nohup: ignoring input
[2023-08-15 10:19:07,828] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-15 10:19:13,415] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-15 10:19:13,485] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-15 10:19:13,491] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-15 10:19:13,529] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-15 10:19:14,339] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-15 10:19:14,339] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-15 10:19:14,419] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-15 10:19:14,419] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-15 10:19:14,496] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-15 10:19:14,496] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-15 10:19:14,496] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-08-15 10:19:14,516] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-15 10:19:14,516] [INFO] [comm.py:616:init_distributed] cdb=None
08/15/2023 10:19:14 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'offload_param': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/15/2023 10:19:14 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'offload_param': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/15/2023 10:19:14 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'offload_param': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/15/2023 10:19:15 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'offload_param': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

loading configuration file /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json
Model config LlamaConfig {
  "_name_or_path": "/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

loading file tokenizer.model
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
loading weights file /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/model.safetensors.index.json
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.31.0"
}

[2023-08-15 10:19:21,168] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9,
  "transformers_version": "4.31.0"
}

Assigning <s> to the bos_token key of the tokenizer
Assigning </s> to the eos_token key of the tokenizer
Assigning <unk> to the unk_token key of the tokenizer
Assigning <pad> to the pad_token key of the tokenizer
Adding <pad> to the vocabulary
08/15/2023 10:19:30 - INFO - __main__ - Sample 18683 of the training set: {'input_ids': tensor([    1,  5122,   338,  1048,   304,   679,   263,  3058,  5253,   310,
         1735,  3109,  1135,   697, 11232,   279,   515,   278,   274,  1161,
         6036, 29889,   960,   540,  4947,   278,  1556,   439, 13868,  1950,
          322,   278,  1791,   297,   282,  2108,   583, 29892,   540,   723,
          817,   304,  7150, 29871, 29941,   282,  2108,   583,   304,  5870,
          278,  5253, 29889,   960,   540,  4947,   278,  1556,   270,  1355,
         1950,   322,   278,  1791,   297,   282,  2108,   583, 29892,   540,
          723,   817,   304,  7150, 29871, 29947,   282,  2108,   583,   304,
         5870,   278,  5253, 29889,  1724,   338,   278,  2533, 29892,   297,
          274,  1237, 29892,   310,   278,  1950, 26999,   310,  1735,   393,
          540,   338,  1811,   304,   679, 29973,    13, 29902,   817,   304,
         1284,   599,   278,  1950, 26999,   310,  1735,  3109,  1135,   697,
        11232,   279,   393, 15523,   278,  2183,  5855, 29889,    13, 12024,
          592,  1369,   491,  3063,   472,   278,   937,  4195, 29901,   540,
         4947,   278,  1556,   439, 13868,  1950,   322,   278,  1791,   297,
          282,  2108,   583, 29892,   322,   540,  4225, 29871, 29941,   282,
         2108,   583,   304,  5870,   278,  5253, 29889,    13,  4013,  2794,
          393,   278,  5253,   310,  1735,  1818,   367,   263,  2999,   310,
        29871, 29906, 29945,   274,  1237,  2298, 29871, 29941,   274,  1237,
        29889,    13,  1576, 19087,  1316,  5253,   338, 29871, 29906, 29947,
          274,  1237, 29892,   322,   278, 10150,   338, 29871, 29955, 29947,
          274,  1237, 29889,    13,  1576,  1950, 26999,   393, 15523,   445,
         4195,   526, 29871, 29906, 29947, 29892, 29871, 29945, 29941, 29892,
          322, 29871, 29955, 29947,   274,  1237, 29889,    13, 10454,  1235,
          592,  1106,   472,   278,  1473,  4195, 29901,   540,  4947,   278,
         1556,   270,  1355,  1950,   322,   278,  1791,   297,   282,  2108,
          583, 29892,   322,   540,  4225, 29871, 29947,   282,  2108,   583,
          304,  5870,   278,  5253, 29889,    13,  4013,  2794,   393,   278,
         5253,   310,  1735,  1818,   367,   263,  2999,   310, 29871, 29896,
        29900,   274,  1237,  2298, 29871, 29947,   274,  1237, 29889,    13,
         1576, 19087,  1316,  5253,   338, 29871, 29896, 29947,   274,  1237,
        29892,   322,   278, 10150,   338, 29871, 29929, 29947,   274,  1237,
        29889,    13,  1576,  1950, 26999,   393, 15523,   445,  4195,   526,
        29871, 29896, 29947, 29892, 29871, 29906, 29947, 29892, 29871, 29941,
        29947, 29892, 29871, 29946, 29947, 29892, 29871, 29945, 29947, 29892,
        29871, 29953, 29947, 29892, 29871, 29955, 29947, 29892, 29871, 29947,
        29947, 29892,   322, 29871, 29929, 29947,   274,  1237, 29889,    13,
         1762,  1284,   278,  1950, 26999,   310,  1735,   393, 15523,  1716,
         5855, 29892,   306,   817,   304,  1284,   278,  3619,  1819,   297,
          278,  1023,  8857, 29889,    13, 29902,  8369,   393, 29871, 29906,
        29947,   322, 29871, 29955, 29947,   274,  1237,  2615,   297,  1716,
         8857, 29892,   577,   896,   526,  1950, 26999,   310,  1735,   393,
          540,   338,  1811,   304,   679, 29889,    13, 29902,   884,  8369,
          393, 29871, 29946, 29947,   322, 29871, 29953, 29947,   274,  1237,
          437,   451,  2615,   297,   278,   937,  1051, 29892,   577,   896,
          526,   451,  1950, 26999,   310,  1735,   393,   540,   338,  1811,
          304,   679, 29889,    13,  1576,   916,  1819,   297,   278,  1473,
         1051,   526,  2845,  2086,  2319,   470,  2086,  2919,   304,   367,
          297,   278,   937,  1051, 29892,   577,   896,   526,   884,   451,
         1950, 26999,   310,  1735,   393,   540,   338,  1811,   304,   679,
        29889,    13,  8439,  1079, 29892,   278,   871,  1950, 26999,   310,
         1735,   393,   540,   338,  1811,   304,   679,   526, 29871, 29906,
        29947,   322, 29871, 29955, 29947,   274,  1237, 29889,    13,  1576,
         2533,   310,  1438, 26999,   338, 29871, 29906, 29947,   718, 29871,
        29955, 29947,   353, 29871, 29896, 29900, 29953,   274,  1237, 29889,
           13, 29937,   673,    13,    13, 29896, 29900, 29953,    13]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        6374, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100,
        -100, -100, -100, -100, -100, -100, 6374]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1])}.
08/15/2023 10:19:30 - INFO - __main__ - Sample 74918 of the training set: {'input_ids': tensor([    1,  2803,   395, 29888,   584,   320,  1995, 29912, 29934, 29913,
          320,   517,   320,  1995, 29912, 29934,  1042,   367,   263,   740,
         1316,   393,    13, 29905, 29961, 29888, 29898, 29888, 29898, 29916,
          448,   343,   876,   353,   285, 29898, 29916, 29897,   285, 29898,
        29891, 29897,   448,   285, 29898, 29916, 29897,   718,   285, 29898,
        29891, 29897,   448,   921, 29891, 18899,  1454,   599,   395, 29916,
         8209,   395, 29891,  7449, 29871, 10987,   278,  2533,   310,   599,
         1950,  1819,   310,   395, 29888, 29898, 29896,   467, 29938,    13,
         4013,   338,   263, 13303,  6306, 29892,   607,  2794,   306,   505,
          304,  1284,   263,   740,   393, 17150,   278,  2183,  6306,   363,
          738,  1881,  1819,   310,   921,   322,   343, 29889,    13, 29909,
         3619, 13705,   363, 17069, 13303, 10693,   338,   304, 18665,   297,
          777,  4266,  1819,   310,   921,   322,   343,   322,  1074,   825,
         5930, 29889,    13,  2831,  1342, 29892,   825,   565,   921,   322,
          343,   526,  1716,  5225, 29973,    13, 11760,   306,   679,   395,
        29888, 29898, 29888, 29898, 29900,   876,   353,   285, 29898, 29900,
         4887, 29906,   448,   285, 29898, 29900, 29897,   718,   285, 29898,
        29900, 29897,   448, 29871, 29900,   353,   285, 29898, 29900,  4887,
        29906,  8209,   607,  2794,   393,   395, 29888, 29898, 29900,  1262,
          338,  2845, 29871, 29900,   470, 29871, 29896, 29889,    13,  5618,
          565,   921,   322,   343,   526,  1716, 29871, 29896, 29973,    13,
        11760,   306,   679,   395, 29888, 29898, 29888, 29898, 29900,   876,
          353,   285, 29898, 29896,  4887, 29906,   448, 29871, 29906, 29888,
        29898, 29896, 29897,   718, 29871, 29896,  8209,   607,   338,   263,
        25904,  6306,   297,   395, 29888, 29898, 29896,   467, 29938,    13]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        6374, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        6374, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
08/15/2023 10:19:30 - INFO - __main__ - Sample 17356 of the training set: {'input_ids': tensor([    1,   450, 14728,   779, 13161, 29871, 29955, 29889, 29945,  3823,
         6034, 29938,   508,   367, 13384,   297,   278,   883,    13, 29905,
         7110, 13161, 29871, 29955, 29889, 29945,  3823,  6034,   353,   320,
         3676, 29912, 29874, 29913,   448,   320,  3676, 29912, 29890, 29913,
          718,   320,  3676, 29912, 29883, 29913,   448,   270,  2053, 29962,
         3062,   395, 29874,   320,   479,   289,   320,   479,   274,   320,
          479,   270, 29938,   526,  6374, 11920, 29889, 29871, 10987,   395,
        29874,   718,   289,   718,   274,   718,   270,  7449,    13, 29902,
         8369,   393,   779, 13161, 29871, 29955, 29889, 29945,  3823,  6034,
        29938,   338,   263,  4203, 29899,  2521,   310,   779, 13161, 29871,
        29896, 29945,  3823,  6034,  1628,   577,   306,   508,   671,   278,
         4203, 29899,  2521,  7063,   363, 18806,   296,   304,  2436,    13,
        29905,  7110, 13161, 29871, 29955, 29889, 29945,  3823,  6034,   353,
          320,  1154,   741,  5223, 29871, 29955, 29889, 29945,  3823,  6034,
         3331,  3944, 29871, 29955, 29889, 29945,  3823,  6034, 29913,   353,
          320,  1154,   741,  5223,   320,  1154, 29912, 29896, 29945,  3823,
         6034,  1157, 29906, 12431,  3944,   320,  1154, 29912, 29896, 29945,
         3823,  6034,  1157, 29906,   930,   353,   320,  1154, 29912, 29896,
          448,   320,  3944, 29871, 29896, 29945,  3823,  6034,  3331,  5223,
        29871, 29896, 29945,  3823,  6034,  1836, 18899,    13, 10454,   306,
          817,   304,  1284,   263,   982,   304,  4653,   779,  3944, 29871,
        29896, 29945,  3823,  6034, 29938,   322,   779,  5223, 29871, 29896,
        29945,  3823,  6034, 29938,   297,  4958,   310,  6862, 16778, 29889,
           13, 29902, 17386,   393,   779,  3944, 29871, 29896, 29945,  3823,
         6034, 29938,   338,   278,  1021,   408,   779,  3944,   313, 29946,
        29945,  3823,  6034,   448, 29871, 29941, 29900,  3823,  6034,  5767,
          322,   306,   508,   671,   278,  6776,   457,  1014,  3018,   428,
         7063,   304,  2436,    13, 29905,  7110,  3944, 29871, 29896, 29945,
         3823,  6034,   353,   320,  3944,   313, 29946, 29945,  3823,  6034,
          448, 29871, 29941, 29900,  3823,  6034, 29897,   353,   320,  3944,
        29871, 29946, 29945,  3823,  6034,   320,  3944, 29871, 29941, 29900,
         3823,  6034,   718,   320,  5223, 29871, 29946, 29945,  3823,  6034,
          320,  5223, 29871, 29941, 29900,  3823,  6034,   353,   320,  1154,
          741,  3676, 29912, 29906,  7585, 29906, 29913,   320,  3822,   320,
         1154,   741,  3676, 29912, 29941,  7585, 29906, 29913,   718,   320,
         1154,   741,  3676, 29912, 29906,  7585, 29906, 29913,   320,  3822,
          320,  1154, 29912, 29896,  1157, 29906, 29913,   353,   320,  1154,
          741,  3676, 29912, 29953, 29913,   718,   320,  3676, 29912, 29906,
         7585, 29946,  1836, 18899,    13,  8942,  2327,   368, 29892,   306,
          508,   671,   278,   269,   457,  1014,  3018,   428,  7063,   304,
         2436,    13, 29905,  7110,  5223, 29871, 29896, 29945,  3823,  6034,
          353,   320,  5223,   313, 29946, 29945,  3823,  6034,   448, 29871,
        29941, 29900,  3823,  6034, 29897,   353,   320,  5223, 29871, 29946,
        29945,  3823,  6034,   320,  3944, 29871, 29941, 29900,  3823,  6034,
          448,   320,  3944, 29871, 29946, 29945,  3823,  6034,   320,  5223,
        29871, 29941, 29900,  3823,  6034,   353,   320,  1154,   741,  3676,
        29912, 29906,  7585, 29906, 29913,   320,  3822,   320,  1154,   741,
         3676, 29912, 29941,  7585, 29906, 29913,   448,   320,  1154,   741,
         3676, 29912, 29906,  7585, 29906, 29913,   320,  3822,   320,  1154,
        29912, 29896,  1157, 29906, 29913,   353,   320,  1154,   741,  3676,
        29912, 29953, 29913,   448,   320,  3676, 29912, 29906,  7585, 29946,
         1836, 18899,    13,  3247,   688,  3460,  1438, 12241,   964,   278,
        15958,   363,   779, 13161, 29871, 29955, 29889, 29945,  3823,  6034,
         1628,   306,   679,    13, 29905,  7110, 13161, 29871, 29955, 29889,
        29945,  3823,  6034,   353,   320,  1154, 29912, 29896,   448,   320,
         1154,   741,  3676, 29912, 29953, 29913,   718,   320,  3676, 29912,
        29906,  7585, 29946, 12431,  1154,   741,  3676, 29912, 29953, 29913,
          448,   320,  3676, 29912, 29906,  7585, 29946,   930,   353,   320,
         1154, 29912, 29946,   448,   320,  3676, 29912, 29953, 29913,   448,
          320,  3676, 29912, 29906, 12431,  3676, 29912, 29953, 29913,   448,
          320,  3676, 29912, 29906,   930,  7790, 29962,    13,  1762, 17903,
          675,   278, 14267,  1061, 29892,   306, 22932,   278,  4825,  1061,
          322, 14267,  1061,   491,   278, 25482,   403,   310,   278, 14267,
         1061, 29892,   607,   338,   779,  3676, 29912, 29953, 29913,   718,
          320,  3676, 29912, 29906,  4311,    13,  4013,  4076,   592,    13,
        29905,  7110, 13161, 29871, 29955, 29889, 29945,  3823,  6034,   353,
          320,  1154,  8001, 29946,   448,   320,  3676, 29912, 29953, 29913,
          448,   320,  3676, 29912, 29906,  1800,  1194,  3676, 29912, 29953,
        29913,   718,   320,  3676, 29912, 29906,  1800,  1157,  1194,  3676,
        29912, 29953, 29913,   448,   320,  3676, 29912, 29906,  1800,  1194,
         3676, 29912, 29953, 29913,   718,   320,  3676, 29912, 29906,  1800,
        29913,   353,   320,  1154, 29912, 29946, 29905,  3676, 29912, 29953,
        29913,   718, 29871, 29946, 29905,  3676, 29912, 29906, 29913,   448,
        29871, 29953,   448, 29871, 29906, 29905,  3676, 29912, 29896, 29906,
        29913,   448, 29871, 29906, 29905,  3676, 29912, 29946, 29913,   448,
        29871, 29906,  1157, 29953,   448, 29871, 29906, 29913,   353,   320,
         1154, 29912, 29946, 29905,  3676, 29912, 29953, 29913,   718, 29871,
        29946, 29905,  3676, 29912, 29906, 29913,   448, 29871, 29947,   448,
        29871, 29946, 29905,  3676, 29912, 29941, 29913,   448, 29871, 29946,
         1157, 29946,  1836, 18899,    13]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        6374, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, 8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1])}.
08/15/2023 10:19:30 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.2847280502319336 seconds
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.32016825675964355 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.3649101257324219 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.42656874656677246 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
08/15/2023 10:19:32 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2023-08-15 10:19:32,824] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
08/15/2023 10:19:32 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
08/15/2023 10:19:32 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
08/15/2023 10:19:32 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
08/15/2023 10:19:32 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/15/2023 10:19:32 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/15/2023 10:19:32 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/15/2023 10:19:32 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-08-15 10:19:33,537] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-15 10:19:33,538] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-08-15 10:19:33,538] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-08-15 10:19:33,548] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-08-15 10:19:33,549] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-08-15 10:19:33,549] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2023-08-15 10:19:33,549] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2023-08-15 10:19:33,666] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
[2023-08-15 10:19:33,666] [INFO] [utils.py:786:see_memory_usage] MA 0.55 GB         Max_MA 1.1 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-15 10:19:33,667] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 104.73 GB, percent = 10.4%
[2023-08-15 10:19:33,668] [INFO] [stage3.py:117:__init__] Reduce bucket size 16777216
[2023-08-15 10:19:33,668] [INFO] [stage3.py:118:__init__] Prefetch bucket size 15099494
[2023-08-15 10:19:33,756] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-15 10:19:33,756] [INFO] [utils.py:786:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-15 10:19:33,756] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 104.73 GB, percent = 10.4%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2023-08-15 10:19:33,931] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-08-15 10:19:33,931] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.55 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-15 10:19:33,932] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 104.86 GB, percent = 10.4%
[2023-08-15 10:19:34,020] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
[2023-08-15 10:19:34,021] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-15 10:19:34,021] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 104.86 GB, percent = 10.4%
[2023-08-15 10:19:36,593] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 2
[2023-08-15 10:19:36,594] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-15 10:19:36,594] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 122.68 GB, percent = 12.2%
[2023-08-15 10:19:36,710] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
[2023-08-15 10:19:36,710] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-15 10:19:36,710] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 123.82 GB, percent = 12.3%
[2023-08-15 10:19:38,820] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
[2023-08-15 10:19:38,820] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-15 10:19:38,820] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 144.65 GB, percent = 14.4%
[2023-08-15 10:19:40,473] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-08-15 10:19:40,473] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-15 10:19:40,474] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 147.92 GB, percent = 14.7%
[2023-08-15 10:19:54,118] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-08-15 10:19:54,118] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-15 10:19:54,119] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 237.26 GB, percent = 23.6%
[2023-08-15 10:19:54,907] [INFO] [stage3.py:424:_setup_for_real_optimizer] optimizer state initialized
[2023-08-15 10:19:59,154] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-08-15 10:19:59,155] [INFO] [utils.py:786:see_memory_usage] MA 0.09 GB         Max_MA 0.58 GB         CA 2.02 GB         Max_CA 2 GB 
[2023-08-15 10:19:59,155] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 246.18 GB, percent = 24.4%
[2023-08-15 10:19:59,155] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-08-15 10:19:59,155] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-15 10:19:59,155] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-15 10:19:59,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-08-15 10:19:59,156] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f271e5b8040>
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-15 10:19:59,156] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 16
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   train_batch_size ............. 128
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-15 10:19:59,157] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-15 10:19:59,158] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-15 10:19:59,158] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-15 10:19:59,158] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-08-15 10:19:59,158] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-15 10:19:59,158] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3
[2023-08-15 10:19:59,158] [INFO] [config.py:950:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
wandb: Currently logged in as: kidrain61. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.8
wandb: Run data is saved locally in /data/users/zhangjunlei/wandb/run-20230815_102000-ygi8r383
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 2023-08-14-2
wandb: ⭐️ View project at https://wandb.ai/kidrain61/open_instruct
wandb: 🚀 View run at https://wandb.ai/kidrain61/open_instruct/runs/ygi8r383
08/15/2023 10:20:05 - INFO - __main__ - ***** Running training *****
08/15/2023 10:20:05 - INFO - __main__ -   Num examples = 87012
08/15/2023 10:20:05 - INFO - __main__ -   Num Epochs = 3
08/15/2023 10:20:05 - INFO - __main__ -   Instantaneous batch size per device = 2
08/15/2023 10:20:05 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128
08/15/2023 10:20:05 - INFO - __main__ -   Gradient Accumulation steps = 16
08/15/2023 10:20:05 - INFO - __main__ -   Total optimization steps = 2040
  0%|          | 0/2040 [00:00<?, ?it/s]  0%|          | 1/2040 [00:39<22:07:23, 39.06s/it]08/15/2023 10:20:44 - INFO - __main__ -   Step: 1, LR: 3.278688524590164e-07, Loss: 18.685558319091797
  0%|          | 2/2040 [01:14<21:00:42, 37.12s/it]08/15/2023 10:21:20 - INFO - __main__ -   Step: 2, LR: 6.557377049180328e-07, Loss: 18.629985809326172
  0%|          | 3/2040 [01:50<20:30:32, 36.25s/it]08/15/2023 10:21:55 - INFO - __main__ -   Step: 3, LR: 9.836065573770493e-07, Loss: 18.737775802612305
  0%|          | 4/2040 [02:25<20:18:41, 35.91s/it]08/15/2023 10:22:31 - INFO - __main__ -   Step: 4, LR: 1.3114754098360657e-06, Loss: 18.75144386291504
  0%|          | 5/2040 [03:02<20:28:23, 36.22s/it]08/15/2023 10:23:08 - INFO - __main__ -   Step: 5, LR: 1.6393442622950819e-06, Loss: 18.71870231628418
  0%|          | 6/2040 [03:36<20:10:13, 35.70s/it]08/15/2023 10:23:42 - INFO - __main__ -   Step: 6, LR: 1.9672131147540985e-06, Loss: 18.323383331298828
  0%|          | 7/2040 [04:12<20:07:32, 35.64s/it]08/15/2023 10:24:18 - INFO - __main__ -   Step: 7, LR: 2.295081967213115e-06, Loss: 17.503849029541016
  0%|          | 8/2040 [04:46<19:55:42, 35.31s/it]08/15/2023 10:24:52 - INFO - __main__ -   Step: 8, LR: 2.6229508196721314e-06, Loss: 17.240516662597656
  0%|          | 9/2040 [05:22<19:52:29, 35.23s/it]08/15/2023 10:25:27 - INFO - __main__ -   Step: 9, LR: 2.9508196721311478e-06, Loss: 13.454977035522461
  0%|          | 10/2040 [05:56<19:48:19, 35.12s/it]08/15/2023 10:26:02 - INFO - __main__ -   Step: 10, LR: 3.2786885245901638e-06, Loss: 12.965484619140625
  1%|          | 11/2040 [06:32<19:51:22, 35.23s/it]08/15/2023 10:26:38 - INFO - __main__ -   Step: 11, LR: 3.6065573770491806e-06, Loss: 11.924460411071777
  1%|          | 12/2040 [07:06<19:44:06, 35.03s/it]08/15/2023 10:27:12 - INFO - __main__ -   Step: 12, LR: 3.934426229508197e-06, Loss: 4.087099552154541
  1%|          | 13/2040 [07:41<19:42:40, 35.01s/it]08/15/2023 10:27:47 - INFO - __main__ -   Step: 13, LR: 4.2622950819672135e-06, Loss: 3.480131149291992
  1%|          | 14/2040 [08:17<19:45:34, 35.11s/it]08/15/2023 10:28:23 - INFO - __main__ -   Step: 14, LR: 4.59016393442623e-06, Loss: 2.357545852661133
  1%|          | 15/2040 [08:52<19:45:07, 35.11s/it]08/15/2023 10:28:58 - INFO - __main__ -   Step: 15, LR: 4.918032786885246e-06, Loss: 2.237055540084839
  1%|          | 16/2040 [09:27<19:48:25, 35.23s/it]08/15/2023 10:29:33 - INFO - __main__ -   Step: 16, LR: 5.245901639344263e-06, Loss: 1.5487152338027954
  1%|          | 17/2040 [10:03<19:47:05, 35.21s/it]08/15/2023 10:30:08 - INFO - __main__ -   Step: 17, LR: 5.573770491803278e-06, Loss: 1.0234262943267822
  1%|          | 18/2040 [10:38<19:46:53, 35.22s/it]08/15/2023 10:30:44 - INFO - __main__ -   Step: 18, LR: 5.9016393442622956e-06, Loss: 1.2339946031570435
  1%|          | 19/2040 [11:13<19:47:52, 35.27s/it]08/15/2023 10:31:19 - INFO - __main__ -   Step: 19, LR: 6.229508196721312e-06, Loss: 1.296950101852417
  1%|          | 20/2040 [11:48<19:47:24, 35.27s/it]08/15/2023 10:31:54 - INFO - __main__ -   Step: 20, LR: 6.5573770491803276e-06, Loss: 0.7027912139892578
  1%|          | 21/2040 [12:24<19:50:02, 35.37s/it]08/15/2023 10:32:30 - INFO - __main__ -   Step: 21, LR: 6.885245901639345e-06, Loss: 0.8995844721794128
  1%|          | 22/2040 [12:59<19:41:54, 35.14s/it]08/15/2023 10:33:05 - INFO - __main__ -   Step: 22, LR: 7.213114754098361e-06, Loss: 0.7720622420310974
  1%|          | 23/2040 [13:34<19:45:37, 35.27s/it]08/15/2023 10:33:40 - INFO - __main__ -   Step: 23, LR: 7.540983606557377e-06, Loss: 0.8607723712921143
  1%|          | 24/2040 [14:09<19:44:49, 35.26s/it]08/15/2023 10:34:15 - INFO - __main__ -   Step: 24, LR: 7.868852459016394e-06, Loss: 0.8177032470703125
  1%|          | 25/2040 [14:44<19:40:53, 35.16s/it]08/15/2023 10:34:50 - INFO - __main__ -   Step: 25, LR: 8.19672131147541e-06, Loss: 0.7179197072982788
  1%|▏         | 26/2040 [15:19<19:38:40, 35.11s/it]08/15/2023 10:35:25 - INFO - __main__ -   Step: 26, LR: 8.524590163934427e-06, Loss: 0.8057759404182434
  1%|▏         | 27/2040 [15:54<19:34:47, 35.02s/it]08/15/2023 10:36:00 - INFO - __main__ -   Step: 27, LR: 8.852459016393443e-06, Loss: 0.8289322853088379
  1%|▏         | 28/2040 [16:30<19:40:09, 35.19s/it]08/15/2023 10:36:36 - INFO - __main__ -   Step: 28, LR: 9.18032786885246e-06, Loss: 0.729368269443512
  1%|▏         | 29/2040 [17:05<19:40:57, 35.24s/it]08/15/2023 10:37:11 - INFO - __main__ -   Step: 29, LR: 9.508196721311476e-06, Loss: 0.7450331449508667
  1%|▏         | 30/2040 [17:40<19:39:26, 35.21s/it]08/15/2023 10:37:46 - INFO - __main__ -   Step: 30, LR: 9.836065573770493e-06, Loss: 0.6449266672134399
  2%|▏         | 31/2040 [18:15<19:36:35, 35.14s/it]08/15/2023 10:38:21 - INFO - __main__ -   Step: 31, LR: 1.0163934426229509e-05, Loss: 0.791624903678894
  2%|▏         | 32/2040 [18:50<19:32:11, 35.03s/it]08/15/2023 10:38:56 - INFO - __main__ -   Step: 32, LR: 1.0491803278688525e-05, Loss: 0.9412656426429749
  2%|▏         | 33/2040 [19:25<19:34:34, 35.11s/it]08/15/2023 10:39:31 - INFO - __main__ -   Step: 33, LR: 1.0819672131147544e-05, Loss: 0.7509458065032959
  2%|▏         | 34/2040 [20:02<19:46:54, 35.50s/it]08/15/2023 10:40:08 - INFO - __main__ -   Step: 34, LR: 1.1147540983606557e-05, Loss: 0.7042869329452515
  2%|▏         | 35/2040 [20:37<19:45:43, 35.48s/it]08/15/2023 10:40:43 - INFO - __main__ -   Step: 35, LR: 1.1475409836065575e-05, Loss: 0.7708337903022766
  2%|▏         | 36/2040 [21:12<19:40:54, 35.36s/it]08/15/2023 10:41:18 - INFO - __main__ -   Step: 36, LR: 1.1803278688524591e-05, Loss: 0.8280015587806702
  2%|▏         | 37/2040 [21:47<19:33:46, 35.16s/it]08/15/2023 10:41:53 - INFO - __main__ -   Step: 37, LR: 1.2131147540983608e-05, Loss: 0.7764096856117249
  2%|▏         | 38/2040 [22:21<19:25:37, 34.93s/it]08/15/2023 10:42:27 - INFO - __main__ -   Step: 38, LR: 1.2459016393442624e-05, Loss: 0.8575609922409058
  2%|▏         | 39/2040 [22:56<19:24:30, 34.92s/it]08/15/2023 10:43:02 - INFO - __main__ -   Step: 39, LR: 1.2786885245901642e-05, Loss: 0.6690458655357361
  2%|▏         | 40/2040 [23:32<19:29:32, 35.09s/it]08/15/2023 10:43:38 - INFO - __main__ -   Step: 40, LR: 1.3114754098360655e-05, Loss: 0.7201085090637207
  2%|▏         | 41/2040 [24:07<19:29:45, 35.11s/it]08/15/2023 10:44:13 - INFO - __main__ -   Step: 41, LR: 1.3442622950819673e-05, Loss: 0.731144905090332
  2%|▏         | 42/2040 [24:42<19:29:17, 35.11s/it]08/15/2023 10:44:48 - INFO - __main__ -   Step: 42, LR: 1.377049180327869e-05, Loss: 0.6908405423164368
  2%|▏         | 43/2040 [25:17<19:27:52, 35.09s/it]08/15/2023 10:45:23 - INFO - __main__ -   Step: 43, LR: 1.4098360655737706e-05, Loss: 0.6064319610595703
  2%|▏         | 44/2040 [25:52<19:28:26, 35.12s/it]08/15/2023 10:45:58 - INFO - __main__ -   Step: 44, LR: 1.4426229508196722e-05, Loss: 0.6010158658027649
  2%|▏         | 45/2040 [26:27<19:28:29, 35.14s/it]08/15/2023 10:46:33 - INFO - __main__ -   Step: 45, LR: 1.4754098360655739e-05, Loss: 0.681576132774353
  2%|▏         | 46/2040 [27:02<19:25:56, 35.08s/it]08/15/2023 10:47:08 - INFO - __main__ -   Step: 46, LR: 1.5081967213114754e-05, Loss: 0.6533772349357605
  2%|▏         | 47/2040 [27:38<19:29:37, 35.21s/it]08/15/2023 10:47:44 - INFO - __main__ -   Step: 47, LR: 1.5409836065573772e-05, Loss: 0.7014403343200684
  2%|▏         | 48/2040 [28:13<19:28:17, 35.19s/it]08/15/2023 10:48:19 - INFO - __main__ -   Step: 48, LR: 1.5737704918032788e-05, Loss: 0.706940770149231
  2%|▏         | 49/2040 [28:48<19:30:28, 35.27s/it]08/15/2023 10:48:54 - INFO - __main__ -   Step: 49, LR: 1.6065573770491805e-05, Loss: 0.7584616541862488
  2%|▏         | 50/2040 [29:24<19:29:26, 35.26s/it]08/15/2023 10:49:30 - INFO - __main__ -   Step: 50, LR: 1.639344262295082e-05, Loss: 0.6827928423881531
  2%|▎         | 51/2040 [29:59<19:25:27, 35.16s/it]08/15/2023 10:50:05 - INFO - __main__ -   Step: 51, LR: 1.6721311475409837e-05, Loss: 0.6895186901092529
  3%|▎         | 52/2040 [30:34<19:25:10, 35.17s/it]08/15/2023 10:50:40 - INFO - __main__ -   Step: 52, LR: 1.7049180327868854e-05, Loss: 0.6392716765403748
  3%|▎         | 53/2040 [31:09<19:19:56, 35.03s/it]08/15/2023 10:51:14 - INFO - __main__ -   Step: 53, LR: 1.737704918032787e-05, Loss: 0.6472481489181519
  3%|▎         | 54/2040 [31:45<19:30:03, 35.35s/it]08/15/2023 10:51:51 - INFO - __main__ -   Step: 54, LR: 1.7704918032786887e-05, Loss: 0.6980567574501038
  3%|▎         | 55/2040 [32:20<19:34:29, 35.50s/it]08/15/2023 10:52:26 - INFO - __main__ -   Step: 55, LR: 1.8032786885245903e-05, Loss: 0.66405189037323
  3%|▎         | 56/2040 [32:56<19:33:00, 35.47s/it]08/15/2023 10:53:02 - INFO - __main__ -   Step: 56, LR: 1.836065573770492e-05, Loss: 0.5777856111526489
  3%|▎         | 57/2040 [33:31<19:28:09, 35.35s/it]08/15/2023 10:53:37 - INFO - __main__ -   Step: 57, LR: 1.8688524590163936e-05, Loss: 0.5911505818367004
  3%|▎         | 58/2040 [34:06<19:21:59, 35.18s/it]08/15/2023 10:54:12 - INFO - __main__ -   Step: 58, LR: 1.9016393442622952e-05, Loss: 0.5647960305213928
  3%|▎         | 59/2040 [34:41<19:19:38, 35.12s/it]08/15/2023 10:54:47 - INFO - __main__ -   Step: 59, LR: 1.934426229508197e-05, Loss: 0.5876146554946899
  3%|▎         | 60/2040 [35:16<19:21:55, 35.21s/it]08/15/2023 10:55:22 - INFO - __main__ -   Step: 60, LR: 1.9672131147540985e-05, Loss: 0.6865384578704834
  3%|▎         | 61/2040 [35:51<19:20:57, 35.20s/it]08/15/2023 10:55:57 - INFO - __main__ -   Step: 61, LR: 2e-05, Loss: 0.6318715810775757
  3%|▎         | 62/2040 [36:26<19:19:14, 35.16s/it]08/15/2023 10:56:32 - INFO - __main__ -   Step: 62, LR: 1.998989388580091e-05, Loss: 0.593194842338562
  3%|▎         | 63/2040 [37:02<19:20:50, 35.23s/it]08/15/2023 10:57:08 - INFO - __main__ -   Step: 63, LR: 1.9979787771601823e-05, Loss: 0.5468055009841919
  3%|▎         | 64/2040 [37:37<19:21:43, 35.28s/it]08/15/2023 10:57:43 - INFO - __main__ -   Step: 64, LR: 1.9969681657402732e-05, Loss: 0.5806121826171875
  3%|▎         | 65/2040 [38:12<19:12:07, 35.00s/it]08/15/2023 10:58:17 - INFO - __main__ -   Step: 65, LR: 1.995957554320364e-05, Loss: 0.6558393239974976
  3%|▎         | 66/2040 [38:47<19:17:19, 35.18s/it]08/15/2023 10:58:53 - INFO - __main__ -   Step: 66, LR: 1.994946942900455e-05, Loss: 0.6915460824966431
  3%|▎         | 67/2040 [39:22<19:15:14, 35.13s/it]08/15/2023 10:59:28 - INFO - __main__ -   Step: 67, LR: 1.993936331480546e-05, Loss: 0.6825894713401794
  3%|▎         | 68/2040 [39:57<19:14:44, 35.13s/it]08/15/2023 11:00:03 - INFO - __main__ -   Step: 68, LR: 1.992925720060637e-05, Loss: 0.583611249923706
  3%|▎         | 69/2040 [40:33<19:17:54, 35.25s/it]08/15/2023 11:00:39 - INFO - __main__ -   Step: 69, LR: 1.9919151086407278e-05, Loss: 0.6106465458869934
  3%|▎         | 70/2040 [41:08<19:12:30, 35.10s/it]08/15/2023 11:01:13 - INFO - __main__ -   Step: 70, LR: 1.9909044972208187e-05, Loss: 0.6484508514404297
  3%|▎         | 71/2040 [41:43<19:13:51, 35.16s/it]08/15/2023 11:01:49 - INFO - __main__ -   Step: 71, LR: 1.98989388580091e-05, Loss: 0.6737432479858398
  4%|▎         | 72/2040 [42:17<19:05:59, 34.94s/it]08/15/2023 11:02:23 - INFO - __main__ -   Step: 72, LR: 1.9888832743810008e-05, Loss: 0.667769193649292
  4%|▎         | 73/2040 [42:52<19:06:20, 34.97s/it]08/15/2023 11:02:58 - INFO - __main__ -   Step: 73, LR: 1.9878726629610917e-05, Loss: 0.6681400537490845
  4%|▎         | 74/2040 [43:27<19:03:32, 34.90s/it]08/15/2023 11:03:33 - INFO - __main__ -   Step: 74, LR: 1.9868620515411826e-05, Loss: 0.6344670057296753
  4%|▎         | 75/2040 [44:02<19:03:01, 34.90s/it]08/15/2023 11:04:08 - INFO - __main__ -   Step: 75, LR: 1.9858514401212735e-05, Loss: 0.6137069463729858
  4%|▎         | 76/2040 [44:37<19:02:15, 34.90s/it]08/15/2023 11:04:43 - INFO - __main__ -   Step: 76, LR: 1.9848408287013645e-05, Loss: 0.6682741045951843
  4%|▍         | 77/2040 [45:12<19:00:04, 34.85s/it]08/15/2023 11:05:17 - INFO - __main__ -   Step: 77, LR: 1.9838302172814554e-05, Loss: 0.691496729850769
  4%|▍         | 78/2040 [45:46<18:59:41, 34.85s/it]08/15/2023 11:05:52 - INFO - __main__ -   Step: 78, LR: 1.9828196058615463e-05, Loss: 0.6395840048789978
  4%|▍         | 79/2040 [46:21<18:58:53, 34.85s/it]08/15/2023 11:06:27 - INFO - __main__ -   Step: 79, LR: 1.9818089944416375e-05, Loss: 0.6131889224052429
  4%|▍         | 80/2040 [46:56<18:59:53, 34.89s/it]08/15/2023 11:07:02 - INFO - __main__ -   Step: 80, LR: 1.9807983830217284e-05, Loss: 0.7618705630302429
  4%|▍         | 81/2040 [47:32<19:02:46, 35.00s/it]08/15/2023 11:07:37 - INFO - __main__ -   Step: 81, LR: 1.9797877716018193e-05, Loss: 0.621153712272644
  4%|▍         | 82/2040 [48:06<18:59:45, 34.93s/it]08/15/2023 11:08:12 - INFO - __main__ -   Step: 82, LR: 1.9787771601819102e-05, Loss: 0.5730640888214111
  4%|▍         | 83/2040 [48:41<18:58:52, 34.92s/it]08/15/2023 11:08:47 - INFO - __main__ -   Step: 83, LR: 1.977766548762001e-05, Loss: 0.5489218831062317
  4%|▍         | 84/2040 [49:16<18:59:14, 34.95s/it]08/15/2023 11:09:22 - INFO - __main__ -   Step: 84, LR: 1.976755937342092e-05, Loss: 0.6387223601341248
  4%|▍         | 85/2040 [49:51<19:00:52, 35.01s/it]08/15/2023 11:09:57 - INFO - __main__ -   Step: 85, LR: 1.975745325922183e-05, Loss: 0.7050899863243103
  4%|▍         | 86/2040 [50:26<18:59:36, 34.99s/it]08/15/2023 11:10:32 - INFO - __main__ -   Step: 86, LR: 1.974734714502274e-05, Loss: 0.5620243549346924
  4%|▍         | 87/2040 [51:02<19:05:37, 35.20s/it]08/15/2023 11:11:08 - INFO - __main__ -   Step: 87, LR: 1.973724103082365e-05, Loss: 0.6026351451873779
  4%|▍         | 88/2040 [51:36<18:58:13, 34.99s/it]08/15/2023 11:11:42 - INFO - __main__ -   Step: 88, LR: 1.972713491662456e-05, Loss: 0.5589847564697266
  4%|▍         | 89/2040 [52:12<18:59:05, 35.03s/it]08/15/2023 11:12:18 - INFO - __main__ -   Step: 89, LR: 1.971702880242547e-05, Loss: 0.595667839050293
  4%|▍         | 90/2040 [52:47<19:01:54, 35.14s/it]08/15/2023 11:12:53 - INFO - __main__ -   Step: 90, LR: 1.970692268822638e-05, Loss: 0.5896825790405273
  4%|▍         | 91/2040 [53:22<19:02:17, 35.17s/it]08/15/2023 11:13:28 - INFO - __main__ -   Step: 91, LR: 1.9696816574027287e-05, Loss: 0.5271782875061035
  5%|▍         | 92/2040 [53:57<18:58:08, 35.06s/it]08/15/2023 11:14:03 - INFO - __main__ -   Step: 92, LR: 1.9686710459828196e-05, Loss: 0.5990623235702515
  5%|▍         | 93/2040 [54:32<18:57:36, 35.06s/it]08/15/2023 11:14:38 - INFO - __main__ -   Step: 93, LR: 1.9676604345629106e-05, Loss: 0.5479444861412048
  5%|▍         | 94/2040 [55:07<18:57:31, 35.07s/it]08/15/2023 11:15:13 - INFO - __main__ -   Step: 94, LR: 1.9666498231430015e-05, Loss: 0.5736854076385498
  5%|▍         | 95/2040 [55:42<18:56:39, 35.06s/it]08/15/2023 11:15:48 - INFO - __main__ -   Step: 95, LR: 1.9656392117230927e-05, Loss: 0.5394915342330933
  5%|▍         | 96/2040 [56:17<18:55:53, 35.06s/it]08/15/2023 11:16:23 - INFO - __main__ -   Step: 96, LR: 1.9646286003031836e-05, Loss: 0.669954240322113
[2023-08-15 11:16:59,364] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 97/2040 [56:53<19:01:27, 35.25s/it]08/15/2023 11:16:59 - INFO - __main__ -   Step: 97, LR: 1.9636179888832745e-05, Loss: 0.6283023357391357
  5%|▍         | 98/2040 [57:28<18:55:34, 35.08s/it]08/15/2023 11:17:34 - INFO - __main__ -   Step: 98, LR: 1.9626073774633658e-05, Loss: 0.6167451739311218
  5%|▍         | 99/2040 [58:03<18:55:55, 35.11s/it]08/15/2023 11:18:09 - INFO - __main__ -   Step: 99, LR: 1.9615967660434563e-05, Loss: 0.6725146770477295
  5%|▍         | 100/2040 [58:38<18:56:14, 35.14s/it]08/15/2023 11:18:44 - INFO - __main__ -   Step: 100, LR: 1.9605861546235472e-05, Loss: 0.6562567949295044
08/15/2023 11:18:44 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100
08/15/2023 11:18:44 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 11:18:44,465] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-08-15 11:18:44,471] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 11:18:44,471] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 11:18:44,471] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 11:18:44,471] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 11:18:44,472] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 11:18:44,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 11:18:44,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 11:18:44,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 11:18:44,483] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 11:18:44,483] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 11:18:44,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 11:18:44,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 11:18:44,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 11:18:44,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 11:19:04,619] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 11:19:04,622] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 11:19:05,432] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 11:19:05,432] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 11:19:06,736] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 11:19:06,745] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 11:19:07,108] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 11:19:07,108] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 11:19:07,113] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 11:19:07,113] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 11:19:07,113] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 11:19:07,113] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 11:19:07 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/pytorch_model
08/15/2023 11:19:07 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/scheduler.bin
08/15/2023 11:19:07 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_100/random_states_0.pkl
  5%|▍         | 101/2040 [59:36<22:41:10, 42.12s/it]08/15/2023 11:19:42 - INFO - __main__ -   Step: 101, LR: 1.959575543203638e-05, Loss: 0.7491064071655273
  5%|▌         | 102/2040 [1:00:12<21:33:13, 40.04s/it]08/15/2023 11:20:18 - INFO - __main__ -   Step: 102, LR: 1.9585649317837294e-05, Loss: 0.7198753356933594
  5%|▌         | 103/2040 [1:00:46<20:40:04, 38.41s/it]08/15/2023 11:20:52 - INFO - __main__ -   Step: 103, LR: 1.9575543203638203e-05, Loss: 0.63018798828125
  5%|▌         | 104/2040 [1:01:21<20:06:05, 37.38s/it]08/15/2023 11:21:27 - INFO - __main__ -   Step: 104, LR: 1.9565437089439112e-05, Loss: 0.6453402042388916
  5%|▌         | 105/2040 [1:01:56<19:44:45, 36.74s/it]08/15/2023 11:22:02 - INFO - __main__ -   Step: 105, LR: 1.955533097524002e-05, Loss: 0.707787811756134
  5%|▌         | 106/2040 [1:02:32<19:28:14, 36.24s/it]08/15/2023 11:22:37 - INFO - __main__ -   Step: 106, LR: 1.9545224861040934e-05, Loss: 0.6516860723495483
  5%|▌         | 107/2040 [1:03:06<19:13:40, 35.81s/it]08/15/2023 11:23:12 - INFO - __main__ -   Step: 107, LR: 1.9535118746841843e-05, Loss: 0.6665135622024536
  5%|▌         | 108/2040 [1:03:42<19:08:56, 35.68s/it]08/15/2023 11:23:48 - INFO - __main__ -   Step: 108, LR: 1.9525012632642752e-05, Loss: 0.5763577818870544
  5%|▌         | 109/2040 [1:04:17<19:01:43, 35.48s/it]08/15/2023 11:24:23 - INFO - __main__ -   Step: 109, LR: 1.951490651844366e-05, Loss: 0.7057721614837646
  5%|▌         | 110/2040 [1:04:52<18:56:42, 35.34s/it]08/15/2023 11:24:58 - INFO - __main__ -   Step: 110, LR: 1.950480040424457e-05, Loss: 0.6299605369567871
  5%|▌         | 111/2040 [1:05:26<18:50:14, 35.16s/it]08/15/2023 11:25:32 - INFO - __main__ -   Step: 111, LR: 1.949469429004548e-05, Loss: 0.5532997250556946
  5%|▌         | 112/2040 [1:06:01<18:48:04, 35.11s/it]08/15/2023 11:26:07 - INFO - __main__ -   Step: 112, LR: 1.9484588175846388e-05, Loss: 0.6007610559463501
  6%|▌         | 113/2040 [1:06:36<18:44:46, 35.02s/it]08/15/2023 11:26:42 - INFO - __main__ -   Step: 113, LR: 1.9474482061647297e-05, Loss: 0.6867578625679016
  6%|▌         | 114/2040 [1:07:11<18:44:35, 35.03s/it]08/15/2023 11:27:17 - INFO - __main__ -   Step: 114, LR: 1.946437594744821e-05, Loss: 0.7661439180374146
  6%|▌         | 115/2040 [1:07:47<18:47:02, 35.13s/it]08/15/2023 11:27:53 - INFO - __main__ -   Step: 115, LR: 1.945426983324912e-05, Loss: 0.6555377840995789
  6%|▌         | 116/2040 [1:08:21<18:42:50, 35.02s/it]08/15/2023 11:28:27 - INFO - __main__ -   Step: 116, LR: 1.9444163719050028e-05, Loss: 0.6080201864242554
  6%|▌         | 117/2040 [1:08:57<18:42:58, 35.04s/it]08/15/2023 11:29:02 - INFO - __main__ -   Step: 117, LR: 1.9434057604850937e-05, Loss: 0.6446326971054077
  6%|▌         | 118/2040 [1:09:32<18:45:05, 35.12s/it]08/15/2023 11:29:38 - INFO - __main__ -   Step: 118, LR: 1.9423951490651846e-05, Loss: 0.6285444498062134
  6%|▌         | 119/2040 [1:10:07<18:43:18, 35.09s/it]08/15/2023 11:30:13 - INFO - __main__ -   Step: 119, LR: 1.9413845376452755e-05, Loss: 0.7096526622772217
  6%|▌         | 120/2040 [1:10:42<18:43:16, 35.10s/it]08/15/2023 11:30:48 - INFO - __main__ -   Step: 120, LR: 1.9403739262253664e-05, Loss: 0.639121413230896
  6%|▌         | 121/2040 [1:11:17<18:45:45, 35.20s/it]08/15/2023 11:31:23 - INFO - __main__ -   Step: 121, LR: 1.9393633148054573e-05, Loss: 0.5620466470718384
  6%|▌         | 122/2040 [1:11:52<18:43:03, 35.13s/it]08/15/2023 11:31:58 - INFO - __main__ -   Step: 122, LR: 1.9383527033855486e-05, Loss: 0.5932613611221313
  6%|▌         | 123/2040 [1:12:27<18:40:45, 35.08s/it]08/15/2023 11:32:33 - INFO - __main__ -   Step: 123, LR: 1.9373420919656395e-05, Loss: 0.6360729932785034
  6%|▌         | 124/2040 [1:13:03<18:41:57, 35.13s/it]08/15/2023 11:33:09 - INFO - __main__ -   Step: 124, LR: 1.9363314805457304e-05, Loss: 0.6132780313491821
  6%|▌         | 125/2040 [1:13:37<18:37:59, 35.03s/it]08/15/2023 11:33:43 - INFO - __main__ -   Step: 125, LR: 1.9353208691258213e-05, Loss: 0.5287319421768188
  6%|▌         | 126/2040 [1:14:12<18:34:51, 34.95s/it]08/15/2023 11:34:18 - INFO - __main__ -   Step: 126, LR: 1.9343102577059122e-05, Loss: 0.615673840045929
  6%|▌         | 127/2040 [1:14:47<18:33:37, 34.93s/it]08/15/2023 11:34:53 - INFO - __main__ -   Step: 127, LR: 1.933299646286003e-05, Loss: 0.6457662582397461
  6%|▋         | 128/2040 [1:15:23<18:38:55, 35.11s/it]08/15/2023 11:35:28 - INFO - __main__ -   Step: 128, LR: 1.932289034866094e-05, Loss: 0.6033493280410767
  6%|▋         | 129/2040 [1:15:57<18:35:44, 35.03s/it]08/15/2023 11:36:03 - INFO - __main__ -   Step: 129, LR: 1.931278423446185e-05, Loss: 0.6312384605407715
  6%|▋         | 130/2040 [1:16:34<18:46:20, 35.38s/it]08/15/2023 11:36:40 - INFO - __main__ -   Step: 130, LR: 1.930267812026276e-05, Loss: 0.6082640290260315
  6%|▋         | 131/2040 [1:17:09<18:43:16, 35.30s/it]08/15/2023 11:37:15 - INFO - __main__ -   Step: 131, LR: 1.929257200606367e-05, Loss: 0.5984775424003601
  6%|▋         | 132/2040 [1:17:44<18:45:01, 35.38s/it]08/15/2023 11:37:50 - INFO - __main__ -   Step: 132, LR: 1.928246589186458e-05, Loss: 0.5824614763259888
  7%|▋         | 133/2040 [1:18:19<18:41:41, 35.29s/it]08/15/2023 11:38:25 - INFO - __main__ -   Step: 133, LR: 1.927235977766549e-05, Loss: 0.60224449634552
  7%|▋         | 134/2040 [1:18:55<18:41:18, 35.30s/it]08/15/2023 11:39:01 - INFO - __main__ -   Step: 134, LR: 1.9262253663466398e-05, Loss: 0.6361016035079956
  7%|▋         | 135/2040 [1:19:29<18:33:37, 35.07s/it]08/15/2023 11:39:35 - INFO - __main__ -   Step: 135, LR: 1.9252147549267307e-05, Loss: 0.5289111137390137
  7%|▋         | 136/2040 [1:20:04<18:34:07, 35.11s/it]08/15/2023 11:40:10 - INFO - __main__ -   Step: 136, LR: 1.9242041435068216e-05, Loss: 0.6483677625656128
  7%|▋         | 137/2040 [1:20:39<18:30:05, 35.00s/it]08/15/2023 11:40:45 - INFO - __main__ -   Step: 137, LR: 1.923193532086913e-05, Loss: 0.5852087736129761
  7%|▋         | 138/2040 [1:21:15<18:33:39, 35.13s/it]08/15/2023 11:41:21 - INFO - __main__ -   Step: 138, LR: 1.9221829206670038e-05, Loss: 0.5668329000473022
  7%|▋         | 139/2040 [1:21:50<18:35:06, 35.20s/it]08/15/2023 11:41:56 - INFO - __main__ -   Step: 139, LR: 1.9211723092470947e-05, Loss: 0.6233627796173096
  7%|▋         | 140/2040 [1:22:25<18:34:49, 35.21s/it]08/15/2023 11:42:31 - INFO - __main__ -   Step: 140, LR: 1.9201616978271856e-05, Loss: 0.6108390688896179
  7%|▋         | 141/2040 [1:23:00<18:29:54, 35.07s/it]08/15/2023 11:43:06 - INFO - __main__ -   Step: 141, LR: 1.9191510864072768e-05, Loss: 0.6322756409645081
  7%|▋         | 142/2040 [1:23:35<18:26:17, 34.97s/it]08/15/2023 11:43:41 - INFO - __main__ -   Step: 142, LR: 1.9181404749873674e-05, Loss: 0.6103891134262085
  7%|▋         | 143/2040 [1:24:10<18:27:34, 35.03s/it]08/15/2023 11:44:16 - INFO - __main__ -   Step: 143, LR: 1.9171298635674583e-05, Loss: 0.656455397605896
  7%|▋         | 144/2040 [1:24:45<18:25:31, 34.98s/it]08/15/2023 11:44:51 - INFO - __main__ -   Step: 144, LR: 1.9161192521475492e-05, Loss: 0.6729931831359863
  7%|▋         | 145/2040 [1:25:20<18:31:29, 35.19s/it]08/15/2023 11:45:26 - INFO - __main__ -   Step: 145, LR: 1.9151086407276404e-05, Loss: 0.6903522610664368
  7%|▋         | 146/2040 [1:25:56<18:31:16, 35.20s/it]08/15/2023 11:46:02 - INFO - __main__ -   Step: 146, LR: 1.9140980293077314e-05, Loss: 0.5621694326400757
  7%|▋         | 147/2040 [1:26:31<18:33:40, 35.30s/it]08/15/2023 11:46:37 - INFO - __main__ -   Step: 147, LR: 1.9130874178878223e-05, Loss: 0.6307541131973267
  7%|▋         | 148/2040 [1:27:06<18:32:31, 35.28s/it]08/15/2023 11:47:12 - INFO - __main__ -   Step: 148, LR: 1.912076806467913e-05, Loss: 0.5967702865600586
  7%|▋         | 149/2040 [1:27:42<18:30:26, 35.23s/it]08/15/2023 11:47:47 - INFO - __main__ -   Step: 149, LR: 1.9110661950480044e-05, Loss: 0.6278440952301025
  7%|▋         | 150/2040 [1:28:16<18:27:05, 35.15s/it]08/15/2023 11:48:22 - INFO - __main__ -   Step: 150, LR: 1.9100555836280953e-05, Loss: 0.5854023098945618
  7%|▋         | 151/2040 [1:28:51<18:21:20, 34.98s/it]08/15/2023 11:48:57 - INFO - __main__ -   Step: 151, LR: 1.9090449722081862e-05, Loss: 0.6037700176239014
  7%|▋         | 152/2040 [1:29:26<18:18:16, 34.90s/it]08/15/2023 11:49:32 - INFO - __main__ -   Step: 152, LR: 1.908034360788277e-05, Loss: 0.6535675525665283
  8%|▊         | 153/2040 [1:30:01<18:21:26, 35.02s/it]08/15/2023 11:50:07 - INFO - __main__ -   Step: 153, LR: 1.907023749368368e-05, Loss: 0.6222306489944458
  8%|▊         | 154/2040 [1:30:36<18:19:21, 34.97s/it]08/15/2023 11:50:42 - INFO - __main__ -   Step: 154, LR: 1.906013137948459e-05, Loss: 0.5772315263748169
  8%|▊         | 155/2040 [1:31:11<18:18:15, 34.96s/it]08/15/2023 11:51:17 - INFO - __main__ -   Step: 155, LR: 1.90500252652855e-05, Loss: 0.6438899636268616
  8%|▊         | 156/2040 [1:31:46<18:15:57, 34.90s/it]08/15/2023 11:51:52 - INFO - __main__ -   Step: 156, LR: 1.9039919151086408e-05, Loss: 0.593951404094696
  8%|▊         | 157/2040 [1:32:22<18:30:38, 35.39s/it]08/15/2023 11:52:28 - INFO - __main__ -   Step: 157, LR: 1.902981303688732e-05, Loss: 0.5369013547897339
  8%|▊         | 158/2040 [1:32:59<18:44:18, 35.84s/it]08/15/2023 11:53:05 - INFO - __main__ -   Step: 158, LR: 1.901970692268823e-05, Loss: 0.5531518459320068
  8%|▊         | 159/2040 [1:33:36<18:53:37, 36.16s/it]08/15/2023 11:53:42 - INFO - __main__ -   Step: 159, LR: 1.9009600808489138e-05, Loss: 0.6311904191970825
  8%|▊         | 160/2040 [1:34:13<18:58:09, 36.32s/it]08/15/2023 11:54:19 - INFO - __main__ -   Step: 160, LR: 1.8999494694290047e-05, Loss: 0.5363468527793884
  8%|▊         | 161/2040 [1:34:50<19:03:19, 36.51s/it]08/15/2023 11:54:56 - INFO - __main__ -   Step: 161, LR: 1.8989388580090956e-05, Loss: 0.603042483329773
  8%|▊         | 162/2040 [1:35:26<19:05:03, 36.58s/it]08/15/2023 11:55:32 - INFO - __main__ -   Step: 162, LR: 1.8979282465891865e-05, Loss: 0.6028891205787659
  8%|▊         | 163/2040 [1:36:03<19:02:21, 36.52s/it]08/15/2023 11:56:09 - INFO - __main__ -   Step: 163, LR: 1.8969176351692775e-05, Loss: 0.631598174571991
  8%|▊         | 164/2040 [1:36:39<19:00:21, 36.47s/it]08/15/2023 11:56:45 - INFO - __main__ -   Step: 164, LR: 1.8959070237493684e-05, Loss: 0.5919116735458374
  8%|▊         | 165/2040 [1:37:16<19:02:34, 36.56s/it]08/15/2023 11:57:22 - INFO - __main__ -   Step: 165, LR: 1.8948964123294596e-05, Loss: 0.6485264897346497
  8%|▊         | 166/2040 [1:37:52<19:01:59, 36.56s/it]08/15/2023 11:57:58 - INFO - __main__ -   Step: 166, LR: 1.8938858009095505e-05, Loss: 0.6116235256195068
  8%|▊         | 167/2040 [1:38:29<19:02:23, 36.60s/it]08/15/2023 11:58:35 - INFO - __main__ -   Step: 167, LR: 1.8928751894896414e-05, Loss: 0.5775291919708252
  8%|▊         | 168/2040 [1:39:06<19:07:18, 36.77s/it]08/15/2023 11:59:12 - INFO - __main__ -   Step: 168, LR: 1.8918645780697323e-05, Loss: 0.6329576969146729
  8%|▊         | 169/2040 [1:39:43<19:08:07, 36.82s/it]08/15/2023 11:59:49 - INFO - __main__ -   Step: 169, LR: 1.8908539666498232e-05, Loss: 0.6074715256690979
  8%|▊         | 170/2040 [1:40:20<19:10:48, 36.92s/it]08/15/2023 12:00:26 - INFO - __main__ -   Step: 170, LR: 1.889843355229914e-05, Loss: 0.6786825060844421
  8%|▊         | 171/2040 [1:40:57<19:08:14, 36.86s/it]08/15/2023 12:01:03 - INFO - __main__ -   Step: 171, LR: 1.888832743810005e-05, Loss: 0.5935841202735901
  8%|▊         | 172/2040 [1:41:34<19:06:40, 36.83s/it]08/15/2023 12:01:40 - INFO - __main__ -   Step: 172, LR: 1.8878221323900963e-05, Loss: 0.5790871977806091
  8%|▊         | 173/2040 [1:42:11<19:09:56, 36.96s/it]08/15/2023 12:02:17 - INFO - __main__ -   Step: 173, LR: 1.8868115209701872e-05, Loss: 0.5552540421485901
  9%|▊         | 174/2040 [1:42:47<18:55:00, 36.50s/it]08/15/2023 12:02:52 - INFO - __main__ -   Step: 174, LR: 1.885800909550278e-05, Loss: 0.6370890736579895
  9%|▊         | 175/2040 [1:43:22<18:40:04, 36.03s/it]08/15/2023 12:03:27 - INFO - __main__ -   Step: 175, LR: 1.884790298130369e-05, Loss: 0.5920337438583374
  9%|▊         | 176/2040 [1:43:56<18:25:53, 35.60s/it]08/15/2023 12:04:02 - INFO - __main__ -   Step: 176, LR: 1.88377968671046e-05, Loss: 0.5830588936805725
  9%|▊         | 177/2040 [1:44:31<18:22:35, 35.51s/it]08/15/2023 12:04:37 - INFO - __main__ -   Step: 177, LR: 1.882769075290551e-05, Loss: 0.5391695499420166
  9%|▊         | 178/2040 [1:45:07<18:19:48, 35.44s/it]08/15/2023 12:05:13 - INFO - __main__ -   Step: 178, LR: 1.8817584638706417e-05, Loss: 0.44523224234580994
  9%|▉         | 179/2040 [1:45:42<18:17:31, 35.38s/it]08/15/2023 12:05:48 - INFO - __main__ -   Step: 179, LR: 1.8807478524507327e-05, Loss: 0.5555717945098877
  9%|▉         | 180/2040 [1:46:17<18:14:53, 35.32s/it]08/15/2023 12:06:23 - INFO - __main__ -   Step: 180, LR: 1.879737241030824e-05, Loss: 0.6269574165344238
  9%|▉         | 181/2040 [1:46:52<18:10:20, 35.19s/it]08/15/2023 12:06:58 - INFO - __main__ -   Step: 181, LR: 1.8787266296109148e-05, Loss: 0.5979998111724854
  9%|▉         | 182/2040 [1:47:27<18:11:51, 35.26s/it]08/15/2023 12:07:33 - INFO - __main__ -   Step: 182, LR: 1.8777160181910057e-05, Loss: 0.5946833491325378
  9%|▉         | 183/2040 [1:48:03<18:11:30, 35.27s/it]08/15/2023 12:08:09 - INFO - __main__ -   Step: 183, LR: 1.8767054067710966e-05, Loss: 0.5961567163467407
  9%|▉         | 184/2040 [1:48:39<18:16:19, 35.44s/it]08/15/2023 12:08:44 - INFO - __main__ -   Step: 184, LR: 1.875694795351188e-05, Loss: 0.5445114374160767
  9%|▉         | 185/2040 [1:49:14<18:18:12, 35.52s/it]08/15/2023 12:09:20 - INFO - __main__ -   Step: 185, LR: 1.8746841839312784e-05, Loss: 0.5971781611442566
  9%|▉         | 186/2040 [1:49:49<18:14:11, 35.41s/it]08/15/2023 12:09:55 - INFO - __main__ -   Step: 186, LR: 1.8736735725113693e-05, Loss: 0.5851861238479614
  9%|▉         | 187/2040 [1:50:25<18:12:32, 35.38s/it]08/15/2023 12:10:31 - INFO - __main__ -   Step: 187, LR: 1.8726629610914602e-05, Loss: 0.7222814559936523
  9%|▉         | 188/2040 [1:50:59<18:04:51, 35.15s/it]08/15/2023 12:11:05 - INFO - __main__ -   Step: 188, LR: 1.8716523496715515e-05, Loss: 0.6723102331161499
  9%|▉         | 189/2040 [1:51:35<18:04:54, 35.17s/it]08/15/2023 12:11:40 - INFO - __main__ -   Step: 189, LR: 1.8706417382516424e-05, Loss: 0.6540677547454834
  9%|▉         | 190/2040 [1:52:09<18:02:28, 35.11s/it]08/15/2023 12:12:15 - INFO - __main__ -   Step: 190, LR: 1.8696311268317333e-05, Loss: 0.6386032700538635
  9%|▉         | 191/2040 [1:52:45<18:05:42, 35.23s/it]08/15/2023 12:12:51 - INFO - __main__ -   Step: 191, LR: 1.8686205154118242e-05, Loss: 0.6822386980056763
  9%|▉         | 192/2040 [1:53:20<18:06:14, 35.27s/it]08/15/2023 12:13:26 - INFO - __main__ -   Step: 192, LR: 1.8676099039919155e-05, Loss: 0.6433707475662231
  9%|▉         | 193/2040 [1:53:56<18:09:37, 35.40s/it]08/15/2023 12:14:02 - INFO - __main__ -   Step: 193, LR: 1.8665992925720064e-05, Loss: 0.5866267085075378
 10%|▉         | 194/2040 [1:54:31<18:04:41, 35.26s/it]08/15/2023 12:14:37 - INFO - __main__ -   Step: 194, LR: 1.8655886811520973e-05, Loss: 0.5373696684837341
 10%|▉         | 195/2040 [1:55:06<18:01:19, 35.16s/it]08/15/2023 12:15:12 - INFO - __main__ -   Step: 195, LR: 1.8645780697321882e-05, Loss: 0.5167585611343384
 10%|▉         | 196/2040 [1:55:41<17:59:59, 35.14s/it]08/15/2023 12:15:47 - INFO - __main__ -   Step: 196, LR: 1.863567458312279e-05, Loss: 0.6313734650611877
 10%|▉         | 197/2040 [1:56:16<17:58:27, 35.11s/it]08/15/2023 12:16:22 - INFO - __main__ -   Step: 197, LR: 1.86255684689237e-05, Loss: 0.75676429271698
 10%|▉         | 198/2040 [1:56:51<17:59:25, 35.16s/it]08/15/2023 12:16:57 - INFO - __main__ -   Step: 198, LR: 1.861546235472461e-05, Loss: 0.5704526901245117
 10%|▉         | 199/2040 [1:57:27<17:59:55, 35.20s/it]08/15/2023 12:17:33 - INFO - __main__ -   Step: 199, LR: 1.8605356240525518e-05, Loss: 0.5978459119796753
 10%|▉         | 200/2040 [1:58:01<17:52:56, 34.99s/it]08/15/2023 12:18:07 - INFO - __main__ -   Step: 200, LR: 1.859525012632643e-05, Loss: 0.5534850358963013
08/15/2023 12:18:07 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200
08/15/2023 12:18:07 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 12:18:07,529] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 12:18:07,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 12:18:07,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 12:18:07,534] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 12:18:07,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 12:18:07,535] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 12:18:07,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 12:18:07,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 12:18:07,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 12:18:07,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 12:18:07,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 12:18:07,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 12:18:07,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 12:18:07,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 12:18:07,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 12:18:28,087] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 12:18:28,087] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 12:18:29,821] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 12:18:29,822] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 12:18:29,913] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 12:18:29,914] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 12:18:30,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 12:18:30,108] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 12:18:30,113] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 12:18:30,113] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 12:18:30,113] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 12:18:30,113] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 12:18:30 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/pytorch_model
08/15/2023 12:18:30 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/scheduler.bin
08/15/2023 12:18:30 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_200/random_states_0.pkl
 10%|▉         | 201/2040 [1:58:59<21:26:05, 41.96s/it]08/15/2023 12:19:05 - INFO - __main__ -   Step: 201, LR: 1.858514401212734e-05, Loss: 0.6445704698562622
 10%|▉         | 202/2040 [1:59:34<20:22:45, 39.92s/it]08/15/2023 12:19:40 - INFO - __main__ -   Step: 202, LR: 1.857503789792825e-05, Loss: 0.5863426327705383
 10%|▉         | 203/2040 [2:00:09<19:30:51, 38.24s/it]08/15/2023 12:20:15 - INFO - __main__ -   Step: 203, LR: 1.8564931783729158e-05, Loss: 0.5695648193359375
 10%|█         | 204/2040 [2:00:44<18:58:24, 37.20s/it]08/15/2023 12:20:50 - INFO - __main__ -   Step: 204, LR: 1.8554825669530067e-05, Loss: 0.6101325750350952
 10%|█         | 205/2040 [2:01:19<18:39:04, 36.59s/it]08/15/2023 12:21:25 - INFO - __main__ -   Step: 205, LR: 1.8544719555330976e-05, Loss: 0.6188023686408997
 10%|█         | 206/2040 [2:01:54<18:26:27, 36.20s/it]08/15/2023 12:22:00 - INFO - __main__ -   Step: 206, LR: 1.8534613441131885e-05, Loss: 0.6129466891288757
 10%|█         | 207/2040 [2:02:29<18:16:24, 35.89s/it]08/15/2023 12:22:35 - INFO - __main__ -   Step: 207, LR: 1.8524507326932798e-05, Loss: 0.6756699085235596
 10%|█         | 208/2040 [2:03:04<18:08:57, 35.66s/it]08/15/2023 12:23:10 - INFO - __main__ -   Step: 208, LR: 1.8514401212733707e-05, Loss: 0.6059170365333557
 10%|█         | 209/2040 [2:03:39<17:55:19, 35.24s/it]08/15/2023 12:23:45 - INFO - __main__ -   Step: 209, LR: 1.8504295098534616e-05, Loss: 0.5855789184570312
 10%|█         | 210/2040 [2:04:13<17:49:58, 35.08s/it]08/15/2023 12:24:19 - INFO - __main__ -   Step: 210, LR: 1.8494188984335525e-05, Loss: 0.6399456858634949
 10%|█         | 211/2040 [2:04:49<17:52:37, 35.19s/it]08/15/2023 12:24:55 - INFO - __main__ -   Step: 211, LR: 1.8484082870136434e-05, Loss: 0.5209686756134033
 10%|█         | 212/2040 [2:05:24<17:51:20, 35.16s/it]08/15/2023 12:25:30 - INFO - __main__ -   Step: 212, LR: 1.8473976755937343e-05, Loss: 0.6289898157119751
 10%|█         | 213/2040 [2:05:59<17:46:29, 35.02s/it]08/15/2023 12:26:04 - INFO - __main__ -   Step: 213, LR: 1.8463870641738252e-05, Loss: 0.5593200922012329
 10%|█         | 214/2040 [2:06:34<17:47:42, 35.08s/it]08/15/2023 12:26:40 - INFO - __main__ -   Step: 214, LR: 1.845376452753916e-05, Loss: 0.6359691023826599
 11%|█         | 215/2040 [2:07:09<17:48:49, 35.14s/it]08/15/2023 12:27:15 - INFO - __main__ -   Step: 215, LR: 1.8443658413340073e-05, Loss: 0.6658596396446228
 11%|█         | 216/2040 [2:07:44<17:42:46, 34.96s/it]08/15/2023 12:27:50 - INFO - __main__ -   Step: 216, LR: 1.8433552299140983e-05, Loss: 0.6220309734344482
 11%|█         | 217/2040 [2:08:19<17:44:42, 35.04s/it]08/15/2023 12:28:25 - INFO - __main__ -   Step: 217, LR: 1.842344618494189e-05, Loss: 0.6013925075531006
 11%|█         | 218/2040 [2:08:53<17:38:38, 34.86s/it]08/15/2023 12:28:59 - INFO - __main__ -   Step: 218, LR: 1.84133400707428e-05, Loss: 0.5317152738571167
 11%|█         | 219/2040 [2:09:29<17:43:24, 35.04s/it]08/15/2023 12:29:35 - INFO - __main__ -   Step: 219, LR: 1.840323395654371e-05, Loss: 0.6021099090576172
 11%|█         | 220/2040 [2:10:03<17:39:11, 34.92s/it]08/15/2023 12:30:09 - INFO - __main__ -   Step: 220, LR: 1.839312784234462e-05, Loss: 0.611907958984375
 11%|█         | 221/2040 [2:10:39<17:42:44, 35.05s/it]08/15/2023 12:30:45 - INFO - __main__ -   Step: 221, LR: 1.8383021728145528e-05, Loss: 0.5457292795181274
 11%|█         | 222/2040 [2:11:13<17:39:03, 34.95s/it]08/15/2023 12:31:19 - INFO - __main__ -   Step: 222, LR: 1.8372915613946437e-05, Loss: 0.6049667000770569
 11%|█         | 223/2040 [2:11:48<17:39:02, 34.97s/it]08/15/2023 12:31:54 - INFO - __main__ -   Step: 223, LR: 1.836280949974735e-05, Loss: 0.5870538353919983
 11%|█         | 224/2040 [2:12:23<17:33:28, 34.81s/it]08/15/2023 12:32:29 - INFO - __main__ -   Step: 224, LR: 1.835270338554826e-05, Loss: 0.5446255803108215
 11%|█         | 225/2040 [2:12:57<17:29:26, 34.69s/it]08/15/2023 12:33:03 - INFO - __main__ -   Step: 225, LR: 1.8342597271349168e-05, Loss: 0.6442453861236572
 11%|█         | 226/2040 [2:13:32<17:30:04, 34.73s/it]08/15/2023 12:33:38 - INFO - __main__ -   Step: 226, LR: 1.8332491157150077e-05, Loss: 0.5411291122436523
 11%|█         | 227/2040 [2:14:07<17:35:10, 34.92s/it]08/15/2023 12:34:13 - INFO - __main__ -   Step: 227, LR: 1.832238504295099e-05, Loss: 0.5728360414505005
 11%|█         | 228/2040 [2:14:42<17:33:42, 34.89s/it]08/15/2023 12:34:48 - INFO - __main__ -   Step: 228, LR: 1.8312278928751895e-05, Loss: 0.5467857122421265
 11%|█         | 229/2040 [2:15:17<17:32:13, 34.86s/it]08/15/2023 12:35:23 - INFO - __main__ -   Step: 229, LR: 1.8302172814552804e-05, Loss: 0.5488330125808716
 11%|█▏        | 230/2040 [2:15:52<17:28:22, 34.75s/it]08/15/2023 12:35:58 - INFO - __main__ -   Step: 230, LR: 1.8292066700353713e-05, Loss: 0.5994889140129089
 11%|█▏        | 231/2040 [2:16:26<17:25:41, 34.68s/it]08/15/2023 12:36:32 - INFO - __main__ -   Step: 231, LR: 1.8281960586154625e-05, Loss: 0.6030086278915405
 11%|█▏        | 232/2040 [2:17:01<17:24:49, 34.67s/it]08/15/2023 12:37:07 - INFO - __main__ -   Step: 232, LR: 1.8271854471955535e-05, Loss: 0.5463000535964966
 11%|█▏        | 233/2040 [2:17:36<17:29:17, 34.84s/it]08/15/2023 12:37:42 - INFO - __main__ -   Step: 233, LR: 1.8261748357756444e-05, Loss: 0.57061767578125
 11%|█▏        | 234/2040 [2:18:11<17:30:48, 34.91s/it]08/15/2023 12:38:17 - INFO - __main__ -   Step: 234, LR: 1.8251642243557353e-05, Loss: 0.6219541430473328
 12%|█▏        | 235/2040 [2:18:46<17:29:23, 34.88s/it]08/15/2023 12:38:52 - INFO - __main__ -   Step: 235, LR: 1.8241536129358265e-05, Loss: 0.5992871522903442
 12%|█▏        | 236/2040 [2:19:20<17:24:52, 34.75s/it]08/15/2023 12:39:26 - INFO - __main__ -   Step: 236, LR: 1.8231430015159174e-05, Loss: 0.6781123876571655
 12%|█▏        | 237/2040 [2:19:55<17:19:09, 34.58s/it]08/15/2023 12:40:00 - INFO - __main__ -   Step: 237, LR: 1.8221323900960083e-05, Loss: 0.5736829042434692
 12%|█▏        | 238/2040 [2:20:29<17:21:44, 34.69s/it]08/15/2023 12:40:35 - INFO - __main__ -   Step: 238, LR: 1.8211217786760992e-05, Loss: 0.5117977261543274
 12%|█▏        | 239/2040 [2:21:04<17:23:09, 34.75s/it]08/15/2023 12:41:10 - INFO - __main__ -   Step: 239, LR: 1.82011116725619e-05, Loss: 0.5814791917800903
 12%|█▏        | 240/2040 [2:21:40<17:26:26, 34.88s/it]08/15/2023 12:41:45 - INFO - __main__ -   Step: 240, LR: 1.819100555836281e-05, Loss: 0.5429661273956299
 12%|█▏        | 241/2040 [2:22:14<17:23:26, 34.80s/it]08/15/2023 12:42:20 - INFO - __main__ -   Step: 241, LR: 1.818089944416372e-05, Loss: 0.6314787864685059
 12%|█▏        | 242/2040 [2:22:49<17:20:51, 34.73s/it]08/15/2023 12:42:55 - INFO - __main__ -   Step: 242, LR: 1.8170793329964632e-05, Loss: 0.5420826077461243
 12%|█▏        | 243/2040 [2:23:23<17:12:54, 34.49s/it]08/15/2023 12:43:29 - INFO - __main__ -   Step: 243, LR: 1.816068721576554e-05, Loss: 0.5930526256561279
 12%|█▏        | 244/2040 [2:23:58<17:17:35, 34.66s/it]08/15/2023 12:44:04 - INFO - __main__ -   Step: 244, LR: 1.815058110156645e-05, Loss: 0.6307569146156311
 12%|█▏        | 245/2040 [2:24:32<17:17:01, 34.66s/it]08/15/2023 12:44:38 - INFO - __main__ -   Step: 245, LR: 1.814047498736736e-05, Loss: 0.61965411901474
 12%|█▏        | 246/2040 [2:25:08<17:21:38, 34.84s/it]08/15/2023 12:45:14 - INFO - __main__ -   Step: 246, LR: 1.813036887316827e-05, Loss: 0.5808995962142944
 12%|█▏        | 247/2040 [2:25:42<17:17:02, 34.70s/it]08/15/2023 12:45:48 - INFO - __main__ -   Step: 247, LR: 1.8120262758969177e-05, Loss: 0.6412276029586792
 12%|█▏        | 248/2040 [2:26:17<17:18:00, 34.75s/it]08/15/2023 12:46:23 - INFO - __main__ -   Step: 248, LR: 1.8110156644770086e-05, Loss: 0.6703734397888184
 12%|█▏        | 249/2040 [2:26:51<17:13:27, 34.62s/it]08/15/2023 12:46:57 - INFO - __main__ -   Step: 249, LR: 1.8100050530570996e-05, Loss: 0.537476658821106
 12%|█▏        | 250/2040 [2:27:26<17:11:42, 34.58s/it]08/15/2023 12:47:32 - INFO - __main__ -   Step: 250, LR: 1.8089944416371908e-05, Loss: 0.5864592790603638
 12%|█▏        | 251/2040 [2:28:00<17:12:51, 34.64s/it]08/15/2023 12:48:06 - INFO - __main__ -   Step: 251, LR: 1.8079838302172817e-05, Loss: 0.5552319288253784
 12%|█▏        | 252/2040 [2:28:36<17:16:17, 34.77s/it]08/15/2023 12:48:41 - INFO - __main__ -   Step: 252, LR: 1.8069732187973726e-05, Loss: 0.5623400211334229
 12%|█▏        | 253/2040 [2:29:10<17:16:37, 34.81s/it]08/15/2023 12:49:16 - INFO - __main__ -   Step: 253, LR: 1.8059626073774635e-05, Loss: 0.5631104707717896
 12%|█▏        | 254/2040 [2:29:46<17:19:17, 34.91s/it]08/15/2023 12:49:52 - INFO - __main__ -   Step: 254, LR: 1.8049519959575544e-05, Loss: 0.5527989864349365
 12%|█▎        | 255/2040 [2:30:20<17:17:35, 34.88s/it]08/15/2023 12:50:26 - INFO - __main__ -   Step: 255, LR: 1.8039413845376453e-05, Loss: 0.580049455165863
 13%|█▎        | 256/2040 [2:30:55<17:16:41, 34.87s/it]08/15/2023 12:51:01 - INFO - __main__ -   Step: 256, LR: 1.8029307731177362e-05, Loss: 0.6044415831565857
 13%|█▎        | 257/2040 [2:31:30<17:13:59, 34.79s/it]08/15/2023 12:51:36 - INFO - __main__ -   Step: 257, LR: 1.801920161697827e-05, Loss: 0.5471196174621582
 13%|█▎        | 258/2040 [2:32:05<17:15:20, 34.86s/it]08/15/2023 12:52:11 - INFO - __main__ -   Step: 258, LR: 1.8009095502779184e-05, Loss: 0.5764335989952087
 13%|█▎        | 259/2040 [2:32:40<17:16:03, 34.90s/it]08/15/2023 12:52:46 - INFO - __main__ -   Step: 259, LR: 1.7998989388580093e-05, Loss: 0.6567813754081726
 13%|█▎        | 260/2040 [2:33:15<17:15:51, 34.92s/it]08/15/2023 12:53:21 - INFO - __main__ -   Step: 260, LR: 1.7988883274381002e-05, Loss: 0.5897310972213745
 13%|█▎        | 261/2040 [2:33:49<17:10:19, 34.75s/it]08/15/2023 12:53:55 - INFO - __main__ -   Step: 261, LR: 1.797877716018191e-05, Loss: 0.6221084594726562
 13%|█▎        | 262/2040 [2:34:23<17:04:01, 34.56s/it]08/15/2023 12:54:29 - INFO - __main__ -   Step: 262, LR: 1.796867104598282e-05, Loss: 0.6154683828353882
 13%|█▎        | 263/2040 [2:34:58<17:05:47, 34.64s/it]08/15/2023 12:55:04 - INFO - __main__ -   Step: 263, LR: 1.795856493178373e-05, Loss: 0.5710760354995728
 13%|█▎        | 264/2040 [2:35:33<17:11:32, 34.85s/it]08/15/2023 12:55:39 - INFO - __main__ -   Step: 264, LR: 1.794845881758464e-05, Loss: 0.6346124410629272
 13%|█▎        | 265/2040 [2:36:08<17:10:38, 34.84s/it]08/15/2023 12:56:14 - INFO - __main__ -   Step: 265, LR: 1.7938352703385547e-05, Loss: 0.6488763689994812
 13%|█▎        | 266/2040 [2:36:44<17:14:18, 34.98s/it]08/15/2023 12:56:50 - INFO - __main__ -   Step: 266, LR: 1.792824658918646e-05, Loss: 0.6852710843086243
 13%|█▎        | 267/2040 [2:37:18<17:10:11, 34.86s/it]08/15/2023 12:57:24 - INFO - __main__ -   Step: 267, LR: 1.791814047498737e-05, Loss: 0.6681784391403198
 13%|█▎        | 268/2040 [2:37:53<17:06:05, 34.74s/it]08/15/2023 12:57:59 - INFO - __main__ -   Step: 268, LR: 1.7908034360788278e-05, Loss: 0.5610290765762329
 13%|█▎        | 269/2040 [2:38:28<17:06:33, 34.78s/it]08/15/2023 12:58:33 - INFO - __main__ -   Step: 269, LR: 1.7897928246589187e-05, Loss: 0.5952581763267517
 13%|█▎        | 270/2040 [2:39:02<17:05:23, 34.76s/it]08/15/2023 12:59:08 - INFO - __main__ -   Step: 270, LR: 1.78878221323901e-05, Loss: 0.572329044342041
 13%|█▎        | 271/2040 [2:39:37<17:09:15, 34.91s/it]08/15/2023 12:59:43 - INFO - __main__ -   Step: 271, LR: 1.7877716018191005e-05, Loss: 0.5958030223846436
 13%|█▎        | 272/2040 [2:40:12<17:06:31, 34.84s/it]08/15/2023 13:00:18 - INFO - __main__ -   Step: 272, LR: 1.7867609903991914e-05, Loss: 0.5611070394515991
 13%|█▎        | 273/2040 [2:40:47<17:06:22, 34.85s/it]08/15/2023 13:00:53 - INFO - __main__ -   Step: 273, LR: 1.7857503789792827e-05, Loss: 0.5445874333381653
 13%|█▎        | 274/2040 [2:41:21<16:59:20, 34.63s/it]08/15/2023 13:01:27 - INFO - __main__ -   Step: 274, LR: 1.7847397675593736e-05, Loss: 0.5941187143325806
 13%|█▎        | 275/2040 [2:41:56<17:00:11, 34.68s/it]08/15/2023 13:02:02 - INFO - __main__ -   Step: 275, LR: 1.7837291561394645e-05, Loss: 0.5936815738677979
 14%|█▎        | 276/2040 [2:42:31<16:58:35, 34.65s/it]08/15/2023 13:02:36 - INFO - __main__ -   Step: 276, LR: 1.7827185447195554e-05, Loss: 0.5799434781074524
 14%|█▎        | 277/2040 [2:43:06<17:01:51, 34.78s/it]08/15/2023 13:03:12 - INFO - __main__ -   Step: 277, LR: 1.7817079332996467e-05, Loss: 0.6285474300384521
 14%|█▎        | 278/2040 [2:43:40<16:57:04, 34.63s/it]08/15/2023 13:03:46 - INFO - __main__ -   Step: 278, LR: 1.7806973218797376e-05, Loss: 0.5755840539932251
 14%|█▎        | 279/2040 [2:44:15<16:56:42, 34.64s/it]08/15/2023 13:04:20 - INFO - __main__ -   Step: 279, LR: 1.7796867104598285e-05, Loss: 0.5391262769699097
 14%|█▎        | 280/2040 [2:44:49<16:53:12, 34.54s/it]08/15/2023 13:04:55 - INFO - __main__ -   Step: 280, LR: 1.7786760990399194e-05, Loss: 0.5301075577735901
 14%|█▍        | 281/2040 [2:45:24<16:58:30, 34.74s/it]08/15/2023 13:05:30 - INFO - __main__ -   Step: 281, LR: 1.7776654876200103e-05, Loss: 0.5550855994224548
 14%|█▍        | 282/2040 [2:45:59<16:59:36, 34.80s/it]08/15/2023 13:06:05 - INFO - __main__ -   Step: 282, LR: 1.7766548762001012e-05, Loss: 0.586032509803772
 14%|█▍        | 283/2040 [2:46:34<17:00:08, 34.84s/it]08/15/2023 13:06:40 - INFO - __main__ -   Step: 283, LR: 1.775644264780192e-05, Loss: 0.5661579966545105
 14%|█▍        | 284/2040 [2:47:08<16:54:38, 34.67s/it]08/15/2023 13:07:14 - INFO - __main__ -   Step: 284, LR: 1.774633653360283e-05, Loss: 0.5840086936950684
 14%|█▍        | 285/2040 [2:47:43<16:54:11, 34.67s/it]08/15/2023 13:07:49 - INFO - __main__ -   Step: 285, LR: 1.7736230419403743e-05, Loss: 0.574354887008667
 14%|█▍        | 286/2040 [2:48:17<16:51:09, 34.59s/it]08/15/2023 13:08:23 - INFO - __main__ -   Step: 286, LR: 1.772612430520465e-05, Loss: 0.6324776411056519
 14%|█▍        | 287/2040 [2:48:52<16:50:34, 34.59s/it]08/15/2023 13:08:58 - INFO - __main__ -   Step: 287, LR: 1.771601819100556e-05, Loss: 0.5280119776725769
 14%|█▍        | 288/2040 [2:49:27<16:50:28, 34.61s/it]08/15/2023 13:09:32 - INFO - __main__ -   Step: 288, LR: 1.770591207680647e-05, Loss: 0.5657030940055847
 14%|█▍        | 289/2040 [2:50:02<16:55:51, 34.81s/it]08/15/2023 13:10:08 - INFO - __main__ -   Step: 289, LR: 1.769580596260738e-05, Loss: 0.5698275566101074
 14%|█▍        | 290/2040 [2:50:36<16:52:08, 34.70s/it]08/15/2023 13:10:42 - INFO - __main__ -   Step: 290, LR: 1.7685699848408288e-05, Loss: 0.6139079332351685
 14%|█▍        | 291/2040 [2:51:11<16:53:18, 34.76s/it]08/15/2023 13:11:17 - INFO - __main__ -   Step: 291, LR: 1.7675593734209197e-05, Loss: 0.5597504377365112
 14%|█▍        | 292/2040 [2:51:46<16:50:05, 34.67s/it]08/15/2023 13:11:52 - INFO - __main__ -   Step: 292, LR: 1.7665487620010106e-05, Loss: 0.7114523649215698
 14%|█▍        | 293/2040 [2:52:20<16:49:43, 34.68s/it]08/15/2023 13:12:26 - INFO - __main__ -   Step: 293, LR: 1.765538150581102e-05, Loss: 0.6014283895492554
 14%|█▍        | 294/2040 [2:52:55<16:53:03, 34.81s/it]08/15/2023 13:13:01 - INFO - __main__ -   Step: 294, LR: 1.7645275391611928e-05, Loss: 0.6636648178100586
 14%|█▍        | 295/2040 [2:53:30<16:53:17, 34.84s/it]08/15/2023 13:13:36 - INFO - __main__ -   Step: 295, LR: 1.7635169277412837e-05, Loss: 0.6095703840255737
 15%|█▍        | 296/2040 [2:54:05<16:54:15, 34.89s/it]08/15/2023 13:14:11 - INFO - __main__ -   Step: 296, LR: 1.7625063163213746e-05, Loss: 0.6182646751403809
 15%|█▍        | 297/2040 [2:54:40<16:52:24, 34.85s/it]08/15/2023 13:14:46 - INFO - __main__ -   Step: 297, LR: 1.7614957049014655e-05, Loss: 0.45886048674583435
 15%|█▍        | 298/2040 [2:55:14<16:47:07, 34.69s/it]08/15/2023 13:15:20 - INFO - __main__ -   Step: 298, LR: 1.7604850934815564e-05, Loss: 0.5736662149429321
 15%|█▍        | 299/2040 [2:55:49<16:48:51, 34.77s/it]08/15/2023 13:15:55 - INFO - __main__ -   Step: 299, LR: 1.7594744820616473e-05, Loss: 0.6386006474494934
 15%|█▍        | 300/2040 [2:56:24<16:49:03, 34.80s/it]08/15/2023 13:16:30 - INFO - __main__ -   Step: 300, LR: 1.7584638706417382e-05, Loss: 0.6749852895736694
08/15/2023 13:16:30 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300
08/15/2023 13:16:30 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 13:16:30,647] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 13:16:30,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 13:16:30,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 13:16:30,653] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 13:16:30,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 13:16:30,653] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 13:16:30,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 13:16:30,664] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 13:16:30,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 13:16:30,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 13:16:30,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 13:16:30,666] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 13:16:30,666] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 13:16:30,666] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 13:16:30,666] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 13:16:50,891] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 13:16:50,891] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 13:16:51,581] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 13:16:51,581] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 13:16:52,350] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 13:16:52,350] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 13:16:53,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 13:16:53,167] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 13:16:53,171] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 13:16:53,171] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 13:16:53,171] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 13:16:53,172] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 13:16:53 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/pytorch_model
08/15/2023 13:16:53 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/scheduler.bin
08/15/2023 13:16:53 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_300/random_states_0.pkl
 15%|█▍        | 301/2040 [2:57:23<20:13:59, 41.89s/it]08/15/2023 13:17:29 - INFO - __main__ -   Step: 301, LR: 1.7574532592218294e-05, Loss: 0.5193898677825928
 15%|█▍        | 302/2040 [2:57:58<19:14:10, 39.84s/it]08/15/2023 13:18:04 - INFO - __main__ -   Step: 302, LR: 1.7564426478019204e-05, Loss: 0.5574895143508911
 15%|█▍        | 303/2040 [2:58:33<18:35:04, 38.52s/it]08/15/2023 13:18:39 - INFO - __main__ -   Step: 303, LR: 1.7554320363820113e-05, Loss: 0.6188157796859741
 15%|█▍        | 304/2040 [2:59:08<18:05:12, 37.51s/it]08/15/2023 13:19:14 - INFO - __main__ -   Step: 304, LR: 1.754421424962102e-05, Loss: 0.6026636958122253
 15%|█▍        | 305/2040 [2:59:43<17:44:11, 36.80s/it]08/15/2023 13:19:49 - INFO - __main__ -   Step: 305, LR: 1.753410813542193e-05, Loss: 0.632283091545105
 15%|█▌        | 306/2040 [3:00:18<17:24:58, 36.16s/it]08/15/2023 13:20:24 - INFO - __main__ -   Step: 306, LR: 1.752400202122284e-05, Loss: 0.5557935237884521
 15%|█▌        | 307/2040 [3:00:53<17:12:23, 35.74s/it]08/15/2023 13:20:59 - INFO - __main__ -   Step: 307, LR: 1.751389590702375e-05, Loss: 0.665107250213623
 15%|█▌        | 308/2040 [3:01:28<17:05:30, 35.53s/it]08/15/2023 13:21:34 - INFO - __main__ -   Step: 308, LR: 1.750378979282466e-05, Loss: 0.618828535079956
 15%|█▌        | 309/2040 [3:02:03<17:04:33, 35.51s/it]08/15/2023 13:22:09 - INFO - __main__ -   Step: 309, LR: 1.749368367862557e-05, Loss: 0.5565555691719055
 15%|█▌        | 310/2040 [3:02:38<16:59:02, 35.34s/it]08/15/2023 13:22:44 - INFO - __main__ -   Step: 310, LR: 1.748357756442648e-05, Loss: 0.5918779373168945
 15%|█▌        | 311/2040 [3:03:14<16:58:07, 35.33s/it]08/15/2023 13:23:20 - INFO - __main__ -   Step: 311, LR: 1.747347145022739e-05, Loss: 0.5939037799835205
 15%|█▌        | 312/2040 [3:03:48<16:50:26, 35.08s/it]08/15/2023 13:23:54 - INFO - __main__ -   Step: 312, LR: 1.74633653360283e-05, Loss: 0.552536129951477
 15%|█▌        | 313/2040 [3:04:23<16:49:47, 35.08s/it]08/15/2023 13:24:29 - INFO - __main__ -   Step: 313, LR: 1.745325922182921e-05, Loss: 0.5606521368026733
 15%|█▌        | 314/2040 [3:04:58<16:48:18, 35.05s/it]08/15/2023 13:25:04 - INFO - __main__ -   Step: 314, LR: 1.7443153107630116e-05, Loss: 0.6193443536758423
 15%|█▌        | 315/2040 [3:05:34<16:50:18, 35.14s/it]08/15/2023 13:25:39 - INFO - __main__ -   Step: 315, LR: 1.7433046993431025e-05, Loss: 0.6324443817138672
 15%|█▌        | 316/2040 [3:06:09<16:50:45, 35.18s/it]08/15/2023 13:26:15 - INFO - __main__ -   Step: 316, LR: 1.7422940879231937e-05, Loss: 0.5422855615615845
 16%|█▌        | 317/2040 [3:06:44<16:48:35, 35.12s/it]08/15/2023 13:26:50 - INFO - __main__ -   Step: 317, LR: 1.7412834765032846e-05, Loss: 0.6057073473930359
 16%|█▌        | 318/2040 [3:07:19<16:48:04, 35.12s/it]08/15/2023 13:27:25 - INFO - __main__ -   Step: 318, LR: 1.7402728650833755e-05, Loss: 0.627671480178833
 16%|█▌        | 319/2040 [3:07:54<16:46:47, 35.10s/it]08/15/2023 13:28:00 - INFO - __main__ -   Step: 319, LR: 1.7392622536634665e-05, Loss: 0.5745924711227417
 16%|█▌        | 320/2040 [3:08:29<16:46:30, 35.11s/it]08/15/2023 13:28:35 - INFO - __main__ -   Step: 320, LR: 1.7382516422435577e-05, Loss: 0.5847387909889221
 16%|█▌        | 321/2040 [3:09:05<16:50:50, 35.28s/it]08/15/2023 13:29:11 - INFO - __main__ -   Step: 321, LR: 1.7372410308236486e-05, Loss: 0.5973387956619263
 16%|█▌        | 322/2040 [3:09:42<17:04:24, 35.78s/it]08/15/2023 13:29:48 - INFO - __main__ -   Step: 322, LR: 1.7362304194037395e-05, Loss: 0.605269193649292
 16%|█▌        | 323/2040 [3:10:17<17:00:02, 35.65s/it]08/15/2023 13:30:23 - INFO - __main__ -   Step: 323, LR: 1.7352198079838304e-05, Loss: 0.6108121275901794
 16%|█▌        | 324/2040 [3:10:52<16:56:55, 35.56s/it]08/15/2023 13:30:58 - INFO - __main__ -   Step: 324, LR: 1.7342091965639213e-05, Loss: 0.5756464600563049
 16%|█▌        | 325/2040 [3:11:28<16:57:16, 35.59s/it]08/15/2023 13:31:34 - INFO - __main__ -   Step: 325, LR: 1.7331985851440122e-05, Loss: 0.6552455425262451
 16%|█▌        | 326/2040 [3:12:04<16:55:19, 35.54s/it]08/15/2023 13:32:09 - INFO - __main__ -   Step: 326, LR: 1.732187973724103e-05, Loss: 0.5316221714019775
 16%|█▌        | 327/2040 [3:12:39<16:53:34, 35.50s/it]08/15/2023 13:32:45 - INFO - __main__ -   Step: 327, LR: 1.731177362304194e-05, Loss: 0.6579018831253052
 16%|█▌        | 328/2040 [3:13:15<16:54:28, 35.55s/it]08/15/2023 13:33:21 - INFO - __main__ -   Step: 328, LR: 1.7301667508842853e-05, Loss: 0.6071895360946655
 16%|█▌        | 329/2040 [3:13:50<16:53:59, 35.56s/it]08/15/2023 13:33:56 - INFO - __main__ -   Step: 329, LR: 1.7291561394643762e-05, Loss: 0.6102777123451233
 16%|█▌        | 330/2040 [3:14:26<16:54:08, 35.58s/it]08/15/2023 13:34:32 - INFO - __main__ -   Step: 330, LR: 1.728145528044467e-05, Loss: 0.5271498560905457
 16%|█▌        | 331/2040 [3:15:01<16:51:48, 35.52s/it]08/15/2023 13:35:07 - INFO - __main__ -   Step: 331, LR: 1.727134916624558e-05, Loss: 0.6218583583831787
 16%|█▋        | 332/2040 [3:15:37<16:50:11, 35.49s/it]08/15/2023 13:35:43 - INFO - __main__ -   Step: 332, LR: 1.726124305204649e-05, Loss: 0.5817620754241943
 16%|█▋        | 333/2040 [3:16:12<16:52:47, 35.60s/it]08/15/2023 13:36:18 - INFO - __main__ -   Step: 333, LR: 1.72511369378474e-05, Loss: 0.5973622798919678
 16%|█▋        | 334/2040 [3:16:47<16:46:18, 35.39s/it]08/15/2023 13:36:53 - INFO - __main__ -   Step: 334, LR: 1.7241030823648307e-05, Loss: 0.5536388158798218
 16%|█▋        | 335/2040 [3:17:23<16:44:58, 35.37s/it]08/15/2023 13:37:29 - INFO - __main__ -   Step: 335, LR: 1.7230924709449217e-05, Loss: 0.5592696070671082
 16%|█▋        | 336/2040 [3:17:57<16:37:04, 35.11s/it]08/15/2023 13:38:03 - INFO - __main__ -   Step: 336, LR: 1.722081859525013e-05, Loss: 0.530333399772644
 17%|█▋        | 337/2040 [3:18:33<16:42:54, 35.33s/it]08/15/2023 13:38:39 - INFO - __main__ -   Step: 337, LR: 1.7210712481051038e-05, Loss: 0.5640195608139038
 17%|█▋        | 338/2040 [3:19:09<16:47:16, 35.51s/it]08/15/2023 13:39:15 - INFO - __main__ -   Step: 338, LR: 1.7200606366851947e-05, Loss: 0.5539740920066833
 17%|█▋        | 339/2040 [3:19:45<16:49:22, 35.60s/it]08/15/2023 13:39:51 - INFO - __main__ -   Step: 339, LR: 1.7190500252652856e-05, Loss: 0.5789692401885986
 17%|█▋        | 340/2040 [3:20:20<16:48:48, 35.61s/it]08/15/2023 13:40:26 - INFO - __main__ -   Step: 340, LR: 1.7180394138453765e-05, Loss: 0.5538942813873291
 17%|█▋        | 341/2040 [3:20:57<16:55:06, 35.85s/it]08/15/2023 13:41:03 - INFO - __main__ -   Step: 341, LR: 1.7170288024254674e-05, Loss: 0.5938490629196167
 17%|█▋        | 342/2040 [3:21:32<16:47:01, 35.58s/it]08/15/2023 13:41:38 - INFO - __main__ -   Step: 342, LR: 1.7160181910055583e-05, Loss: 0.5444893836975098
 17%|█▋        | 343/2040 [3:22:07<16:40:31, 35.38s/it]08/15/2023 13:42:13 - INFO - __main__ -   Step: 343, LR: 1.7150075795856496e-05, Loss: 0.6247966289520264
 17%|█▋        | 344/2040 [3:22:41<16:32:41, 35.12s/it]08/15/2023 13:42:47 - INFO - __main__ -   Step: 344, LR: 1.7139969681657405e-05, Loss: 0.6603958606719971
 17%|█▋        | 345/2040 [3:23:16<16:32:48, 35.14s/it]08/15/2023 13:43:22 - INFO - __main__ -   Step: 345, LR: 1.7129863567458314e-05, Loss: 0.5451706051826477
 17%|█▋        | 346/2040 [3:23:52<16:35:05, 35.25s/it]08/15/2023 13:43:58 - INFO - __main__ -   Step: 346, LR: 1.7119757453259223e-05, Loss: 0.5900927782058716
 17%|█▋        | 347/2040 [3:24:27<16:36:46, 35.33s/it]08/15/2023 13:44:33 - INFO - __main__ -   Step: 347, LR: 1.7109651339060132e-05, Loss: 0.6060087084770203
 17%|█▋        | 348/2040 [3:25:03<16:35:07, 35.29s/it]08/15/2023 13:45:09 - INFO - __main__ -   Step: 348, LR: 1.709954522486104e-05, Loss: 0.6232753992080688
 17%|█▋        | 349/2040 [3:25:42<17:10:09, 36.55s/it]08/15/2023 13:45:48 - INFO - __main__ -   Step: 349, LR: 1.708943911066195e-05, Loss: 0.6230770945549011
 17%|█▋        | 350/2040 [3:26:21<17:26:41, 37.16s/it]08/15/2023 13:46:27 - INFO - __main__ -   Step: 350, LR: 1.707933299646286e-05, Loss: 0.6216617822647095
 17%|█▋        | 351/2040 [3:26:58<17:28:18, 37.24s/it]08/15/2023 13:47:04 - INFO - __main__ -   Step: 351, LR: 1.7069226882263772e-05, Loss: 0.551033616065979
 17%|█▋        | 352/2040 [3:27:33<17:11:25, 36.66s/it]08/15/2023 13:47:39 - INFO - __main__ -   Step: 352, LR: 1.705912076806468e-05, Loss: 0.6484779119491577
 17%|█▋        | 353/2040 [3:28:11<17:17:47, 36.91s/it]08/15/2023 13:48:17 - INFO - __main__ -   Step: 353, LR: 1.704901465386559e-05, Loss: 0.6198912858963013
 17%|█▋        | 354/2040 [3:28:46<17:03:35, 36.43s/it]08/15/2023 13:48:52 - INFO - __main__ -   Step: 354, LR: 1.70389085396665e-05, Loss: 0.5841690301895142
 17%|█▋        | 355/2040 [3:29:21<16:51:24, 36.01s/it]08/15/2023 13:49:27 - INFO - __main__ -   Step: 355, LR: 1.702880242546741e-05, Loss: 0.5399613380432129
 17%|█▋        | 356/2040 [3:29:57<16:46:42, 35.87s/it]08/15/2023 13:50:03 - INFO - __main__ -   Step: 356, LR: 1.701869631126832e-05, Loss: 0.6537968516349792
 18%|█▊        | 357/2040 [3:30:32<16:37:56, 35.58s/it]08/15/2023 13:50:38 - INFO - __main__ -   Step: 357, LR: 1.7008590197069226e-05, Loss: 0.6306067705154419
 18%|█▊        | 358/2040 [3:31:07<16:33:39, 35.45s/it]08/15/2023 13:51:13 - INFO - __main__ -   Step: 358, LR: 1.6998484082870135e-05, Loss: 0.6283574104309082
 18%|█▊        | 359/2040 [3:31:43<16:38:41, 35.65s/it]08/15/2023 13:51:49 - INFO - __main__ -   Step: 359, LR: 1.6988377968671048e-05, Loss: 0.5464053153991699
 18%|█▊        | 360/2040 [3:32:19<16:37:50, 35.64s/it]08/15/2023 13:52:24 - INFO - __main__ -   Step: 360, LR: 1.6978271854471957e-05, Loss: 0.6543252468109131
 18%|█▊        | 361/2040 [3:32:54<16:34:19, 35.53s/it]08/15/2023 13:53:00 - INFO - __main__ -   Step: 361, LR: 1.6968165740272866e-05, Loss: 0.5419036149978638
 18%|█▊        | 362/2040 [3:33:29<16:32:54, 35.50s/it]08/15/2023 13:53:35 - INFO - __main__ -   Step: 362, LR: 1.6958059626073775e-05, Loss: 0.49325913190841675
 18%|█▊        | 363/2040 [3:34:04<16:24:12, 35.21s/it]08/15/2023 13:54:10 - INFO - __main__ -   Step: 363, LR: 1.6947953511874688e-05, Loss: 0.518856406211853
 18%|█▊        | 364/2040 [3:34:38<16:11:01, 34.76s/it]08/15/2023 13:54:43 - INFO - __main__ -   Step: 364, LR: 1.6937847397675597e-05, Loss: 0.5372729301452637
 18%|█▊        | 365/2040 [3:35:12<16:06:02, 34.60s/it]08/15/2023 13:55:18 - INFO - __main__ -   Step: 365, LR: 1.6927741283476506e-05, Loss: 0.6682274341583252
 18%|█▊        | 366/2040 [3:35:45<15:56:02, 34.27s/it]08/15/2023 13:55:51 - INFO - __main__ -   Step: 366, LR: 1.6917635169277415e-05, Loss: 0.5850778818130493
 18%|█▊        | 367/2040 [3:36:19<15:51:21, 34.12s/it]08/15/2023 13:56:25 - INFO - __main__ -   Step: 367, LR: 1.6907529055078324e-05, Loss: 0.6212903261184692
 18%|█▊        | 368/2040 [3:36:53<15:50:32, 34.11s/it]08/15/2023 13:56:59 - INFO - __main__ -   Step: 368, LR: 1.6897422940879233e-05, Loss: 0.5576275587081909
 18%|█▊        | 369/2040 [3:37:27<15:50:03, 34.11s/it]08/15/2023 13:57:33 - INFO - __main__ -   Step: 369, LR: 1.6887316826680142e-05, Loss: 0.5279347896575928
 18%|█▊        | 370/2040 [3:38:01<15:48:38, 34.08s/it]08/15/2023 13:58:07 - INFO - __main__ -   Step: 370, LR: 1.687721071248105e-05, Loss: 0.5301018953323364
 18%|█▊        | 371/2040 [3:38:35<15:44:53, 33.97s/it]08/15/2023 13:58:41 - INFO - __main__ -   Step: 371, LR: 1.6867104598281963e-05, Loss: 0.6482877731323242
 18%|█▊        | 372/2040 [3:39:08<15:39:47, 33.81s/it]08/15/2023 13:59:14 - INFO - __main__ -   Step: 372, LR: 1.6856998484082873e-05, Loss: 0.5803760290145874
 18%|█▊        | 373/2040 [3:39:42<15:36:53, 33.72s/it]08/15/2023 13:59:48 - INFO - __main__ -   Step: 373, LR: 1.684689236988378e-05, Loss: 0.5706607103347778
 18%|█▊        | 374/2040 [3:40:17<15:49:36, 34.20s/it]08/15/2023 14:00:23 - INFO - __main__ -   Step: 374, LR: 1.683678625568469e-05, Loss: 0.5599356293678284
 18%|█▊        | 375/2040 [3:40:53<16:01:00, 34.63s/it]08/15/2023 14:00:59 - INFO - __main__ -   Step: 375, LR: 1.68266801414856e-05, Loss: 0.5668931603431702
 18%|█▊        | 376/2040 [3:41:28<16:03:42, 34.75s/it]08/15/2023 14:01:34 - INFO - __main__ -   Step: 376, LR: 1.681657402728651e-05, Loss: 0.634903073310852
 18%|█▊        | 377/2040 [3:42:03<16:07:08, 34.89s/it]08/15/2023 14:02:09 - INFO - __main__ -   Step: 377, LR: 1.6806467913087418e-05, Loss: 0.5726429224014282
 19%|█▊        | 378/2040 [3:42:38<16:03:16, 34.78s/it]08/15/2023 14:02:44 - INFO - __main__ -   Step: 378, LR: 1.679636179888833e-05, Loss: 0.5591520071029663
 19%|█▊        | 379/2040 [3:43:13<16:06:39, 34.92s/it]08/15/2023 14:03:19 - INFO - __main__ -   Step: 379, LR: 1.678625568468924e-05, Loss: 0.647514820098877
 19%|█▊        | 380/2040 [3:43:48<16:07:35, 34.97s/it]08/15/2023 14:03:54 - INFO - __main__ -   Step: 380, LR: 1.677614957049015e-05, Loss: 0.5796408653259277
 19%|█▊        | 381/2040 [3:44:23<16:10:22, 35.09s/it]08/15/2023 14:04:29 - INFO - __main__ -   Step: 381, LR: 1.6766043456291058e-05, Loss: 0.5952195525169373
 19%|█▊        | 382/2040 [3:44:59<16:13:13, 35.22s/it]08/15/2023 14:05:05 - INFO - __main__ -   Step: 382, LR: 1.6755937342091967e-05, Loss: 0.5933938026428223
 19%|█▉        | 383/2040 [3:45:34<16:08:04, 35.05s/it]08/15/2023 14:05:39 - INFO - __main__ -   Step: 383, LR: 1.6745831227892876e-05, Loss: 0.5773343443870544
 19%|█▉        | 384/2040 [3:46:09<16:08:35, 35.09s/it]08/15/2023 14:06:15 - INFO - __main__ -   Step: 384, LR: 1.6735725113693785e-05, Loss: 0.5640773773193359
 19%|█▉        | 385/2040 [3:46:43<16:01:58, 34.88s/it]08/15/2023 14:06:49 - INFO - __main__ -   Step: 385, LR: 1.6725618999494694e-05, Loss: 0.5832672119140625
 19%|█▉        | 386/2040 [3:47:19<16:06:21, 35.06s/it]08/15/2023 14:07:24 - INFO - __main__ -   Step: 386, LR: 1.6715512885295606e-05, Loss: 0.584465742111206
 19%|█▉        | 387/2040 [3:47:53<16:00:59, 34.88s/it]08/15/2023 14:07:59 - INFO - __main__ -   Step: 387, LR: 1.6705406771096515e-05, Loss: 0.6383274793624878
 19%|█▉        | 388/2040 [3:48:27<15:53:39, 34.64s/it]08/15/2023 14:08:33 - INFO - __main__ -   Step: 388, LR: 1.6695300656897425e-05, Loss: 0.5477608442306519
 19%|█▉        | 389/2040 [3:49:00<15:40:45, 34.19s/it]08/15/2023 14:09:06 - INFO - __main__ -   Step: 389, LR: 1.6685194542698334e-05, Loss: 0.5275585651397705
 19%|█▉        | 390/2040 [3:49:34<15:34:14, 33.97s/it]08/15/2023 14:09:40 - INFO - __main__ -   Step: 390, LR: 1.6675088428499243e-05, Loss: 0.5729579925537109
 19%|█▉        | 391/2040 [3:50:08<15:35:45, 34.05s/it]08/15/2023 14:10:14 - INFO - __main__ -   Step: 391, LR: 1.6664982314300152e-05, Loss: 0.545901358127594
 19%|█▉        | 392/2040 [3:50:42<15:37:05, 34.12s/it]08/15/2023 14:10:48 - INFO - __main__ -   Step: 392, LR: 1.665487620010106e-05, Loss: 0.5952852368354797
 19%|█▉        | 393/2040 [3:51:16<15:37:03, 34.14s/it]08/15/2023 14:11:22 - INFO - __main__ -   Step: 393, LR: 1.664477008590197e-05, Loss: 0.5791866779327393
 19%|█▉        | 394/2040 [3:51:51<15:38:21, 34.21s/it]08/15/2023 14:11:57 - INFO - __main__ -   Step: 394, LR: 1.6634663971702882e-05, Loss: 0.5793929100036621
 19%|█▉        | 395/2040 [3:52:25<15:41:03, 34.32s/it]08/15/2023 14:12:31 - INFO - __main__ -   Step: 395, LR: 1.662455785750379e-05, Loss: 0.591191291809082
 19%|█▉        | 396/2040 [3:53:00<15:39:35, 34.29s/it]08/15/2023 14:13:05 - INFO - __main__ -   Step: 396, LR: 1.66144517433047e-05, Loss: 0.5089987516403198
 19%|█▉        | 397/2040 [3:53:34<15:42:43, 34.43s/it]08/15/2023 14:13:40 - INFO - __main__ -   Step: 397, LR: 1.660434562910561e-05, Loss: 0.5524550080299377
 20%|█▉        | 398/2040 [3:54:09<15:44:54, 34.53s/it]08/15/2023 14:14:15 - INFO - __main__ -   Step: 398, LR: 1.6594239514906522e-05, Loss: 0.5087645649909973
 20%|█▉        | 399/2040 [3:54:43<15:42:38, 34.47s/it]08/15/2023 14:14:49 - INFO - __main__ -   Step: 399, LR: 1.658413340070743e-05, Loss: 0.6587823629379272
 20%|█▉        | 400/2040 [3:55:18<15:39:55, 34.39s/it]08/15/2023 14:15:23 - INFO - __main__ -   Step: 400, LR: 1.6574027286508337e-05, Loss: 0.5888147354125977
08/15/2023 14:15:23 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400
08/15/2023 14:15:23 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 14:15:24,004] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 14:15:24,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 14:15:24,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 14:15:24,011] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 14:15:24,011] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 14:15:24,011] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 14:15:24,011] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 14:15:24,021] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 14:15:24,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 14:15:24,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 14:15:24,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 14:15:24,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 14:15:24,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 14:15:24,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 14:15:24,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 14:15:45,568] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 14:15:45,568] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 14:15:46,795] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 14:15:46,795] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 14:15:47,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 14:15:47,207] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 14:15:47,636] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 14:15:47,636] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 14:15:47,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 14:15:47,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 14:15:47,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 14:15:47,708] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 14:15:47 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/pytorch_model
08/15/2023 14:15:47 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/scheduler.bin
08/15/2023 14:15:47 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_400/random_states_0.pkl
 20%|█▉        | 401/2040 [3:56:17<19:00:32, 41.75s/it]08/15/2023 14:16:22 - INFO - __main__ -   Step: 401, LR: 1.6563921172309246e-05, Loss: 0.5747770071029663
 20%|█▉        | 402/2040 [3:56:53<18:13:05, 40.04s/it]08/15/2023 14:16:58 - INFO - __main__ -   Step: 402, LR: 1.655381505811016e-05, Loss: 0.5854394435882568
 20%|█▉        | 403/2040 [3:57:29<17:42:29, 38.94s/it]08/15/2023 14:17:35 - INFO - __main__ -   Step: 403, LR: 1.6543708943911067e-05, Loss: 0.6120344996452332
 20%|█▉        | 404/2040 [3:58:05<17:14:09, 37.93s/it]08/15/2023 14:18:10 - INFO - __main__ -   Step: 404, LR: 1.6533602829711976e-05, Loss: 0.5975271463394165
 20%|█▉        | 405/2040 [3:58:41<16:58:30, 37.38s/it]08/15/2023 14:18:47 - INFO - __main__ -   Step: 405, LR: 1.6523496715512886e-05, Loss: 0.484959214925766
 20%|█▉        | 406/2040 [3:59:16<16:44:05, 36.87s/it]08/15/2023 14:19:22 - INFO - __main__ -   Step: 406, LR: 1.6513390601313798e-05, Loss: 0.5335726737976074
 20%|█▉        | 407/2040 [3:59:51<16:29:35, 36.36s/it]08/15/2023 14:19:57 - INFO - __main__ -   Step: 407, LR: 1.6503284487114707e-05, Loss: 0.6054583787918091
 20%|██        | 408/2040 [4:00:27<16:23:13, 36.15s/it]08/15/2023 14:20:33 - INFO - __main__ -   Step: 408, LR: 1.6493178372915616e-05, Loss: 0.588241696357727
 20%|██        | 409/2040 [4:01:02<16:14:45, 35.86s/it]08/15/2023 14:21:08 - INFO - __main__ -   Step: 409, LR: 1.6483072258716525e-05, Loss: 0.5430954098701477
 20%|██        | 410/2040 [4:01:37<16:06:20, 35.57s/it]08/15/2023 14:21:43 - INFO - __main__ -   Step: 410, LR: 1.6472966144517434e-05, Loss: 0.5345520377159119
 20%|██        | 411/2040 [4:02:12<16:00:48, 35.39s/it]08/15/2023 14:22:18 - INFO - __main__ -   Step: 411, LR: 1.6462860030318343e-05, Loss: 0.6116889715194702
 20%|██        | 412/2040 [4:02:47<15:59:02, 35.35s/it]08/15/2023 14:22:53 - INFO - __main__ -   Step: 412, LR: 1.6452753916119252e-05, Loss: 0.544991672039032
 20%|██        | 413/2040 [4:03:23<16:00:54, 35.44s/it]08/15/2023 14:23:29 - INFO - __main__ -   Step: 413, LR: 1.6442647801920165e-05, Loss: 0.5828098654747009
 20%|██        | 414/2040 [4:03:58<15:59:48, 35.42s/it]08/15/2023 14:24:04 - INFO - __main__ -   Step: 414, LR: 1.6432541687721074e-05, Loss: 0.5417321920394897
 20%|██        | 415/2040 [4:04:34<15:58:52, 35.40s/it]08/15/2023 14:24:40 - INFO - __main__ -   Step: 415, LR: 1.6422435573521983e-05, Loss: 0.6260622143745422
 20%|██        | 416/2040 [4:05:08<15:51:31, 35.15s/it]08/15/2023 14:25:14 - INFO - __main__ -   Step: 416, LR: 1.6412329459322892e-05, Loss: 0.5596112012863159
 20%|██        | 417/2040 [4:05:44<15:54:27, 35.29s/it]08/15/2023 14:25:50 - INFO - __main__ -   Step: 417, LR: 1.64022233451238e-05, Loss: 0.5119022130966187
 20%|██        | 418/2040 [4:06:20<15:56:08, 35.37s/it]08/15/2023 14:26:25 - INFO - __main__ -   Step: 418, LR: 1.639211723092471e-05, Loss: 0.5695703029632568
 21%|██        | 419/2040 [4:06:55<15:57:34, 35.44s/it]08/15/2023 14:27:01 - INFO - __main__ -   Step: 419, LR: 1.638201111672562e-05, Loss: 0.5410138368606567
 21%|██        | 420/2040 [4:07:30<15:50:15, 35.19s/it]08/15/2023 14:27:36 - INFO - __main__ -   Step: 420, LR: 1.637190500252653e-05, Loss: 0.5373940467834473
 21%|██        | 421/2040 [4:08:04<15:43:06, 34.95s/it]08/15/2023 14:28:10 - INFO - __main__ -   Step: 421, LR: 1.636179888832744e-05, Loss: 0.6117550730705261
 21%|██        | 422/2040 [4:08:39<15:39:33, 34.84s/it]08/15/2023 14:28:45 - INFO - __main__ -   Step: 422, LR: 1.635169277412835e-05, Loss: 0.5469870567321777
 21%|██        | 423/2040 [4:09:14<15:39:39, 34.87s/it]08/15/2023 14:29:20 - INFO - __main__ -   Step: 423, LR: 1.634158665992926e-05, Loss: 0.5706220865249634
 21%|██        | 424/2040 [4:09:49<15:41:18, 34.95s/it]08/15/2023 14:29:55 - INFO - __main__ -   Step: 424, LR: 1.6331480545730168e-05, Loss: 0.6563407182693481
 21%|██        | 425/2040 [4:10:23<15:38:19, 34.86s/it]08/15/2023 14:30:29 - INFO - __main__ -   Step: 425, LR: 1.6321374431531077e-05, Loss: 0.6142438054084778
 21%|██        | 426/2040 [4:10:58<15:38:44, 34.90s/it]08/15/2023 14:31:04 - INFO - __main__ -   Step: 426, LR: 1.6311268317331986e-05, Loss: 0.5586050152778625
 21%|██        | 427/2040 [4:11:33<15:35:52, 34.81s/it]08/15/2023 14:31:39 - INFO - __main__ -   Step: 427, LR: 1.6301162203132895e-05, Loss: 0.5402318239212036
 21%|██        | 428/2040 [4:12:08<15:35:02, 34.80s/it]08/15/2023 14:32:14 - INFO - __main__ -   Step: 428, LR: 1.6291056088933804e-05, Loss: 0.5620476007461548
 21%|██        | 429/2040 [4:12:42<15:32:09, 34.72s/it]08/15/2023 14:32:48 - INFO - __main__ -   Step: 429, LR: 1.6280949974734717e-05, Loss: 0.5841642618179321
 21%|██        | 430/2040 [4:13:17<15:31:53, 34.73s/it]08/15/2023 14:33:23 - INFO - __main__ -   Step: 430, LR: 1.6270843860535626e-05, Loss: 0.6591111421585083
 21%|██        | 431/2040 [4:13:52<15:30:30, 34.70s/it]08/15/2023 14:33:58 - INFO - __main__ -   Step: 431, LR: 1.6260737746336535e-05, Loss: 0.5291603207588196
 21%|██        | 432/2040 [4:14:27<15:31:42, 34.77s/it]08/15/2023 14:34:33 - INFO - __main__ -   Step: 432, LR: 1.6250631632137444e-05, Loss: 0.677171528339386
 21%|██        | 433/2040 [4:15:01<15:31:12, 34.77s/it]08/15/2023 14:35:07 - INFO - __main__ -   Step: 433, LR: 1.6240525517938353e-05, Loss: 0.6292315721511841
 21%|██▏       | 434/2040 [4:15:36<15:28:34, 34.69s/it]08/15/2023 14:35:42 - INFO - __main__ -   Step: 434, LR: 1.6230419403739262e-05, Loss: 0.5769056081771851
 21%|██▏       | 435/2040 [4:16:11<15:32:46, 34.87s/it]08/15/2023 14:36:17 - INFO - __main__ -   Step: 435, LR: 1.622031328954017e-05, Loss: 0.5282170176506042
 21%|██▏       | 436/2040 [4:16:47<15:35:42, 35.00s/it]08/15/2023 14:36:52 - INFO - __main__ -   Step: 436, LR: 1.621020717534108e-05, Loss: 0.6387443542480469
 21%|██▏       | 437/2040 [4:17:21<15:31:16, 34.86s/it]08/15/2023 14:37:27 - INFO - __main__ -   Step: 437, LR: 1.6200101061141993e-05, Loss: 0.56805419921875
 21%|██▏       | 438/2040 [4:17:56<15:32:53, 34.94s/it]08/15/2023 14:38:02 - INFO - __main__ -   Step: 438, LR: 1.6189994946942902e-05, Loss: 0.5048717260360718
 22%|██▏       | 439/2040 [4:18:31<15:28:13, 34.79s/it]08/15/2023 14:38:37 - INFO - __main__ -   Step: 439, LR: 1.617988883274381e-05, Loss: 0.5939545035362244
 22%|██▏       | 440/2040 [4:19:06<15:31:40, 34.94s/it]08/15/2023 14:39:12 - INFO - __main__ -   Step: 440, LR: 1.616978271854472e-05, Loss: 0.5160384774208069
 22%|██▏       | 441/2040 [4:19:41<15:34:05, 35.05s/it]08/15/2023 14:39:47 - INFO - __main__ -   Step: 441, LR: 1.6159676604345633e-05, Loss: 0.5611200332641602
 22%|██▏       | 442/2040 [4:20:16<15:29:27, 34.90s/it]08/15/2023 14:40:22 - INFO - __main__ -   Step: 442, LR: 1.614957049014654e-05, Loss: 0.5750030279159546
 22%|██▏       | 443/2040 [4:20:51<15:34:52, 35.12s/it]08/15/2023 14:40:57 - INFO - __main__ -   Step: 443, LR: 1.6139464375947447e-05, Loss: 0.5006318092346191
 22%|██▏       | 444/2040 [4:21:27<15:34:49, 35.14s/it]08/15/2023 14:41:33 - INFO - __main__ -   Step: 444, LR: 1.612935826174836e-05, Loss: 0.5425740480422974
 22%|██▏       | 445/2040 [4:22:02<15:35:12, 35.18s/it]08/15/2023 14:42:08 - INFO - __main__ -   Step: 445, LR: 1.611925214754927e-05, Loss: 0.5536888837814331
 22%|██▏       | 446/2040 [4:22:37<15:37:08, 35.28s/it]08/15/2023 14:42:43 - INFO - __main__ -   Step: 446, LR: 1.6109146033350178e-05, Loss: 0.6332609057426453
 22%|██▏       | 447/2040 [4:23:13<15:35:28, 35.23s/it]08/15/2023 14:43:18 - INFO - __main__ -   Step: 447, LR: 1.6099039919151087e-05, Loss: 0.5634000301361084
 22%|██▏       | 448/2040 [4:23:47<15:31:45, 35.12s/it]08/15/2023 14:43:53 - INFO - __main__ -   Step: 448, LR: 1.6088933804952e-05, Loss: 0.551609218120575
 22%|██▏       | 449/2040 [4:24:24<15:43:13, 35.57s/it]08/15/2023 14:44:30 - INFO - __main__ -   Step: 449, LR: 1.607882769075291e-05, Loss: 0.5073807835578918
 22%|██▏       | 450/2040 [4:25:01<15:50:30, 35.87s/it]08/15/2023 14:45:06 - INFO - __main__ -   Step: 450, LR: 1.6068721576553818e-05, Loss: 0.5644429326057434
 22%|██▏       | 451/2040 [4:25:36<15:48:08, 35.80s/it]08/15/2023 14:45:42 - INFO - __main__ -   Step: 451, LR: 1.6058615462354727e-05, Loss: 0.45860031247138977
 22%|██▏       | 452/2040 [4:26:13<15:52:11, 35.98s/it]08/15/2023 14:46:18 - INFO - __main__ -   Step: 452, LR: 1.6048509348155636e-05, Loss: 0.5477492809295654
 22%|██▏       | 453/2040 [4:26:50<16:00:25, 36.31s/it]08/15/2023 14:46:56 - INFO - __main__ -   Step: 453, LR: 1.6038403233956545e-05, Loss: 0.5838181376457214
 22%|██▏       | 454/2040 [4:27:25<15:52:54, 36.05s/it]08/15/2023 14:47:31 - INFO - __main__ -   Step: 454, LR: 1.6028297119757454e-05, Loss: 0.5466721057891846
 22%|██▏       | 455/2040 [4:28:01<15:51:39, 36.02s/it]08/15/2023 14:48:07 - INFO - __main__ -   Step: 455, LR: 1.6018191005558363e-05, Loss: 0.5526158809661865
 22%|██▏       | 456/2040 [4:28:38<15:54:22, 36.15s/it]08/15/2023 14:48:43 - INFO - __main__ -   Step: 456, LR: 1.6008084891359275e-05, Loss: 0.5889004468917847
 22%|██▏       | 457/2040 [4:29:13<15:45:00, 35.82s/it]08/15/2023 14:49:18 - INFO - __main__ -   Step: 457, LR: 1.5997978777160184e-05, Loss: 0.5559959411621094
 22%|██▏       | 458/2040 [4:29:48<15:41:14, 35.70s/it]08/15/2023 14:49:54 - INFO - __main__ -   Step: 458, LR: 1.5987872662961094e-05, Loss: 0.573540210723877
 22%|██▎       | 459/2040 [4:30:23<15:37:39, 35.59s/it]08/15/2023 14:50:29 - INFO - __main__ -   Step: 459, LR: 1.5977766548762003e-05, Loss: 0.6316040754318237
 23%|██▎       | 460/2040 [4:30:58<15:32:50, 35.42s/it]08/15/2023 14:51:04 - INFO - __main__ -   Step: 460, LR: 1.5967660434562912e-05, Loss: 0.639363169670105
 23%|██▎       | 461/2040 [4:31:33<15:29:54, 35.34s/it]08/15/2023 14:51:39 - INFO - __main__ -   Step: 461, LR: 1.595755432036382e-05, Loss: 0.5824123024940491
 23%|██▎       | 462/2040 [4:32:09<15:28:14, 35.29s/it]08/15/2023 14:52:15 - INFO - __main__ -   Step: 462, LR: 1.594744820616473e-05, Loss: 0.5621823072433472
 23%|██▎       | 463/2040 [4:32:45<15:34:39, 35.56s/it]08/15/2023 14:52:51 - INFO - __main__ -   Step: 463, LR: 1.593734209196564e-05, Loss: 0.5332000255584717
 23%|██▎       | 464/2040 [4:33:21<15:35:44, 35.62s/it]08/15/2023 14:53:27 - INFO - __main__ -   Step: 464, LR: 1.592723597776655e-05, Loss: 0.5456600189208984
 23%|██▎       | 465/2040 [4:33:56<15:36:12, 35.66s/it]08/15/2023 14:54:02 - INFO - __main__ -   Step: 465, LR: 1.591712986356746e-05, Loss: 0.5350996851921082
 23%|██▎       | 466/2040 [4:34:32<15:34:39, 35.63s/it]08/15/2023 14:54:38 - INFO - __main__ -   Step: 466, LR: 1.590702374936837e-05, Loss: 0.5312696695327759
 23%|██▎       | 467/2040 [4:35:07<15:28:39, 35.42s/it]08/15/2023 14:55:13 - INFO - __main__ -   Step: 467, LR: 1.589691763516928e-05, Loss: 0.6005473136901855
 23%|██▎       | 468/2040 [4:35:42<15:26:27, 35.36s/it]08/15/2023 14:55:48 - INFO - __main__ -   Step: 468, LR: 1.5886811520970188e-05, Loss: 0.5820620059967041
 23%|██▎       | 469/2040 [4:36:17<15:20:46, 35.17s/it]08/15/2023 14:56:23 - INFO - __main__ -   Step: 469, LR: 1.5876705406771097e-05, Loss: 0.5521777868270874
 23%|██▎       | 470/2040 [4:36:53<15:27:36, 35.45s/it]08/15/2023 14:56:59 - INFO - __main__ -   Step: 470, LR: 1.5866599292572006e-05, Loss: 0.5481482744216919
 23%|██▎       | 471/2040 [4:37:29<15:34:54, 35.75s/it]08/15/2023 14:57:35 - INFO - __main__ -   Step: 471, LR: 1.5856493178372915e-05, Loss: 0.5973832011222839
 23%|██▎       | 472/2040 [4:38:04<15:27:27, 35.49s/it]08/15/2023 14:58:10 - INFO - __main__ -   Step: 472, LR: 1.5846387064173827e-05, Loss: 0.5316892862319946
 23%|██▎       | 473/2040 [4:38:40<15:31:44, 35.68s/it]08/15/2023 14:58:46 - INFO - __main__ -   Step: 473, LR: 1.5836280949974736e-05, Loss: 0.5548370480537415
 23%|██▎       | 474/2040 [4:39:16<15:33:24, 35.76s/it]08/15/2023 14:59:22 - INFO - __main__ -   Step: 474, LR: 1.5826174835775646e-05, Loss: 0.5480053424835205
 23%|██▎       | 475/2040 [4:39:51<15:23:37, 35.41s/it]08/15/2023 14:59:57 - INFO - __main__ -   Step: 475, LR: 1.5816068721576555e-05, Loss: 0.5247617959976196
 23%|██▎       | 476/2040 [4:40:26<15:23:59, 35.45s/it]08/15/2023 15:00:32 - INFO - __main__ -   Step: 476, LR: 1.5805962607377464e-05, Loss: 0.5776214599609375
 23%|██▎       | 477/2040 [4:41:02<15:22:00, 35.39s/it]08/15/2023 15:01:08 - INFO - __main__ -   Step: 477, LR: 1.5795856493178373e-05, Loss: 0.5530985593795776
 23%|██▎       | 478/2040 [4:41:36<15:13:22, 35.08s/it]08/15/2023 15:01:42 - INFO - __main__ -   Step: 478, LR: 1.5785750378979282e-05, Loss: 0.5835052132606506
 23%|██▎       | 479/2040 [4:42:12<15:15:57, 35.21s/it]08/15/2023 15:02:17 - INFO - __main__ -   Step: 479, LR: 1.5775644264780194e-05, Loss: 0.5750753879547119
 24%|██▎       | 480/2040 [4:42:47<15:18:30, 35.33s/it]08/15/2023 15:02:53 - INFO - __main__ -   Step: 480, LR: 1.5765538150581103e-05, Loss: 0.5720279216766357
 24%|██▎       | 481/2040 [4:43:23<15:21:22, 35.46s/it]08/15/2023 15:03:29 - INFO - __main__ -   Step: 481, LR: 1.5755432036382012e-05, Loss: 0.5999783277511597
 24%|██▎       | 482/2040 [4:43:59<15:24:27, 35.60s/it]08/15/2023 15:04:05 - INFO - __main__ -   Step: 482, LR: 1.574532592218292e-05, Loss: 0.5308972597122192
 24%|██▎       | 483/2040 [4:44:36<15:34:26, 36.01s/it]08/15/2023 15:04:42 - INFO - __main__ -   Step: 483, LR: 1.5735219807983834e-05, Loss: 0.5825198292732239
 24%|██▎       | 484/2040 [4:45:11<15:28:53, 35.82s/it]08/15/2023 15:05:17 - INFO - __main__ -   Step: 484, LR: 1.5725113693784743e-05, Loss: 0.6142066717147827
 24%|██▍       | 485/2040 [4:45:47<15:27:21, 35.78s/it]08/15/2023 15:05:53 - INFO - __main__ -   Step: 485, LR: 1.5715007579585652e-05, Loss: 0.5664283037185669
 24%|██▍       | 486/2040 [4:46:22<15:23:20, 35.65s/it]08/15/2023 15:06:28 - INFO - __main__ -   Step: 486, LR: 1.5704901465386558e-05, Loss: 0.5294274687767029
 24%|██▍       | 487/2040 [4:46:57<15:16:24, 35.41s/it]08/15/2023 15:07:03 - INFO - __main__ -   Step: 487, LR: 1.569479535118747e-05, Loss: 0.5885319709777832
 24%|██▍       | 488/2040 [4:47:32<15:10:11, 35.19s/it]08/15/2023 15:07:38 - INFO - __main__ -   Step: 488, LR: 1.568468923698838e-05, Loss: 0.5699255466461182
 24%|██▍       | 489/2040 [4:48:06<15:03:56, 34.97s/it]08/15/2023 15:08:12 - INFO - __main__ -   Step: 489, LR: 1.567458312278929e-05, Loss: 0.579928994178772
 24%|██▍       | 490/2040 [4:48:41<14:59:19, 34.81s/it]08/15/2023 15:08:47 - INFO - __main__ -   Step: 490, LR: 1.5664477008590197e-05, Loss: 0.5827715396881104
 24%|██▍       | 491/2040 [4:49:15<14:54:18, 34.64s/it]08/15/2023 15:09:21 - INFO - __main__ -   Step: 491, LR: 1.565437089439111e-05, Loss: 0.6497195959091187
 24%|██▍       | 492/2040 [4:49:49<14:51:46, 34.56s/it]08/15/2023 15:09:55 - INFO - __main__ -   Step: 492, LR: 1.564426478019202e-05, Loss: 0.6000178456306458
 24%|██▍       | 493/2040 [4:50:24<14:51:32, 34.58s/it]08/15/2023 15:10:30 - INFO - __main__ -   Step: 493, LR: 1.5634158665992928e-05, Loss: 0.5752168893814087
 24%|██▍       | 494/2040 [4:50:58<14:51:00, 34.58s/it]08/15/2023 15:11:04 - INFO - __main__ -   Step: 494, LR: 1.5624052551793837e-05, Loss: 0.612675666809082
 24%|██▍       | 495/2040 [4:51:32<14:43:00, 34.29s/it]08/15/2023 15:11:38 - INFO - __main__ -   Step: 495, LR: 1.5613946437594746e-05, Loss: 0.5962933897972107
 24%|██▍       | 496/2040 [4:52:06<14:38:46, 34.15s/it]08/15/2023 15:12:12 - INFO - __main__ -   Step: 496, LR: 1.5603840323395655e-05, Loss: 0.5708485841751099
 24%|██▍       | 497/2040 [4:52:40<14:38:19, 34.15s/it]08/15/2023 15:12:46 - INFO - __main__ -   Step: 497, LR: 1.5593734209196564e-05, Loss: 0.5834641456604004
 24%|██▍       | 498/2040 [4:53:15<14:41:38, 34.31s/it]08/15/2023 15:13:21 - INFO - __main__ -   Step: 498, LR: 1.5583628094997473e-05, Loss: 0.5114552974700928
 24%|██▍       | 499/2040 [4:53:50<14:44:55, 34.45s/it]08/15/2023 15:13:55 - INFO - __main__ -   Step: 499, LR: 1.5573521980798386e-05, Loss: 0.5727986693382263
 25%|██▍       | 500/2040 [4:54:24<14:41:26, 34.34s/it]08/15/2023 15:14:30 - INFO - __main__ -   Step: 500, LR: 1.5563415866599295e-05, Loss: 0.5190321207046509
08/15/2023 15:14:30 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500
08/15/2023 15:14:30 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 15:14:30,047] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 15:14:30,052] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 15:14:30,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 15:14:30,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 15:14:30,053] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 15:14:30,053] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 15:14:30,053] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 15:14:30,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 15:14:30,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 15:14:30,064] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 15:14:30,065] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 15:14:30,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 15:14:30,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 15:14:30,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 15:14:30,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 15:14:51,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 15:14:51,786] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 15:14:51,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 15:14:51,797] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 15:14:52,287] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 15:14:52,287] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 15:14:52,371] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 15:14:52,371] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 15:14:52,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 15:14:52,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 15:14:52,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 15:14:52,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 15:14:52 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/pytorch_model
08/15/2023 15:14:52 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/scheduler.bin
08/15/2023 15:14:52 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_500/random_states_0.pkl
 25%|██▍       | 501/2040 [4:55:21<17:38:11, 41.26s/it]08/15/2023 15:15:27 - INFO - __main__ -   Step: 501, LR: 1.5553309752400204e-05, Loss: 0.5720885992050171
 25%|██▍       | 502/2040 [4:55:56<16:47:54, 39.32s/it]08/15/2023 15:16:02 - INFO - __main__ -   Step: 502, LR: 1.5543203638201113e-05, Loss: 0.5724034309387207
 25%|██▍       | 503/2040 [4:56:30<16:10:51, 37.90s/it]08/15/2023 15:16:36 - INFO - __main__ -   Step: 503, LR: 1.5533097524002022e-05, Loss: 0.5315994620323181
 25%|██▍       | 504/2040 [4:57:05<15:45:59, 36.95s/it]08/15/2023 15:17:11 - INFO - __main__ -   Step: 504, LR: 1.552299140980293e-05, Loss: 0.5592926740646362
 25%|██▍       | 505/2040 [4:57:40<15:28:39, 36.30s/it]08/15/2023 15:17:46 - INFO - __main__ -   Step: 505, LR: 1.551288529560384e-05, Loss: 0.5351657271385193
 25%|██▍       | 506/2040 [4:58:15<15:19:20, 35.96s/it]08/15/2023 15:18:21 - INFO - __main__ -   Step: 506, LR: 1.550277918140475e-05, Loss: 0.550891101360321
 25%|██▍       | 507/2040 [4:58:50<15:13:06, 35.74s/it]08/15/2023 15:18:56 - INFO - __main__ -   Step: 507, LR: 1.5492673067205662e-05, Loss: 0.5996532440185547
 25%|██▍       | 508/2040 [4:59:25<15:05:01, 35.45s/it]08/15/2023 15:19:31 - INFO - __main__ -   Step: 508, LR: 1.548256695300657e-05, Loss: 0.5372950434684753
 25%|██▍       | 509/2040 [5:00:00<14:59:20, 35.25s/it]08/15/2023 15:20:06 - INFO - __main__ -   Step: 509, LR: 1.547246083880748e-05, Loss: 0.5553786158561707
 25%|██▌       | 510/2040 [5:00:34<14:50:39, 34.93s/it]08/15/2023 15:20:40 - INFO - __main__ -   Step: 510, LR: 1.546235472460839e-05, Loss: 0.6014891266822815
 25%|██▌       | 511/2040 [5:01:09<14:49:46, 34.92s/it]08/15/2023 15:21:15 - INFO - __main__ -   Step: 511, LR: 1.5452248610409298e-05, Loss: 0.5641176700592041
 25%|██▌       | 512/2040 [5:01:44<14:50:17, 34.96s/it]08/15/2023 15:21:50 - INFO - __main__ -   Step: 512, LR: 1.5442142496210207e-05, Loss: 0.5889724493026733
 25%|██▌       | 513/2040 [5:02:19<14:52:22, 35.06s/it]08/15/2023 15:22:25 - INFO - __main__ -   Step: 513, LR: 1.5432036382011116e-05, Loss: 0.5609022974967957
 25%|██▌       | 514/2040 [5:02:55<14:56:12, 35.24s/it]08/15/2023 15:23:01 - INFO - __main__ -   Step: 514, LR: 1.542193026781203e-05, Loss: 0.5854275226593018
 25%|██▌       | 515/2040 [5:03:31<15:01:12, 35.46s/it]08/15/2023 15:23:37 - INFO - __main__ -   Step: 515, LR: 1.5411824153612938e-05, Loss: 0.587907075881958
 25%|██▌       | 516/2040 [5:04:07<15:02:37, 35.54s/it]08/15/2023 15:24:13 - INFO - __main__ -   Step: 516, LR: 1.5401718039413847e-05, Loss: 0.5569554567337036
 25%|██▌       | 517/2040 [5:04:42<14:59:42, 35.44s/it]08/15/2023 15:24:48 - INFO - __main__ -   Step: 517, LR: 1.5391611925214756e-05, Loss: 0.5839006900787354
 25%|██▌       | 518/2040 [5:05:17<14:54:19, 35.26s/it]08/15/2023 15:25:23 - INFO - __main__ -   Step: 518, LR: 1.5381505811015665e-05, Loss: 0.6218442320823669
 25%|██▌       | 519/2040 [5:05:52<14:50:54, 35.14s/it]08/15/2023 15:25:57 - INFO - __main__ -   Step: 519, LR: 1.5371399696816574e-05, Loss: 0.496019572019577
 25%|██▌       | 520/2040 [5:06:27<14:51:54, 35.21s/it]08/15/2023 15:26:33 - INFO - __main__ -   Step: 520, LR: 1.5361293582617483e-05, Loss: 0.5631839036941528
 26%|██▌       | 521/2040 [5:07:02<14:52:02, 35.24s/it]08/15/2023 15:27:08 - INFO - __main__ -   Step: 521, LR: 1.5351187468418392e-05, Loss: 0.6285125017166138
 26%|██▌       | 522/2040 [5:07:38<14:54:52, 35.37s/it]08/15/2023 15:27:44 - INFO - __main__ -   Step: 522, LR: 1.5341081354219305e-05, Loss: 0.4968072772026062
 26%|██▌       | 523/2040 [5:08:13<14:50:13, 35.21s/it]08/15/2023 15:28:19 - INFO - __main__ -   Step: 523, LR: 1.5330975240020214e-05, Loss: 0.5717310905456543
 26%|██▌       | 524/2040 [5:08:47<14:44:11, 34.99s/it]08/15/2023 15:28:53 - INFO - __main__ -   Step: 524, LR: 1.5320869125821123e-05, Loss: 0.6361721754074097
 26%|██▌       | 525/2040 [5:09:22<14:42:02, 34.93s/it]08/15/2023 15:29:28 - INFO - __main__ -   Step: 525, LR: 1.5310763011622032e-05, Loss: 0.5577487945556641
 26%|██▌       | 526/2040 [5:09:57<14:43:18, 35.01s/it]08/15/2023 15:30:03 - INFO - __main__ -   Step: 526, LR: 1.5300656897422944e-05, Loss: 0.5480262041091919
 26%|██▌       | 527/2040 [5:10:32<14:40:10, 34.90s/it]08/15/2023 15:30:38 - INFO - __main__ -   Step: 527, LR: 1.5290550783223854e-05, Loss: 0.5492432713508606
 26%|██▌       | 528/2040 [5:11:06<14:33:37, 34.67s/it]08/15/2023 15:31:12 - INFO - __main__ -   Step: 528, LR: 1.5280444669024763e-05, Loss: 0.5805191397666931
 26%|██▌       | 529/2040 [5:11:40<14:25:31, 34.37s/it]08/15/2023 15:31:46 - INFO - __main__ -   Step: 529, LR: 1.5270338554825668e-05, Loss: 0.5918841361999512
 26%|██▌       | 530/2040 [5:12:14<14:25:50, 34.40s/it]08/15/2023 15:32:20 - INFO - __main__ -   Step: 530, LR: 1.526023244062658e-05, Loss: 0.5503607988357544
 26%|██▌       | 531/2040 [5:12:49<14:25:19, 34.41s/it]08/15/2023 15:32:54 - INFO - __main__ -   Step: 531, LR: 1.525012632642749e-05, Loss: 0.5857973098754883
 26%|██▌       | 532/2040 [5:13:23<14:26:51, 34.49s/it]08/15/2023 15:33:29 - INFO - __main__ -   Step: 532, LR: 1.5240020212228399e-05, Loss: 0.5294011831283569
 26%|██▌       | 533/2040 [5:13:58<14:30:01, 34.64s/it]08/15/2023 15:34:04 - INFO - __main__ -   Step: 533, LR: 1.5229914098029308e-05, Loss: 0.5593300461769104
 26%|██▌       | 534/2040 [5:14:33<14:32:16, 34.75s/it]08/15/2023 15:34:39 - INFO - __main__ -   Step: 534, LR: 1.5219807983830219e-05, Loss: 0.5733238458633423
 26%|██▌       | 535/2040 [5:15:09<14:37:03, 34.97s/it]08/15/2023 15:35:15 - INFO - __main__ -   Step: 535, LR: 1.5209701869631128e-05, Loss: 0.5780049562454224
 26%|██▋       | 536/2040 [5:15:43<14:31:46, 34.78s/it]08/15/2023 15:35:49 - INFO - __main__ -   Step: 536, LR: 1.5199595755432037e-05, Loss: 0.5249603986740112
 26%|██▋       | 537/2040 [5:16:18<14:30:20, 34.74s/it]08/15/2023 15:36:24 - INFO - __main__ -   Step: 537, LR: 1.5189489641232946e-05, Loss: 0.6476563215255737
 26%|██▋       | 538/2040 [5:16:52<14:25:24, 34.57s/it]08/15/2023 15:36:58 - INFO - __main__ -   Step: 538, LR: 1.5179383527033857e-05, Loss: 0.5845474004745483
 26%|██▋       | 539/2040 [5:17:27<14:26:14, 34.63s/it]08/15/2023 15:37:33 - INFO - __main__ -   Step: 539, LR: 1.5169277412834766e-05, Loss: 0.6193113327026367
 26%|██▋       | 540/2040 [5:18:01<14:26:49, 34.67s/it]08/15/2023 15:38:07 - INFO - __main__ -   Step: 540, LR: 1.5159171298635675e-05, Loss: 0.4956754446029663
 27%|██▋       | 541/2040 [5:18:35<14:19:53, 34.42s/it]08/15/2023 15:38:41 - INFO - __main__ -   Step: 541, LR: 1.5149065184436584e-05, Loss: 0.5702023506164551
 27%|██▋       | 542/2040 [5:19:10<14:18:15, 34.38s/it]08/15/2023 15:39:15 - INFO - __main__ -   Step: 542, LR: 1.5138959070237496e-05, Loss: 0.6172248125076294
 27%|██▋       | 543/2040 [5:19:43<14:11:20, 34.12s/it]08/15/2023 15:39:49 - INFO - __main__ -   Step: 543, LR: 1.5128852956038404e-05, Loss: 0.5471271872520447
 27%|██▋       | 544/2040 [5:20:18<14:13:29, 34.23s/it]08/15/2023 15:40:23 - INFO - __main__ -   Step: 544, LR: 1.5118746841839313e-05, Loss: 0.6778686046600342
 27%|██▋       | 545/2040 [5:20:52<14:17:01, 34.40s/it]08/15/2023 15:40:58 - INFO - __main__ -   Step: 545, LR: 1.5108640727640222e-05, Loss: 0.5814588069915771
 27%|██▋       | 546/2040 [5:21:26<14:13:34, 34.28s/it]08/15/2023 15:41:32 - INFO - __main__ -   Step: 546, LR: 1.5098534613441134e-05, Loss: 0.6432875990867615
 27%|██▋       | 547/2040 [5:22:00<14:10:40, 34.19s/it]08/15/2023 15:42:06 - INFO - __main__ -   Step: 547, LR: 1.5088428499242043e-05, Loss: 0.7065544128417969
 27%|██▋       | 548/2040 [5:22:34<14:05:37, 34.01s/it]08/15/2023 15:42:40 - INFO - __main__ -   Step: 548, LR: 1.507832238504295e-05, Loss: 0.6365979909896851
 27%|██▋       | 549/2040 [5:23:08<14:06:21, 34.06s/it]08/15/2023 15:43:14 - INFO - __main__ -   Step: 549, LR: 1.5068216270843863e-05, Loss: 0.5840287804603577
 27%|██▋       | 550/2040 [5:23:43<14:10:16, 34.24s/it]08/15/2023 15:43:49 - INFO - __main__ -   Step: 550, LR: 1.5058110156644772e-05, Loss: 0.6411451101303101
 27%|██▋       | 551/2040 [5:24:17<14:07:48, 34.16s/it]08/15/2023 15:44:23 - INFO - __main__ -   Step: 551, LR: 1.5048004042445681e-05, Loss: 0.5564429759979248
 27%|██▋       | 552/2040 [5:24:52<14:12:24, 34.37s/it]08/15/2023 15:44:57 - INFO - __main__ -   Step: 552, LR: 1.503789792824659e-05, Loss: 0.5762843489646912
 27%|██▋       | 553/2040 [5:25:25<14:06:20, 34.15s/it]08/15/2023 15:45:31 - INFO - __main__ -   Step: 553, LR: 1.5027791814047501e-05, Loss: 0.5595563054084778
 27%|██▋       | 554/2040 [5:25:59<14:05:37, 34.14s/it]08/15/2023 15:46:05 - INFO - __main__ -   Step: 554, LR: 1.501768569984841e-05, Loss: 0.5576642751693726
 27%|██▋       | 555/2040 [5:26:34<14:09:15, 34.31s/it]08/15/2023 15:46:40 - INFO - __main__ -   Step: 555, LR: 1.500757958564932e-05, Loss: 0.5492479801177979
 27%|██▋       | 556/2040 [5:27:08<14:08:51, 34.32s/it]08/15/2023 15:47:14 - INFO - __main__ -   Step: 556, LR: 1.4997473471450228e-05, Loss: 0.6089904308319092
 27%|██▋       | 557/2040 [5:27:42<14:05:12, 34.20s/it]08/15/2023 15:47:48 - INFO - __main__ -   Step: 557, LR: 1.498736735725114e-05, Loss: 0.5496546626091003
 27%|██▋       | 558/2040 [5:28:17<14:06:56, 34.29s/it]08/15/2023 15:48:23 - INFO - __main__ -   Step: 558, LR: 1.4977261243052048e-05, Loss: 0.5019806027412415
 27%|██▋       | 559/2040 [5:28:51<14:06:33, 34.30s/it]08/15/2023 15:48:57 - INFO - __main__ -   Step: 559, LR: 1.4967155128852957e-05, Loss: 0.5780036449432373
 27%|██▋       | 560/2040 [5:29:25<14:05:09, 34.26s/it]08/15/2023 15:49:31 - INFO - __main__ -   Step: 560, LR: 1.4957049014653866e-05, Loss: 0.5709652900695801
 28%|██▊       | 561/2040 [5:30:00<14:05:54, 34.32s/it]08/15/2023 15:50:06 - INFO - __main__ -   Step: 561, LR: 1.4946942900454777e-05, Loss: 0.5425947308540344
 28%|██▊       | 562/2040 [5:30:34<14:02:19, 34.19s/it]08/15/2023 15:50:40 - INFO - __main__ -   Step: 562, LR: 1.4936836786255686e-05, Loss: 0.7046881914138794
 28%|██▊       | 563/2040 [5:31:07<13:59:20, 34.10s/it]08/15/2023 15:51:13 - INFO - __main__ -   Step: 563, LR: 1.4926730672056595e-05, Loss: 0.5846883058547974
 28%|██▊       | 564/2040 [5:31:42<14:03:13, 34.28s/it]08/15/2023 15:51:48 - INFO - __main__ -   Step: 564, LR: 1.4916624557857504e-05, Loss: 0.5412299633026123
 28%|██▊       | 565/2040 [5:32:16<14:02:31, 34.27s/it]08/15/2023 15:52:22 - INFO - __main__ -   Step: 565, LR: 1.4906518443658415e-05, Loss: 0.5714660882949829
 28%|██▊       | 566/2040 [5:32:52<14:11:58, 34.68s/it]08/15/2023 15:52:58 - INFO - __main__ -   Step: 566, LR: 1.4896412329459324e-05, Loss: 0.5590702295303345
 28%|██▊       | 567/2040 [5:33:27<14:14:47, 34.82s/it]08/15/2023 15:53:33 - INFO - __main__ -   Step: 567, LR: 1.4886306215260233e-05, Loss: 0.5365495085716248
 28%|██▊       | 568/2040 [5:34:03<14:18:00, 34.97s/it]08/15/2023 15:54:08 - INFO - __main__ -   Step: 568, LR: 1.4876200101061142e-05, Loss: 0.551084578037262
 28%|██▊       | 569/2040 [5:34:37<14:16:36, 34.94s/it]08/15/2023 15:54:43 - INFO - __main__ -   Step: 569, LR: 1.4866093986862053e-05, Loss: 0.5991687774658203
 28%|██▊       | 570/2040 [5:35:12<14:10:45, 34.72s/it]08/15/2023 15:55:18 - INFO - __main__ -   Step: 570, LR: 1.4855987872662962e-05, Loss: 0.4813077449798584
 28%|██▊       | 571/2040 [5:35:47<14:13:00, 34.84s/it]08/15/2023 15:55:53 - INFO - __main__ -   Step: 571, LR: 1.4845881758463871e-05, Loss: 0.5244899988174438
 28%|██▊       | 572/2040 [5:36:21<14:09:23, 34.72s/it]08/15/2023 15:56:27 - INFO - __main__ -   Step: 572, LR: 1.483577564426478e-05, Loss: 0.5734400153160095
 28%|██▊       | 573/2040 [5:36:56<14:13:13, 34.90s/it]08/15/2023 15:57:02 - INFO - __main__ -   Step: 573, LR: 1.4825669530065691e-05, Loss: 0.5401175022125244
 28%|██▊       | 574/2040 [5:37:31<14:12:56, 34.91s/it]08/15/2023 15:57:37 - INFO - __main__ -   Step: 574, LR: 1.48155634158666e-05, Loss: 0.5250613689422607
 28%|██▊       | 575/2040 [5:38:07<14:18:41, 35.17s/it]08/15/2023 15:58:13 - INFO - __main__ -   Step: 575, LR: 1.480545730166751e-05, Loss: 0.5696643590927124
 28%|██▊       | 576/2040 [5:38:42<14:12:21, 34.93s/it]08/15/2023 15:58:47 - INFO - __main__ -   Step: 576, LR: 1.4795351187468418e-05, Loss: 0.581913948059082
 28%|██▊       | 577/2040 [5:39:16<14:11:28, 34.92s/it]08/15/2023 15:59:22 - INFO - __main__ -   Step: 577, LR: 1.478524507326933e-05, Loss: 0.5288028717041016
 28%|██▊       | 578/2040 [5:39:51<14:05:48, 34.71s/it]08/15/2023 15:59:57 - INFO - __main__ -   Step: 578, LR: 1.4775138959070238e-05, Loss: 0.6309819221496582
 28%|██▊       | 579/2040 [5:40:25<14:02:59, 34.62s/it]08/15/2023 16:00:31 - INFO - __main__ -   Step: 579, LR: 1.4765032844871147e-05, Loss: 0.6104350090026855
 28%|██▊       | 580/2040 [5:41:00<14:04:40, 34.71s/it]08/15/2023 16:01:06 - INFO - __main__ -   Step: 580, LR: 1.4754926730672056e-05, Loss: 0.654975175857544
 28%|██▊       | 581/2040 [5:41:35<14:07:59, 34.87s/it]08/15/2023 16:01:41 - INFO - __main__ -   Step: 581, LR: 1.4744820616472967e-05, Loss: 0.5680539608001709
 29%|██▊       | 582/2040 [5:42:11<14:13:50, 35.14s/it]08/15/2023 16:02:17 - INFO - __main__ -   Step: 582, LR: 1.4734714502273876e-05, Loss: 0.5356298089027405
 29%|██▊       | 583/2040 [5:42:46<14:14:39, 35.19s/it]08/15/2023 16:02:52 - INFO - __main__ -   Step: 583, LR: 1.4724608388074785e-05, Loss: 0.5481126308441162
 29%|██▊       | 584/2040 [5:43:21<14:12:41, 35.14s/it]08/15/2023 16:03:27 - INFO - __main__ -   Step: 584, LR: 1.4714502273875698e-05, Loss: 0.6241140365600586
 29%|██▊       | 585/2040 [5:43:57<14:14:00, 35.22s/it]08/15/2023 16:04:03 - INFO - __main__ -   Step: 585, LR: 1.4704396159676607e-05, Loss: 0.5577415227890015
 29%|██▊       | 586/2040 [5:44:31<14:06:32, 34.93s/it]08/15/2023 16:04:37 - INFO - __main__ -   Step: 586, LR: 1.4694290045477514e-05, Loss: 0.6386764645576477
 29%|██▉       | 587/2040 [5:45:06<14:06:25, 34.95s/it]08/15/2023 16:05:12 - INFO - __main__ -   Step: 587, LR: 1.4684183931278423e-05, Loss: 0.6033205986022949
 29%|██▉       | 588/2040 [5:45:42<14:10:41, 35.15s/it]08/15/2023 16:05:48 - INFO - __main__ -   Step: 588, LR: 1.4674077817079336e-05, Loss: 0.5136470794677734
 29%|██▉       | 589/2040 [5:46:17<14:13:41, 35.30s/it]08/15/2023 16:06:23 - INFO - __main__ -   Step: 589, LR: 1.4663971702880245e-05, Loss: 0.5173306465148926
 29%|██▉       | 590/2040 [5:46:52<14:10:47, 35.21s/it]08/15/2023 16:06:58 - INFO - __main__ -   Step: 590, LR: 1.4653865588681154e-05, Loss: 0.520946204662323
 29%|██▉       | 591/2040 [5:47:27<14:03:35, 34.93s/it]08/15/2023 16:07:32 - INFO - __main__ -   Step: 591, LR: 1.4643759474482061e-05, Loss: 0.5861063003540039
 29%|██▉       | 592/2040 [5:48:00<13:52:57, 34.51s/it]08/15/2023 16:08:06 - INFO - __main__ -   Step: 592, LR: 1.4633653360282974e-05, Loss: 0.5545748472213745
 29%|██▉       | 593/2040 [5:48:35<13:51:47, 34.49s/it]08/15/2023 16:08:40 - INFO - __main__ -   Step: 593, LR: 1.4623547246083883e-05, Loss: 0.5857806205749512
 29%|██▉       | 594/2040 [5:49:09<13:50:09, 34.45s/it]08/15/2023 16:09:15 - INFO - __main__ -   Step: 594, LR: 1.4613441131884792e-05, Loss: 0.5886110067367554
 29%|██▉       | 595/2040 [5:49:43<13:50:04, 34.47s/it]08/15/2023 16:09:49 - INFO - __main__ -   Step: 595, LR: 1.4603335017685701e-05, Loss: 0.5632070899009705
 29%|██▉       | 596/2040 [5:50:19<13:54:41, 34.68s/it]08/15/2023 16:10:25 - INFO - __main__ -   Step: 596, LR: 1.4593228903486612e-05, Loss: 0.5381871461868286
 29%|██▉       | 597/2040 [5:50:54<13:56:54, 34.80s/it]08/15/2023 16:11:00 - INFO - __main__ -   Step: 597, LR: 1.458312278928752e-05, Loss: 0.5729640126228333
 29%|██▉       | 598/2040 [5:51:29<13:58:34, 34.89s/it]08/15/2023 16:11:35 - INFO - __main__ -   Step: 598, LR: 1.457301667508843e-05, Loss: 0.5072907209396362
 29%|██▉       | 599/2040 [5:52:04<13:58:53, 34.93s/it]08/15/2023 16:12:10 - INFO - __main__ -   Step: 599, LR: 1.4562910560889339e-05, Loss: 0.6150487661361694
 29%|██▉       | 600/2040 [5:52:40<14:05:48, 35.24s/it]08/15/2023 16:12:46 - INFO - __main__ -   Step: 600, LR: 1.455280444669025e-05, Loss: 0.5060146450996399
08/15/2023 16:12:46 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600
08/15/2023 16:12:46 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 16:12:46,191] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 16:12:46,197] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 16:12:46,197] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 16:12:46,197] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 16:12:46,198] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 16:12:46,198] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 16:12:46,198] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 16:12:46,209] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 16:12:46,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 16:12:46,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 16:12:46,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 16:12:46,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 16:12:46,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 16:12:46,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 16:12:46,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 16:13:08,292] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 16:13:08,292] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 16:13:09,836] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 16:13:09,836] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 16:13:10,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 16:13:10,130] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 16:13:10,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 16:13:10,131] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 16:13:10,135] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 16:13:10,135] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 16:13:10,135] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 16:13:10,135] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 16:13:10 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/pytorch_model
08/15/2023 16:13:10 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/scheduler.bin
08/15/2023 16:13:10 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_600/random_states_0.pkl
 29%|██▉       | 601/2040 [5:53:40<17:02:13, 42.62s/it]08/15/2023 16:13:46 - INFO - __main__ -   Step: 601, LR: 1.4542698332491159e-05, Loss: 0.5418688654899597
 30%|██▉       | 602/2040 [5:54:15<16:08:22, 40.41s/it]08/15/2023 16:14:21 - INFO - __main__ -   Step: 602, LR: 1.4532592218292068e-05, Loss: 0.541488766670227
 30%|██▉       | 603/2040 [5:54:50<15:28:56, 38.79s/it]08/15/2023 16:14:56 - INFO - __main__ -   Step: 603, LR: 1.4522486104092977e-05, Loss: 0.5184012055397034
 30%|██▉       | 604/2040 [5:55:27<15:17:08, 38.32s/it]08/15/2023 16:15:33 - INFO - __main__ -   Step: 604, LR: 1.4512379989893888e-05, Loss: 0.5263879299163818
 30%|██▉       | 605/2040 [5:56:03<14:58:06, 37.55s/it]08/15/2023 16:16:09 - INFO - __main__ -   Step: 605, LR: 1.4502273875694797e-05, Loss: 0.5961161255836487
 30%|██▉       | 606/2040 [5:56:37<14:36:09, 36.66s/it]08/15/2023 16:16:43 - INFO - __main__ -   Step: 606, LR: 1.4492167761495706e-05, Loss: 0.5581603050231934
 30%|██▉       | 607/2040 [5:57:13<14:29:58, 36.43s/it]08/15/2023 16:17:19 - INFO - __main__ -   Step: 607, LR: 1.4482061647296615e-05, Loss: 0.6116456985473633
 30%|██▉       | 608/2040 [5:57:48<14:20:27, 36.05s/it]08/15/2023 16:17:54 - INFO - __main__ -   Step: 608, LR: 1.4471955533097526e-05, Loss: 0.5857287049293518
 30%|██▉       | 609/2040 [5:58:24<14:12:36, 35.75s/it]08/15/2023 16:18:29 - INFO - __main__ -   Step: 609, LR: 1.4461849418898435e-05, Loss: 0.44906729459762573
 30%|██▉       | 610/2040 [5:58:58<14:06:04, 35.50s/it]08/15/2023 16:19:04 - INFO - __main__ -   Step: 610, LR: 1.4451743304699344e-05, Loss: 0.6341807842254639
 30%|██▉       | 611/2040 [5:59:33<14:00:06, 35.27s/it]08/15/2023 16:19:39 - INFO - __main__ -   Step: 611, LR: 1.4441637190500253e-05, Loss: 0.6024315357208252
 30%|███       | 612/2040 [6:00:08<13:54:11, 35.05s/it]08/15/2023 16:20:14 - INFO - __main__ -   Step: 612, LR: 1.4431531076301164e-05, Loss: 0.5910395383834839
 30%|███       | 613/2040 [6:00:43<13:52:22, 35.00s/it]08/15/2023 16:20:49 - INFO - __main__ -   Step: 613, LR: 1.4421424962102073e-05, Loss: 0.5835432410240173
 30%|███       | 614/2040 [6:01:18<13:55:17, 35.15s/it]08/15/2023 16:21:24 - INFO - __main__ -   Step: 614, LR: 1.4411318847902982e-05, Loss: 0.6408523321151733
 30%|███       | 615/2040 [6:01:53<13:50:27, 34.97s/it]08/15/2023 16:21:59 - INFO - __main__ -   Step: 615, LR: 1.4401212733703891e-05, Loss: 0.5485128164291382
 30%|███       | 616/2040 [6:02:28<13:54:21, 35.16s/it]08/15/2023 16:22:34 - INFO - __main__ -   Step: 616, LR: 1.4391106619504802e-05, Loss: 0.5742409229278564
 30%|███       | 617/2040 [6:03:04<13:54:38, 35.19s/it]08/15/2023 16:23:09 - INFO - __main__ -   Step: 617, LR: 1.438100050530571e-05, Loss: 0.5549962520599365
 30%|███       | 618/2040 [6:03:39<13:52:56, 35.14s/it]08/15/2023 16:23:44 - INFO - __main__ -   Step: 618, LR: 1.437089439110662e-05, Loss: 0.5625555515289307
 30%|███       | 619/2040 [6:04:14<13:51:25, 35.11s/it]08/15/2023 16:24:19 - INFO - __main__ -   Step: 619, LR: 1.436078827690753e-05, Loss: 0.5657082796096802
 30%|███       | 620/2040 [6:04:49<13:54:26, 35.26s/it]08/15/2023 16:24:55 - INFO - __main__ -   Step: 620, LR: 1.435068216270844e-05, Loss: 0.6467475891113281
 30%|███       | 621/2040 [6:05:24<13:53:03, 35.22s/it]08/15/2023 16:25:30 - INFO - __main__ -   Step: 621, LR: 1.4340576048509349e-05, Loss: 0.5575451254844666
 30%|███       | 622/2040 [6:06:00<13:53:47, 35.28s/it]08/15/2023 16:26:06 - INFO - __main__ -   Step: 622, LR: 1.4330469934310258e-05, Loss: 0.5585198402404785
 31%|███       | 623/2040 [6:06:35<13:51:53, 35.23s/it]08/15/2023 16:26:41 - INFO - __main__ -   Step: 623, LR: 1.4320363820111169e-05, Loss: 0.6710951328277588
 31%|███       | 624/2040 [6:07:10<13:48:21, 35.10s/it]08/15/2023 16:27:16 - INFO - __main__ -   Step: 624, LR: 1.4310257705912078e-05, Loss: 0.6168549656867981
 31%|███       | 625/2040 [6:07:46<13:53:47, 35.36s/it]08/15/2023 16:27:51 - INFO - __main__ -   Step: 625, LR: 1.4300151591712987e-05, Loss: 0.5889537334442139
 31%|███       | 626/2040 [6:08:22<13:57:55, 35.56s/it]08/15/2023 16:28:28 - INFO - __main__ -   Step: 626, LR: 1.4290045477513896e-05, Loss: 0.6026269197463989
 31%|███       | 627/2040 [6:08:56<13:52:12, 35.34s/it]08/15/2023 16:29:02 - INFO - __main__ -   Step: 627, LR: 1.4279939363314808e-05, Loss: 0.5332657098770142
 31%|███       | 628/2040 [6:09:31<13:49:29, 35.25s/it]08/15/2023 16:29:37 - INFO - __main__ -   Step: 628, LR: 1.4269833249115717e-05, Loss: 0.5343954563140869
 31%|███       | 629/2040 [6:10:07<13:49:51, 35.29s/it]08/15/2023 16:30:13 - INFO - __main__ -   Step: 629, LR: 1.4259727134916625e-05, Loss: 0.5935466289520264
 31%|███       | 630/2040 [6:10:42<13:51:23, 35.38s/it]08/15/2023 16:30:48 - INFO - __main__ -   Step: 630, LR: 1.4249621020717534e-05, Loss: 0.5719588398933411
 31%|███       | 631/2040 [6:11:18<13:55:08, 35.56s/it]08/15/2023 16:31:24 - INFO - __main__ -   Step: 631, LR: 1.4239514906518446e-05, Loss: 0.5783466696739197
 31%|███       | 632/2040 [6:11:54<13:54:47, 35.57s/it]08/15/2023 16:32:00 - INFO - __main__ -   Step: 632, LR: 1.4229408792319355e-05, Loss: 0.6327741742134094
 31%|███       | 633/2040 [6:12:29<13:51:36, 35.46s/it]08/15/2023 16:32:35 - INFO - __main__ -   Step: 633, LR: 1.4219302678120264e-05, Loss: 0.6753895282745361
 31%|███       | 634/2040 [6:13:05<13:56:05, 35.68s/it]08/15/2023 16:33:11 - INFO - __main__ -   Step: 634, LR: 1.4209196563921172e-05, Loss: 0.5155893564224243
 31%|███       | 635/2040 [6:13:41<13:52:00, 35.53s/it]08/15/2023 16:33:47 - INFO - __main__ -   Step: 635, LR: 1.4199090449722084e-05, Loss: 0.5620998740196228
 31%|███       | 636/2040 [6:14:15<13:46:40, 35.33s/it]08/15/2023 16:34:21 - INFO - __main__ -   Step: 636, LR: 1.4188984335522993e-05, Loss: 0.6224206686019897
 31%|███       | 637/2040 [6:14:51<13:46:11, 35.33s/it]08/15/2023 16:34:57 - INFO - __main__ -   Step: 637, LR: 1.4178878221323902e-05, Loss: 0.5860567092895508
 31%|███▏      | 638/2040 [6:15:26<13:45:35, 35.33s/it]08/15/2023 16:35:32 - INFO - __main__ -   Step: 638, LR: 1.4168772107124811e-05, Loss: 0.6274600028991699
 31%|███▏      | 639/2040 [6:16:01<13:41:49, 35.20s/it]08/15/2023 16:36:07 - INFO - __main__ -   Step: 639, LR: 1.4158665992925722e-05, Loss: 0.6100912094116211
 31%|███▏      | 640/2040 [6:16:36<13:42:28, 35.25s/it]08/15/2023 16:36:42 - INFO - __main__ -   Step: 640, LR: 1.4148559878726631e-05, Loss: 0.552779495716095
 31%|███▏      | 641/2040 [6:17:11<13:39:54, 35.16s/it]08/15/2023 16:37:17 - INFO - __main__ -   Step: 641, LR: 1.413845376452754e-05, Loss: 0.5549690127372742
 31%|███▏      | 642/2040 [6:17:47<13:39:18, 35.16s/it]08/15/2023 16:37:52 - INFO - __main__ -   Step: 642, LR: 1.412834765032845e-05, Loss: 0.6639353036880493
 32%|███▏      | 643/2040 [6:18:23<13:49:57, 35.65s/it]08/15/2023 16:38:29 - INFO - __main__ -   Step: 643, LR: 1.411824153612936e-05, Loss: 0.5278398394584656
 32%|███▏      | 644/2040 [6:18:59<13:52:26, 35.78s/it]08/15/2023 16:39:05 - INFO - __main__ -   Step: 644, LR: 1.410813542193027e-05, Loss: 0.4814978837966919
 32%|███▏      | 645/2040 [6:19:35<13:52:51, 35.82s/it]08/15/2023 16:39:41 - INFO - __main__ -   Step: 645, LR: 1.4098029307731178e-05, Loss: 0.5617546439170837
 32%|███▏      | 646/2040 [6:20:11<13:50:09, 35.73s/it]08/15/2023 16:40:17 - INFO - __main__ -   Step: 646, LR: 1.4087923193532087e-05, Loss: 0.5740317106246948
 32%|███▏      | 647/2040 [6:20:48<13:59:20, 36.15s/it]08/15/2023 16:40:54 - INFO - __main__ -   Step: 647, LR: 1.4077817079332998e-05, Loss: 0.571373462677002
 32%|███▏      | 648/2040 [6:21:26<14:11:54, 36.72s/it]08/15/2023 16:41:32 - INFO - __main__ -   Step: 648, LR: 1.4067710965133907e-05, Loss: 0.5319644808769226
 32%|███▏      | 649/2040 [6:22:04<14:17:59, 37.01s/it]08/15/2023 16:42:10 - INFO - __main__ -   Step: 649, LR: 1.4057604850934816e-05, Loss: 0.5155894756317139
 32%|███▏      | 650/2040 [6:22:40<14:15:19, 36.92s/it]08/15/2023 16:42:46 - INFO - __main__ -   Step: 650, LR: 1.4047498736735725e-05, Loss: 0.6013389825820923
 32%|███▏      | 651/2040 [6:23:18<14:16:42, 37.01s/it]08/15/2023 16:43:24 - INFO - __main__ -   Step: 651, LR: 1.4037392622536636e-05, Loss: 0.6247934103012085
 32%|███▏      | 652/2040 [6:23:55<14:19:04, 37.14s/it]08/15/2023 16:44:01 - INFO - __main__ -   Step: 652, LR: 1.4027286508337545e-05, Loss: 0.5639871954917908
 32%|███▏      | 653/2040 [6:24:32<14:16:08, 37.04s/it]08/15/2023 16:44:38 - INFO - __main__ -   Step: 653, LR: 1.4017180394138454e-05, Loss: 0.575710654258728
 32%|███▏      | 654/2040 [6:25:09<14:15:03, 37.02s/it]08/15/2023 16:45:15 - INFO - __main__ -   Step: 654, LR: 1.4007074279939365e-05, Loss: 0.5454472899436951
 32%|███▏      | 655/2040 [6:25:45<14:11:45, 36.90s/it]08/15/2023 16:45:51 - INFO - __main__ -   Step: 655, LR: 1.3996968165740274e-05, Loss: 0.507910966873169
 32%|███▏      | 656/2040 [6:26:23<14:18:28, 37.22s/it]08/15/2023 16:46:29 - INFO - __main__ -   Step: 656, LR: 1.3986862051541183e-05, Loss: 0.5772213935852051
 32%|███▏      | 657/2040 [6:27:01<14:22:04, 37.40s/it]08/15/2023 16:47:07 - INFO - __main__ -   Step: 657, LR: 1.3976755937342092e-05, Loss: 0.5306617021560669
 32%|███▏      | 658/2040 [6:27:39<14:26:14, 37.61s/it]08/15/2023 16:47:45 - INFO - __main__ -   Step: 658, LR: 1.3966649823143003e-05, Loss: 0.48093485832214355
 32%|███▏      | 659/2040 [6:28:17<14:28:31, 37.73s/it]08/15/2023 16:48:23 - INFO - __main__ -   Step: 659, LR: 1.3956543708943912e-05, Loss: 0.620598316192627
 32%|███▏      | 660/2040 [6:28:56<14:31:35, 37.90s/it]08/15/2023 16:49:02 - INFO - __main__ -   Step: 660, LR: 1.3946437594744821e-05, Loss: 0.5800681114196777
 32%|███▏      | 661/2040 [6:29:33<14:29:05, 37.81s/it]08/15/2023 16:49:39 - INFO - __main__ -   Step: 661, LR: 1.393633148054573e-05, Loss: 0.5392346382141113
 32%|███▏      | 662/2040 [6:30:11<14:29:30, 37.86s/it]08/15/2023 16:50:17 - INFO - __main__ -   Step: 662, LR: 1.3926225366346641e-05, Loss: 0.5843388438224792
 32%|███▎      | 663/2040 [6:30:47<14:16:52, 37.34s/it]08/15/2023 16:50:53 - INFO - __main__ -   Step: 663, LR: 1.391611925214755e-05, Loss: 0.5299174785614014
 33%|███▎      | 664/2040 [6:31:25<14:18:56, 37.45s/it]08/15/2023 16:51:31 - INFO - __main__ -   Step: 664, LR: 1.390601313794846e-05, Loss: 0.5818283557891846
 33%|███▎      | 665/2040 [6:32:03<14:19:18, 37.50s/it]08/15/2023 16:52:09 - INFO - __main__ -   Step: 665, LR: 1.3895907023749368e-05, Loss: 0.49068930745124817
 33%|███▎      | 666/2040 [6:32:41<14:21:22, 37.61s/it]08/15/2023 16:52:46 - INFO - __main__ -   Step: 666, LR: 1.3885800909550279e-05, Loss: 0.5864098072052002
 33%|███▎      | 667/2040 [6:33:18<14:18:16, 37.51s/it]08/15/2023 16:53:24 - INFO - __main__ -   Step: 667, LR: 1.3875694795351188e-05, Loss: 0.6628007888793945
 33%|███▎      | 668/2040 [6:33:56<14:20:37, 37.64s/it]08/15/2023 16:54:02 - INFO - __main__ -   Step: 668, LR: 1.3865588681152097e-05, Loss: 0.6092514991760254
 33%|███▎      | 669/2040 [6:34:33<14:16:08, 37.47s/it]08/15/2023 16:54:39 - INFO - __main__ -   Step: 669, LR: 1.3855482566953006e-05, Loss: 0.5635561943054199
 33%|███▎      | 670/2040 [6:35:10<14:15:19, 37.46s/it]08/15/2023 16:55:16 - INFO - __main__ -   Step: 670, LR: 1.3845376452753919e-05, Loss: 0.5744696855545044
 33%|███▎      | 671/2040 [6:35:48<14:18:21, 37.62s/it]08/15/2023 16:55:54 - INFO - __main__ -   Step: 671, LR: 1.3835270338554828e-05, Loss: 0.5853143930435181
 33%|███▎      | 672/2040 [6:36:26<14:20:33, 37.74s/it]08/15/2023 16:56:32 - INFO - __main__ -   Step: 672, LR: 1.3825164224355735e-05, Loss: 0.5158262252807617
 33%|███▎      | 673/2040 [6:37:04<14:17:13, 37.63s/it]08/15/2023 16:57:10 - INFO - __main__ -   Step: 673, LR: 1.3815058110156644e-05, Loss: 0.5815576314926147
 33%|███▎      | 674/2040 [6:37:41<14:18:14, 37.70s/it]08/15/2023 16:57:47 - INFO - __main__ -   Step: 674, LR: 1.3804951995957557e-05, Loss: 0.5873525738716125
 33%|███▎      | 675/2040 [6:38:19<14:14:14, 37.55s/it]08/15/2023 16:58:25 - INFO - __main__ -   Step: 675, LR: 1.3794845881758466e-05, Loss: 0.5369406938552856
 33%|███▎      | 676/2040 [6:38:56<14:10:27, 37.41s/it]08/15/2023 16:59:02 - INFO - __main__ -   Step: 676, LR: 1.3784739767559375e-05, Loss: 0.5710746049880981
 33%|███▎      | 677/2040 [6:39:34<14:18:05, 37.77s/it]08/15/2023 16:59:40 - INFO - __main__ -   Step: 677, LR: 1.3774633653360282e-05, Loss: 0.5748323202133179
 33%|███▎      | 678/2040 [6:40:12<14:14:24, 37.64s/it]08/15/2023 17:00:18 - INFO - __main__ -   Step: 678, LR: 1.3764527539161195e-05, Loss: 0.573499321937561
 33%|███▎      | 679/2040 [6:40:50<14:16:00, 37.74s/it]08/15/2023 17:00:56 - INFO - __main__ -   Step: 679, LR: 1.3754421424962104e-05, Loss: 0.5583336353302002
 33%|███▎      | 680/2040 [6:41:06<11:49:18, 31.29s/it]08/15/2023 17:01:12 - INFO - __main__ -   Step: 680, LR: 1.3744315310763013e-05, Loss: 0.425874263048172
 33%|███▎      | 681/2040 [6:41:43<12:28:24, 33.04s/it]08/15/2023 17:01:49 - INFO - __main__ -   Step: 681, LR: 1.3734209196563922e-05, Loss: 0.5320619344711304
 33%|███▎      | 682/2040 [6:42:20<12:57:13, 34.34s/it]08/15/2023 17:02:26 - INFO - __main__ -   Step: 682, LR: 1.3724103082364833e-05, Loss: 0.48580941557884216
 33%|███▎      | 683/2040 [6:42:59<13:22:27, 35.48s/it]08/15/2023 17:03:04 - INFO - __main__ -   Step: 683, LR: 1.3713996968165742e-05, Loss: 0.4435414671897888
 34%|███▎      | 684/2040 [6:43:36<13:35:00, 36.06s/it]08/15/2023 17:03:42 - INFO - __main__ -   Step: 684, LR: 1.3703890853966651e-05, Loss: 0.5035685300827026
 34%|███▎      | 685/2040 [6:44:14<13:49:44, 36.74s/it]08/15/2023 17:04:20 - INFO - __main__ -   Step: 685, LR: 1.369378473976756e-05, Loss: 0.5051857233047485
 34%|███▎      | 686/2040 [6:44:52<13:52:48, 36.90s/it]08/15/2023 17:04:58 - INFO - __main__ -   Step: 686, LR: 1.368367862556847e-05, Loss: 0.5238991975784302
 34%|███▎      | 687/2040 [6:45:29<13:52:30, 36.92s/it]08/15/2023 17:05:34 - INFO - __main__ -   Step: 687, LR: 1.367357251136938e-05, Loss: 0.5450582504272461
 34%|███▎      | 688/2040 [6:46:06<13:57:24, 37.16s/it]08/15/2023 17:06:12 - INFO - __main__ -   Step: 688, LR: 1.3663466397170289e-05, Loss: 0.5124545097351074
 34%|███▍      | 689/2040 [6:46:44<14:00:24, 37.32s/it]08/15/2023 17:06:50 - INFO - __main__ -   Step: 689, LR: 1.36533602829712e-05, Loss: 0.48984232544898987
 34%|███▍      | 690/2040 [6:47:22<14:03:37, 37.49s/it]08/15/2023 17:07:28 - INFO - __main__ -   Step: 690, LR: 1.3643254168772109e-05, Loss: 0.5352358818054199
 34%|███▍      | 691/2040 [6:48:00<14:09:00, 37.76s/it]08/15/2023 17:08:06 - INFO - __main__ -   Step: 691, LR: 1.3633148054573018e-05, Loss: 0.5215493440628052
 34%|███▍      | 692/2040 [6:48:39<14:12:22, 37.94s/it]08/15/2023 17:08:45 - INFO - __main__ -   Step: 692, LR: 1.3623041940373927e-05, Loss: 0.445054292678833
 34%|███▍      | 693/2040 [6:49:17<14:12:11, 37.96s/it]08/15/2023 17:09:23 - INFO - __main__ -   Step: 693, LR: 1.3612935826174838e-05, Loss: 0.555307149887085
 34%|███▍      | 694/2040 [6:49:55<14:11:46, 37.97s/it]08/15/2023 17:10:01 - INFO - __main__ -   Step: 694, LR: 1.3602829711975747e-05, Loss: 0.4738020896911621
 34%|███▍      | 695/2040 [6:50:31<14:03:42, 37.64s/it]08/15/2023 17:10:37 - INFO - __main__ -   Step: 695, LR: 1.3592723597776656e-05, Loss: 0.45417505502700806
 34%|███▍      | 696/2040 [6:51:10<14:09:07, 37.91s/it]08/15/2023 17:11:16 - INFO - __main__ -   Step: 696, LR: 1.3582617483577565e-05, Loss: 0.46222811937332153
 34%|███▍      | 697/2040 [6:51:48<14:08:12, 37.89s/it]08/15/2023 17:11:54 - INFO - __main__ -   Step: 697, LR: 1.3572511369378476e-05, Loss: 0.6201074123382568
 34%|███▍      | 698/2040 [6:52:26<14:08:06, 37.92s/it]08/15/2023 17:12:32 - INFO - __main__ -   Step: 698, LR: 1.3562405255179385e-05, Loss: 0.5168642997741699
 34%|███▍      | 699/2040 [6:53:00<13:39:12, 36.65s/it]08/15/2023 17:13:05 - INFO - __main__ -   Step: 699, LR: 1.3552299140980294e-05, Loss: 0.502869725227356
 34%|███▍      | 700/2040 [6:53:33<13:16:24, 35.66s/it]08/15/2023 17:13:39 - INFO - __main__ -   Step: 700, LR: 1.3542193026781203e-05, Loss: 0.5083611607551575
08/15/2023 17:13:39 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700
08/15/2023 17:13:39 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 17:13:39,330] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 17:13:39,335] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 17:13:39,335] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 17:13:39,335] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 17:13:39,335] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 17:13:39,337] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 17:13:39,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 17:13:39,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 17:13:39,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 17:13:39,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 17:13:39,348] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 17:13:39,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 17:13:39,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 17:13:39,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 17:13:39,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 17:14:00,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 17:14:00,707] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 17:14:02,414] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 17:14:02,415] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 17:14:02,739] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 17:14:02,740] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 17:14:03,446] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 17:14:03,446] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 17:14:03,450] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 17:14:03,450] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 17:14:03,450] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 17:14:03,486] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 17:14:03 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/pytorch_model
08/15/2023 17:14:03 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/scheduler.bin
08/15/2023 17:14:03 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_700/random_states_0.pkl
 34%|███▍      | 701/2040 [6:54:32<15:49:43, 42.56s/it]08/15/2023 17:14:37 - INFO - __main__ -   Step: 701, LR: 1.3532086912582114e-05, Loss: 0.5237330198287964
 34%|███▍      | 702/2040 [6:55:06<14:57:26, 40.24s/it]08/15/2023 17:15:12 - INFO - __main__ -   Step: 702, LR: 1.3521980798383023e-05, Loss: 0.5639927387237549
 34%|███▍      | 703/2040 [6:55:42<14:22:25, 38.70s/it]08/15/2023 17:15:47 - INFO - __main__ -   Step: 703, LR: 1.3511874684183932e-05, Loss: 0.4709543287754059
 35%|███▍      | 704/2040 [6:56:17<13:58:21, 37.65s/it]08/15/2023 17:16:23 - INFO - __main__ -   Step: 704, LR: 1.350176856998484e-05, Loss: 0.48619699478149414
 35%|███▍      | 705/2040 [6:56:52<13:42:54, 36.98s/it]08/15/2023 17:16:58 - INFO - __main__ -   Step: 705, LR: 1.3491662455785752e-05, Loss: 0.554662823677063
 35%|███▍      | 706/2040 [6:57:27<13:27:33, 36.32s/it]08/15/2023 17:17:33 - INFO - __main__ -   Step: 706, LR: 1.348155634158666e-05, Loss: 0.48802927136421204
 35%|███▍      | 707/2040 [6:58:02<13:16:02, 35.83s/it]08/15/2023 17:18:08 - INFO - __main__ -   Step: 707, LR: 1.347145022738757e-05, Loss: 0.4456380307674408
 35%|███▍      | 708/2040 [6:58:36<13:03:14, 35.28s/it]08/15/2023 17:18:42 - INFO - __main__ -   Step: 708, LR: 1.3461344113188479e-05, Loss: 0.5376266837120056
 35%|███▍      | 709/2040 [6:59:10<12:58:35, 35.10s/it]08/15/2023 17:19:16 - INFO - __main__ -   Step: 709, LR: 1.345123799898939e-05, Loss: 0.5073740482330322
 35%|███▍      | 710/2040 [6:59:45<12:57:03, 35.06s/it]08/15/2023 17:19:51 - INFO - __main__ -   Step: 710, LR: 1.3441131884790299e-05, Loss: 0.48938438296318054
 35%|███▍      | 711/2040 [7:00:20<12:57:11, 35.09s/it]08/15/2023 17:20:26 - INFO - __main__ -   Step: 711, LR: 1.3431025770591208e-05, Loss: 0.5258705615997314
 35%|███▍      | 712/2040 [7:00:55<12:53:34, 34.95s/it]08/15/2023 17:21:01 - INFO - __main__ -   Step: 712, LR: 1.3420919656392117e-05, Loss: 0.4946620762348175
 35%|███▍      | 713/2040 [7:01:30<12:53:42, 34.98s/it]08/15/2023 17:21:36 - INFO - __main__ -   Step: 713, LR: 1.341081354219303e-05, Loss: 0.47637325525283813
 35%|███▌      | 714/2040 [7:02:04<12:47:16, 34.72s/it]08/15/2023 17:22:10 - INFO - __main__ -   Step: 714, LR: 1.3400707427993938e-05, Loss: 0.4498431980609894
 35%|███▌      | 715/2040 [7:02:39<12:46:48, 34.72s/it]08/15/2023 17:22:45 - INFO - __main__ -   Step: 715, LR: 1.3390601313794846e-05, Loss: 0.5526651740074158
 35%|███▌      | 716/2040 [7:03:14<12:46:01, 34.71s/it]08/15/2023 17:23:20 - INFO - __main__ -   Step: 716, LR: 1.3380495199595755e-05, Loss: 0.5176639556884766
 35%|███▌      | 717/2040 [7:03:49<12:47:50, 34.82s/it]08/15/2023 17:23:55 - INFO - __main__ -   Step: 717, LR: 1.3370389085396667e-05, Loss: 0.5912119746208191
 35%|███▌      | 718/2040 [7:04:24<12:49:26, 34.92s/it]08/15/2023 17:24:30 - INFO - __main__ -   Step: 718, LR: 1.3360282971197576e-05, Loss: 0.5012043118476868
 35%|███▌      | 719/2040 [7:04:59<12:47:54, 34.88s/it]08/15/2023 17:25:05 - INFO - __main__ -   Step: 719, LR: 1.3350176856998485e-05, Loss: 0.4932232201099396
 35%|███▌      | 720/2040 [7:05:33<12:42:26, 34.66s/it]08/15/2023 17:25:39 - INFO - __main__ -   Step: 720, LR: 1.3340070742799393e-05, Loss: 0.5134633183479309
 35%|███▌      | 721/2040 [7:06:07<12:41:43, 34.65s/it]08/15/2023 17:26:13 - INFO - __main__ -   Step: 721, LR: 1.3329964628600305e-05, Loss: 0.5267598628997803
 35%|███▌      | 722/2040 [7:06:42<12:44:07, 34.79s/it]08/15/2023 17:26:48 - INFO - __main__ -   Step: 722, LR: 1.3319858514401214e-05, Loss: 0.4664061367511749
 35%|███▌      | 723/2040 [7:07:17<12:44:49, 34.84s/it]08/15/2023 17:27:23 - INFO - __main__ -   Step: 723, LR: 1.3309752400202123e-05, Loss: 0.5921671986579895
 35%|███▌      | 724/2040 [7:07:52<12:42:45, 34.78s/it]08/15/2023 17:27:58 - INFO - __main__ -   Step: 724, LR: 1.3299646286003034e-05, Loss: 0.6135839223861694
 36%|███▌      | 725/2040 [7:08:27<12:42:48, 34.81s/it]08/15/2023 17:28:33 - INFO - __main__ -   Step: 725, LR: 1.3289540171803943e-05, Loss: 0.4246629476547241
 36%|███▌      | 726/2040 [7:09:02<12:40:37, 34.73s/it]08/15/2023 17:29:07 - INFO - __main__ -   Step: 726, LR: 1.3279434057604852e-05, Loss: 0.5329508781433105
 36%|███▌      | 727/2040 [7:09:36<12:40:47, 34.77s/it]08/15/2023 17:29:42 - INFO - __main__ -   Step: 727, LR: 1.3269327943405761e-05, Loss: 0.527544379234314
 36%|███▌      | 728/2040 [7:10:11<12:39:00, 34.71s/it]08/15/2023 17:30:17 - INFO - __main__ -   Step: 728, LR: 1.3259221829206672e-05, Loss: 0.5016056299209595
 36%|███▌      | 729/2040 [7:10:46<12:40:29, 34.81s/it]08/15/2023 17:30:52 - INFO - __main__ -   Step: 729, LR: 1.3249115715007581e-05, Loss: 0.4624471962451935
 36%|███▌      | 730/2040 [7:11:21<12:40:55, 34.85s/it]08/15/2023 17:31:27 - INFO - __main__ -   Step: 730, LR: 1.323900960080849e-05, Loss: 0.5336622595787048
 36%|███▌      | 731/2040 [7:11:56<12:39:21, 34.81s/it]08/15/2023 17:32:02 - INFO - __main__ -   Step: 731, LR: 1.32289034866094e-05, Loss: 0.508191704750061
 36%|███▌      | 732/2040 [7:12:30<12:36:57, 34.72s/it]08/15/2023 17:32:36 - INFO - __main__ -   Step: 732, LR: 1.321879737241031e-05, Loss: 0.5947487354278564
 36%|███▌      | 733/2040 [7:13:05<12:35:33, 34.69s/it]08/15/2023 17:33:11 - INFO - __main__ -   Step: 733, LR: 1.320869125821122e-05, Loss: 0.5410256385803223
 36%|███▌      | 734/2040 [7:13:40<12:35:58, 34.73s/it]08/15/2023 17:33:46 - INFO - __main__ -   Step: 734, LR: 1.3198585144012128e-05, Loss: 0.45137399435043335
 36%|███▌      | 735/2040 [7:14:15<12:37:32, 34.83s/it]08/15/2023 17:34:21 - INFO - __main__ -   Step: 735, LR: 1.3188479029813037e-05, Loss: 0.5223405361175537
 36%|███▌      | 736/2040 [7:14:50<12:38:30, 34.90s/it]08/15/2023 17:34:56 - INFO - __main__ -   Step: 736, LR: 1.3178372915613948e-05, Loss: 0.49895748496055603
 36%|███▌      | 737/2040 [7:15:25<12:37:35, 34.89s/it]08/15/2023 17:35:30 - INFO - __main__ -   Step: 737, LR: 1.3168266801414857e-05, Loss: 0.5773943662643433
 36%|███▌      | 738/2040 [7:15:59<12:33:21, 34.72s/it]08/15/2023 17:36:05 - INFO - __main__ -   Step: 738, LR: 1.3158160687215766e-05, Loss: 0.46156948804855347
 36%|███▌      | 739/2040 [7:16:34<12:34:21, 34.79s/it]08/15/2023 17:36:40 - INFO - __main__ -   Step: 739, LR: 1.3148054573016675e-05, Loss: 0.49227893352508545
 36%|███▋      | 740/2040 [7:17:09<12:36:16, 34.91s/it]08/15/2023 17:37:15 - INFO - __main__ -   Step: 740, LR: 1.3137948458817586e-05, Loss: 0.5423526763916016
 36%|███▋      | 741/2040 [7:17:43<12:31:07, 34.69s/it]08/15/2023 17:37:49 - INFO - __main__ -   Step: 741, LR: 1.3127842344618495e-05, Loss: 0.5522319078445435
 36%|███▋      | 742/2040 [7:18:18<12:32:41, 34.79s/it]08/15/2023 17:38:24 - INFO - __main__ -   Step: 742, LR: 1.3117736230419404e-05, Loss: 0.5538172721862793
 36%|███▋      | 743/2040 [7:18:53<12:33:08, 34.84s/it]08/15/2023 17:38:59 - INFO - __main__ -   Step: 743, LR: 1.3107630116220313e-05, Loss: 0.6028568744659424
 36%|███▋      | 744/2040 [7:19:28<12:31:59, 34.81s/it]08/15/2023 17:39:34 - INFO - __main__ -   Step: 744, LR: 1.3097524002021224e-05, Loss: 0.5619877576828003
 37%|███▋      | 745/2040 [7:20:03<12:30:40, 34.78s/it]08/15/2023 17:40:09 - INFO - __main__ -   Step: 745, LR: 1.3087417887822133e-05, Loss: 0.5496042966842651
 37%|███▋      | 746/2040 [7:20:38<12:31:53, 34.86s/it]08/15/2023 17:40:44 - INFO - __main__ -   Step: 746, LR: 1.3077311773623042e-05, Loss: 0.5471529364585876
 37%|███▋      | 747/2040 [7:21:12<12:26:34, 34.64s/it]08/15/2023 17:41:18 - INFO - __main__ -   Step: 747, LR: 1.3067205659423951e-05, Loss: 0.5152789950370789
 37%|███▋      | 748/2040 [7:21:46<12:25:48, 34.64s/it]08/15/2023 17:41:52 - INFO - __main__ -   Step: 748, LR: 1.3057099545224862e-05, Loss: 0.5580326914787292
 37%|███▋      | 749/2040 [7:22:21<12:25:00, 34.62s/it]08/15/2023 17:42:27 - INFO - __main__ -   Step: 749, LR: 1.3046993431025771e-05, Loss: 0.5038846731185913
 37%|███▋      | 750/2040 [7:22:56<12:28:23, 34.81s/it]08/15/2023 17:43:02 - INFO - __main__ -   Step: 750, LR: 1.303688731682668e-05, Loss: 0.6040239334106445
 37%|███▋      | 751/2040 [7:23:31<12:27:07, 34.78s/it]08/15/2023 17:43:37 - INFO - __main__ -   Step: 751, LR: 1.302678120262759e-05, Loss: 0.471430242061615
 37%|███▋      | 752/2040 [7:24:06<12:28:01, 34.85s/it]08/15/2023 17:44:12 - INFO - __main__ -   Step: 752, LR: 1.30166750884285e-05, Loss: 0.5389719009399414
 37%|███▋      | 753/2040 [7:24:40<12:24:28, 34.71s/it]08/15/2023 17:44:46 - INFO - __main__ -   Step: 753, LR: 1.3006568974229409e-05, Loss: 0.46397411823272705
 37%|███▋      | 754/2040 [7:25:15<12:24:10, 34.72s/it]08/15/2023 17:45:21 - INFO - __main__ -   Step: 754, LR: 1.2996462860030318e-05, Loss: 0.5709738731384277
 37%|███▋      | 755/2040 [7:25:50<12:26:45, 34.87s/it]08/15/2023 17:45:56 - INFO - __main__ -   Step: 755, LR: 1.2986356745831227e-05, Loss: 0.4752683937549591
 37%|███▋      | 756/2040 [7:26:26<12:31:11, 35.10s/it]08/15/2023 17:46:32 - INFO - __main__ -   Step: 756, LR: 1.297625063163214e-05, Loss: 0.48327261209487915
 37%|███▋      | 757/2040 [7:27:01<12:32:59, 35.21s/it]08/15/2023 17:47:07 - INFO - __main__ -   Step: 757, LR: 1.2966144517433049e-05, Loss: 0.5377224683761597
 37%|███▋      | 758/2040 [7:27:37<12:31:55, 35.19s/it]08/15/2023 17:47:43 - INFO - __main__ -   Step: 758, LR: 1.2956038403233956e-05, Loss: 0.5189619064331055
 37%|███▋      | 759/2040 [7:28:11<12:26:24, 34.96s/it]08/15/2023 17:48:17 - INFO - __main__ -   Step: 759, LR: 1.2945932289034869e-05, Loss: 0.5303014516830444
 37%|███▋      | 760/2040 [7:28:45<12:21:32, 34.76s/it]08/15/2023 17:48:51 - INFO - __main__ -   Step: 760, LR: 1.2935826174835778e-05, Loss: 0.4979720413684845
 37%|███▋      | 761/2040 [7:29:20<12:18:21, 34.64s/it]08/15/2023 17:49:26 - INFO - __main__ -   Step: 761, LR: 1.2925720060636687e-05, Loss: 0.477956086397171
 37%|███▋      | 762/2040 [7:29:55<12:20:11, 34.75s/it]08/15/2023 17:50:01 - INFO - __main__ -   Step: 762, LR: 1.2915613946437596e-05, Loss: 0.4998111128807068
 37%|███▋      | 763/2040 [7:30:29<12:17:10, 34.64s/it]08/15/2023 17:50:35 - INFO - __main__ -   Step: 763, LR: 1.2905507832238507e-05, Loss: 0.5509504079818726
 37%|███▋      | 764/2040 [7:31:04<12:17:34, 34.68s/it]08/15/2023 17:51:10 - INFO - __main__ -   Step: 764, LR: 1.2895401718039416e-05, Loss: 0.546929121017456
 38%|███▊      | 765/2040 [7:31:38<12:16:05, 34.64s/it]08/15/2023 17:51:44 - INFO - __main__ -   Step: 765, LR: 1.2885295603840325e-05, Loss: 0.5678408145904541
 38%|███▊      | 766/2040 [7:32:13<12:14:47, 34.61s/it]08/15/2023 17:52:19 - INFO - __main__ -   Step: 766, LR: 1.2875189489641234e-05, Loss: 0.5880343317985535
 38%|███▊      | 767/2040 [7:32:47<12:10:16, 34.42s/it]08/15/2023 17:52:53 - INFO - __main__ -   Step: 767, LR: 1.2865083375442145e-05, Loss: 0.5719729065895081
 38%|███▊      | 768/2040 [7:33:22<12:12:42, 34.56s/it]08/15/2023 17:53:28 - INFO - __main__ -   Step: 768, LR: 1.2854977261243054e-05, Loss: 0.47593411803245544
 38%|███▊      | 769/2040 [7:33:56<12:11:04, 34.51s/it]08/15/2023 17:54:02 - INFO - __main__ -   Step: 769, LR: 1.2844871147043963e-05, Loss: 0.5209710597991943
 38%|███▊      | 770/2040 [7:34:31<12:12:59, 34.63s/it]08/15/2023 17:54:37 - INFO - __main__ -   Step: 770, LR: 1.2834765032844872e-05, Loss: 0.5478636026382446
 38%|███▊      | 771/2040 [7:35:05<12:07:19, 34.39s/it]08/15/2023 17:55:11 - INFO - __main__ -   Step: 771, LR: 1.2824658918645783e-05, Loss: 0.5388495922088623
 38%|███▊      | 772/2040 [7:35:39<12:07:09, 34.41s/it]08/15/2023 17:55:45 - INFO - __main__ -   Step: 772, LR: 1.2814552804446692e-05, Loss: 0.5343431830406189
 38%|███▊      | 773/2040 [7:36:14<12:08:06, 34.48s/it]08/15/2023 17:56:20 - INFO - __main__ -   Step: 773, LR: 1.28044466902476e-05, Loss: 0.49627619981765747
 38%|███▊      | 774/2040 [7:36:48<12:06:52, 34.45s/it]08/15/2023 17:56:54 - INFO - __main__ -   Step: 774, LR: 1.279434057604851e-05, Loss: 0.5983254909515381
 38%|███▊      | 775/2040 [7:37:23<12:08:29, 34.55s/it]08/15/2023 17:57:29 - INFO - __main__ -   Step: 775, LR: 1.278423446184942e-05, Loss: 0.46174195408821106
 38%|███▊      | 776/2040 [7:37:58<12:07:41, 34.54s/it]08/15/2023 17:58:04 - INFO - __main__ -   Step: 776, LR: 1.277412834765033e-05, Loss: 0.5274565815925598
 38%|███▊      | 777/2040 [7:38:32<12:05:34, 34.47s/it]08/15/2023 17:58:38 - INFO - __main__ -   Step: 777, LR: 1.2764022233451239e-05, Loss: 0.5024476051330566
 38%|███▊      | 778/2040 [7:39:06<12:03:29, 34.40s/it]08/15/2023 17:59:12 - INFO - __main__ -   Step: 778, LR: 1.2753916119252148e-05, Loss: 0.45889416337013245
 38%|███▊      | 779/2040 [7:39:41<12:03:15, 34.41s/it]08/15/2023 17:59:47 - INFO - __main__ -   Step: 779, LR: 1.2743810005053059e-05, Loss: 0.45361143350601196
 38%|███▊      | 780/2040 [7:40:15<12:03:00, 34.43s/it]08/15/2023 18:00:21 - INFO - __main__ -   Step: 780, LR: 1.2733703890853968e-05, Loss: 0.4467732608318329
 38%|███▊      | 781/2040 [7:40:50<12:03:55, 34.50s/it]08/15/2023 18:00:56 - INFO - __main__ -   Step: 781, LR: 1.2723597776654877e-05, Loss: 0.5383027791976929
 38%|███▊      | 782/2040 [7:41:24<12:00:50, 34.38s/it]08/15/2023 18:01:30 - INFO - __main__ -   Step: 782, LR: 1.2713491662455786e-05, Loss: 0.5275159478187561
 38%|███▊      | 783/2040 [7:41:59<12:01:45, 34.45s/it]08/15/2023 18:02:04 - INFO - __main__ -   Step: 783, LR: 1.2703385548256697e-05, Loss: 0.4550766944885254
 38%|███▊      | 784/2040 [7:42:33<11:58:06, 34.30s/it]08/15/2023 18:02:38 - INFO - __main__ -   Step: 784, LR: 1.2693279434057606e-05, Loss: 0.5320461988449097
 38%|███▊      | 785/2040 [7:43:07<11:57:57, 34.33s/it]08/15/2023 18:03:13 - INFO - __main__ -   Step: 785, LR: 1.2683173319858515e-05, Loss: 0.5466094613075256
 39%|███▊      | 786/2040 [7:43:41<11:57:47, 34.34s/it]08/15/2023 18:03:47 - INFO - __main__ -   Step: 786, LR: 1.2673067205659424e-05, Loss: 0.49498459696769714
 39%|███▊      | 787/2040 [7:44:16<11:58:30, 34.41s/it]08/15/2023 18:04:22 - INFO - __main__ -   Step: 787, LR: 1.2662961091460335e-05, Loss: 0.49377816915512085
 39%|███▊      | 788/2040 [7:44:50<11:57:12, 34.37s/it]08/15/2023 18:04:56 - INFO - __main__ -   Step: 788, LR: 1.2652854977261244e-05, Loss: 0.539661169052124
 39%|███▊      | 789/2040 [7:45:24<11:55:12, 34.30s/it]08/15/2023 18:05:30 - INFO - __main__ -   Step: 789, LR: 1.2642748863062153e-05, Loss: 0.5651832818984985
 39%|███▊      | 790/2040 [7:45:58<11:52:59, 34.22s/it]08/15/2023 18:06:04 - INFO - __main__ -   Step: 790, LR: 1.2632642748863062e-05, Loss: 0.4503709673881531
 39%|███▉      | 791/2040 [7:46:33<11:54:51, 34.34s/it]08/15/2023 18:06:39 - INFO - __main__ -   Step: 791, LR: 1.2622536634663973e-05, Loss: 0.4875917434692383
 39%|███▉      | 792/2040 [7:47:08<11:56:12, 34.43s/it]08/15/2023 18:07:13 - INFO - __main__ -   Step: 792, LR: 1.2612430520464882e-05, Loss: 0.4957239627838135
 39%|███▉      | 793/2040 [7:47:42<11:56:22, 34.47s/it]08/15/2023 18:07:48 - INFO - __main__ -   Step: 793, LR: 1.260232440626579e-05, Loss: 0.5204473733901978
 39%|███▉      | 794/2040 [7:48:17<11:55:45, 34.47s/it]08/15/2023 18:08:22 - INFO - __main__ -   Step: 794, LR: 1.2592218292066703e-05, Loss: 0.44933587312698364
 39%|███▉      | 795/2040 [7:48:50<11:51:47, 34.30s/it]08/15/2023 18:08:56 - INFO - __main__ -   Step: 795, LR: 1.258211217786761e-05, Loss: 0.49903956055641174
 39%|███▉      | 796/2040 [7:49:25<11:51:12, 34.30s/it]08/15/2023 18:09:31 - INFO - __main__ -   Step: 796, LR: 1.257200606366852e-05, Loss: 0.49354541301727295
 39%|███▉      | 797/2040 [7:49:59<11:51:35, 34.35s/it]08/15/2023 18:10:05 - INFO - __main__ -   Step: 797, LR: 1.2561899949469429e-05, Loss: 0.5099083185195923
 39%|███▉      | 798/2040 [7:50:34<11:53:37, 34.47s/it]08/15/2023 18:10:40 - INFO - __main__ -   Step: 798, LR: 1.2551793835270341e-05, Loss: 0.5464986562728882
 39%|███▉      | 799/2040 [7:51:09<11:53:21, 34.49s/it]08/15/2023 18:11:14 - INFO - __main__ -   Step: 799, LR: 1.254168772107125e-05, Loss: 0.5066375136375427
 39%|███▉      | 800/2040 [7:51:43<11:53:46, 34.54s/it]08/15/2023 18:11:49 - INFO - __main__ -   Step: 800, LR: 1.253158160687216e-05, Loss: 0.5108383297920227
08/15/2023 18:11:49 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800
08/15/2023 18:11:49 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 18:11:49,611] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 18:11:49,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 18:11:49,616] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 18:11:49,617] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 18:11:49,617] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 18:11:49,617] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 18:11:49,617] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 18:11:49,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 18:11:49,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 18:11:49,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 18:11:49,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 18:11:49,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 18:11:49,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 18:11:49,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 18:11:49,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 18:12:10,810] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 18:12:10,810] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 18:12:11,724] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 18:12:11,724] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 18:12:12,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 18:12:12,751] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 18:12:12,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 18:12:12,849] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 18:12:12,854] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 18:12:12,854] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 18:12:12,854] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 18:12:12,855] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 18:12:12 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/pytorch_model
08/15/2023 18:12:12 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/scheduler.bin
08/15/2023 18:12:12 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_800/random_states_0.pkl
 39%|███▉      | 801/2040 [7:52:41<14:16:20, 41.47s/it]08/15/2023 18:12:47 - INFO - __main__ -   Step: 801, LR: 1.2521475492673067e-05, Loss: 0.49108195304870605
 39%|███▉      | 802/2040 [7:53:16<13:35:01, 39.50s/it]08/15/2023 18:13:22 - INFO - __main__ -   Step: 802, LR: 1.251136937847398e-05, Loss: 0.4873303472995758
 39%|███▉      | 803/2040 [7:53:50<13:00:44, 37.87s/it]08/15/2023 18:13:56 - INFO - __main__ -   Step: 803, LR: 1.2501263264274888e-05, Loss: 0.5799630880355835
 39%|███▉      | 804/2040 [7:54:24<12:40:27, 36.92s/it]08/15/2023 18:14:30 - INFO - __main__ -   Step: 804, LR: 1.2491157150075797e-05, Loss: 0.4595422148704529
 39%|███▉      | 805/2040 [7:55:00<12:30:46, 36.48s/it]08/15/2023 18:15:06 - INFO - __main__ -   Step: 805, LR: 1.2481051035876706e-05, Loss: 0.47215256094932556
 40%|███▉      | 806/2040 [7:55:35<12:19:48, 35.97s/it]08/15/2023 18:15:41 - INFO - __main__ -   Step: 806, LR: 1.2470944921677617e-05, Loss: 0.49990296363830566
 40%|███▉      | 807/2040 [7:56:10<12:14:06, 35.72s/it]08/15/2023 18:16:16 - INFO - __main__ -   Step: 807, LR: 1.2460838807478526e-05, Loss: 0.6077436208724976
 40%|███▉      | 808/2040 [7:56:44<12:05:54, 35.35s/it]08/15/2023 18:16:50 - INFO - __main__ -   Step: 808, LR: 1.2450732693279435e-05, Loss: 0.5083615183830261
 40%|███▉      | 809/2040 [7:57:20<12:08:42, 35.52s/it]08/15/2023 18:17:26 - INFO - __main__ -   Step: 809, LR: 1.2440626579080344e-05, Loss: 0.47971898317337036
 40%|███▉      | 810/2040 [7:57:55<12:03:56, 35.31s/it]08/15/2023 18:18:01 - INFO - __main__ -   Step: 810, LR: 1.2430520464881255e-05, Loss: 0.5444834232330322
 40%|███▉      | 811/2040 [7:58:30<12:00:56, 35.20s/it]08/15/2023 18:18:36 - INFO - __main__ -   Step: 811, LR: 1.2420414350682164e-05, Loss: 0.5048288702964783
 40%|███▉      | 812/2040 [7:59:05<11:57:33, 35.06s/it]08/15/2023 18:19:11 - INFO - __main__ -   Step: 812, LR: 1.2410308236483073e-05, Loss: 0.471191942691803
 40%|███▉      | 813/2040 [7:59:40<11:55:37, 34.99s/it]08/15/2023 18:19:46 - INFO - __main__ -   Step: 813, LR: 1.2400202122283982e-05, Loss: 0.4206431806087494
 40%|███▉      | 814/2040 [8:00:14<11:52:03, 34.85s/it]08/15/2023 18:20:20 - INFO - __main__ -   Step: 814, LR: 1.2390096008084893e-05, Loss: 0.4328628182411194
 40%|███▉      | 815/2040 [8:00:49<11:53:12, 34.93s/it]08/15/2023 18:20:55 - INFO - __main__ -   Step: 815, LR: 1.2379989893885802e-05, Loss: 0.5567038059234619
 40%|████      | 816/2040 [8:01:24<11:51:17, 34.87s/it]08/15/2023 18:21:30 - INFO - __main__ -   Step: 816, LR: 1.2369883779686711e-05, Loss: 0.5999842882156372
 40%|████      | 817/2040 [8:01:59<11:51:24, 34.90s/it]08/15/2023 18:22:05 - INFO - __main__ -   Step: 817, LR: 1.235977766548762e-05, Loss: 0.4929896593093872
 40%|████      | 818/2040 [8:02:34<11:50:22, 34.88s/it]08/15/2023 18:22:40 - INFO - __main__ -   Step: 818, LR: 1.2349671551288531e-05, Loss: 0.43560391664505005
 40%|████      | 819/2040 [8:03:08<11:47:52, 34.79s/it]08/15/2023 18:23:14 - INFO - __main__ -   Step: 819, LR: 1.233956543708944e-05, Loss: 0.4728832244873047
 40%|████      | 820/2040 [8:03:43<11:46:19, 34.74s/it]08/15/2023 18:23:49 - INFO - __main__ -   Step: 820, LR: 1.232945932289035e-05, Loss: 0.48808372020721436
 40%|████      | 821/2040 [8:04:17<11:44:14, 34.66s/it]08/15/2023 18:24:23 - INFO - __main__ -   Step: 821, LR: 1.2319353208691258e-05, Loss: 0.5196248292922974
 40%|████      | 822/2040 [8:04:52<11:44:12, 34.69s/it]08/15/2023 18:24:58 - INFO - __main__ -   Step: 822, LR: 1.2309247094492169e-05, Loss: 0.5779568552970886
 40%|████      | 823/2040 [8:05:27<11:44:57, 34.76s/it]08/15/2023 18:25:33 - INFO - __main__ -   Step: 823, LR: 1.2299140980293078e-05, Loss: 0.6249973773956299
 40%|████      | 824/2040 [8:06:01<11:41:18, 34.60s/it]08/15/2023 18:26:07 - INFO - __main__ -   Step: 824, LR: 1.2289034866093987e-05, Loss: 0.5248048901557922
 40%|████      | 825/2040 [8:06:36<11:40:34, 34.60s/it]08/15/2023 18:26:42 - INFO - __main__ -   Step: 825, LR: 1.2278928751894898e-05, Loss: 0.554328441619873
 40%|████      | 826/2040 [8:07:11<11:40:43, 34.63s/it]08/15/2023 18:27:17 - INFO - __main__ -   Step: 826, LR: 1.2268822637695807e-05, Loss: 0.52660071849823
 41%|████      | 827/2040 [8:07:46<11:43:34, 34.80s/it]08/15/2023 18:27:52 - INFO - __main__ -   Step: 827, LR: 1.2258716523496716e-05, Loss: 0.4761427044868469
 41%|████      | 828/2040 [8:08:21<11:44:12, 34.86s/it]08/15/2023 18:28:27 - INFO - __main__ -   Step: 828, LR: 1.2248610409297625e-05, Loss: 0.5005706548690796
 41%|████      | 829/2040 [8:08:56<11:42:27, 34.80s/it]08/15/2023 18:29:01 - INFO - __main__ -   Step: 829, LR: 1.2238504295098536e-05, Loss: 0.47923195362091064
 41%|████      | 830/2040 [8:09:30<11:41:39, 34.79s/it]08/15/2023 18:29:36 - INFO - __main__ -   Step: 830, LR: 1.2228398180899445e-05, Loss: 0.451144814491272
 41%|████      | 831/2040 [8:10:04<11:37:00, 34.59s/it]08/15/2023 18:30:10 - INFO - __main__ -   Step: 831, LR: 1.2218292066700354e-05, Loss: 0.5221810340881348
 41%|████      | 832/2040 [8:10:39<11:39:07, 34.73s/it]08/15/2023 18:30:45 - INFO - __main__ -   Step: 832, LR: 1.2208185952501263e-05, Loss: 0.45159676671028137
 41%|████      | 833/2040 [8:11:14<11:36:31, 34.62s/it]08/15/2023 18:31:20 - INFO - __main__ -   Step: 833, LR: 1.2198079838302174e-05, Loss: 0.48077476024627686
 41%|████      | 834/2040 [8:11:49<11:38:53, 34.77s/it]08/15/2023 18:31:55 - INFO - __main__ -   Step: 834, LR: 1.2187973724103083e-05, Loss: 0.5044432878494263
 41%|████      | 835/2040 [8:12:23<11:36:32, 34.68s/it]08/15/2023 18:32:29 - INFO - __main__ -   Step: 835, LR: 1.2177867609903992e-05, Loss: 0.46158167719841003
 41%|████      | 836/2040 [8:12:58<11:34:38, 34.62s/it]08/15/2023 18:33:04 - INFO - __main__ -   Step: 836, LR: 1.2167761495704901e-05, Loss: 0.5015296936035156
 41%|████      | 837/2040 [8:13:32<11:31:12, 34.47s/it]08/15/2023 18:33:38 - INFO - __main__ -   Step: 837, LR: 1.2157655381505814e-05, Loss: 0.5105928182601929
 41%|████      | 838/2040 [8:14:07<11:31:38, 34.52s/it]08/15/2023 18:34:13 - INFO - __main__ -   Step: 838, LR: 1.2147549267306723e-05, Loss: 0.5886262655258179
 41%|████      | 839/2040 [8:14:42<11:32:55, 34.62s/it]08/15/2023 18:34:47 - INFO - __main__ -   Step: 839, LR: 1.213744315310763e-05, Loss: 0.5294874310493469
 41%|████      | 840/2040 [8:15:16<11:34:08, 34.71s/it]08/15/2023 18:35:22 - INFO - __main__ -   Step: 840, LR: 1.212733703890854e-05, Loss: 0.49771103262901306
 41%|████      | 841/2040 [8:15:51<11:34:19, 34.74s/it]08/15/2023 18:35:57 - INFO - __main__ -   Step: 841, LR: 1.2117230924709452e-05, Loss: 0.61308753490448
 41%|████▏     | 842/2040 [8:16:26<11:31:57, 34.66s/it]08/15/2023 18:36:32 - INFO - __main__ -   Step: 842, LR: 1.210712481051036e-05, Loss: 0.43816816806793213
 41%|████▏     | 843/2040 [8:17:01<11:34:20, 34.80s/it]08/15/2023 18:37:07 - INFO - __main__ -   Step: 843, LR: 1.209701869631127e-05, Loss: 0.5485401749610901
 41%|████▏     | 844/2040 [8:17:35<11:32:13, 34.73s/it]08/15/2023 18:37:41 - INFO - __main__ -   Step: 844, LR: 1.2086912582112177e-05, Loss: 0.5535624027252197
 41%|████▏     | 845/2040 [8:18:11<11:34:23, 34.86s/it]08/15/2023 18:38:17 - INFO - __main__ -   Step: 845, LR: 1.207680646791309e-05, Loss: 0.46303117275238037
 41%|████▏     | 846/2040 [8:18:45<11:30:54, 34.72s/it]08/15/2023 18:38:51 - INFO - __main__ -   Step: 846, LR: 1.2066700353713999e-05, Loss: 0.520033597946167
 42%|████▏     | 847/2040 [8:19:20<11:32:34, 34.83s/it]08/15/2023 18:39:26 - INFO - __main__ -   Step: 847, LR: 1.2056594239514908e-05, Loss: 0.5656629800796509
 42%|████▏     | 848/2040 [8:19:54<11:27:37, 34.61s/it]08/15/2023 18:40:00 - INFO - __main__ -   Step: 848, LR: 1.2046488125315817e-05, Loss: 0.5601397752761841
 42%|████▏     | 849/2040 [8:20:29<11:27:26, 34.63s/it]08/15/2023 18:40:35 - INFO - __main__ -   Step: 849, LR: 1.2036382011116728e-05, Loss: 0.4921211898326874
 42%|████▏     | 850/2040 [8:21:03<11:26:43, 34.62s/it]08/15/2023 18:41:09 - INFO - __main__ -   Step: 850, LR: 1.2026275896917637e-05, Loss: 0.4764060974121094
 42%|████▏     | 851/2040 [8:21:38<11:27:00, 34.67s/it]08/15/2023 18:41:44 - INFO - __main__ -   Step: 851, LR: 1.2016169782718546e-05, Loss: 0.48345640301704407
 42%|████▏     | 852/2040 [8:22:13<11:25:46, 34.64s/it]08/15/2023 18:42:19 - INFO - __main__ -   Step: 852, LR: 1.2006063668519455e-05, Loss: 0.5137602090835571
 42%|████▏     | 853/2040 [8:22:48<11:27:54, 34.77s/it]08/15/2023 18:42:54 - INFO - __main__ -   Step: 853, LR: 1.1995957554320366e-05, Loss: 0.5741910934448242
 42%|████▏     | 854/2040 [8:23:23<11:28:22, 34.82s/it]08/15/2023 18:43:29 - INFO - __main__ -   Step: 854, LR: 1.1985851440121275e-05, Loss: 0.4420945644378662
 42%|████▏     | 855/2040 [8:23:57<11:25:28, 34.71s/it]08/15/2023 18:44:03 - INFO - __main__ -   Step: 855, LR: 1.1975745325922184e-05, Loss: 0.48397645354270935
 42%|████▏     | 856/2040 [8:24:32<11:23:43, 34.65s/it]08/15/2023 18:44:38 - INFO - __main__ -   Step: 856, LR: 1.1965639211723093e-05, Loss: 0.5086772441864014
 42%|████▏     | 857/2040 [8:25:07<11:27:26, 34.87s/it]08/15/2023 18:45:13 - INFO - __main__ -   Step: 857, LR: 1.1955533097524004e-05, Loss: 0.4729364514350891
 42%|████▏     | 858/2040 [8:25:42<11:24:57, 34.77s/it]08/15/2023 18:45:48 - INFO - __main__ -   Step: 858, LR: 1.1945426983324913e-05, Loss: 0.48877111077308655
 42%|████▏     | 859/2040 [8:26:16<11:23:55, 34.75s/it]08/15/2023 18:46:22 - INFO - __main__ -   Step: 859, LR: 1.1935320869125822e-05, Loss: 0.5133007168769836
 42%|████▏     | 860/2040 [8:26:51<11:25:11, 34.84s/it]08/15/2023 18:46:57 - INFO - __main__ -   Step: 860, LR: 1.1925214754926733e-05, Loss: 0.5249292850494385
 42%|████▏     | 861/2040 [8:27:26<11:21:30, 34.68s/it]08/15/2023 18:47:32 - INFO - __main__ -   Step: 861, LR: 1.1915108640727642e-05, Loss: 0.5202109813690186
 42%|████▏     | 862/2040 [8:28:01<11:23:00, 34.79s/it]08/15/2023 18:48:07 - INFO - __main__ -   Step: 862, LR: 1.190500252652855e-05, Loss: 0.4971756935119629
 42%|████▏     | 863/2040 [8:28:36<11:24:28, 34.89s/it]08/15/2023 18:48:42 - INFO - __main__ -   Step: 863, LR: 1.189489641232946e-05, Loss: 0.49764689803123474
 42%|████▏     | 864/2040 [8:29:11<11:24:28, 34.92s/it]08/15/2023 18:49:17 - INFO - __main__ -   Step: 864, LR: 1.188479029813037e-05, Loss: 0.5932508111000061
 42%|████▏     | 865/2040 [8:29:46<11:24:28, 34.95s/it]08/15/2023 18:49:52 - INFO - __main__ -   Step: 865, LR: 1.187468418393128e-05, Loss: 0.546484649181366
 42%|████▏     | 866/2040 [8:30:21<11:21:59, 34.85s/it]08/15/2023 18:50:26 - INFO - __main__ -   Step: 866, LR: 1.1864578069732189e-05, Loss: 0.5957750082015991
 42%|████▎     | 867/2040 [8:30:55<11:21:36, 34.87s/it]08/15/2023 18:51:01 - INFO - __main__ -   Step: 867, LR: 1.1854471955533098e-05, Loss: 0.5309499502182007
 43%|████▎     | 868/2040 [8:31:30<11:19:10, 34.77s/it]08/15/2023 18:51:36 - INFO - __main__ -   Step: 868, LR: 1.1844365841334009e-05, Loss: 0.4917902946472168
 43%|████▎     | 869/2040 [8:32:05<11:20:46, 34.88s/it]08/15/2023 18:52:11 - INFO - __main__ -   Step: 869, LR: 1.1834259727134918e-05, Loss: 0.5120961666107178
 43%|████▎     | 870/2040 [8:32:39<11:16:58, 34.72s/it]08/15/2023 18:52:45 - INFO - __main__ -   Step: 870, LR: 1.1824153612935827e-05, Loss: 0.48823773860931396
 43%|████▎     | 871/2040 [8:33:14<11:16:52, 34.74s/it]08/15/2023 18:53:20 - INFO - __main__ -   Step: 871, LR: 1.1814047498736736e-05, Loss: 0.5028252005577087
 43%|████▎     | 872/2040 [8:33:49<11:15:26, 34.70s/it]08/15/2023 18:53:55 - INFO - __main__ -   Step: 872, LR: 1.1803941384537646e-05, Loss: 0.5082074999809265
 43%|████▎     | 873/2040 [8:34:23<11:14:01, 34.65s/it]08/15/2023 18:54:29 - INFO - __main__ -   Step: 873, LR: 1.1793835270338556e-05, Loss: 0.5099643468856812
 43%|████▎     | 874/2040 [8:34:59<11:17:33, 34.87s/it]08/15/2023 18:55:05 - INFO - __main__ -   Step: 874, LR: 1.1783729156139465e-05, Loss: 0.49077314138412476
 43%|████▎     | 875/2040 [8:35:34<11:18:16, 34.93s/it]08/15/2023 18:55:40 - INFO - __main__ -   Step: 875, LR: 1.1773623041940374e-05, Loss: 0.5593668818473816
 43%|████▎     | 876/2040 [8:36:08<11:15:51, 34.84s/it]08/15/2023 18:56:14 - INFO - __main__ -   Step: 876, LR: 1.1763516927741284e-05, Loss: 0.4933711290359497
 43%|████▎     | 877/2040 [8:36:43<11:12:48, 34.71s/it]08/15/2023 18:56:49 - INFO - __main__ -   Step: 877, LR: 1.1753410813542194e-05, Loss: 0.4404008686542511
 43%|████▎     | 878/2040 [8:37:20<11:23:38, 35.30s/it]08/15/2023 18:57:25 - INFO - __main__ -   Step: 878, LR: 1.1743304699343103e-05, Loss: 0.556466281414032
 43%|████▎     | 879/2040 [8:37:54<11:18:46, 35.08s/it]08/15/2023 18:58:00 - INFO - __main__ -   Step: 879, LR: 1.1733198585144012e-05, Loss: 0.4814112186431885
 43%|████▎     | 880/2040 [8:38:29<11:17:25, 35.04s/it]08/15/2023 18:58:35 - INFO - __main__ -   Step: 880, LR: 1.1723092470944924e-05, Loss: 0.476820170879364
 43%|████▎     | 881/2040 [8:39:04<11:18:01, 35.10s/it]08/15/2023 18:59:10 - INFO - __main__ -   Step: 881, LR: 1.1712986356745833e-05, Loss: 0.5309940576553345
 43%|████▎     | 882/2040 [8:39:39<11:15:23, 34.99s/it]08/15/2023 18:59:45 - INFO - __main__ -   Step: 882, LR: 1.170288024254674e-05, Loss: 0.519946277141571
 43%|████▎     | 883/2040 [8:40:13<11:10:17, 34.76s/it]08/15/2023 19:00:19 - INFO - __main__ -   Step: 883, LR: 1.169277412834765e-05, Loss: 0.5316329002380371
 43%|████▎     | 884/2040 [8:40:48<11:06:44, 34.61s/it]08/15/2023 19:00:53 - INFO - __main__ -   Step: 884, LR: 1.1682668014148562e-05, Loss: 0.4577782154083252
 43%|████▎     | 885/2040 [8:41:23<11:10:26, 34.83s/it]08/15/2023 19:01:29 - INFO - __main__ -   Step: 885, LR: 1.1672561899949471e-05, Loss: 0.5098415613174438
 43%|████▎     | 886/2040 [8:41:58<11:08:57, 34.78s/it]08/15/2023 19:02:03 - INFO - __main__ -   Step: 886, LR: 1.166245578575038e-05, Loss: 0.491935670375824
 43%|████▎     | 887/2040 [8:42:33<11:11:48, 34.96s/it]08/15/2023 19:02:39 - INFO - __main__ -   Step: 887, LR: 1.1652349671551288e-05, Loss: 0.3937370479106903
 44%|████▎     | 888/2040 [8:43:08<11:10:01, 34.90s/it]08/15/2023 19:03:14 - INFO - __main__ -   Step: 888, LR: 1.16422435573522e-05, Loss: 0.5261977910995483
 44%|████▎     | 889/2040 [8:43:42<11:08:47, 34.86s/it]08/15/2023 19:03:48 - INFO - __main__ -   Step: 889, LR: 1.163213744315311e-05, Loss: 0.5373644828796387
 44%|████▎     | 890/2040 [8:44:17<11:08:29, 34.88s/it]08/15/2023 19:04:23 - INFO - __main__ -   Step: 890, LR: 1.1622031328954018e-05, Loss: 0.4892549216747284
 44%|████▎     | 891/2040 [8:44:52<11:05:51, 34.77s/it]08/15/2023 19:04:58 - INFO - __main__ -   Step: 891, LR: 1.1611925214754927e-05, Loss: 0.566726565361023
 44%|████▎     | 892/2040 [8:45:27<11:06:37, 34.84s/it]08/15/2023 19:05:33 - INFO - __main__ -   Step: 892, LR: 1.1601819100555838e-05, Loss: 0.5350356698036194
 44%|████▍     | 893/2040 [8:46:02<11:08:50, 34.99s/it]08/15/2023 19:06:08 - INFO - __main__ -   Step: 893, LR: 1.1591712986356747e-05, Loss: 0.5669523477554321
 44%|████▍     | 894/2040 [8:46:37<11:09:10, 35.04s/it]08/15/2023 19:06:43 - INFO - __main__ -   Step: 894, LR: 1.1581606872157656e-05, Loss: 0.4648747146129608
 44%|████▍     | 895/2040 [8:47:13<11:09:55, 35.11s/it]08/15/2023 19:07:19 - INFO - __main__ -   Step: 895, LR: 1.1571500757958567e-05, Loss: 0.509393036365509
 44%|████▍     | 896/2040 [8:47:47<11:06:39, 34.96s/it]08/15/2023 19:07:53 - INFO - __main__ -   Step: 896, LR: 1.1561394643759476e-05, Loss: 0.4845346510410309
 44%|████▍     | 897/2040 [8:48:22<11:05:10, 34.92s/it]08/15/2023 19:08:28 - INFO - __main__ -   Step: 897, LR: 1.1551288529560385e-05, Loss: 0.5049507021903992
 44%|████▍     | 898/2040 [8:48:57<11:04:20, 34.90s/it]08/15/2023 19:09:03 - INFO - __main__ -   Step: 898, LR: 1.1541182415361294e-05, Loss: 0.4926178455352783
 44%|████▍     | 899/2040 [8:49:32<11:05:38, 35.00s/it]08/15/2023 19:09:38 - INFO - __main__ -   Step: 899, LR: 1.1531076301162205e-05, Loss: 0.4749771058559418
 44%|████▍     | 900/2040 [8:50:07<11:05:48, 35.04s/it]08/15/2023 19:10:13 - INFO - __main__ -   Step: 900, LR: 1.1520970186963114e-05, Loss: 0.4726385176181793
08/15/2023 19:10:13 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900
08/15/2023 19:10:13 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 19:10:13,755] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 19:10:13,760] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 19:10:13,761] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 19:10:13,761] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 19:10:13,761] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 19:10:13,761] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 19:10:13,762] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 19:10:13,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 19:10:13,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 19:10:13,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 19:10:13,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 19:10:13,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 19:10:13,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 19:10:13,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 19:10:13,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 19:10:36,687] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 19:10:36,688] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 19:10:36,723] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 19:10:36,723] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 19:10:37,603] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 19:10:37,603] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 19:10:37,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 19:10:37,808] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 19:10:37,812] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 19:10:37,812] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 19:10:37,812] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 19:10:37,812] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 19:10:37 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/pytorch_model
08/15/2023 19:10:37 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/scheduler.bin
08/15/2023 19:10:37 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_900/random_states_0.pkl
 44%|████▍     | 901/2040 [8:51:06<13:19:02, 42.09s/it]08/15/2023 19:11:12 - INFO - __main__ -   Step: 901, LR: 1.1510864072764023e-05, Loss: 0.5089092254638672
 44%|████▍     | 902/2040 [8:51:41<12:37:11, 39.92s/it]08/15/2023 19:11:47 - INFO - __main__ -   Step: 902, LR: 1.1500757958564932e-05, Loss: 0.5242844820022583
 44%|████▍     | 903/2040 [8:52:15<12:05:54, 38.31s/it]08/15/2023 19:12:21 - INFO - __main__ -   Step: 903, LR: 1.1490651844365843e-05, Loss: 0.4512261748313904
 44%|████▍     | 904/2040 [8:52:49<11:41:29, 37.05s/it]08/15/2023 19:12:55 - INFO - __main__ -   Step: 904, LR: 1.1480545730166752e-05, Loss: 0.5016273260116577
 44%|████▍     | 905/2040 [8:53:24<11:26:50, 36.31s/it]08/15/2023 19:13:30 - INFO - __main__ -   Step: 905, LR: 1.1470439615967661e-05, Loss: 0.5157818794250488
 44%|████▍     | 906/2040 [8:53:59<11:17:32, 35.85s/it]08/15/2023 19:14:05 - INFO - __main__ -   Step: 906, LR: 1.146033350176857e-05, Loss: 0.49605444073677063
 44%|████▍     | 907/2040 [8:54:34<11:11:43, 35.57s/it]08/15/2023 19:14:40 - INFO - __main__ -   Step: 907, LR: 1.1450227387569481e-05, Loss: 0.47321486473083496
 45%|████▍     | 908/2040 [8:55:09<11:08:25, 35.43s/it]08/15/2023 19:15:15 - INFO - __main__ -   Step: 908, LR: 1.144012127337039e-05, Loss: 0.4958886504173279
 45%|████▍     | 909/2040 [8:55:44<11:05:41, 35.32s/it]08/15/2023 19:15:50 - INFO - __main__ -   Step: 909, LR: 1.14300151591713e-05, Loss: 0.4517391324043274
 45%|████▍     | 910/2040 [8:56:19<11:02:30, 35.18s/it]08/15/2023 19:16:25 - INFO - __main__ -   Step: 910, LR: 1.1419909044972208e-05, Loss: 0.5165314674377441
 45%|████▍     | 911/2040 [8:56:53<10:58:01, 34.97s/it]08/15/2023 19:16:59 - INFO - __main__ -   Step: 911, LR: 1.1409802930773119e-05, Loss: 0.530921220779419
 45%|████▍     | 912/2040 [8:57:28<10:56:40, 34.93s/it]08/15/2023 19:17:34 - INFO - __main__ -   Step: 912, LR: 1.1399696816574028e-05, Loss: 0.547870397567749
 45%|████▍     | 913/2040 [8:58:03<10:57:56, 35.03s/it]08/15/2023 19:18:09 - INFO - __main__ -   Step: 913, LR: 1.1389590702374937e-05, Loss: 0.45266562700271606
 45%|████▍     | 914/2040 [8:58:38<10:57:16, 35.02s/it]08/15/2023 19:18:44 - INFO - __main__ -   Step: 914, LR: 1.1379484588175846e-05, Loss: 0.5301977396011353
 45%|████▍     | 915/2040 [8:59:13<10:54:22, 34.90s/it]08/15/2023 19:19:19 - INFO - __main__ -   Step: 915, LR: 1.1369378473976757e-05, Loss: 0.4541229009628296
 45%|████▍     | 916/2040 [8:59:48<10:52:47, 34.85s/it]08/15/2023 19:19:54 - INFO - __main__ -   Step: 916, LR: 1.1359272359777666e-05, Loss: 0.5331013202667236
 45%|████▍     | 917/2040 [9:00:22<10:49:56, 34.73s/it]08/15/2023 19:20:28 - INFO - __main__ -   Step: 917, LR: 1.1349166245578575e-05, Loss: 0.4973318576812744
 45%|████▌     | 918/2040 [9:00:57<10:50:04, 34.76s/it]08/15/2023 19:21:03 - INFO - __main__ -   Step: 918, LR: 1.1339060131379484e-05, Loss: 0.481204092502594
 45%|████▌     | 919/2040 [9:01:32<10:50:42, 34.83s/it]08/15/2023 19:21:38 - INFO - __main__ -   Step: 919, LR: 1.1328954017180395e-05, Loss: 0.5453928709030151
 45%|████▌     | 920/2040 [9:02:07<10:50:48, 34.87s/it]08/15/2023 19:22:13 - INFO - __main__ -   Step: 920, LR: 1.1318847902981304e-05, Loss: 0.5342341065406799
 45%|████▌     | 921/2040 [9:02:42<10:50:58, 34.91s/it]08/15/2023 19:22:48 - INFO - __main__ -   Step: 921, LR: 1.1308741788782213e-05, Loss: 0.500727117061615
 45%|████▌     | 922/2040 [9:03:16<10:46:38, 34.70s/it]08/15/2023 19:23:22 - INFO - __main__ -   Step: 922, LR: 1.1298635674583122e-05, Loss: 0.5104619264602661
 45%|████▌     | 923/2040 [9:03:51<10:45:30, 34.67s/it]08/15/2023 19:23:57 - INFO - __main__ -   Step: 923, LR: 1.1288529560384035e-05, Loss: 0.491005539894104
 45%|████▌     | 924/2040 [9:04:25<10:45:46, 34.72s/it]08/15/2023 19:24:31 - INFO - __main__ -   Step: 924, LR: 1.1278423446184944e-05, Loss: 0.5483509302139282
 45%|████▌     | 925/2040 [9:05:01<10:47:14, 34.83s/it]08/15/2023 19:25:06 - INFO - __main__ -   Step: 925, LR: 1.1268317331985851e-05, Loss: 0.6296517848968506
 45%|████▌     | 926/2040 [9:05:35<10:44:22, 34.71s/it]08/15/2023 19:25:41 - INFO - __main__ -   Step: 926, LR: 1.125821121778676e-05, Loss: 0.5456641316413879
 45%|████▌     | 927/2040 [9:06:10<10:45:41, 34.81s/it]08/15/2023 19:26:16 - INFO - __main__ -   Step: 927, LR: 1.1248105103587673e-05, Loss: 0.5083956718444824
 45%|████▌     | 928/2040 [9:06:44<10:41:15, 34.60s/it]08/15/2023 19:26:50 - INFO - __main__ -   Step: 928, LR: 1.1237998989388582e-05, Loss: 0.5484126806259155
 46%|████▌     | 929/2040 [9:07:19<10:41:24, 34.64s/it]08/15/2023 19:27:25 - INFO - __main__ -   Step: 929, LR: 1.122789287518949e-05, Loss: 0.5117732286453247
 46%|████▌     | 930/2040 [9:07:54<10:40:47, 34.64s/it]08/15/2023 19:27:59 - INFO - __main__ -   Step: 930, LR: 1.1217786760990402e-05, Loss: 0.5955715179443359
 46%|████▌     | 931/2040 [9:08:28<10:40:12, 34.64s/it]08/15/2023 19:28:34 - INFO - __main__ -   Step: 931, LR: 1.120768064679131e-05, Loss: 0.5099204778671265
 46%|████▌     | 932/2040 [9:09:03<10:40:10, 34.67s/it]08/15/2023 19:29:09 - INFO - __main__ -   Step: 932, LR: 1.119757453259222e-05, Loss: 0.5085932016372681
 46%|████▌     | 933/2040 [9:09:38<10:40:57, 34.74s/it]08/15/2023 19:29:44 - INFO - __main__ -   Step: 933, LR: 1.1187468418393129e-05, Loss: 0.551802933216095
 46%|████▌     | 934/2040 [9:10:12<10:38:04, 34.61s/it]08/15/2023 19:30:18 - INFO - __main__ -   Step: 934, LR: 1.117736230419404e-05, Loss: 0.5193278789520264
 46%|████▌     | 935/2040 [9:10:47<10:38:37, 34.68s/it]08/15/2023 19:30:53 - INFO - __main__ -   Step: 935, LR: 1.1167256189994949e-05, Loss: 0.5415531992912292
 46%|████▌     | 936/2040 [9:11:22<10:37:55, 34.67s/it]08/15/2023 19:31:28 - INFO - __main__ -   Step: 936, LR: 1.1157150075795858e-05, Loss: 0.5299514532089233
 46%|████▌     | 937/2040 [9:11:57<10:40:20, 34.83s/it]08/15/2023 19:32:03 - INFO - __main__ -   Step: 937, LR: 1.1147043961596767e-05, Loss: 0.47190722823143005
 46%|████▌     | 938/2040 [9:12:31<10:38:33, 34.77s/it]08/15/2023 19:32:37 - INFO - __main__ -   Step: 938, LR: 1.1136937847397678e-05, Loss: 0.5379668474197388
 46%|████▌     | 939/2040 [9:13:06<10:39:00, 34.82s/it]08/15/2023 19:33:12 - INFO - __main__ -   Step: 939, LR: 1.1126831733198587e-05, Loss: 0.4732534885406494
 46%|████▌     | 940/2040 [9:13:41<10:37:48, 34.79s/it]08/15/2023 19:33:47 - INFO - __main__ -   Step: 940, LR: 1.1116725618999496e-05, Loss: 0.5486977100372314
 46%|████▌     | 941/2040 [9:14:16<10:36:52, 34.77s/it]08/15/2023 19:34:22 - INFO - __main__ -   Step: 941, LR: 1.1106619504800405e-05, Loss: 0.5901107788085938
 46%|████▌     | 942/2040 [9:14:50<10:35:37, 34.73s/it]08/15/2023 19:34:56 - INFO - __main__ -   Step: 942, LR: 1.1096513390601316e-05, Loss: 0.522843599319458
 46%|████▌     | 943/2040 [9:15:25<10:36:37, 34.82s/it]08/15/2023 19:35:31 - INFO - __main__ -   Step: 943, LR: 1.1086407276402225e-05, Loss: 0.5050449967384338
 46%|████▋     | 944/2040 [9:16:00<10:34:10, 34.72s/it]08/15/2023 19:36:06 - INFO - __main__ -   Step: 944, LR: 1.1076301162203134e-05, Loss: 0.412555068731308
 46%|████▋     | 945/2040 [9:16:35<10:35:28, 34.82s/it]08/15/2023 19:36:41 - INFO - __main__ -   Step: 945, LR: 1.1066195048004043e-05, Loss: 0.5244474411010742
 46%|████▋     | 946/2040 [9:17:10<10:34:39, 34.81s/it]08/15/2023 19:37:16 - INFO - __main__ -   Step: 946, LR: 1.1056088933804954e-05, Loss: 0.6009638905525208
 46%|████▋     | 947/2040 [9:17:44<10:33:21, 34.77s/it]08/15/2023 19:37:50 - INFO - __main__ -   Step: 947, LR: 1.1045982819605863e-05, Loss: 0.5025544762611389
 46%|████▋     | 948/2040 [9:18:20<10:34:53, 34.88s/it]08/15/2023 19:38:26 - INFO - __main__ -   Step: 948, LR: 1.1035876705406772e-05, Loss: 0.5172929763793945
 47%|████▋     | 949/2040 [9:18:54<10:30:37, 34.68s/it]08/15/2023 19:39:00 - INFO - __main__ -   Step: 949, LR: 1.102577059120768e-05, Loss: 0.5551145076751709
 47%|████▋     | 950/2040 [9:19:28<10:29:43, 34.66s/it]08/15/2023 19:39:34 - INFO - __main__ -   Step: 950, LR: 1.1015664477008591e-05, Loss: 0.5752356052398682
 47%|████▋     | 951/2040 [9:20:03<10:30:27, 34.74s/it]08/15/2023 19:40:09 - INFO - __main__ -   Step: 951, LR: 1.10055583628095e-05, Loss: 0.5025604963302612
 47%|████▋     | 952/2040 [9:20:38<10:27:56, 34.63s/it]08/15/2023 19:40:44 - INFO - __main__ -   Step: 952, LR: 1.099545224861041e-05, Loss: 0.5319490432739258
 47%|████▋     | 953/2040 [9:21:12<10:27:54, 34.66s/it]08/15/2023 19:41:18 - INFO - __main__ -   Step: 953, LR: 1.0985346134411319e-05, Loss: 0.49873656034469604
 47%|████▋     | 954/2040 [9:21:47<10:28:52, 34.74s/it]08/15/2023 19:41:53 - INFO - __main__ -   Step: 954, LR: 1.097524002021223e-05, Loss: 0.5367502570152283
 47%|████▋     | 955/2040 [9:22:22<10:28:24, 34.75s/it]08/15/2023 19:42:28 - INFO - __main__ -   Step: 955, LR: 1.0965133906013139e-05, Loss: 0.5446886420249939
 47%|████▋     | 956/2040 [9:22:57<10:26:24, 34.67s/it]08/15/2023 19:43:03 - INFO - __main__ -   Step: 956, LR: 1.0955027791814048e-05, Loss: 0.5657752156257629
 47%|████▋     | 957/2040 [9:23:32<10:28:33, 34.82s/it]08/15/2023 19:43:38 - INFO - __main__ -   Step: 957, LR: 1.0944921677614957e-05, Loss: 0.5016388893127441
 47%|████▋     | 958/2040 [9:24:06<10:25:39, 34.69s/it]08/15/2023 19:44:12 - INFO - __main__ -   Step: 958, LR: 1.0934815563415867e-05, Loss: 0.523120641708374
 47%|████▋     | 959/2040 [9:24:41<10:24:22, 34.66s/it]08/15/2023 19:44:47 - INFO - __main__ -   Step: 959, LR: 1.0924709449216777e-05, Loss: 0.4691058397293091
 47%|████▋     | 960/2040 [9:25:16<10:25:50, 34.77s/it]08/15/2023 19:45:22 - INFO - __main__ -   Step: 960, LR: 1.0914603335017686e-05, Loss: 0.5117960572242737
 47%|████▋     | 961/2040 [9:25:50<10:23:32, 34.67s/it]08/15/2023 19:45:56 - INFO - __main__ -   Step: 961, LR: 1.0904497220818595e-05, Loss: 0.5102851986885071
 47%|████▋     | 962/2040 [9:26:25<10:25:05, 34.79s/it]08/15/2023 19:46:31 - INFO - __main__ -   Step: 962, LR: 1.0894391106619505e-05, Loss: 0.5188423991203308
 47%|████▋     | 963/2040 [9:27:00<10:22:28, 34.68s/it]08/15/2023 19:47:06 - INFO - __main__ -   Step: 963, LR: 1.0884284992420415e-05, Loss: 0.5030248165130615
 47%|████▋     | 964/2040 [9:27:35<10:23:18, 34.76s/it]08/15/2023 19:47:41 - INFO - __main__ -   Step: 964, LR: 1.0874178878221324e-05, Loss: 0.4943487346172333
 47%|████▋     | 965/2040 [9:28:10<10:23:06, 34.78s/it]08/15/2023 19:48:15 - INFO - __main__ -   Step: 965, LR: 1.0864072764022236e-05, Loss: 0.4865587055683136
 47%|████▋     | 966/2040 [9:28:45<10:26:32, 35.00s/it]08/15/2023 19:48:51 - INFO - __main__ -   Step: 966, LR: 1.0853966649823145e-05, Loss: 0.5279274582862854
 47%|████▋     | 967/2040 [9:29:21<10:28:53, 35.17s/it]08/15/2023 19:49:27 - INFO - __main__ -   Step: 967, LR: 1.0843860535624054e-05, Loss: 0.4825676679611206
 47%|████▋     | 968/2040 [9:29:56<10:29:32, 35.24s/it]08/15/2023 19:50:02 - INFO - __main__ -   Step: 968, LR: 1.0833754421424962e-05, Loss: 0.4656107425689697
 48%|████▊     | 969/2040 [9:30:30<10:24:06, 34.96s/it]08/15/2023 19:50:36 - INFO - __main__ -   Step: 969, LR: 1.0823648307225874e-05, Loss: 0.47806695103645325
 48%|████▊     | 970/2040 [9:31:06<10:24:58, 35.05s/it]08/15/2023 19:51:11 - INFO - __main__ -   Step: 970, LR: 1.0813542193026783e-05, Loss: 0.5210094451904297
 48%|████▊     | 971/2040 [9:31:40<10:20:48, 34.84s/it]08/15/2023 19:51:46 - INFO - __main__ -   Step: 971, LR: 1.0803436078827692e-05, Loss: 0.5526503324508667
 48%|████▊     | 972/2040 [9:32:14<10:18:33, 34.75s/it]08/15/2023 19:52:20 - INFO - __main__ -   Step: 972, LR: 1.0793329964628601e-05, Loss: 0.5462918281555176
 48%|████▊     | 973/2040 [9:32:49<10:17:31, 34.73s/it]08/15/2023 19:52:55 - INFO - __main__ -   Step: 973, LR: 1.0783223850429512e-05, Loss: 0.539277970790863
 48%|████▊     | 974/2040 [9:33:24<10:15:19, 34.63s/it]08/15/2023 19:53:29 - INFO - __main__ -   Step: 974, LR: 1.0773117736230421e-05, Loss: 0.4754832983016968
 48%|████▊     | 975/2040 [9:33:58<10:14:29, 34.62s/it]08/15/2023 19:54:04 - INFO - __main__ -   Step: 975, LR: 1.076301162203133e-05, Loss: 0.5302196145057678
 48%|████▊     | 976/2040 [9:34:33<10:15:39, 34.72s/it]08/15/2023 19:54:39 - INFO - __main__ -   Step: 976, LR: 1.075290550783224e-05, Loss: 0.48707056045532227
 48%|████▊     | 977/2040 [9:35:08<10:13:36, 34.63s/it]08/15/2023 19:55:13 - INFO - __main__ -   Step: 977, LR: 1.074279939363315e-05, Loss: 0.5449602603912354
 48%|████▊     | 978/2040 [9:35:42<10:11:09, 34.53s/it]08/15/2023 19:55:48 - INFO - __main__ -   Step: 978, LR: 1.0732693279434059e-05, Loss: 0.5602718591690063
 48%|████▊     | 979/2040 [9:36:16<10:10:16, 34.51s/it]08/15/2023 19:56:22 - INFO - __main__ -   Step: 979, LR: 1.0722587165234968e-05, Loss: 0.49127280712127686
 48%|████▊     | 980/2040 [9:36:50<10:07:56, 34.41s/it]08/15/2023 19:56:56 - INFO - __main__ -   Step: 980, LR: 1.0712481051035877e-05, Loss: 0.4928048849105835
 48%|████▊     | 981/2040 [9:37:25<10:06:37, 34.37s/it]08/15/2023 19:57:31 - INFO - __main__ -   Step: 981, LR: 1.0702374936836788e-05, Loss: 0.5152198672294617
 48%|████▊     | 982/2040 [9:37:59<10:07:59, 34.48s/it]08/15/2023 19:58:05 - INFO - __main__ -   Step: 982, LR: 1.0692268822637697e-05, Loss: 0.6025071144104004
 48%|████▊     | 983/2040 [9:38:34<10:06:12, 34.41s/it]08/15/2023 19:58:40 - INFO - __main__ -   Step: 983, LR: 1.0682162708438606e-05, Loss: 0.5352659225463867
 48%|████▊     | 984/2040 [9:39:08<10:04:57, 34.37s/it]08/15/2023 19:59:14 - INFO - __main__ -   Step: 984, LR: 1.0672056594239515e-05, Loss: 0.5056734085083008
 48%|████▊     | 985/2040 [9:39:42<10:04:52, 34.40s/it]08/15/2023 19:59:48 - INFO - __main__ -   Step: 985, LR: 1.0661950480040426e-05, Loss: 0.4944300055503845
 48%|████▊     | 986/2040 [9:40:17<10:02:46, 34.31s/it]08/15/2023 20:00:23 - INFO - __main__ -   Step: 986, LR: 1.0651844365841335e-05, Loss: 0.548209547996521
 48%|████▊     | 987/2040 [9:40:51<10:03:15, 34.37s/it]08/15/2023 20:00:57 - INFO - __main__ -   Step: 987, LR: 1.0641738251642244e-05, Loss: 0.57750004529953
 48%|████▊     | 988/2040 [9:41:26<10:05:09, 34.51s/it]08/15/2023 20:01:32 - INFO - __main__ -   Step: 988, LR: 1.0631632137443153e-05, Loss: 0.4604785442352295
 48%|████▊     | 989/2040 [9:42:01<10:06:54, 34.65s/it]08/15/2023 20:02:07 - INFO - __main__ -   Step: 989, LR: 1.0621526023244064e-05, Loss: 0.45695924758911133
 49%|████▊     | 990/2040 [9:42:35<10:05:52, 34.62s/it]08/15/2023 20:02:41 - INFO - __main__ -   Step: 990, LR: 1.0611419909044973e-05, Loss: 0.41013869643211365
 49%|████▊     | 991/2040 [9:43:10<10:03:26, 34.52s/it]08/15/2023 20:03:16 - INFO - __main__ -   Step: 991, LR: 1.0601313794845882e-05, Loss: 0.5083475112915039
 49%|████▊     | 992/2040 [9:43:44<9:59:48, 34.34s/it] 08/15/2023 20:03:50 - INFO - __main__ -   Step: 992, LR: 1.0591207680646791e-05, Loss: 0.630306601524353
 49%|████▊     | 993/2040 [9:44:18<9:59:33, 34.36s/it]08/15/2023 20:04:24 - INFO - __main__ -   Step: 993, LR: 1.0581101566447702e-05, Loss: 0.462565541267395
 49%|████▊     | 994/2040 [9:44:52<9:56:56, 34.24s/it]08/15/2023 20:04:58 - INFO - __main__ -   Step: 994, LR: 1.0570995452248611e-05, Loss: 0.39771339297294617
 49%|████▉     | 995/2040 [9:45:27<9:57:55, 34.33s/it]08/15/2023 20:05:32 - INFO - __main__ -   Step: 995, LR: 1.056088933804952e-05, Loss: 0.5434677600860596
 49%|████▉     | 996/2040 [9:46:01<9:58:48, 34.41s/it]08/15/2023 20:06:07 - INFO - __main__ -   Step: 996, LR: 1.055078322385043e-05, Loss: 0.45265913009643555
 49%|████▉     | 997/2040 [9:46:35<9:57:28, 34.37s/it]08/15/2023 20:06:41 - INFO - __main__ -   Step: 997, LR: 1.054067710965134e-05, Loss: 0.527540922164917
 49%|████▉     | 998/2040 [9:47:09<9:54:55, 34.26s/it]08/15/2023 20:07:15 - INFO - __main__ -   Step: 998, LR: 1.0530570995452249e-05, Loss: 0.43876925110816956
 49%|████▉     | 999/2040 [9:47:44<9:56:58, 34.41s/it]08/15/2023 20:07:50 - INFO - __main__ -   Step: 999, LR: 1.0520464881253158e-05, Loss: 0.46953654289245605
 49%|████▉     | 1000/2040 [9:48:19<9:57:41, 34.48s/it]08/15/2023 20:08:25 - INFO - __main__ -   Step: 1000, LR: 1.0510358767054069e-05, Loss: 0.515576958656311
08/15/2023 20:08:25 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000
08/15/2023 20:08:25 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 20:08:25,278] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 20:08:25,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 20:08:25,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 20:08:25,284] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 20:08:25,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 20:08:25,285] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 20:08:25,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 20:08:25,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 20:08:25,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 20:08:25,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 20:08:25,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 20:08:25,297] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 20:08:25,297] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 20:08:25,297] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 20:08:25,297] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 20:08:46,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 20:08:46,574] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 20:08:48,214] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 20:08:48,215] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 20:08:48,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 20:08:48,676] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 20:08:48,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 20:08:48,696] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 20:08:48,700] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 20:08:48,700] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 20:08:48,700] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 20:08:48,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 20:08:48 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/pytorch_model
08/15/2023 20:08:48 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/scheduler.bin
08/15/2023 20:08:48 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1000/random_states_0.pkl
 49%|████▉     | 1001/2040 [9:49:16<11:57:08, 41.41s/it]08/15/2023 20:09:22 - INFO - __main__ -   Step: 1001, LR: 1.0500252652854978e-05, Loss: 0.5202372074127197
 49%|████▉     | 1002/2040 [9:49:51<11:20:45, 39.35s/it]08/15/2023 20:09:57 - INFO - __main__ -   Step: 1002, LR: 1.0490146538655887e-05, Loss: 0.5356682538986206
 49%|████▉     | 1003/2040 [9:50:26<10:57:05, 38.02s/it]08/15/2023 20:10:32 - INFO - __main__ -   Step: 1003, LR: 1.0480040424456796e-05, Loss: 0.5206831693649292
 49%|████▉     | 1004/2040 [9:51:01<10:42:38, 37.22s/it]08/15/2023 20:11:07 - INFO - __main__ -   Step: 1004, LR: 1.0469934310257709e-05, Loss: 0.4628642201423645
 49%|████▉     | 1005/2040 [9:51:37<10:34:38, 36.79s/it]08/15/2023 20:11:43 - INFO - __main__ -   Step: 1005, LR: 1.0459828196058616e-05, Loss: 0.4738016128540039
 49%|████▉     | 1006/2040 [9:52:13<10:31:20, 36.63s/it]08/15/2023 20:12:19 - INFO - __main__ -   Step: 1006, LR: 1.0449722081859525e-05, Loss: 0.5635737776756287
 49%|████▉     | 1007/2040 [9:52:49<10:25:51, 36.35s/it]08/15/2023 20:12:55 - INFO - __main__ -   Step: 1007, LR: 1.0439615967660434e-05, Loss: 0.4960007667541504
 49%|████▉     | 1008/2040 [9:53:24<10:20:20, 36.07s/it]08/15/2023 20:13:30 - INFO - __main__ -   Step: 1008, LR: 1.0429509853461347e-05, Loss: 0.4973748028278351
 49%|████▉     | 1009/2040 [9:53:59<10:12:30, 35.65s/it]08/15/2023 20:14:05 - INFO - __main__ -   Step: 1009, LR: 1.0419403739262256e-05, Loss: 0.4411803185939789
 50%|████▉     | 1010/2040 [9:54:34<10:10:11, 35.55s/it]08/15/2023 20:14:40 - INFO - __main__ -   Step: 1010, LR: 1.0409297625063165e-05, Loss: 0.42476463317871094
 50%|████▉     | 1011/2040 [9:55:10<10:11:53, 35.68s/it]08/15/2023 20:15:16 - INFO - __main__ -   Step: 1011, LR: 1.0399191510864072e-05, Loss: 0.5163130760192871
 50%|████▉     | 1012/2040 [9:55:45<10:07:49, 35.48s/it]08/15/2023 20:15:51 - INFO - __main__ -   Step: 1012, LR: 1.0389085396664985e-05, Loss: 0.5332627296447754
 50%|████▉     | 1013/2040 [9:56:20<10:04:58, 35.34s/it]08/15/2023 20:16:26 - INFO - __main__ -   Step: 1013, LR: 1.0378979282465894e-05, Loss: 0.5797980427742004
 50%|████▉     | 1014/2040 [9:56:56<10:04:35, 35.36s/it]08/15/2023 20:17:02 - INFO - __main__ -   Step: 1014, LR: 1.0368873168266803e-05, Loss: 0.5749335289001465
 50%|████▉     | 1015/2040 [9:57:31<10:02:18, 35.26s/it]08/15/2023 20:17:37 - INFO - __main__ -   Step: 1015, LR: 1.0358767054067712e-05, Loss: 0.5648094415664673
 50%|████▉     | 1016/2040 [9:58:06<10:00:48, 35.20s/it]08/15/2023 20:18:12 - INFO - __main__ -   Step: 1016, LR: 1.0348660939868623e-05, Loss: 0.47754135727882385
 50%|████▉     | 1017/2040 [9:58:41<10:02:07, 35.31s/it]08/15/2023 20:18:47 - INFO - __main__ -   Step: 1017, LR: 1.0338554825669532e-05, Loss: 0.5021984577178955
 50%|████▉     | 1018/2040 [9:59:17<10:03:13, 35.41s/it]08/15/2023 20:19:23 - INFO - __main__ -   Step: 1018, LR: 1.032844871147044e-05, Loss: 0.5401833057403564
 50%|████▉     | 1019/2040 [9:59:53<10:03:37, 35.47s/it]08/15/2023 20:19:59 - INFO - __main__ -   Step: 1019, LR: 1.031834259727135e-05, Loss: 0.5066921710968018
 50%|█████     | 1020/2040 [10:00:29<10:07:01, 35.71s/it]08/15/2023 20:20:35 - INFO - __main__ -   Step: 1020, LR: 1.030823648307226e-05, Loss: 0.5333608388900757
 50%|█████     | 1021/2040 [10:01:04<10:04:44, 35.61s/it]08/15/2023 20:21:10 - INFO - __main__ -   Step: 1021, LR: 1.029813036887317e-05, Loss: 0.483003705739975
 50%|█████     | 1022/2040 [10:01:39<9:57:41, 35.23s/it] 08/15/2023 20:21:45 - INFO - __main__ -   Step: 1022, LR: 1.0288024254674079e-05, Loss: 0.4646453857421875
 50%|█████     | 1023/2040 [10:02:14<9:55:49, 35.15s/it]08/15/2023 20:22:20 - INFO - __main__ -   Step: 1023, LR: 1.0277918140474988e-05, Loss: 0.4471967816352844
 50%|█████     | 1024/2040 [10:02:49<9:56:37, 35.23s/it]08/15/2023 20:22:55 - INFO - __main__ -   Step: 1024, LR: 1.0267812026275899e-05, Loss: 0.5840796232223511
 50%|█████     | 1025/2040 [10:03:24<9:56:17, 35.25s/it]08/15/2023 20:23:30 - INFO - __main__ -   Step: 1025, LR: 1.0257705912076808e-05, Loss: 0.5497646331787109
 50%|█████     | 1026/2040 [10:04:00<9:57:07, 35.33s/it]08/15/2023 20:24:06 - INFO - __main__ -   Step: 1026, LR: 1.0247599797877717e-05, Loss: 0.4636504650115967
 50%|█████     | 1027/2040 [10:04:35<9:55:31, 35.27s/it]08/15/2023 20:24:41 - INFO - __main__ -   Step: 1027, LR: 1.0237493683678626e-05, Loss: 0.5143332481384277
 50%|█████     | 1028/2040 [10:05:10<9:53:43, 35.20s/it]08/15/2023 20:25:16 - INFO - __main__ -   Step: 1028, LR: 1.0227387569479536e-05, Loss: 0.49376678466796875
 50%|█████     | 1029/2040 [10:05:45<9:50:40, 35.06s/it]08/15/2023 20:25:51 - INFO - __main__ -   Step: 1029, LR: 1.0217281455280446e-05, Loss: 0.5477656126022339
 50%|█████     | 1030/2040 [10:06:20<9:49:40, 35.03s/it]08/15/2023 20:26:26 - INFO - __main__ -   Step: 1030, LR: 1.0207175341081355e-05, Loss: 0.5438752174377441
 51%|█████     | 1031/2040 [10:06:55<9:48:10, 34.98s/it]08/15/2023 20:27:01 - INFO - __main__ -   Step: 1031, LR: 1.0197069226882264e-05, Loss: 0.5032262206077576
 51%|█████     | 1032/2040 [10:07:30<9:49:56, 35.12s/it]08/15/2023 20:27:36 - INFO - __main__ -   Step: 1032, LR: 1.0186963112683174e-05, Loss: 0.47401025891304016
 51%|█████     | 1033/2040 [10:08:05<9:48:56, 35.09s/it]08/15/2023 20:28:11 - INFO - __main__ -   Step: 1033, LR: 1.0176856998484084e-05, Loss: 0.5208430290222168
 51%|█████     | 1034/2040 [10:08:40<9:47:19, 35.03s/it]08/15/2023 20:28:46 - INFO - __main__ -   Step: 1034, LR: 1.0166750884284993e-05, Loss: 0.5162495374679565
 51%|█████     | 1035/2040 [10:09:15<9:45:11, 34.94s/it]08/15/2023 20:29:21 - INFO - __main__ -   Step: 1035, LR: 1.0156644770085903e-05, Loss: 0.44947803020477295
 51%|█████     | 1036/2040 [10:09:49<9:43:30, 34.87s/it]08/15/2023 20:29:55 - INFO - __main__ -   Step: 1036, LR: 1.0146538655886812e-05, Loss: 0.5720230340957642
 51%|█████     | 1037/2040 [10:10:24<9:43:00, 34.88s/it]08/15/2023 20:30:30 - INFO - __main__ -   Step: 1037, LR: 1.0136432541687722e-05, Loss: 0.5036559104919434
 51%|█████     | 1038/2040 [10:11:00<9:44:44, 35.01s/it]08/15/2023 20:31:06 - INFO - __main__ -   Step: 1038, LR: 1.012632642748863e-05, Loss: 0.5207148194313049
 51%|█████     | 1039/2040 [10:11:34<9:42:46, 34.93s/it]08/15/2023 20:31:40 - INFO - __main__ -   Step: 1039, LR: 1.0116220313289541e-05, Loss: 0.5243920087814331
 51%|█████     | 1040/2040 [10:12:09<9:42:44, 34.96s/it]08/15/2023 20:32:15 - INFO - __main__ -   Step: 1040, LR: 1.010611419909045e-05, Loss: 0.47826671600341797
 51%|█████     | 1041/2040 [10:12:44<9:40:50, 34.89s/it]08/15/2023 20:32:50 - INFO - __main__ -   Step: 1041, LR: 1.009600808489136e-05, Loss: 0.5391520261764526
 51%|█████     | 1042/2040 [10:13:19<9:38:33, 34.78s/it]08/15/2023 20:33:25 - INFO - __main__ -   Step: 1042, LR: 1.0085901970692269e-05, Loss: 0.5410052537918091
 51%|█████     | 1043/2040 [10:13:54<9:38:43, 34.83s/it]08/15/2023 20:34:00 - INFO - __main__ -   Step: 1043, LR: 1.007579585649318e-05, Loss: 0.5123088359832764
 51%|█████     | 1044/2040 [10:14:29<9:39:54, 34.93s/it]08/15/2023 20:34:35 - INFO - __main__ -   Step: 1044, LR: 1.0065689742294088e-05, Loss: 0.5646995902061462
 51%|█████     | 1045/2040 [10:15:04<9:38:47, 34.90s/it]08/15/2023 20:35:10 - INFO - __main__ -   Step: 1045, LR: 1.0055583628094998e-05, Loss: 0.4410022795200348
 51%|█████▏    | 1046/2040 [10:15:39<9:41:05, 35.08s/it]08/15/2023 20:35:45 - INFO - __main__ -   Step: 1046, LR: 1.0045477513895907e-05, Loss: 0.46776121854782104
 51%|█████▏    | 1047/2040 [10:16:13<9:37:03, 34.87s/it]08/15/2023 20:36:19 - INFO - __main__ -   Step: 1047, LR: 1.0035371399696819e-05, Loss: 0.4614221155643463
 51%|█████▏    | 1048/2040 [10:16:49<9:37:42, 34.94s/it]08/15/2023 20:36:54 - INFO - __main__ -   Step: 1048, LR: 1.0025265285497726e-05, Loss: 0.5049935579299927
 51%|█████▏    | 1049/2040 [10:17:24<9:39:13, 35.07s/it]08/15/2023 20:37:30 - INFO - __main__ -   Step: 1049, LR: 1.0015159171298636e-05, Loss: 0.47377079725265503
 51%|█████▏    | 1050/2040 [10:17:59<9:39:40, 35.13s/it]08/15/2023 20:38:05 - INFO - __main__ -   Step: 1050, LR: 1.0005053057099545e-05, Loss: 0.47093915939331055
 52%|█████▏    | 1051/2040 [10:18:35<9:43:41, 35.41s/it]08/15/2023 20:38:41 - INFO - __main__ -   Step: 1051, LR: 9.994946942900455e-06, Loss: 0.5031057596206665
 52%|█████▏    | 1052/2040 [10:19:10<9:39:21, 35.18s/it]08/15/2023 20:39:16 - INFO - __main__ -   Step: 1052, LR: 9.984840828701366e-06, Loss: 0.5088167190551758
 52%|█████▏    | 1053/2040 [10:19:45<9:36:29, 35.05s/it]08/15/2023 20:39:51 - INFO - __main__ -   Step: 1053, LR: 9.974734714502275e-06, Loss: 0.47989457845687866
 52%|█████▏    | 1054/2040 [10:20:19<9:34:46, 34.98s/it]08/15/2023 20:40:25 - INFO - __main__ -   Step: 1054, LR: 9.964628600303184e-06, Loss: 0.5524176955223083
 52%|█████▏    | 1055/2040 [10:20:55<9:34:59, 35.02s/it]08/15/2023 20:41:01 - INFO - __main__ -   Step: 1055, LR: 9.954522486104093e-06, Loss: 0.47039031982421875
 52%|█████▏    | 1056/2040 [10:21:30<9:36:09, 35.13s/it]08/15/2023 20:41:36 - INFO - __main__ -   Step: 1056, LR: 9.944416371905004e-06, Loss: 0.4849572479724884
 52%|█████▏    | 1057/2040 [10:22:05<9:32:58, 34.97s/it]08/15/2023 20:42:11 - INFO - __main__ -   Step: 1057, LR: 9.934310257705913e-06, Loss: 0.5065240859985352
 52%|█████▏    | 1058/2040 [10:22:41<9:38:22, 35.34s/it]08/15/2023 20:42:47 - INFO - __main__ -   Step: 1058, LR: 9.924204143506822e-06, Loss: 0.5402845144271851
 52%|█████▏    | 1059/2040 [10:23:16<9:34:56, 35.17s/it]08/15/2023 20:43:21 - INFO - __main__ -   Step: 1059, LR: 9.914098029307731e-06, Loss: 0.5245497226715088
 52%|█████▏    | 1060/2040 [10:23:50<9:31:41, 35.00s/it]08/15/2023 20:43:56 - INFO - __main__ -   Step: 1060, LR: 9.903991915108642e-06, Loss: 0.46797946095466614
 52%|█████▏    | 1061/2040 [10:24:25<9:30:48, 34.98s/it]08/15/2023 20:44:31 - INFO - __main__ -   Step: 1061, LR: 9.893885800909551e-06, Loss: 0.4847097396850586
 52%|█████▏    | 1062/2040 [10:25:00<9:31:55, 35.09s/it]08/15/2023 20:45:06 - INFO - __main__ -   Step: 1062, LR: 9.88377968671046e-06, Loss: 0.48093166947364807
 52%|█████▏    | 1063/2040 [10:25:35<9:29:26, 34.97s/it]08/15/2023 20:45:41 - INFO - __main__ -   Step: 1063, LR: 9.87367357251137e-06, Loss: 0.534425675868988
 52%|█████▏    | 1064/2040 [10:26:10<9:27:52, 34.91s/it]08/15/2023 20:46:16 - INFO - __main__ -   Step: 1064, LR: 9.86356745831228e-06, Loss: 0.4921634793281555
 52%|█████▏    | 1065/2040 [10:26:45<9:28:59, 35.01s/it]08/15/2023 20:46:51 - INFO - __main__ -   Step: 1065, LR: 9.85346134411319e-06, Loss: 0.5694985389709473
 52%|█████▏    | 1066/2040 [10:27:20<9:26:35, 34.90s/it]08/15/2023 20:47:26 - INFO - __main__ -   Step: 1066, LR: 9.843355229914098e-06, Loss: 0.5851782560348511
 52%|█████▏    | 1067/2040 [10:27:55<9:25:24, 34.87s/it]08/15/2023 20:48:00 - INFO - __main__ -   Step: 1067, LR: 9.833249115715007e-06, Loss: 0.5095388889312744
 52%|█████▏    | 1068/2040 [10:28:30<9:27:02, 35.00s/it]08/15/2023 20:48:36 - INFO - __main__ -   Step: 1068, LR: 9.823143001515918e-06, Loss: 0.5411000847816467
 52%|█████▏    | 1069/2040 [10:29:05<9:26:25, 35.00s/it]08/15/2023 20:49:11 - INFO - __main__ -   Step: 1069, LR: 9.813036887316829e-06, Loss: 0.483506977558136
 52%|█████▏    | 1070/2040 [10:29:40<9:26:41, 35.05s/it]08/15/2023 20:49:46 - INFO - __main__ -   Step: 1070, LR: 9.802930773117736e-06, Loss: 0.47726285457611084
 52%|█████▎    | 1071/2040 [10:30:15<9:27:38, 35.15s/it]08/15/2023 20:50:21 - INFO - __main__ -   Step: 1071, LR: 9.792824658918647e-06, Loss: 0.5236430764198303
 53%|█████▎    | 1072/2040 [10:30:51<9:28:00, 35.21s/it]08/15/2023 20:50:57 - INFO - __main__ -   Step: 1072, LR: 9.782718544719556e-06, Loss: 0.5079706907272339
 53%|█████▎    | 1073/2040 [10:31:26<9:26:50, 35.17s/it]08/15/2023 20:51:32 - INFO - __main__ -   Step: 1073, LR: 9.772612430520467e-06, Loss: 0.5149139165878296
 53%|█████▎    | 1074/2040 [10:32:01<9:28:05, 35.29s/it]08/15/2023 20:52:07 - INFO - __main__ -   Step: 1074, LR: 9.762506316321376e-06, Loss: 0.4537670910358429
 53%|█████▎    | 1075/2040 [10:32:36<9:26:13, 35.21s/it]08/15/2023 20:52:42 - INFO - __main__ -   Step: 1075, LR: 9.752400202122285e-06, Loss: 0.5737966299057007
 53%|█████▎    | 1076/2040 [10:33:12<9:25:37, 35.20s/it]08/15/2023 20:53:18 - INFO - __main__ -   Step: 1076, LR: 9.742294087923194e-06, Loss: 0.5398241281509399
 53%|█████▎    | 1077/2040 [10:33:47<9:25:51, 35.26s/it]08/15/2023 20:53:53 - INFO - __main__ -   Step: 1077, LR: 9.732187973724105e-06, Loss: 0.45438870787620544
 53%|█████▎    | 1078/2040 [10:34:22<9:23:24, 35.14s/it]08/15/2023 20:54:28 - INFO - __main__ -   Step: 1078, LR: 9.722081859525014e-06, Loss: 0.5576510429382324
 53%|█████▎    | 1079/2040 [10:34:58<9:27:21, 35.42s/it]08/15/2023 20:55:04 - INFO - __main__ -   Step: 1079, LR: 9.711975745325923e-06, Loss: 0.45803573727607727
 53%|█████▎    | 1080/2040 [10:35:34<9:30:25, 35.65s/it]08/15/2023 20:55:40 - INFO - __main__ -   Step: 1080, LR: 9.701869631126832e-06, Loss: 0.48865261673927307
 53%|█████▎    | 1081/2040 [10:36:10<9:28:38, 35.58s/it]08/15/2023 20:56:15 - INFO - __main__ -   Step: 1081, LR: 9.691763516927743e-06, Loss: 0.5373126864433289
 53%|█████▎    | 1082/2040 [10:36:45<9:25:07, 35.39s/it]08/15/2023 20:56:50 - INFO - __main__ -   Step: 1082, LR: 9.681657402728652e-06, Loss: 0.5357247591018677
 53%|█████▎    | 1083/2040 [10:37:19<9:21:31, 35.21s/it]08/15/2023 20:57:25 - INFO - __main__ -   Step: 1083, LR: 9.671551288529561e-06, Loss: 0.5254732370376587
 53%|█████▎    | 1084/2040 [10:37:54<9:19:20, 35.10s/it]08/15/2023 20:58:00 - INFO - __main__ -   Step: 1084, LR: 9.66144517433047e-06, Loss: 0.5172405242919922
 53%|█████▎    | 1085/2040 [10:38:31<9:27:02, 35.63s/it]08/15/2023 20:58:37 - INFO - __main__ -   Step: 1085, LR: 9.65133906013138e-06, Loss: 0.489231675863266
 53%|█████▎    | 1086/2040 [10:39:07<9:26:07, 35.60s/it]08/15/2023 20:59:12 - INFO - __main__ -   Step: 1086, LR: 9.64123294593229e-06, Loss: 0.5026845335960388
 53%|█████▎    | 1087/2040 [10:39:42<9:25:51, 35.63s/it]08/15/2023 20:59:48 - INFO - __main__ -   Step: 1087, LR: 9.631126831733199e-06, Loss: 0.5087083578109741
 53%|█████▎    | 1088/2040 [10:40:18<9:24:02, 35.55s/it]08/15/2023 21:00:24 - INFO - __main__ -   Step: 1088, LR: 9.621020717534108e-06, Loss: 0.4921209216117859
 53%|█████▎    | 1089/2040 [10:40:53<9:22:19, 35.48s/it]08/15/2023 21:00:59 - INFO - __main__ -   Step: 1089, LR: 9.610914603335019e-06, Loss: 0.5206689834594727
 53%|█████▎    | 1090/2040 [10:41:28<9:19:36, 35.34s/it]08/15/2023 21:01:34 - INFO - __main__ -   Step: 1090, LR: 9.600808489135928e-06, Loss: 0.508529782295227
 53%|█████▎    | 1091/2040 [10:42:03<9:19:46, 35.39s/it]08/15/2023 21:02:09 - INFO - __main__ -   Step: 1091, LR: 9.590702374936837e-06, Loss: 0.5128419995307922
 54%|█████▎    | 1092/2040 [10:42:38<9:17:00, 35.25s/it]08/15/2023 21:02:44 - INFO - __main__ -   Step: 1092, LR: 9.580596260737746e-06, Loss: 0.4754863679409027
 54%|█████▎    | 1093/2040 [10:43:14<9:17:41, 35.33s/it]08/15/2023 21:03:20 - INFO - __main__ -   Step: 1093, LR: 9.570490146538657e-06, Loss: 0.5085716247558594
 54%|█████▎    | 1094/2040 [10:43:49<9:16:51, 35.32s/it]08/15/2023 21:03:55 - INFO - __main__ -   Step: 1094, LR: 9.560384032339566e-06, Loss: 0.5531954765319824
 54%|█████▎    | 1095/2040 [10:44:25<9:16:47, 35.35s/it]08/15/2023 21:04:31 - INFO - __main__ -   Step: 1095, LR: 9.550277918140477e-06, Loss: 0.5052899718284607
 54%|█████▎    | 1096/2040 [10:45:00<9:15:33, 35.31s/it]08/15/2023 21:05:06 - INFO - __main__ -   Step: 1096, LR: 9.540171803941386e-06, Loss: 0.5058878660202026
 54%|█████▍    | 1097/2040 [10:45:35<9:13:31, 35.22s/it]08/15/2023 21:05:41 - INFO - __main__ -   Step: 1097, LR: 9.530065689742295e-06, Loss: 0.5032377243041992
 54%|█████▍    | 1098/2040 [10:46:11<9:15:11, 35.36s/it]08/15/2023 21:06:16 - INFO - __main__ -   Step: 1098, LR: 9.519959575543204e-06, Loss: 0.4916762113571167
 54%|█████▍    | 1099/2040 [10:46:46<9:16:14, 35.47s/it]08/15/2023 21:06:52 - INFO - __main__ -   Step: 1099, LR: 9.509853461344115e-06, Loss: 0.5416292548179626
 54%|█████▍    | 1100/2040 [10:47:21<9:13:38, 35.34s/it]08/15/2023 21:07:27 - INFO - __main__ -   Step: 1100, LR: 9.499747347145024e-06, Loss: 0.5197657346725464
08/15/2023 21:07:27 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100
08/15/2023 21:07:27 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 21:07:27,713] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 21:07:27,719] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 21:07:27,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 21:07:27,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 21:07:27,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 21:07:27,720] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 21:07:27,720] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 21:07:27,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 21:07:27,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 21:07:27,731] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 21:07:27,731] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 21:07:27,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 21:07:27,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 21:07:27,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 21:07:27,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 21:07:48,696] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 21:07:48,697] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 21:07:49,913] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 21:07:49,913] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 21:07:50,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 21:07:50,536] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 21:07:50,646] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 21:07:50,647] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 21:07:50,651] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 21:07:50,651] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 21:07:50,651] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 21:07:50,651] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 21:07:50 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/pytorch_model
08/15/2023 21:07:50 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/scheduler.bin
08/15/2023 21:07:50 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1100/random_states_0.pkl
 54%|█████▍    | 1101/2040 [10:48:19<10:56:53, 41.97s/it]08/15/2023 21:08:25 - INFO - __main__ -   Step: 1101, LR: 9.489641232945933e-06, Loss: 0.4341418147087097
 54%|█████▍    | 1102/2040 [10:48:54<10:23:28, 39.88s/it]08/15/2023 21:09:00 - INFO - __main__ -   Step: 1102, LR: 9.479535118746842e-06, Loss: 0.4895533621311188
 54%|█████▍    | 1103/2040 [10:49:28<9:58:43, 38.34s/it] 08/15/2023 21:09:34 - INFO - __main__ -   Step: 1103, LR: 9.469429004547753e-06, Loss: 0.46789705753326416
 54%|█████▍    | 1104/2040 [10:50:03<9:39:10, 37.13s/it]08/15/2023 21:10:09 - INFO - __main__ -   Step: 1104, LR: 9.459322890348662e-06, Loss: 0.4749028980731964
 54%|█████▍    | 1105/2040 [10:50:37<9:26:51, 36.38s/it]08/15/2023 21:10:43 - INFO - __main__ -   Step: 1105, LR: 9.44921677614957e-06, Loss: 0.5504122972488403
 54%|█████▍    | 1106/2040 [10:51:13<9:20:20, 36.00s/it]08/15/2023 21:11:18 - INFO - __main__ -   Step: 1106, LR: 9.439110661950482e-06, Loss: 0.4795912504196167
 54%|█████▍    | 1107/2040 [10:51:48<9:16:49, 35.81s/it]08/15/2023 21:11:54 - INFO - __main__ -   Step: 1107, LR: 9.42900454775139e-06, Loss: 0.4585532546043396
 54%|█████▍    | 1108/2040 [10:52:23<9:10:56, 35.47s/it]08/15/2023 21:12:28 - INFO - __main__ -   Step: 1108, LR: 9.4188984335523e-06, Loss: 0.5525311827659607
 54%|█████▍    | 1109/2040 [10:52:57<9:06:28, 35.22s/it]08/15/2023 21:13:03 - INFO - __main__ -   Step: 1109, LR: 9.408792319353209e-06, Loss: 0.5327568650245667
 54%|█████▍    | 1110/2040 [10:53:32<9:01:52, 34.96s/it]08/15/2023 21:13:37 - INFO - __main__ -   Step: 1110, LR: 9.39868620515412e-06, Loss: 0.4992092251777649
 54%|█████▍    | 1111/2040 [10:54:07<9:04:06, 35.14s/it]08/15/2023 21:14:13 - INFO - __main__ -   Step: 1111, LR: 9.388580090955029e-06, Loss: 0.5303766131401062
 55%|█████▍    | 1112/2040 [10:54:42<9:03:55, 35.17s/it]08/15/2023 21:14:48 - INFO - __main__ -   Step: 1112, LR: 9.37847397675594e-06, Loss: 0.478859543800354
 55%|█████▍    | 1113/2040 [10:55:18<9:03:25, 35.17s/it]08/15/2023 21:15:23 - INFO - __main__ -   Step: 1113, LR: 9.368367862556847e-06, Loss: 0.5058484077453613
 55%|█████▍    | 1114/2040 [10:55:53<9:03:19, 35.20s/it]08/15/2023 21:15:59 - INFO - __main__ -   Step: 1114, LR: 9.358261748357757e-06, Loss: 0.518510103225708
 55%|█████▍    | 1115/2040 [10:56:28<9:01:21, 35.12s/it]08/15/2023 21:16:34 - INFO - __main__ -   Step: 1115, LR: 9.348155634158667e-06, Loss: 0.45041877031326294
 55%|█████▍    | 1116/2040 [10:57:03<9:00:10, 35.08s/it]08/15/2023 21:17:09 - INFO - __main__ -   Step: 1116, LR: 9.338049519959577e-06, Loss: 0.5121212005615234
 55%|█████▍    | 1117/2040 [10:57:37<8:57:51, 34.96s/it]08/15/2023 21:17:43 - INFO - __main__ -   Step: 1117, LR: 9.327943405760486e-06, Loss: 0.524529218673706
 55%|█████▍    | 1118/2040 [10:58:13<9:01:12, 35.22s/it]08/15/2023 21:18:19 - INFO - __main__ -   Step: 1118, LR: 9.317837291561395e-06, Loss: 0.534984827041626
 55%|█████▍    | 1119/2040 [10:58:49<9:02:52, 35.37s/it]08/15/2023 21:18:55 - INFO - __main__ -   Step: 1119, LR: 9.307731177362305e-06, Loss: 0.4642348885536194
 55%|█████▍    | 1120/2040 [10:59:24<9:02:08, 35.36s/it]08/15/2023 21:19:30 - INFO - __main__ -   Step: 1120, LR: 9.297625063163215e-06, Loss: 0.49914655089378357
 55%|█████▍    | 1121/2040 [10:59:59<8:59:23, 35.22s/it]08/15/2023 21:20:05 - INFO - __main__ -   Step: 1121, LR: 9.287518948964124e-06, Loss: 0.49107006192207336
 55%|█████▌    | 1122/2040 [11:00:34<8:55:15, 34.98s/it]08/15/2023 21:20:40 - INFO - __main__ -   Step: 1122, LR: 9.277412834765033e-06, Loss: 0.4308732748031616
 55%|█████▌    | 1123/2040 [11:01:08<8:53:59, 34.94s/it]08/15/2023 21:21:14 - INFO - __main__ -   Step: 1123, LR: 9.267306720565943e-06, Loss: 0.46993929147720337
 55%|█████▌    | 1124/2040 [11:01:43<8:51:37, 34.82s/it]08/15/2023 21:21:49 - INFO - __main__ -   Step: 1124, LR: 9.257200606366853e-06, Loss: 0.5316812992095947
 55%|█████▌    | 1125/2040 [11:02:18<8:52:23, 34.91s/it]08/15/2023 21:22:24 - INFO - __main__ -   Step: 1125, LR: 9.247094492167762e-06, Loss: 0.44336581230163574
 55%|█████▌    | 1126/2040 [11:02:53<8:51:19, 34.88s/it]08/15/2023 21:22:59 - INFO - __main__ -   Step: 1126, LR: 9.236988377968671e-06, Loss: 0.4934680461883545
 55%|█████▌    | 1127/2040 [11:03:28<8:49:37, 34.81s/it]08/15/2023 21:23:33 - INFO - __main__ -   Step: 1127, LR: 9.22688226376958e-06, Loss: 0.5046461820602417
 55%|█████▌    | 1128/2040 [11:04:03<8:50:01, 34.87s/it]08/15/2023 21:24:08 - INFO - __main__ -   Step: 1128, LR: 9.216776149570491e-06, Loss: 0.45767003297805786
 55%|█████▌    | 1129/2040 [11:04:37<8:48:46, 34.83s/it]08/15/2023 21:24:43 - INFO - __main__ -   Step: 1129, LR: 9.2066700353714e-06, Loss: 0.5151684284210205
 55%|█████▌    | 1130/2040 [11:05:12<8:47:38, 34.79s/it]08/15/2023 21:25:18 - INFO - __main__ -   Step: 1130, LR: 9.19656392117231e-06, Loss: 0.5361523032188416
 55%|█████▌    | 1131/2040 [11:05:47<8:48:36, 34.89s/it]08/15/2023 21:25:53 - INFO - __main__ -   Step: 1131, LR: 9.186457806973219e-06, Loss: 0.4920629858970642
 55%|█████▌    | 1132/2040 [11:06:22<8:46:19, 34.78s/it]08/15/2023 21:26:28 - INFO - __main__ -   Step: 1132, LR: 9.17635169277413e-06, Loss: 0.48073792457580566
 56%|█████▌    | 1133/2040 [11:06:57<8:47:03, 34.87s/it]08/15/2023 21:27:03 - INFO - __main__ -   Step: 1133, LR: 9.166245578575038e-06, Loss: 0.5750738978385925
 56%|█████▌    | 1134/2040 [11:07:31<8:45:27, 34.80s/it]08/15/2023 21:27:37 - INFO - __main__ -   Step: 1134, LR: 9.156139464375947e-06, Loss: 0.5408773422241211
 56%|█████▌    | 1135/2040 [11:08:07<8:46:42, 34.92s/it]08/15/2023 21:28:12 - INFO - __main__ -   Step: 1135, LR: 9.146033350176856e-06, Loss: 0.528798520565033
 56%|█████▌    | 1136/2040 [11:08:41<8:44:21, 34.80s/it]08/15/2023 21:28:47 - INFO - __main__ -   Step: 1136, LR: 9.135927235977767e-06, Loss: 0.4830074906349182
 56%|█████▌    | 1137/2040 [11:09:17<8:46:41, 35.00s/it]08/15/2023 21:29:22 - INFO - __main__ -   Step: 1137, LR: 9.125821121778676e-06, Loss: 0.47909635305404663
 56%|█████▌    | 1138/2040 [11:09:52<8:46:07, 35.00s/it]08/15/2023 21:29:57 - INFO - __main__ -   Step: 1138, LR: 9.115715007579587e-06, Loss: 0.4796431064605713
 56%|█████▌    | 1139/2040 [11:10:27<8:46:18, 35.05s/it]08/15/2023 21:30:33 - INFO - __main__ -   Step: 1139, LR: 9.105608893380496e-06, Loss: 0.5581690073013306
 56%|█████▌    | 1140/2040 [11:11:02<8:45:46, 35.05s/it]08/15/2023 21:31:08 - INFO - __main__ -   Step: 1140, LR: 9.095502779181405e-06, Loss: 0.49895668029785156
 56%|█████▌    | 1141/2040 [11:11:37<8:43:48, 34.96s/it]08/15/2023 21:31:42 - INFO - __main__ -   Step: 1141, LR: 9.085396664982316e-06, Loss: 0.4633049964904785
 56%|█████▌    | 1142/2040 [11:12:11<8:41:47, 34.86s/it]08/15/2023 21:32:17 - INFO - __main__ -   Step: 1142, LR: 9.075290550783225e-06, Loss: 0.4943280816078186
 56%|█████▌    | 1143/2040 [11:12:47<8:44:00, 35.05s/it]08/15/2023 21:32:53 - INFO - __main__ -   Step: 1143, LR: 9.065184436584134e-06, Loss: 0.49780043959617615
 56%|█████▌    | 1144/2040 [11:13:21<8:42:17, 34.98s/it]08/15/2023 21:33:27 - INFO - __main__ -   Step: 1144, LR: 9.055078322385043e-06, Loss: 0.5032378435134888
 56%|█████▌    | 1145/2040 [11:13:56<8:39:00, 34.79s/it]08/15/2023 21:34:02 - INFO - __main__ -   Step: 1145, LR: 9.044972208185954e-06, Loss: 0.5051929950714111
 56%|█████▌    | 1146/2040 [11:14:31<8:39:08, 34.84s/it]08/15/2023 21:34:37 - INFO - __main__ -   Step: 1146, LR: 9.034866093986863e-06, Loss: 0.6098333597183228
 56%|█████▌    | 1147/2040 [11:15:05<8:38:07, 34.81s/it]08/15/2023 21:35:11 - INFO - __main__ -   Step: 1147, LR: 9.024759979787772e-06, Loss: 0.5503785610198975
 56%|█████▋    | 1148/2040 [11:15:40<8:38:06, 34.85s/it]08/15/2023 21:35:46 - INFO - __main__ -   Step: 1148, LR: 9.014653865588681e-06, Loss: 0.49823614954948425
 56%|█████▋    | 1149/2040 [11:16:15<8:37:06, 34.82s/it]08/15/2023 21:36:21 - INFO - __main__ -   Step: 1149, LR: 9.004547751389592e-06, Loss: 0.5451948642730713
 56%|█████▋    | 1150/2040 [11:16:50<8:37:01, 34.86s/it]08/15/2023 21:36:56 - INFO - __main__ -   Step: 1150, LR: 8.994441637190501e-06, Loss: 0.5005335211753845
 56%|█████▋    | 1151/2040 [11:17:25<8:35:01, 34.76s/it]08/15/2023 21:37:31 - INFO - __main__ -   Step: 1151, LR: 8.98433552299141e-06, Loss: 0.6051493287086487
 56%|█████▋    | 1152/2040 [11:18:00<8:36:04, 34.87s/it]08/15/2023 21:38:06 - INFO - __main__ -   Step: 1152, LR: 8.97422940879232e-06, Loss: 0.530019998550415
 57%|█████▋    | 1153/2040 [11:18:35<8:36:43, 34.95s/it]08/15/2023 21:38:41 - INFO - __main__ -   Step: 1153, LR: 8.96412329459323e-06, Loss: 0.4572885036468506
 57%|█████▋    | 1154/2040 [11:19:10<8:38:18, 35.10s/it]08/15/2023 21:39:16 - INFO - __main__ -   Step: 1154, LR: 8.954017180394139e-06, Loss: 0.41614195704460144
 57%|█████▋    | 1155/2040 [11:19:45<8:35:26, 34.95s/it]08/15/2023 21:39:51 - INFO - __main__ -   Step: 1155, LR: 8.94391106619505e-06, Loss: 0.5649672746658325
 57%|█████▋    | 1156/2040 [11:20:20<8:33:37, 34.86s/it]08/15/2023 21:40:26 - INFO - __main__ -   Step: 1156, LR: 8.933804951995957e-06, Loss: 0.5583261847496033
 57%|█████▋    | 1157/2040 [11:20:54<8:29:50, 34.64s/it]08/15/2023 21:41:00 - INFO - __main__ -   Step: 1157, LR: 8.923698837796868e-06, Loss: 0.42713138461112976
 57%|█████▋    | 1158/2040 [11:21:28<8:28:56, 34.62s/it]08/15/2023 21:41:34 - INFO - __main__ -   Step: 1158, LR: 8.913592723597777e-06, Loss: 0.5307369828224182
 57%|█████▋    | 1159/2040 [11:22:03<8:29:52, 34.73s/it]08/15/2023 21:42:09 - INFO - __main__ -   Step: 1159, LR: 8.903486609398688e-06, Loss: 0.49030739068984985
 57%|█████▋    | 1160/2040 [11:22:38<8:30:09, 34.78s/it]08/15/2023 21:42:44 - INFO - __main__ -   Step: 1160, LR: 8.893380495199597e-06, Loss: 0.4645039439201355
 57%|█████▋    | 1161/2040 [11:23:13<8:28:26, 34.71s/it]08/15/2023 21:43:19 - INFO - __main__ -   Step: 1161, LR: 8.883274381000506e-06, Loss: 0.5231001973152161
 57%|█████▋    | 1162/2040 [11:23:47<8:26:32, 34.62s/it]08/15/2023 21:43:53 - INFO - __main__ -   Step: 1162, LR: 8.873168266801415e-06, Loss: 0.4874693751335144
 57%|█████▋    | 1163/2040 [11:24:22<8:25:29, 34.58s/it]08/15/2023 21:44:28 - INFO - __main__ -   Step: 1163, LR: 8.863062152602326e-06, Loss: 0.4995827078819275
 57%|█████▋    | 1164/2040 [11:24:57<8:26:36, 34.70s/it]08/15/2023 21:45:03 - INFO - __main__ -   Step: 1164, LR: 8.852956038403235e-06, Loss: 0.5470988750457764
 57%|█████▋    | 1165/2040 [11:25:31<8:26:14, 34.71s/it]08/15/2023 21:45:37 - INFO - __main__ -   Step: 1165, LR: 8.842849924204144e-06, Loss: 0.45021480321884155
 57%|█████▋    | 1166/2040 [11:26:06<8:26:24, 34.76s/it]08/15/2023 21:46:12 - INFO - __main__ -   Step: 1166, LR: 8.832743810005053e-06, Loss: 0.5077036023139954
 57%|█████▋    | 1167/2040 [11:26:41<8:26:48, 34.83s/it]08/15/2023 21:46:47 - INFO - __main__ -   Step: 1167, LR: 8.822637695805964e-06, Loss: 0.47435885667800903
 57%|█████▋    | 1168/2040 [11:27:15<8:23:28, 34.64s/it]08/15/2023 21:47:21 - INFO - __main__ -   Step: 1168, LR: 8.812531581606873e-06, Loss: 0.4984501302242279
 57%|█████▋    | 1169/2040 [11:27:50<8:21:43, 34.56s/it]08/15/2023 21:47:56 - INFO - __main__ -   Step: 1169, LR: 8.802425467407782e-06, Loss: 0.5083019137382507
 57%|█████▋    | 1170/2040 [11:28:25<8:23:20, 34.71s/it]08/15/2023 21:48:31 - INFO - __main__ -   Step: 1170, LR: 8.792319353208691e-06, Loss: 0.4852488040924072
 57%|█████▋    | 1171/2040 [11:29:00<8:23:56, 34.80s/it]08/15/2023 21:49:06 - INFO - __main__ -   Step: 1171, LR: 8.782213239009602e-06, Loss: 0.5168732404708862
 57%|█████▋    | 1172/2040 [11:29:35<8:25:47, 34.96s/it]08/15/2023 21:49:41 - INFO - __main__ -   Step: 1172, LR: 8.77210712481051e-06, Loss: 0.5272563695907593
 57%|█████▊    | 1173/2040 [11:30:10<8:22:56, 34.81s/it]08/15/2023 21:50:16 - INFO - __main__ -   Step: 1173, LR: 8.76200101061142e-06, Loss: 0.5401633977890015
 58%|█████▊    | 1174/2040 [11:30:44<8:18:21, 34.53s/it]08/15/2023 21:50:49 - INFO - __main__ -   Step: 1174, LR: 8.75189489641233e-06, Loss: 0.44482213258743286
 58%|█████▊    | 1175/2040 [11:31:18<8:19:02, 34.62s/it]08/15/2023 21:51:24 - INFO - __main__ -   Step: 1175, LR: 8.74178878221324e-06, Loss: 0.49406981468200684
 58%|█████▊    | 1176/2040 [11:31:53<8:19:10, 34.66s/it]08/15/2023 21:51:59 - INFO - __main__ -   Step: 1176, LR: 8.73168266801415e-06, Loss: 0.5532042980194092
 58%|█████▊    | 1177/2040 [11:32:29<8:22:28, 34.93s/it]08/15/2023 21:52:35 - INFO - __main__ -   Step: 1177, LR: 8.721576553815058e-06, Loss: 0.5104697346687317
 58%|█████▊    | 1178/2040 [11:33:03<8:19:19, 34.76s/it]08/15/2023 21:53:09 - INFO - __main__ -   Step: 1178, LR: 8.711470439615969e-06, Loss: 0.4664677381515503
 58%|█████▊    | 1179/2040 [11:33:38<8:18:13, 34.72s/it]08/15/2023 21:53:44 - INFO - __main__ -   Step: 1179, LR: 8.701364325416878e-06, Loss: 0.46415597200393677
 58%|█████▊    | 1180/2040 [11:34:12<8:15:00, 34.53s/it]08/15/2023 21:54:18 - INFO - __main__ -   Step: 1180, LR: 8.691258211217789e-06, Loss: 0.513413667678833
 58%|█████▊    | 1181/2040 [11:34:46<8:14:45, 34.56s/it]08/15/2023 21:54:52 - INFO - __main__ -   Step: 1181, LR: 8.681152097018698e-06, Loss: 0.48323601484298706
 58%|█████▊    | 1182/2040 [11:35:21<8:15:12, 34.63s/it]08/15/2023 21:55:27 - INFO - __main__ -   Step: 1182, LR: 8.671045982819607e-06, Loss: 0.4767514169216156
 58%|█████▊    | 1183/2040 [11:35:56<8:17:07, 34.80s/it]08/15/2023 21:56:02 - INFO - __main__ -   Step: 1183, LR: 8.660939868620516e-06, Loss: 0.4557415246963501
 58%|█████▊    | 1184/2040 [11:36:31<8:14:27, 34.66s/it]08/15/2023 21:56:37 - INFO - __main__ -   Step: 1184, LR: 8.650833754421427e-06, Loss: 0.48972851037979126
 58%|█████▊    | 1185/2040 [11:37:05<8:13:15, 34.61s/it]08/15/2023 21:57:11 - INFO - __main__ -   Step: 1185, LR: 8.640727640222336e-06, Loss: 0.540392279624939
 58%|█████▊    | 1186/2040 [11:37:40<8:14:06, 34.71s/it]08/15/2023 21:57:46 - INFO - __main__ -   Step: 1186, LR: 8.630621526023245e-06, Loss: 0.44461193680763245
 58%|█████▊    | 1187/2040 [11:38:15<8:14:26, 34.78s/it]08/15/2023 21:58:21 - INFO - __main__ -   Step: 1187, LR: 8.620515411824154e-06, Loss: 0.5759018659591675
 58%|█████▊    | 1188/2040 [11:38:50<8:14:52, 34.85s/it]08/15/2023 21:58:56 - INFO - __main__ -   Step: 1188, LR: 8.610409297625064e-06, Loss: 0.5416779518127441
 58%|█████▊    | 1189/2040 [11:39:25<8:15:47, 34.96s/it]08/15/2023 21:59:31 - INFO - __main__ -   Step: 1189, LR: 8.600303183425974e-06, Loss: 0.4921153783798218
 58%|█████▊    | 1190/2040 [11:40:00<8:12:48, 34.79s/it]08/15/2023 22:00:06 - INFO - __main__ -   Step: 1190, LR: 8.590197069226883e-06, Loss: 0.5378333330154419
 58%|█████▊    | 1191/2040 [11:40:34<8:11:59, 34.77s/it]08/15/2023 22:00:40 - INFO - __main__ -   Step: 1191, LR: 8.580090955027792e-06, Loss: 0.5301182866096497
 58%|█████▊    | 1192/2040 [11:41:09<8:10:09, 34.68s/it]08/15/2023 22:01:15 - INFO - __main__ -   Step: 1192, LR: 8.569984840828702e-06, Loss: 0.4697994589805603
 58%|█████▊    | 1193/2040 [11:41:44<8:10:25, 34.74s/it]08/15/2023 22:01:50 - INFO - __main__ -   Step: 1193, LR: 8.559878726629612e-06, Loss: 0.4741217792034149
 59%|█████▊    | 1194/2040 [11:42:18<8:08:38, 34.65s/it]08/15/2023 22:02:24 - INFO - __main__ -   Step: 1194, LR: 8.54977261243052e-06, Loss: 0.46312469244003296
 59%|█████▊    | 1195/2040 [11:42:53<8:09:30, 34.76s/it]08/15/2023 22:02:59 - INFO - __main__ -   Step: 1195, LR: 8.53966649823143e-06, Loss: 0.4141419231891632
 59%|█████▊    | 1196/2040 [11:43:28<8:09:14, 34.78s/it]08/15/2023 22:03:34 - INFO - __main__ -   Step: 1196, LR: 8.52956038403234e-06, Loss: 0.487973690032959
 59%|█████▊    | 1197/2040 [11:44:02<8:06:28, 34.62s/it]08/15/2023 22:04:08 - INFO - __main__ -   Step: 1197, LR: 8.51945426983325e-06, Loss: 0.5099506378173828
 59%|█████▊    | 1198/2040 [11:44:37<8:06:46, 34.69s/it]08/15/2023 22:04:43 - INFO - __main__ -   Step: 1198, LR: 8.50934815563416e-06, Loss: 0.4993433952331543
 59%|█████▉    | 1199/2040 [11:45:12<8:05:29, 34.64s/it]08/15/2023 22:05:18 - INFO - __main__ -   Step: 1199, LR: 8.499242041435068e-06, Loss: 0.5857962965965271
 59%|█████▉    | 1200/2040 [11:45:46<8:05:06, 34.65s/it]08/15/2023 22:05:52 - INFO - __main__ -   Step: 1200, LR: 8.489135927235978e-06, Loss: 0.5279852151870728
08/15/2023 22:05:52 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200
08/15/2023 22:05:52 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 22:05:52,813] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 22:05:52,818] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 22:05:52,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 22:05:52,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 22:05:52,819] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 22:05:52,819] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 22:05:52,819] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 22:05:52,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 22:05:52,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 22:05:52,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 22:05:52,956] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 22:05:52,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 22:05:52,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 22:05:52,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 22:05:52,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 22:06:15,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 22:06:15,729] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 22:06:16,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 22:06:16,304] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 22:06:17,548] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 22:06:17,549] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 22:06:17,584] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 22:06:17,584] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 22:06:17,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 22:06:17,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 22:06:17,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 22:06:17,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 22:06:17 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/pytorch_model
08/15/2023 22:06:17 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/scheduler.bin
08/15/2023 22:06:17 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1200/random_states_0.pkl
 59%|█████▉    | 1201/2040 [11:46:45<9:44:48, 41.82s/it]08/15/2023 22:06:51 - INFO - __main__ -   Step: 1201, LR: 8.479029813036888e-06, Loss: 0.4787203073501587
 59%|█████▉    | 1202/2040 [11:47:20<9:14:26, 39.70s/it]08/15/2023 22:07:26 - INFO - __main__ -   Step: 1202, LR: 8.468923698837798e-06, Loss: 0.46221309900283813
 59%|█████▉    | 1203/2040 [11:47:54<8:51:11, 38.08s/it]08/15/2023 22:08:00 - INFO - __main__ -   Step: 1203, LR: 8.458817584638707e-06, Loss: 0.5278897285461426
 59%|█████▉    | 1204/2040 [11:48:28<8:35:05, 36.97s/it]08/15/2023 22:08:34 - INFO - __main__ -   Step: 1204, LR: 8.448711470439616e-06, Loss: 0.5435608625411987
 59%|█████▉    | 1205/2040 [11:49:02<8:21:52, 36.06s/it]08/15/2023 22:09:08 - INFO - __main__ -   Step: 1205, LR: 8.438605356240526e-06, Loss: 0.5063028931617737
 59%|█████▉    | 1206/2040 [11:49:37<8:14:17, 35.56s/it]08/15/2023 22:09:43 - INFO - __main__ -   Step: 1206, LR: 8.428499242041436e-06, Loss: 0.5004889965057373
 59%|█████▉    | 1207/2040 [11:50:11<8:10:07, 35.30s/it]08/15/2023 22:10:17 - INFO - __main__ -   Step: 1207, LR: 8.418393127842345e-06, Loss: 0.47165706753730774
 59%|█████▉    | 1208/2040 [11:50:46<8:08:28, 35.23s/it]08/15/2023 22:10:52 - INFO - __main__ -   Step: 1208, LR: 8.408287013643254e-06, Loss: 0.41187795996665955
 59%|█████▉    | 1209/2040 [11:51:21<8:05:16, 35.04s/it]08/15/2023 22:11:27 - INFO - __main__ -   Step: 1209, LR: 8.398180899444165e-06, Loss: 0.5248409509658813
 59%|█████▉    | 1210/2040 [11:51:56<8:03:05, 34.92s/it]08/15/2023 22:12:02 - INFO - __main__ -   Step: 1210, LR: 8.388074785245074e-06, Loss: 0.5266748070716858
 59%|█████▉    | 1211/2040 [11:52:30<8:01:25, 34.84s/it]08/15/2023 22:12:36 - INFO - __main__ -   Step: 1211, LR: 8.377968671045983e-06, Loss: 0.4575393497943878
 59%|█████▉    | 1212/2040 [11:53:05<8:00:59, 34.86s/it]08/15/2023 22:13:11 - INFO - __main__ -   Step: 1212, LR: 8.367862556846892e-06, Loss: 0.46490877866744995
 59%|█████▉    | 1213/2040 [11:53:40<7:58:34, 34.72s/it]08/15/2023 22:13:46 - INFO - __main__ -   Step: 1213, LR: 8.357756442647803e-06, Loss: 0.48715877532958984
 60%|█████▉    | 1214/2040 [11:54:15<7:58:43, 34.77s/it]08/15/2023 22:14:20 - INFO - __main__ -   Step: 1214, LR: 8.347650328448712e-06, Loss: 0.4671556055545807
 60%|█████▉    | 1215/2040 [11:54:49<7:56:30, 34.66s/it]08/15/2023 22:14:55 - INFO - __main__ -   Step: 1215, LR: 8.337544214249621e-06, Loss: 0.6055452823638916
 60%|█████▉    | 1216/2040 [11:55:24<7:56:33, 34.70s/it]08/15/2023 22:15:30 - INFO - __main__ -   Step: 1216, LR: 8.32743810005053e-06, Loss: 0.561732292175293
 60%|█████▉    | 1217/2040 [11:55:58<7:55:27, 34.66s/it]08/15/2023 22:16:04 - INFO - __main__ -   Step: 1217, LR: 8.317331985851441e-06, Loss: 0.5005321502685547
 60%|█████▉    | 1218/2040 [11:56:33<7:55:05, 34.68s/it]08/15/2023 22:16:39 - INFO - __main__ -   Step: 1218, LR: 8.30722587165235e-06, Loss: 0.4936411678791046
 60%|█████▉    | 1219/2040 [11:57:07<7:53:19, 34.59s/it]08/15/2023 22:17:13 - INFO - __main__ -   Step: 1219, LR: 8.297119757453261e-06, Loss: 0.48256969451904297
 60%|█████▉    | 1220/2040 [11:57:42<7:53:13, 34.63s/it]08/15/2023 22:17:48 - INFO - __main__ -   Step: 1220, LR: 8.287013643254168e-06, Loss: 0.5285911560058594
 60%|█████▉    | 1221/2040 [11:58:17<7:51:37, 34.55s/it]08/15/2023 22:18:22 - INFO - __main__ -   Step: 1221, LR: 8.27690752905508e-06, Loss: 0.45666587352752686
 60%|█████▉    | 1222/2040 [11:58:51<7:51:40, 34.60s/it]08/15/2023 22:18:57 - INFO - __main__ -   Step: 1222, LR: 8.266801414855988e-06, Loss: 0.5313607454299927
 60%|█████▉    | 1223/2040 [11:59:26<7:50:43, 34.57s/it]08/15/2023 22:19:32 - INFO - __main__ -   Step: 1223, LR: 8.256695300656899e-06, Loss: 0.5398421287536621
 60%|██████    | 1224/2040 [12:00:01<7:51:05, 34.64s/it]08/15/2023 22:20:06 - INFO - __main__ -   Step: 1224, LR: 8.246589186457808e-06, Loss: 0.5025802850723267
 60%|██████    | 1225/2040 [12:00:35<7:50:42, 34.65s/it]08/15/2023 22:20:41 - INFO - __main__ -   Step: 1225, LR: 8.236483072258717e-06, Loss: 0.43611282110214233
 60%|██████    | 1226/2040 [12:01:10<7:51:52, 34.78s/it]08/15/2023 22:21:16 - INFO - __main__ -   Step: 1226, LR: 8.226376958059626e-06, Loss: 0.5440884232521057
 60%|██████    | 1227/2040 [12:01:45<7:52:13, 34.85s/it]08/15/2023 22:21:51 - INFO - __main__ -   Step: 1227, LR: 8.216270843860537e-06, Loss: 0.48333311080932617
 60%|██████    | 1228/2040 [12:02:20<7:50:47, 34.79s/it]08/15/2023 22:22:26 - INFO - __main__ -   Step: 1228, LR: 8.206164729661446e-06, Loss: 0.45791229605674744
 60%|██████    | 1229/2040 [12:02:55<7:50:01, 34.77s/it]08/15/2023 22:23:01 - INFO - __main__ -   Step: 1229, LR: 8.196058615462355e-06, Loss: 0.4606069028377533
 60%|██████    | 1230/2040 [12:03:29<7:49:12, 34.76s/it]08/15/2023 22:23:35 - INFO - __main__ -   Step: 1230, LR: 8.185952501263264e-06, Loss: 0.49521899223327637
 60%|██████    | 1231/2040 [12:04:04<7:47:46, 34.69s/it]08/15/2023 22:24:10 - INFO - __main__ -   Step: 1231, LR: 8.175846387064175e-06, Loss: 0.5262372493743896
 60%|██████    | 1232/2040 [12:04:39<7:49:49, 34.89s/it]08/15/2023 22:24:45 - INFO - __main__ -   Step: 1232, LR: 8.165740272865084e-06, Loss: 0.5432643890380859
 60%|██████    | 1233/2040 [12:05:21<8:15:46, 36.86s/it]08/15/2023 22:25:27 - INFO - __main__ -   Step: 1233, LR: 8.155634158665993e-06, Loss: 0.5027371048927307
 60%|██████    | 1234/2040 [12:06:08<8:58:54, 40.12s/it]08/15/2023 22:26:14 - INFO - __main__ -   Step: 1234, LR: 8.145528044466902e-06, Loss: 0.44825711846351624
 61%|██████    | 1235/2040 [12:06:53<9:15:26, 41.40s/it]08/15/2023 22:26:59 - INFO - __main__ -   Step: 1235, LR: 8.135421930267813e-06, Loss: 0.49615931510925293
 61%|██████    | 1236/2040 [12:07:32<9:07:02, 40.82s/it]08/15/2023 22:27:38 - INFO - __main__ -   Step: 1236, LR: 8.125315816068722e-06, Loss: 0.46254292130470276
 61%|██████    | 1237/2040 [12:08:12<9:01:53, 40.49s/it]08/15/2023 22:28:18 - INFO - __main__ -   Step: 1237, LR: 8.115209701869631e-06, Loss: 0.5780579447746277
 61%|██████    | 1238/2040 [12:08:47<8:39:24, 38.86s/it]08/15/2023 22:28:53 - INFO - __main__ -   Step: 1238, LR: 8.10510358767054e-06, Loss: 0.47621971368789673
 61%|██████    | 1239/2040 [12:09:22<8:22:33, 37.64s/it]08/15/2023 22:29:28 - INFO - __main__ -   Step: 1239, LR: 8.094997473471451e-06, Loss: 0.45435184240341187
 61%|██████    | 1240/2040 [12:09:57<8:11:05, 36.83s/it]08/15/2023 22:30:03 - INFO - __main__ -   Step: 1240, LR: 8.08489135927236e-06, Loss: 0.45053428411483765
 61%|██████    | 1241/2040 [12:10:32<8:02:08, 36.21s/it]08/15/2023 22:30:37 - INFO - __main__ -   Step: 1241, LR: 8.07478524507327e-06, Loss: 0.47385314106941223
 61%|██████    | 1242/2040 [12:11:06<7:54:14, 35.66s/it]08/15/2023 22:31:12 - INFO - __main__ -   Step: 1242, LR: 8.06467913087418e-06, Loss: 0.5066816806793213
 61%|██████    | 1243/2040 [12:11:41<7:49:43, 35.36s/it]08/15/2023 22:31:47 - INFO - __main__ -   Step: 1243, LR: 8.054573016675089e-06, Loss: 0.48540008068084717
 61%|██████    | 1244/2040 [12:12:15<7:46:05, 35.13s/it]08/15/2023 22:32:21 - INFO - __main__ -   Step: 1244, LR: 8.044466902476e-06, Loss: 0.5134315490722656
 61%|██████    | 1245/2040 [12:12:50<7:43:25, 34.98s/it]08/15/2023 22:32:56 - INFO - __main__ -   Step: 1245, LR: 8.034360788276909e-06, Loss: 0.48291128873825073
 61%|██████    | 1246/2040 [12:13:25<7:42:24, 34.94s/it]08/15/2023 22:33:31 - INFO - __main__ -   Step: 1246, LR: 8.024254674077818e-06, Loss: 0.4847288727760315
 61%|██████    | 1247/2040 [12:13:59<7:39:56, 34.80s/it]08/15/2023 22:34:05 - INFO - __main__ -   Step: 1247, LR: 8.014148559878727e-06, Loss: 0.44800829887390137
 61%|██████    | 1248/2040 [12:14:34<7:37:34, 34.67s/it]08/15/2023 22:34:39 - INFO - __main__ -   Step: 1248, LR: 8.004042445679638e-06, Loss: 0.5008322596549988
 61%|██████    | 1249/2040 [12:15:08<7:36:24, 34.62s/it]08/15/2023 22:35:14 - INFO - __main__ -   Step: 1249, LR: 7.993936331480547e-06, Loss: 0.5929514169692993
 61%|██████▏   | 1250/2040 [12:15:43<7:36:29, 34.67s/it]08/15/2023 22:35:49 - INFO - __main__ -   Step: 1250, LR: 7.983830217281456e-06, Loss: 0.49989503622055054
 61%|██████▏   | 1251/2040 [12:16:18<7:39:25, 34.94s/it]08/15/2023 22:36:24 - INFO - __main__ -   Step: 1251, LR: 7.973724103082365e-06, Loss: 0.4187847077846527
 61%|██████▏   | 1252/2040 [12:16:53<7:39:17, 34.97s/it]08/15/2023 22:36:59 - INFO - __main__ -   Step: 1252, LR: 7.963617988883276e-06, Loss: 0.4586225748062134
 61%|██████▏   | 1253/2040 [12:17:28<7:35:09, 34.70s/it]08/15/2023 22:37:33 - INFO - __main__ -   Step: 1253, LR: 7.953511874684185e-06, Loss: 0.5447472333908081
 61%|██████▏   | 1254/2040 [12:18:02<7:33:03, 34.59s/it]08/15/2023 22:38:08 - INFO - __main__ -   Step: 1254, LR: 7.943405760485094e-06, Loss: 0.48337262868881226
 62%|██████▏   | 1255/2040 [12:18:36<7:31:31, 34.51s/it]08/15/2023 22:38:42 - INFO - __main__ -   Step: 1255, LR: 7.933299646286003e-06, Loss: 0.48666301369667053
 62%|██████▏   | 1256/2040 [12:19:11<7:30:19, 34.46s/it]08/15/2023 22:39:16 - INFO - __main__ -   Step: 1256, LR: 7.923193532086914e-06, Loss: 0.5192984342575073
 62%|██████▏   | 1257/2040 [12:19:45<7:30:51, 34.55s/it]08/15/2023 22:39:51 - INFO - __main__ -   Step: 1257, LR: 7.913087417887823e-06, Loss: 0.44499439001083374
 62%|██████▏   | 1258/2040 [12:20:19<7:28:33, 34.42s/it]08/15/2023 22:40:25 - INFO - __main__ -   Step: 1258, LR: 7.902981303688732e-06, Loss: 0.5190088152885437
 62%|██████▏   | 1259/2040 [12:20:54<7:27:29, 34.38s/it]08/15/2023 22:41:00 - INFO - __main__ -   Step: 1259, LR: 7.892875189489641e-06, Loss: 0.5441259741783142
 62%|██████▏   | 1260/2040 [12:21:28<7:27:29, 34.42s/it]08/15/2023 22:41:34 - INFO - __main__ -   Step: 1260, LR: 7.882769075290552e-06, Loss: 0.5285305976867676
 62%|██████▏   | 1261/2040 [12:22:03<7:27:08, 34.44s/it]08/15/2023 22:42:09 - INFO - __main__ -   Step: 1261, LR: 7.87266296109146e-06, Loss: 0.48195311427116394
 62%|██████▏   | 1262/2040 [12:22:38<7:28:53, 34.62s/it]08/15/2023 22:42:44 - INFO - __main__ -   Step: 1262, LR: 7.862556846892372e-06, Loss: 0.47652119398117065
 62%|██████▏   | 1263/2040 [12:23:12<7:28:54, 34.66s/it]08/15/2023 22:43:18 - INFO - __main__ -   Step: 1263, LR: 7.852450732693279e-06, Loss: 0.524374783039093
 62%|██████▏   | 1264/2040 [12:23:47<7:26:31, 34.52s/it]08/15/2023 22:43:53 - INFO - __main__ -   Step: 1264, LR: 7.84234461849419e-06, Loss: 0.5861643552780151
 62%|██████▏   | 1265/2040 [12:24:21<7:25:51, 34.52s/it]08/15/2023 22:44:27 - INFO - __main__ -   Step: 1265, LR: 7.832238504295099e-06, Loss: 0.49337688088417053
 62%|██████▏   | 1266/2040 [12:24:56<7:25:08, 34.51s/it]08/15/2023 22:45:02 - INFO - __main__ -   Step: 1266, LR: 7.82213239009601e-06, Loss: 0.49844682216644287
 62%|██████▏   | 1267/2040 [12:25:30<7:25:25, 34.57s/it]08/15/2023 22:45:36 - INFO - __main__ -   Step: 1267, LR: 7.812026275896919e-06, Loss: 0.5094282627105713
 62%|██████▏   | 1268/2040 [12:26:06<7:27:21, 34.77s/it]08/15/2023 22:46:12 - INFO - __main__ -   Step: 1268, LR: 7.801920161697828e-06, Loss: 0.5189576148986816
 62%|██████▏   | 1269/2040 [12:26:40<7:26:22, 34.74s/it]08/15/2023 22:46:46 - INFO - __main__ -   Step: 1269, LR: 7.791814047498737e-06, Loss: 0.5200687050819397
 62%|██████▏   | 1270/2040 [12:27:15<7:24:16, 34.62s/it]08/15/2023 22:47:21 - INFO - __main__ -   Step: 1270, LR: 7.781707933299647e-06, Loss: 0.4810691475868225
 62%|██████▏   | 1271/2040 [12:27:49<7:21:39, 34.46s/it]08/15/2023 22:47:55 - INFO - __main__ -   Step: 1271, LR: 7.771601819100557e-06, Loss: 0.5031740069389343
 62%|██████▏   | 1272/2040 [12:28:23<7:21:44, 34.51s/it]08/15/2023 22:48:29 - INFO - __main__ -   Step: 1272, LR: 7.761495704901466e-06, Loss: 0.4838147759437561
 62%|██████▏   | 1273/2040 [12:28:58<7:22:51, 34.64s/it]08/15/2023 22:49:04 - INFO - __main__ -   Step: 1273, LR: 7.751389590702375e-06, Loss: 0.46173179149627686
 62%|██████▏   | 1274/2040 [12:29:33<7:23:04, 34.71s/it]08/15/2023 22:49:39 - INFO - __main__ -   Step: 1274, LR: 7.741283476503285e-06, Loss: 0.46052485704421997
 62%|██████▎   | 1275/2040 [12:30:08<7:21:12, 34.60s/it]08/15/2023 22:50:13 - INFO - __main__ -   Step: 1275, LR: 7.731177362304195e-06, Loss: 0.43004003167152405
 63%|██████▎   | 1276/2040 [12:30:42<7:18:43, 34.46s/it]08/15/2023 22:50:48 - INFO - __main__ -   Step: 1276, LR: 7.721071248105104e-06, Loss: 0.48252972960472107
 63%|██████▎   | 1277/2040 [12:31:16<7:17:49, 34.43s/it]08/15/2023 22:51:22 - INFO - __main__ -   Step: 1277, LR: 7.710965133906014e-06, Loss: 0.5421711802482605
 63%|██████▎   | 1278/2040 [12:31:51<7:17:47, 34.47s/it]08/15/2023 22:51:56 - INFO - __main__ -   Step: 1278, LR: 7.700859019706923e-06, Loss: 0.5284057855606079
 63%|██████▎   | 1279/2040 [12:32:26<7:19:28, 34.65s/it]08/15/2023 22:52:32 - INFO - __main__ -   Step: 1279, LR: 7.690752905507833e-06, Loss: 0.5386554598808289
 63%|██████▎   | 1280/2040 [12:33:00<7:19:19, 34.68s/it]08/15/2023 22:53:06 - INFO - __main__ -   Step: 1280, LR: 7.680646791308742e-06, Loss: 0.4753662943840027
 63%|██████▎   | 1281/2040 [12:33:34<7:16:30, 34.51s/it]08/15/2023 22:53:40 - INFO - __main__ -   Step: 1281, LR: 7.670540677109652e-06, Loss: 0.4747655987739563
 63%|██████▎   | 1282/2040 [12:34:08<7:14:02, 34.36s/it]08/15/2023 22:54:14 - INFO - __main__ -   Step: 1282, LR: 7.660434562910561e-06, Loss: 0.5249859690666199
 63%|██████▎   | 1283/2040 [12:34:43<7:12:58, 34.32s/it]08/15/2023 22:54:49 - INFO - __main__ -   Step: 1283, LR: 7.650328448711472e-06, Loss: 0.4456639289855957
 63%|██████▎   | 1284/2040 [12:35:17<7:13:50, 34.43s/it]08/15/2023 22:55:23 - INFO - __main__ -   Step: 1284, LR: 7.640222334512381e-06, Loss: 0.4307910203933716
 63%|██████▎   | 1285/2040 [12:35:52<7:15:33, 34.61s/it]08/15/2023 22:55:58 - INFO - __main__ -   Step: 1285, LR: 7.63011622031329e-06, Loss: 0.4519171416759491
 63%|██████▎   | 1286/2040 [12:36:27<7:15:56, 34.69s/it]08/15/2023 22:56:33 - INFO - __main__ -   Step: 1286, LR: 7.6200101061141994e-06, Loss: 0.5267559885978699
 63%|██████▎   | 1287/2040 [12:37:02<7:15:30, 34.70s/it]08/15/2023 22:57:08 - INFO - __main__ -   Step: 1287, LR: 7.609903991915109e-06, Loss: 0.45707452297210693
 63%|██████▎   | 1288/2040 [12:37:37<7:17:40, 34.92s/it]08/15/2023 22:57:43 - INFO - __main__ -   Step: 1288, LR: 7.5997978777160184e-06, Loss: 0.42801082134246826
 63%|██████▎   | 1289/2040 [12:38:12<7:16:29, 34.87s/it]08/15/2023 22:58:18 - INFO - __main__ -   Step: 1289, LR: 7.589691763516928e-06, Loss: 0.47715863585472107
 63%|██████▎   | 1290/2040 [12:38:47<7:16:58, 34.96s/it]08/15/2023 22:58:53 - INFO - __main__ -   Step: 1290, LR: 7.5795856493178374e-06, Loss: 0.5539569854736328
 63%|██████▎   | 1291/2040 [12:39:22<7:15:50, 34.91s/it]08/15/2023 22:59:28 - INFO - __main__ -   Step: 1291, LR: 7.569479535118748e-06, Loss: 0.48723089694976807
 63%|██████▎   | 1292/2040 [12:39:57<7:13:55, 34.81s/it]08/15/2023 23:00:03 - INFO - __main__ -   Step: 1292, LR: 7.559373420919656e-06, Loss: 0.5467692613601685
 63%|██████▎   | 1293/2040 [12:40:32<7:13:37, 34.83s/it]08/15/2023 23:00:38 - INFO - __main__ -   Step: 1293, LR: 7.549267306720567e-06, Loss: 0.5916967391967773
 63%|██████▎   | 1294/2040 [12:41:06<7:12:02, 34.75s/it]08/15/2023 23:01:12 - INFO - __main__ -   Step: 1294, LR: 7.539161192521475e-06, Loss: 0.4690244793891907
 63%|██████▎   | 1295/2040 [12:41:41<7:12:02, 34.80s/it]08/15/2023 23:01:47 - INFO - __main__ -   Step: 1295, LR: 7.529055078322386e-06, Loss: 0.47695034742355347
 64%|██████▎   | 1296/2040 [12:42:16<7:13:17, 34.94s/it]08/15/2023 23:02:22 - INFO - __main__ -   Step: 1296, LR: 7.518948964123295e-06, Loss: 0.4703446924686432
 64%|██████▎   | 1297/2040 [12:42:51<7:12:25, 34.92s/it]08/15/2023 23:02:57 - INFO - __main__ -   Step: 1297, LR: 7.508842849924205e-06, Loss: 0.45910099148750305
 64%|██████▎   | 1298/2040 [12:43:26<7:11:18, 34.88s/it]08/15/2023 23:03:32 - INFO - __main__ -   Step: 1298, LR: 7.498736735725114e-06, Loss: 0.49756920337677
 64%|██████▎   | 1299/2040 [12:44:01<7:10:13, 34.84s/it]08/15/2023 23:04:07 - INFO - __main__ -   Step: 1299, LR: 7.488630621526024e-06, Loss: 0.4948413372039795
 64%|██████▎   | 1300/2040 [12:44:36<7:09:27, 34.82s/it]08/15/2023 23:04:41 - INFO - __main__ -   Step: 1300, LR: 7.478524507326933e-06, Loss: 0.5679005980491638
08/15/2023 23:04:41 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300
08/15/2023 23:04:41 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-15 23:04:41,981] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-15 23:04:41,987] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-15 23:04:41,987] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-15 23:04:41,987] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-15 23:04:41,987] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-15 23:04:41,988] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-15 23:04:41,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-15 23:04:41,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-15 23:04:41,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-15 23:04:41,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-15 23:04:42,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-15 23:04:42,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-15 23:04:42,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-15 23:04:42,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-15 23:04:42,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-15 23:05:05,045] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-15 23:05:05,046] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-15 23:05:05,065] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-15 23:05:05,065] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-15 23:05:05,453] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-15 23:05:05,453] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-15 23:05:06,172] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-15 23:05:06,172] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-15 23:05:06,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 23:05:06,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 23:05:06,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-15 23:05:06,178] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/15/2023 23:05:06 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/pytorch_model
08/15/2023 23:05:06 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/scheduler.bin
08/15/2023 23:05:06 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1300/random_states_0.pkl
 64%|██████▍   | 1301/2040 [12:45:34<8:35:47, 41.88s/it]08/15/2023 23:05:40 - INFO - __main__ -   Step: 1301, LR: 7.468418393127843e-06, Loss: 0.4693247675895691
 64%|██████▍   | 1302/2040 [12:46:09<8:11:08, 39.93s/it]08/15/2023 23:06:15 - INFO - __main__ -   Step: 1302, LR: 7.458312278928752e-06, Loss: 0.5629799365997314
 64%|██████▍   | 1303/2040 [12:46:44<7:52:08, 38.44s/it]08/15/2023 23:06:50 - INFO - __main__ -   Step: 1303, LR: 7.448206164729662e-06, Loss: 0.43538904190063477
 64%|██████▍   | 1304/2040 [12:47:20<7:40:07, 37.51s/it]08/15/2023 23:07:26 - INFO - __main__ -   Step: 1304, LR: 7.438100050530571e-06, Loss: 0.6119784116744995
 64%|██████▍   | 1305/2040 [12:47:55<7:30:11, 36.75s/it]08/15/2023 23:08:00 - INFO - __main__ -   Step: 1305, LR: 7.427993936331481e-06, Loss: 0.470016747713089
 64%|██████▍   | 1306/2040 [12:48:30<7:22:55, 36.21s/it]08/15/2023 23:08:35 - INFO - __main__ -   Step: 1306, LR: 7.41788782213239e-06, Loss: 0.5799479484558105
 64%|██████▍   | 1307/2040 [12:49:04<7:16:15, 35.71s/it]08/15/2023 23:09:10 - INFO - __main__ -   Step: 1307, LR: 7.4077817079333e-06, Loss: 0.4879431128501892
 64%|██████▍   | 1308/2040 [12:49:39<7:13:33, 35.54s/it]08/15/2023 23:09:45 - INFO - __main__ -   Step: 1308, LR: 7.397675593734209e-06, Loss: 0.4686657786369324
 64%|██████▍   | 1309/2040 [12:50:15<7:12:18, 35.48s/it]08/15/2023 23:10:20 - INFO - __main__ -   Step: 1309, LR: 7.387569479535119e-06, Loss: 0.5039453506469727
 64%|██████▍   | 1310/2040 [12:50:50<7:12:36, 35.56s/it]08/15/2023 23:10:56 - INFO - __main__ -   Step: 1310, LR: 7.377463365336028e-06, Loss: 0.5025726556777954
 64%|██████▍   | 1311/2040 [12:51:25<7:10:32, 35.44s/it]08/15/2023 23:11:31 - INFO - __main__ -   Step: 1311, LR: 7.367357251136938e-06, Loss: 0.4991665780544281
 64%|██████▍   | 1312/2040 [12:52:00<7:06:48, 35.18s/it]08/15/2023 23:12:06 - INFO - __main__ -   Step: 1312, LR: 7.357251136937849e-06, Loss: 0.5077375769615173
 64%|██████▍   | 1313/2040 [12:52:36<7:07:26, 35.28s/it]08/15/2023 23:12:41 - INFO - __main__ -   Step: 1313, LR: 7.347145022738757e-06, Loss: 0.5210599899291992
 64%|██████▍   | 1314/2040 [12:53:10<7:05:21, 35.15s/it]08/15/2023 23:13:16 - INFO - __main__ -   Step: 1314, LR: 7.337038908539668e-06, Loss: 0.5031219720840454
 64%|██████▍   | 1315/2040 [12:53:46<7:04:47, 35.16s/it]08/15/2023 23:13:51 - INFO - __main__ -   Step: 1315, LR: 7.326932794340577e-06, Loss: 0.4457205533981323
 65%|██████▍   | 1316/2040 [12:54:21<7:03:55, 35.13s/it]08/15/2023 23:14:27 - INFO - __main__ -   Step: 1316, LR: 7.316826680141487e-06, Loss: 0.4247063100337982
 65%|██████▍   | 1317/2040 [12:54:56<7:03:28, 35.14s/it]08/15/2023 23:15:02 - INFO - __main__ -   Step: 1317, LR: 7.306720565942396e-06, Loss: 0.48184800148010254
 65%|██████▍   | 1318/2040 [12:55:30<7:01:09, 35.00s/it]08/15/2023 23:15:36 - INFO - __main__ -   Step: 1318, LR: 7.296614451743306e-06, Loss: 0.42340534925460815
 65%|██████▍   | 1319/2040 [12:56:05<6:59:07, 34.88s/it]08/15/2023 23:16:11 - INFO - __main__ -   Step: 1319, LR: 7.286508337544215e-06, Loss: 0.5558627843856812
 65%|██████▍   | 1320/2040 [12:56:40<6:59:53, 34.99s/it]08/15/2023 23:16:46 - INFO - __main__ -   Step: 1320, LR: 7.276402223345125e-06, Loss: 0.601420521736145
 65%|██████▍   | 1321/2040 [12:57:15<6:58:46, 34.95s/it]08/15/2023 23:17:21 - INFO - __main__ -   Step: 1321, LR: 7.266296109146034e-06, Loss: 0.4359287619590759
 65%|██████▍   | 1322/2040 [12:57:51<7:00:32, 35.14s/it]08/15/2023 23:17:57 - INFO - __main__ -   Step: 1322, LR: 7.256189994946944e-06, Loss: 0.506946325302124
 65%|██████▍   | 1323/2040 [12:58:26<7:00:35, 35.20s/it]08/15/2023 23:18:32 - INFO - __main__ -   Step: 1323, LR: 7.246083880747853e-06, Loss: 0.5247712135314941
 65%|██████▍   | 1324/2040 [12:59:01<6:59:04, 35.12s/it]08/15/2023 23:19:07 - INFO - __main__ -   Step: 1324, LR: 7.235977766548763e-06, Loss: 0.4525197446346283
 65%|██████▍   | 1325/2040 [12:59:36<6:58:03, 35.08s/it]08/15/2023 23:19:42 - INFO - __main__ -   Step: 1325, LR: 7.225871652349672e-06, Loss: 0.5498341917991638
 65%|██████▌   | 1326/2040 [13:00:11<6:57:51, 35.11s/it]08/15/2023 23:20:17 - INFO - __main__ -   Step: 1326, LR: 7.215765538150582e-06, Loss: 0.4975614547729492
 65%|██████▌   | 1327/2040 [13:00:46<6:55:33, 34.97s/it]08/15/2023 23:20:52 - INFO - __main__ -   Step: 1327, LR: 7.205659423951491e-06, Loss: 0.4615074694156647
 65%|██████▌   | 1328/2040 [13:01:21<6:56:50, 35.13s/it]08/15/2023 23:21:27 - INFO - __main__ -   Step: 1328, LR: 7.195553309752401e-06, Loss: 0.5063775777816772
 65%|██████▌   | 1329/2040 [13:01:56<6:55:38, 35.08s/it]08/15/2023 23:22:02 - INFO - __main__ -   Step: 1329, LR: 7.18544719555331e-06, Loss: 0.5010161995887756
 65%|██████▌   | 1330/2040 [13:02:31<6:54:16, 35.01s/it]08/15/2023 23:22:37 - INFO - __main__ -   Step: 1330, LR: 7.17534108135422e-06, Loss: 0.5249330997467041
 65%|██████▌   | 1331/2040 [13:03:07<6:57:34, 35.34s/it]08/15/2023 23:23:13 - INFO - __main__ -   Step: 1331, LR: 7.165234967155129e-06, Loss: 0.4320055842399597
 65%|██████▌   | 1332/2040 [13:03:42<6:54:47, 35.15s/it]08/15/2023 23:23:48 - INFO - __main__ -   Step: 1332, LR: 7.155128852956039e-06, Loss: 0.5256460905075073
 65%|██████▌   | 1333/2040 [13:04:17<6:52:53, 35.04s/it]08/15/2023 23:24:23 - INFO - __main__ -   Step: 1333, LR: 7.145022738756948e-06, Loss: 0.4198667109012604
 65%|██████▌   | 1334/2040 [13:04:53<6:55:03, 35.27s/it]08/15/2023 23:24:58 - INFO - __main__ -   Step: 1334, LR: 7.134916624557859e-06, Loss: 0.4408365786075592
 65%|██████▌   | 1335/2040 [13:05:28<6:53:53, 35.22s/it]08/15/2023 23:25:34 - INFO - __main__ -   Step: 1335, LR: 7.124810510358767e-06, Loss: 0.4544292986392975
 65%|██████▌   | 1336/2040 [13:06:02<6:51:50, 35.10s/it]08/15/2023 23:26:08 - INFO - __main__ -   Step: 1336, LR: 7.114704396159678e-06, Loss: 0.5513261556625366
 66%|██████▌   | 1337/2040 [13:06:38<6:51:14, 35.10s/it]08/15/2023 23:26:43 - INFO - __main__ -   Step: 1337, LR: 7.104598281960586e-06, Loss: 0.48174577951431274
 66%|██████▌   | 1338/2040 [13:07:13<6:52:28, 35.25s/it]08/15/2023 23:27:19 - INFO - __main__ -   Step: 1338, LR: 7.094492167761497e-06, Loss: 0.44785887002944946
 66%|██████▌   | 1339/2040 [13:07:48<6:51:20, 35.21s/it]08/15/2023 23:27:54 - INFO - __main__ -   Step: 1339, LR: 7.084386053562406e-06, Loss: 0.48843449354171753
 66%|██████▌   | 1340/2040 [13:08:26<6:58:49, 35.90s/it]08/15/2023 23:28:32 - INFO - __main__ -   Step: 1340, LR: 7.074279939363316e-06, Loss: 0.5332363843917847
 66%|██████▌   | 1341/2040 [13:09:01<6:54:57, 35.62s/it]08/15/2023 23:29:07 - INFO - __main__ -   Step: 1341, LR: 7.064173825164225e-06, Loss: 0.5160655975341797
 66%|██████▌   | 1342/2040 [13:09:36<6:51:18, 35.36s/it]08/15/2023 23:29:41 - INFO - __main__ -   Step: 1342, LR: 7.054067710965135e-06, Loss: 0.45540186762809753
 66%|██████▌   | 1343/2040 [13:10:11<6:50:12, 35.31s/it]08/15/2023 23:30:17 - INFO - __main__ -   Step: 1343, LR: 7.043961596766044e-06, Loss: 0.4782559275627136
 66%|██████▌   | 1344/2040 [13:10:46<6:48:03, 35.18s/it]08/15/2023 23:30:51 - INFO - __main__ -   Step: 1344, LR: 7.033855482566954e-06, Loss: 0.5132306218147278
 66%|██████▌   | 1345/2040 [13:11:21<6:47:31, 35.18s/it]08/15/2023 23:31:27 - INFO - __main__ -   Step: 1345, LR: 7.023749368367863e-06, Loss: 0.48935630917549133
 66%|██████▌   | 1346/2040 [13:11:56<6:46:11, 35.12s/it]08/15/2023 23:32:02 - INFO - __main__ -   Step: 1346, LR: 7.013643254168773e-06, Loss: 0.5236256718635559
 66%|██████▌   | 1347/2040 [13:12:32<6:48:07, 35.34s/it]08/15/2023 23:32:37 - INFO - __main__ -   Step: 1347, LR: 7.0035371399696826e-06, Loss: 0.6052718758583069
 66%|██████▌   | 1348/2040 [13:13:06<6:45:30, 35.16s/it]08/15/2023 23:33:12 - INFO - __main__ -   Step: 1348, LR: 6.993431025770592e-06, Loss: 0.4661226272583008
 66%|██████▌   | 1349/2040 [13:13:41<6:43:14, 35.01s/it]08/15/2023 23:33:47 - INFO - __main__ -   Step: 1349, LR: 6.9833249115715016e-06, Loss: 0.5496102571487427
 66%|██████▌   | 1350/2040 [13:14:16<6:41:44, 34.93s/it]08/15/2023 23:34:22 - INFO - __main__ -   Step: 1350, LR: 6.973218797372411e-06, Loss: 0.459087073802948
 66%|██████▌   | 1351/2040 [13:14:51<6:42:45, 35.07s/it]08/15/2023 23:34:57 - INFO - __main__ -   Step: 1351, LR: 6.9631126831733206e-06, Loss: 0.493990033864975
 66%|██████▋   | 1352/2040 [13:15:26<6:40:47, 34.95s/it]08/15/2023 23:35:32 - INFO - __main__ -   Step: 1352, LR: 6.95300656897423e-06, Loss: 0.486167848110199
 66%|██████▋   | 1353/2040 [13:16:01<6:41:04, 35.03s/it]08/15/2023 23:36:07 - INFO - __main__ -   Step: 1353, LR: 6.9429004547751395e-06, Loss: 0.4349956810474396
 66%|██████▋   | 1354/2040 [13:16:36<6:38:57, 34.89s/it]08/15/2023 23:36:42 - INFO - __main__ -   Step: 1354, LR: 6.932794340576049e-06, Loss: 0.4938250780105591
 66%|██████▋   | 1355/2040 [13:17:11<6:39:16, 34.97s/it]08/15/2023 23:37:17 - INFO - __main__ -   Step: 1355, LR: 6.922688226376959e-06, Loss: 0.5624938011169434
 66%|██████▋   | 1356/2040 [13:17:46<6:38:10, 34.93s/it]08/15/2023 23:37:51 - INFO - __main__ -   Step: 1356, LR: 6.912582112177868e-06, Loss: 0.479560911655426
 67%|██████▋   | 1357/2040 [13:18:21<6:38:25, 35.00s/it]08/15/2023 23:38:27 - INFO - __main__ -   Step: 1357, LR: 6.902475997978778e-06, Loss: 0.5300754308700562
 67%|██████▋   | 1358/2040 [13:18:56<6:37:02, 34.93s/it]08/15/2023 23:39:01 - INFO - __main__ -   Step: 1358, LR: 6.8923698837796875e-06, Loss: 0.4913678765296936
 67%|██████▋   | 1359/2040 [13:19:32<6:40:59, 35.33s/it]08/15/2023 23:39:38 - INFO - __main__ -   Step: 1359, LR: 6.882263769580597e-06, Loss: 0.4649296998977661
 67%|██████▋   | 1360/2040 [13:20:11<6:54:52, 36.61s/it]08/15/2023 23:40:17 - INFO - __main__ -   Step: 1360, LR: 6.8721576553815065e-06, Loss: 0.35536089539527893
 67%|██████▋   | 1361/2040 [13:20:57<7:26:07, 39.42s/it]08/15/2023 23:41:03 - INFO - __main__ -   Step: 1361, LR: 6.862051541182416e-06, Loss: 0.40624183416366577
 67%|██████▋   | 1362/2040 [13:21:33<7:11:38, 38.20s/it]08/15/2023 23:41:39 - INFO - __main__ -   Step: 1362, LR: 6.8519454269833254e-06, Loss: 0.40119919180870056
 67%|██████▋   | 1363/2040 [13:22:08<7:01:11, 37.33s/it]08/15/2023 23:42:14 - INFO - __main__ -   Step: 1363, LR: 6.841839312784235e-06, Loss: 0.36809852719306946
 67%|██████▋   | 1364/2040 [13:22:43<6:53:07, 36.67s/it]08/15/2023 23:42:49 - INFO - __main__ -   Step: 1364, LR: 6.8317331985851444e-06, Loss: 0.42344245314598083
 67%|██████▋   | 1365/2040 [13:23:18<6:45:56, 36.08s/it]08/15/2023 23:43:24 - INFO - __main__ -   Step: 1365, LR: 6.821627084386054e-06, Loss: 0.4073018431663513
 67%|██████▋   | 1366/2040 [13:23:53<6:40:41, 35.67s/it]08/15/2023 23:43:58 - INFO - __main__ -   Step: 1366, LR: 6.8115209701869634e-06, Loss: 0.3619273006916046
 67%|██████▋   | 1367/2040 [13:24:28<6:39:57, 35.66s/it]08/15/2023 23:44:34 - INFO - __main__ -   Step: 1367, LR: 6.801414855987873e-06, Loss: 0.39441928267478943
 67%|██████▋   | 1368/2040 [13:25:04<6:38:59, 35.62s/it]08/15/2023 23:45:10 - INFO - __main__ -   Step: 1368, LR: 6.7913087417887824e-06, Loss: 0.43243712186813354
 67%|██████▋   | 1369/2040 [13:25:39<6:37:46, 35.57s/it]08/15/2023 23:45:45 - INFO - __main__ -   Step: 1369, LR: 6.781202627589692e-06, Loss: 0.33934539556503296
 67%|██████▋   | 1370/2040 [13:26:14<6:34:05, 35.29s/it]08/15/2023 23:46:20 - INFO - __main__ -   Step: 1370, LR: 6.771096513390601e-06, Loss: 0.34061333537101746
 67%|██████▋   | 1371/2040 [13:26:49<6:33:09, 35.26s/it]08/15/2023 23:46:55 - INFO - __main__ -   Step: 1371, LR: 6.760990399191511e-06, Loss: 0.3197508454322815
 67%|██████▋   | 1372/2040 [13:27:24<6:30:55, 35.11s/it]08/15/2023 23:47:30 - INFO - __main__ -   Step: 1372, LR: 6.75088428499242e-06, Loss: 0.44620081782341003
 67%|██████▋   | 1373/2040 [13:27:58<6:28:34, 34.95s/it]08/15/2023 23:48:04 - INFO - __main__ -   Step: 1373, LR: 6.74077817079333e-06, Loss: 0.46140211820602417
 67%|██████▋   | 1374/2040 [13:28:35<6:32:50, 35.39s/it]08/15/2023 23:48:41 - INFO - __main__ -   Step: 1374, LR: 6.730672056594239e-06, Loss: 0.41722869873046875
 67%|██████▋   | 1375/2040 [13:29:18<6:59:18, 37.83s/it]08/15/2023 23:49:24 - INFO - __main__ -   Step: 1375, LR: 6.720565942395149e-06, Loss: 0.37251847982406616
 67%|██████▋   | 1376/2040 [13:29:58<7:05:32, 38.45s/it]08/15/2023 23:50:04 - INFO - __main__ -   Step: 1376, LR: 6.710459828196058e-06, Loss: 0.33435776829719543
 68%|██████▊   | 1377/2040 [13:30:33<6:53:26, 37.42s/it]08/15/2023 23:50:39 - INFO - __main__ -   Step: 1377, LR: 6.700353713996969e-06, Loss: 0.34136879444122314
 68%|██████▊   | 1378/2040 [13:31:09<6:46:08, 36.81s/it]08/15/2023 23:51:14 - INFO - __main__ -   Step: 1378, LR: 6.690247599797877e-06, Loss: 0.35928475856781006
 68%|██████▊   | 1379/2040 [13:31:43<6:38:52, 36.21s/it]08/15/2023 23:51:49 - INFO - __main__ -   Step: 1379, LR: 6.680141485598788e-06, Loss: 0.3769994080066681
 68%|██████▊   | 1380/2040 [13:32:18<6:33:08, 35.74s/it]08/15/2023 23:52:24 - INFO - __main__ -   Step: 1380, LR: 6.670035371399696e-06, Loss: 0.3291403651237488
 68%|██████▊   | 1381/2040 [13:32:53<6:29:16, 35.44s/it]08/15/2023 23:52:59 - INFO - __main__ -   Step: 1381, LR: 6.659929257200607e-06, Loss: 0.3658466935157776
 68%|██████▊   | 1382/2040 [13:33:28<6:27:17, 35.32s/it]08/15/2023 23:53:34 - INFO - __main__ -   Step: 1382, LR: 6.649823143001517e-06, Loss: 0.41378694772720337
 68%|██████▊   | 1383/2040 [13:34:03<6:25:21, 35.19s/it]08/15/2023 23:54:09 - INFO - __main__ -   Step: 1383, LR: 6.639717028802426e-06, Loss: 0.3388528823852539
 68%|██████▊   | 1384/2040 [13:34:38<6:26:26, 35.35s/it]08/15/2023 23:54:44 - INFO - __main__ -   Step: 1384, LR: 6.629610914603336e-06, Loss: 0.4250503182411194
 68%|██████▊   | 1385/2040 [13:35:14<6:25:22, 35.30s/it]08/15/2023 23:55:20 - INFO - __main__ -   Step: 1385, LR: 6.619504800404245e-06, Loss: 0.3982045650482178
 68%|██████▊   | 1386/2040 [13:35:48<6:22:55, 35.13s/it]08/15/2023 23:55:54 - INFO - __main__ -   Step: 1386, LR: 6.609398686205155e-06, Loss: 0.34505003690719604
 68%|██████▊   | 1387/2040 [13:36:23<6:21:06, 35.02s/it]08/15/2023 23:56:29 - INFO - __main__ -   Step: 1387, LR: 6.599292572006064e-06, Loss: 0.4418448805809021
 68%|██████▊   | 1388/2040 [13:36:58<6:20:47, 35.04s/it]08/15/2023 23:57:04 - INFO - __main__ -   Step: 1388, LR: 6.589186457806974e-06, Loss: 0.39785870909690857
 68%|██████▊   | 1389/2040 [13:37:33<6:20:22, 35.06s/it]08/15/2023 23:57:39 - INFO - __main__ -   Step: 1389, LR: 6.579080343607883e-06, Loss: 0.42267921566963196
 68%|██████▊   | 1390/2040 [13:38:09<6:21:16, 35.19s/it]08/15/2023 23:58:15 - INFO - __main__ -   Step: 1390, LR: 6.568974229408793e-06, Loss: 0.42388999462127686
 68%|██████▊   | 1391/2040 [13:38:44<6:21:05, 35.23s/it]08/15/2023 23:58:50 - INFO - __main__ -   Step: 1391, LR: 6.558868115209702e-06, Loss: 0.4536451995372772
 68%|██████▊   | 1392/2040 [13:39:19<6:18:56, 35.09s/it]08/15/2023 23:59:25 - INFO - __main__ -   Step: 1392, LR: 6.548762001010612e-06, Loss: 0.3443566560745239
 68%|██████▊   | 1393/2040 [13:39:54<6:16:55, 34.95s/it]08/15/2023 23:59:59 - INFO - __main__ -   Step: 1393, LR: 6.538655886811521e-06, Loss: 0.4091275930404663
 68%|██████▊   | 1394/2040 [13:40:29<6:17:18, 35.04s/it]08/16/2023 00:00:35 - INFO - __main__ -   Step: 1394, LR: 6.528549772612431e-06, Loss: 0.3831826448440552
 68%|██████▊   | 1395/2040 [13:41:05<6:21:23, 35.48s/it]08/16/2023 00:01:11 - INFO - __main__ -   Step: 1395, LR: 6.51844365841334e-06, Loss: 0.42593514919281006
 68%|██████▊   | 1396/2040 [13:41:42<6:25:18, 35.90s/it]08/16/2023 00:01:48 - INFO - __main__ -   Step: 1396, LR: 6.50833754421425e-06, Loss: 0.3985019326210022
 68%|██████▊   | 1397/2040 [13:42:16<6:19:16, 35.39s/it]08/16/2023 00:02:22 - INFO - __main__ -   Step: 1397, LR: 6.498231430015159e-06, Loss: 0.34495115280151367
 69%|██████▊   | 1398/2040 [13:42:51<6:16:49, 35.22s/it]08/16/2023 00:02:57 - INFO - __main__ -   Step: 1398, LR: 6.48812531581607e-06, Loss: 0.3852618336677551
 69%|██████▊   | 1399/2040 [13:43:25<6:12:49, 34.90s/it]08/16/2023 00:03:31 - INFO - __main__ -   Step: 1399, LR: 6.478019201616978e-06, Loss: 0.337958425283432
 69%|██████▊   | 1400/2040 [13:44:00<6:11:24, 34.82s/it]08/16/2023 00:04:06 - INFO - __main__ -   Step: 1400, LR: 6.467913087417889e-06, Loss: 0.39522606134414673
08/16/2023 00:04:06 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400
08/16/2023 00:04:06 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-16 00:04:06,366] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-16 00:04:06,371] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-16 00:04:06,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-16 00:04:06,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-16 00:04:06,372] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-16 00:04:06,372] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-16 00:04:06,372] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-16 00:04:06,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-16 00:04:06,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-16 00:04:06,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-16 00:04:06,384] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-16 00:04:06,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-16 00:04:06,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-16 00:04:06,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-16 00:04:06,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-16 00:04:28,131] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-16 00:04:28,131] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-16 00:04:29,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-16 00:04:29,335] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-16 00:04:29,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-16 00:04:29,906] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-16 00:04:30,047] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-16 00:04:30,047] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-16 00:04:30,051] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 00:04:30,051] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 00:04:30,051] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 00:04:30,051] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/16/2023 00:04:30 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/pytorch_model
08/16/2023 00:04:30 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/scheduler.bin
08/16/2023 00:04:30 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1400/random_states_0.pkl
 69%|██████▊   | 1401/2040 [13:44:58<7:23:50, 41.68s/it]08/16/2023 00:05:04 - INFO - __main__ -   Step: 1401, LR: 6.457806973218798e-06, Loss: 0.420833557844162
 69%|██████▊   | 1402/2040 [13:45:33<7:04:19, 39.90s/it]08/16/2023 00:05:39 - INFO - __main__ -   Step: 1402, LR: 6.447700859019708e-06, Loss: 0.310465931892395
 69%|██████▉   | 1403/2040 [13:46:09<6:49:38, 38.58s/it]08/16/2023 00:06:15 - INFO - __main__ -   Step: 1403, LR: 6.437594744820617e-06, Loss: 0.30323144793510437
 69%|██████▉   | 1404/2040 [13:46:43<6:35:46, 37.34s/it]08/16/2023 00:06:49 - INFO - __main__ -   Step: 1404, LR: 6.427488630621527e-06, Loss: 0.39540308713912964
 69%|██████▉   | 1405/2040 [13:47:18<6:26:53, 36.56s/it]08/16/2023 00:07:24 - INFO - __main__ -   Step: 1405, LR: 6.417382516422436e-06, Loss: 0.3221891522407532
 69%|██████▉   | 1406/2040 [13:47:52<6:19:22, 35.90s/it]08/16/2023 00:07:58 - INFO - __main__ -   Step: 1406, LR: 6.407276402223346e-06, Loss: 0.3048652112483978
 69%|██████▉   | 1407/2040 [13:48:28<6:16:52, 35.72s/it]08/16/2023 00:08:34 - INFO - __main__ -   Step: 1407, LR: 6.397170288024255e-06, Loss: 0.42082950472831726
 69%|██████▉   | 1408/2040 [13:49:03<6:15:03, 35.61s/it]08/16/2023 00:09:09 - INFO - __main__ -   Step: 1408, LR: 6.387064173825165e-06, Loss: 0.37789493799209595
 69%|██████▉   | 1409/2040 [13:49:38<6:13:43, 35.54s/it]08/16/2023 00:09:44 - INFO - __main__ -   Step: 1409, LR: 6.376958059626074e-06, Loss: 0.3790055513381958
 69%|██████▉   | 1410/2040 [13:50:13<6:11:09, 35.35s/it]08/16/2023 00:10:19 - INFO - __main__ -   Step: 1410, LR: 6.366851945426984e-06, Loss: 0.37568119168281555
 69%|██████▉   | 1411/2040 [13:50:48<6:09:51, 35.28s/it]08/16/2023 00:10:54 - INFO - __main__ -   Step: 1411, LR: 6.356745831227893e-06, Loss: 0.3177123963832855
 69%|██████▉   | 1412/2040 [13:51:23<6:07:37, 35.12s/it]08/16/2023 00:11:29 - INFO - __main__ -   Step: 1412, LR: 6.346639717028803e-06, Loss: 0.39097917079925537
 69%|██████▉   | 1413/2040 [13:51:59<6:08:09, 35.23s/it]08/16/2023 00:12:05 - INFO - __main__ -   Step: 1413, LR: 6.336533602829712e-06, Loss: 0.3294380307197571
 69%|██████▉   | 1414/2040 [13:52:35<6:11:46, 35.63s/it]08/16/2023 00:12:41 - INFO - __main__ -   Step: 1414, LR: 6.326427488630622e-06, Loss: 0.37319836020469666
 69%|██████▉   | 1415/2040 [13:53:11<6:10:01, 35.52s/it]08/16/2023 00:13:16 - INFO - __main__ -   Step: 1415, LR: 6.316321374431531e-06, Loss: 0.47220173478126526
 69%|██████▉   | 1416/2040 [13:53:46<6:09:05, 35.49s/it]08/16/2023 00:13:52 - INFO - __main__ -   Step: 1416, LR: 6.306215260232441e-06, Loss: 0.3217660188674927
 69%|██████▉   | 1417/2040 [13:54:21<6:07:28, 35.39s/it]08/16/2023 00:14:27 - INFO - __main__ -   Step: 1417, LR: 6.296109146033352e-06, Loss: 0.34867051243782043
 70%|██████▉   | 1418/2040 [13:54:56<6:05:49, 35.29s/it]08/16/2023 00:15:02 - INFO - __main__ -   Step: 1418, LR: 6.28600303183426e-06, Loss: 0.4023457169532776
 70%|██████▉   | 1419/2040 [13:55:32<6:06:40, 35.43s/it]08/16/2023 00:15:38 - INFO - __main__ -   Step: 1419, LR: 6.275896917635171e-06, Loss: 0.3674060106277466
 70%|██████▉   | 1420/2040 [13:56:09<6:11:37, 35.96s/it]08/16/2023 00:16:15 - INFO - __main__ -   Step: 1420, LR: 6.26579080343608e-06, Loss: 0.34212493896484375
 70%|██████▉   | 1421/2040 [13:56:44<6:07:55, 35.66s/it]08/16/2023 00:16:50 - INFO - __main__ -   Step: 1421, LR: 6.25568468923699e-06, Loss: 0.4602225422859192
 70%|██████▉   | 1422/2040 [13:57:22<6:13:47, 36.29s/it]08/16/2023 00:17:28 - INFO - __main__ -   Step: 1422, LR: 6.245578575037899e-06, Loss: 0.3686745762825012
 70%|██████▉   | 1423/2040 [13:57:56<6:07:40, 35.75s/it]08/16/2023 00:18:02 - INFO - __main__ -   Step: 1423, LR: 6.2354724608388086e-06, Loss: 0.3784692883491516
 70%|██████▉   | 1424/2040 [13:58:33<6:08:21, 35.88s/it]08/16/2023 00:18:38 - INFO - __main__ -   Step: 1424, LR: 6.225366346639718e-06, Loss: 0.34084999561309814
 70%|██████▉   | 1425/2040 [13:59:07<6:03:48, 35.49s/it]08/16/2023 00:19:13 - INFO - __main__ -   Step: 1425, LR: 6.2152602324406276e-06, Loss: 0.36799901723861694
 70%|██████▉   | 1426/2040 [13:59:42<6:01:36, 35.34s/it]08/16/2023 00:19:48 - INFO - __main__ -   Step: 1426, LR: 6.205154118241537e-06, Loss: 0.3631099760532379
 70%|██████▉   | 1427/2040 [14:00:17<6:00:48, 35.32s/it]08/16/2023 00:20:23 - INFO - __main__ -   Step: 1427, LR: 6.1950480040424466e-06, Loss: 0.3041874170303345
 70%|███████   | 1428/2040 [14:00:52<5:59:27, 35.24s/it]08/16/2023 00:20:58 - INFO - __main__ -   Step: 1428, LR: 6.184941889843356e-06, Loss: 0.39349037408828735
 70%|███████   | 1429/2040 [14:01:28<6:01:21, 35.49s/it]08/16/2023 00:21:34 - INFO - __main__ -   Step: 1429, LR: 6.1748357756442656e-06, Loss: 0.33983850479125977
 70%|███████   | 1430/2040 [14:02:04<6:01:41, 35.58s/it]08/16/2023 00:22:10 - INFO - __main__ -   Step: 1430, LR: 6.164729661445175e-06, Loss: 0.33782023191452026
 70%|███████   | 1431/2040 [14:02:39<5:59:35, 35.43s/it]08/16/2023 00:22:45 - INFO - __main__ -   Step: 1431, LR: 6.1546235472460846e-06, Loss: 0.37695688009262085
 70%|███████   | 1432/2040 [14:03:16<6:02:11, 35.74s/it]08/16/2023 00:23:22 - INFO - __main__ -   Step: 1432, LR: 6.144517433046994e-06, Loss: 0.30260106921195984
 70%|███████   | 1433/2040 [14:03:51<6:00:56, 35.68s/it]08/16/2023 00:23:57 - INFO - __main__ -   Step: 1433, LR: 6.1344113188479035e-06, Loss: 0.36457955837249756
 70%|███████   | 1434/2040 [14:04:26<5:58:18, 35.48s/it]08/16/2023 00:24:32 - INFO - __main__ -   Step: 1434, LR: 6.124305204648813e-06, Loss: 0.3201771378517151
 70%|███████   | 1435/2040 [14:05:01<5:56:13, 35.33s/it]08/16/2023 00:25:07 - INFO - __main__ -   Step: 1435, LR: 6.1141990904497225e-06, Loss: 0.37583041191101074
 70%|███████   | 1436/2040 [14:05:36<5:54:14, 35.19s/it]08/16/2023 00:25:42 - INFO - __main__ -   Step: 1436, LR: 6.104092976250632e-06, Loss: 0.3905753493309021
 70%|███████   | 1437/2040 [14:06:11<5:52:18, 35.06s/it]08/16/2023 00:26:17 - INFO - __main__ -   Step: 1437, LR: 6.0939868620515415e-06, Loss: 0.359935462474823
 70%|███████   | 1438/2040 [14:06:48<5:56:29, 35.53s/it]08/16/2023 00:26:54 - INFO - __main__ -   Step: 1438, LR: 6.083880747852451e-06, Loss: 0.3822743594646454
 71%|███████   | 1439/2040 [14:07:33<6:26:53, 38.62s/it]08/16/2023 00:27:39 - INFO - __main__ -   Step: 1439, LR: 6.073774633653361e-06, Loss: 0.39216122031211853
 71%|███████   | 1440/2040 [14:08:11<6:22:54, 38.29s/it]08/16/2023 00:28:17 - INFO - __main__ -   Step: 1440, LR: 6.06366851945427e-06, Loss: 0.34317001700401306
 71%|███████   | 1441/2040 [14:08:52<6:31:12, 39.19s/it]08/16/2023 00:28:58 - INFO - __main__ -   Step: 1441, LR: 6.05356240525518e-06, Loss: 0.36276131868362427
 71%|███████   | 1442/2040 [14:09:36<6:43:16, 40.46s/it]08/16/2023 00:29:42 - INFO - __main__ -   Step: 1442, LR: 6.043456291056089e-06, Loss: 0.37298232316970825
 71%|███████   | 1443/2040 [14:10:13<6:34:11, 39.62s/it]08/16/2023 00:30:19 - INFO - __main__ -   Step: 1443, LR: 6.033350176856999e-06, Loss: 0.2848270535469055
 71%|███████   | 1444/2040 [14:10:54<6:37:46, 40.04s/it]08/16/2023 00:31:00 - INFO - __main__ -   Step: 1444, LR: 6.0232440626579084e-06, Loss: 0.3233467936515808
 71%|███████   | 1445/2040 [14:11:33<6:31:30, 39.48s/it]08/16/2023 00:31:38 - INFO - __main__ -   Step: 1445, LR: 6.013137948458818e-06, Loss: 0.5110746622085571
 71%|███████   | 1446/2040 [14:12:10<6:24:30, 38.84s/it]08/16/2023 00:32:16 - INFO - __main__ -   Step: 1446, LR: 6.0030318342597274e-06, Loss: 0.3483547866344452
 71%|███████   | 1447/2040 [14:12:46<6:17:08, 38.16s/it]08/16/2023 00:32:52 - INFO - __main__ -   Step: 1447, LR: 5.992925720060637e-06, Loss: 0.3669857382774353
 71%|███████   | 1448/2040 [14:13:25<6:17:34, 38.27s/it]08/16/2023 00:33:31 - INFO - __main__ -   Step: 1448, LR: 5.9828196058615464e-06, Loss: 0.3891598582267761
 71%|███████   | 1449/2040 [14:14:01<6:11:33, 37.72s/it]08/16/2023 00:34:07 - INFO - __main__ -   Step: 1449, LR: 5.972713491662456e-06, Loss: 0.35677146911621094
 71%|███████   | 1450/2040 [14:14:40<6:14:53, 38.12s/it]08/16/2023 00:34:46 - INFO - __main__ -   Step: 1450, LR: 5.962607377463366e-06, Loss: 0.3770318031311035
 71%|███████   | 1451/2040 [14:15:17<6:10:16, 37.72s/it]08/16/2023 00:35:23 - INFO - __main__ -   Step: 1451, LR: 5.952501263264275e-06, Loss: 0.3860521912574768
 71%|███████   | 1452/2040 [14:15:56<6:14:02, 38.17s/it]08/16/2023 00:36:02 - INFO - __main__ -   Step: 1452, LR: 5.942395149065185e-06, Loss: 0.33245715498924255
 71%|███████   | 1453/2040 [14:16:33<6:08:16, 37.64s/it]08/16/2023 00:36:39 - INFO - __main__ -   Step: 1453, LR: 5.932289034866094e-06, Loss: 0.41938573122024536
 71%|███████▏  | 1454/2040 [14:17:11<6:10:15, 37.91s/it]08/16/2023 00:37:17 - INFO - __main__ -   Step: 1454, LR: 5.922182920667004e-06, Loss: 0.38239169120788574
 71%|███████▏  | 1455/2040 [14:17:48<6:04:51, 37.42s/it]08/16/2023 00:37:54 - INFO - __main__ -   Step: 1455, LR: 5.912076806467913e-06, Loss: 0.33180156350135803
 71%|███████▏  | 1456/2040 [14:18:23<5:58:56, 36.88s/it]08/16/2023 00:38:29 - INFO - __main__ -   Step: 1456, LR: 5.901970692268823e-06, Loss: 0.39102596044540405
 71%|███████▏  | 1457/2040 [14:19:00<5:57:12, 36.76s/it]08/16/2023 00:39:06 - INFO - __main__ -   Step: 1457, LR: 5.891864578069732e-06, Loss: 0.3974347710609436
 71%|███████▏  | 1458/2040 [14:19:36<5:53:48, 36.48s/it]08/16/2023 00:39:42 - INFO - __main__ -   Step: 1458, LR: 5.881758463870642e-06, Loss: 0.3538772761821747
 72%|███████▏  | 1459/2040 [14:20:12<5:52:06, 36.36s/it]08/16/2023 00:40:18 - INFO - __main__ -   Step: 1459, LR: 5.871652349671551e-06, Loss: 0.37186843156814575
 72%|███████▏  | 1460/2040 [14:20:47<5:49:05, 36.11s/it]08/16/2023 00:40:53 - INFO - __main__ -   Step: 1460, LR: 5.861546235472462e-06, Loss: 0.3378055691719055
 72%|███████▏  | 1461/2040 [14:21:23<5:47:17, 35.99s/it]08/16/2023 00:41:29 - INFO - __main__ -   Step: 1461, LR: 5.85144012127337e-06, Loss: 0.3721805214881897
 72%|███████▏  | 1462/2040 [14:21:59<5:48:11, 36.15s/it]08/16/2023 00:42:05 - INFO - __main__ -   Step: 1462, LR: 5.841334007074281e-06, Loss: 0.502372682094574
 72%|███████▏  | 1463/2040 [14:22:36<5:48:18, 36.22s/it]08/16/2023 00:42:42 - INFO - __main__ -   Step: 1463, LR: 5.83122789287519e-06, Loss: 0.3125154674053192
 72%|███████▏  | 1464/2040 [14:23:12<5:47:00, 36.15s/it]08/16/2023 00:43:18 - INFO - __main__ -   Step: 1464, LR: 5.8211217786761e-06, Loss: 0.33412235975265503
 72%|███████▏  | 1465/2040 [14:23:48<5:45:51, 36.09s/it]08/16/2023 00:43:54 - INFO - __main__ -   Step: 1465, LR: 5.811015664477009e-06, Loss: 0.35343700647354126
 72%|███████▏  | 1466/2040 [14:24:23<5:43:23, 35.89s/it]08/16/2023 00:44:29 - INFO - __main__ -   Step: 1466, LR: 5.800909550277919e-06, Loss: 0.42275500297546387
 72%|███████▏  | 1467/2040 [14:24:59<5:42:31, 35.87s/it]08/16/2023 00:45:05 - INFO - __main__ -   Step: 1467, LR: 5.790803436078828e-06, Loss: 0.3167712092399597
 72%|███████▏  | 1468/2040 [14:25:35<5:41:40, 35.84s/it]08/16/2023 00:45:41 - INFO - __main__ -   Step: 1468, LR: 5.780697321879738e-06, Loss: 0.3417280316352844
 72%|███████▏  | 1469/2040 [14:26:11<5:41:18, 35.86s/it]08/16/2023 00:46:17 - INFO - __main__ -   Step: 1469, LR: 5.770591207680647e-06, Loss: 0.31697186827659607
 72%|███████▏  | 1470/2040 [14:26:46<5:40:08, 35.80s/it]08/16/2023 00:46:52 - INFO - __main__ -   Step: 1470, LR: 5.760485093481557e-06, Loss: 0.34588760137557983
 72%|███████▏  | 1471/2040 [14:27:23<5:42:43, 36.14s/it]08/16/2023 00:47:29 - INFO - __main__ -   Step: 1471, LR: 5.750378979282466e-06, Loss: 0.40090399980545044
 72%|███████▏  | 1472/2040 [14:28:00<5:43:15, 36.26s/it]08/16/2023 00:48:06 - INFO - __main__ -   Step: 1472, LR: 5.740272865083376e-06, Loss: 0.3591247797012329
 72%|███████▏  | 1473/2040 [14:28:36<5:41:56, 36.18s/it]08/16/2023 00:48:42 - INFO - __main__ -   Step: 1473, LR: 5.730166750884285e-06, Loss: 0.34671151638031006
 72%|███████▏  | 1474/2040 [14:29:12<5:40:39, 36.11s/it]08/16/2023 00:49:18 - INFO - __main__ -   Step: 1474, LR: 5.720060636685195e-06, Loss: 0.31799107789993286
 72%|███████▏  | 1475/2040 [14:29:49<5:42:39, 36.39s/it]08/16/2023 00:49:55 - INFO - __main__ -   Step: 1475, LR: 5.709954522486104e-06, Loss: 0.36023080348968506
 72%|███████▏  | 1476/2040 [14:30:25<5:40:25, 36.22s/it]08/16/2023 00:50:31 - INFO - __main__ -   Step: 1476, LR: 5.699848408287014e-06, Loss: 0.4169372022151947
 72%|███████▏  | 1477/2040 [14:31:00<5:37:18, 35.95s/it]08/16/2023 00:51:06 - INFO - __main__ -   Step: 1477, LR: 5.689742294087923e-06, Loss: 0.37353411316871643
 72%|███████▏  | 1478/2040 [14:31:35<5:34:58, 35.76s/it]08/16/2023 00:51:41 - INFO - __main__ -   Step: 1478, LR: 5.679636179888833e-06, Loss: 0.43829870223999023
 72%|███████▎  | 1479/2040 [14:32:12<5:36:11, 35.96s/it]08/16/2023 00:52:18 - INFO - __main__ -   Step: 1479, LR: 5.669530065689742e-06, Loss: 0.40985846519470215
 73%|███████▎  | 1480/2040 [14:32:48<5:36:14, 36.03s/it]08/16/2023 00:52:54 - INFO - __main__ -   Step: 1480, LR: 5.659423951490652e-06, Loss: 0.31576481461524963
 73%|███████▎  | 1481/2040 [14:33:23<5:33:54, 35.84s/it]08/16/2023 00:53:29 - INFO - __main__ -   Step: 1481, LR: 5.649317837291561e-06, Loss: 0.40533581376075745
 73%|███████▎  | 1482/2040 [14:33:59<5:33:02, 35.81s/it]08/16/2023 00:54:05 - INFO - __main__ -   Step: 1482, LR: 5.639211723092472e-06, Loss: 0.3110188841819763
 73%|███████▎  | 1483/2040 [14:34:34<5:31:11, 35.68s/it]08/16/2023 00:54:40 - INFO - __main__ -   Step: 1483, LR: 5.62910560889338e-06, Loss: 0.3542814552783966
 73%|███████▎  | 1484/2040 [14:35:10<5:29:58, 35.61s/it]08/16/2023 00:55:16 - INFO - __main__ -   Step: 1484, LR: 5.618999494694291e-06, Loss: 0.33215487003326416
 73%|███████▎  | 1485/2040 [14:35:45<5:28:02, 35.46s/it]08/16/2023 00:55:51 - INFO - __main__ -   Step: 1485, LR: 5.608893380495201e-06, Loss: 0.3429337739944458
 73%|███████▎  | 1486/2040 [14:36:20<5:26:16, 35.34s/it]08/16/2023 00:56:26 - INFO - __main__ -   Step: 1486, LR: 5.59878726629611e-06, Loss: 0.4462745785713196
 73%|███████▎  | 1487/2040 [14:36:55<5:25:23, 35.31s/it]08/16/2023 00:57:01 - INFO - __main__ -   Step: 1487, LR: 5.58868115209702e-06, Loss: 0.325277715921402
 73%|███████▎  | 1488/2040 [14:37:31<5:25:22, 35.37s/it]08/16/2023 00:57:37 - INFO - __main__ -   Step: 1488, LR: 5.578575037897929e-06, Loss: 0.35059264302253723
 73%|███████▎  | 1489/2040 [14:38:07<5:26:05, 35.51s/it]08/16/2023 00:58:13 - INFO - __main__ -   Step: 1489, LR: 5.568468923698839e-06, Loss: 0.37979137897491455
 73%|███████▎  | 1490/2040 [14:38:42<5:25:23, 35.50s/it]08/16/2023 00:58:48 - INFO - __main__ -   Step: 1490, LR: 5.558362809499748e-06, Loss: 0.393128365278244
 73%|███████▎  | 1491/2040 [14:39:17<5:22:57, 35.30s/it]08/16/2023 00:59:23 - INFO - __main__ -   Step: 1491, LR: 5.548256695300658e-06, Loss: 0.38678091764450073
 73%|███████▎  | 1492/2040 [14:39:52<5:22:39, 35.33s/it]08/16/2023 00:59:58 - INFO - __main__ -   Step: 1492, LR: 5.538150581101567e-06, Loss: 0.31962788105010986
 73%|███████▎  | 1493/2040 [14:40:28<5:22:16, 35.35s/it]08/16/2023 01:00:34 - INFO - __main__ -   Step: 1493, LR: 5.528044466902477e-06, Loss: 0.3554261028766632
 73%|███████▎  | 1494/2040 [14:41:05<5:26:14, 35.85s/it]08/16/2023 01:01:11 - INFO - __main__ -   Step: 1494, LR: 5.517938352703386e-06, Loss: 0.34501105546951294
 73%|███████▎  | 1495/2040 [14:41:40<5:24:00, 35.67s/it]08/16/2023 01:01:46 - INFO - __main__ -   Step: 1495, LR: 5.507832238504296e-06, Loss: 0.27379098534584045
 73%|███████▎  | 1496/2040 [14:42:16<5:23:20, 35.66s/it]08/16/2023 01:02:22 - INFO - __main__ -   Step: 1496, LR: 5.497726124305205e-06, Loss: 0.305677205324173
 73%|███████▎  | 1497/2040 [14:42:51<5:22:39, 35.65s/it]08/16/2023 01:02:57 - INFO - __main__ -   Step: 1497, LR: 5.487620010106115e-06, Loss: 0.3830653727054596
 73%|███████▎  | 1498/2040 [14:43:27<5:21:15, 35.56s/it]08/16/2023 01:03:33 - INFO - __main__ -   Step: 1498, LR: 5.477513895907024e-06, Loss: 0.34769564867019653
 73%|███████▎  | 1499/2040 [14:44:02<5:20:26, 35.54s/it]08/16/2023 01:04:08 - INFO - __main__ -   Step: 1499, LR: 5.467407781707934e-06, Loss: 0.36368781328201294
 74%|███████▎  | 1500/2040 [14:44:39<5:24:10, 36.02s/it]08/16/2023 01:04:45 - INFO - __main__ -   Step: 1500, LR: 5.457301667508843e-06, Loss: 0.35312211513519287
08/16/2023 01:04:45 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500
08/16/2023 01:04:45 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-16 01:04:45,632] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-16 01:04:45,637] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-16 01:04:45,638] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-16 01:04:45,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-16 01:04:45,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-16 01:04:45,638] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-16 01:04:45,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-16 01:04:45,649] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-16 01:04:45,649] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-16 01:04:45,649] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-16 01:04:45,650] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-16 01:04:45,651] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-16 01:04:45,651] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-16 01:04:45,651] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-16 01:04:45,651] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-16 01:05:06,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-16 01:05:06,583] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-16 01:05:06,878] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-16 01:05:06,879] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-16 01:05:07,911] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-16 01:05:07,911] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-16 01:05:09,359] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-16 01:05:09,359] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-16 01:05:09,363] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 01:05:09,363] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 01:05:09,364] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 01:05:09,364] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/16/2023 01:05:09 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/pytorch_model
08/16/2023 01:05:09 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/scheduler.bin
08/16/2023 01:05:09 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1500/random_states_0.pkl
 74%|███████▎  | 1501/2040 [14:45:38<6:26:02, 42.97s/it]08/16/2023 01:05:44 - INFO - __main__ -   Step: 1501, LR: 5.447195553309753e-06, Loss: 0.31344592571258545
 74%|███████▎  | 1502/2040 [14:46:14<6:06:29, 40.87s/it]08/16/2023 01:06:20 - INFO - __main__ -   Step: 1502, LR: 5.437089439110662e-06, Loss: 0.41935795545578003
 74%|███████▎  | 1503/2040 [14:46:49<5:49:48, 39.08s/it]08/16/2023 01:06:55 - INFO - __main__ -   Step: 1503, LR: 5.4269833249115726e-06, Loss: 0.42172035574913025
 74%|███████▎  | 1504/2040 [14:47:25<5:39:57, 38.06s/it]08/16/2023 01:07:31 - INFO - __main__ -   Step: 1504, LR: 5.416877210712481e-06, Loss: 0.34150105714797974
 74%|███████▍  | 1505/2040 [14:48:00<5:31:44, 37.20s/it]08/16/2023 01:08:06 - INFO - __main__ -   Step: 1505, LR: 5.4067710965133916e-06, Loss: 0.352247416973114
 74%|███████▍  | 1506/2040 [14:48:36<5:26:14, 36.66s/it]08/16/2023 01:08:41 - INFO - __main__ -   Step: 1506, LR: 5.396664982314301e-06, Loss: 0.34514912962913513
 74%|███████▍  | 1507/2040 [14:49:11<5:23:13, 36.39s/it]08/16/2023 01:09:17 - INFO - __main__ -   Step: 1507, LR: 5.3865588681152106e-06, Loss: 0.37885695695877075
 74%|███████▍  | 1508/2040 [14:49:48<5:22:40, 36.39s/it]08/16/2023 01:09:54 - INFO - __main__ -   Step: 1508, LR: 5.37645275391612e-06, Loss: 0.3382776379585266
 74%|███████▍  | 1509/2040 [14:50:23<5:19:55, 36.15s/it]08/16/2023 01:10:29 - INFO - __main__ -   Step: 1509, LR: 5.3663466397170296e-06, Loss: 0.3687582015991211
 74%|███████▍  | 1510/2040 [14:50:59<5:17:38, 35.96s/it]08/16/2023 01:11:05 - INFO - __main__ -   Step: 1510, LR: 5.356240525517939e-06, Loss: 0.3229893147945404
 74%|███████▍  | 1511/2040 [14:51:34<5:13:59, 35.61s/it]08/16/2023 01:11:40 - INFO - __main__ -   Step: 1511, LR: 5.3461344113188485e-06, Loss: 0.35415369272232056
 74%|███████▍  | 1512/2040 [14:52:09<5:12:34, 35.52s/it]08/16/2023 01:12:15 - INFO - __main__ -   Step: 1512, LR: 5.336028297119758e-06, Loss: 0.41157159209251404
 74%|███████▍  | 1513/2040 [14:52:44<5:11:56, 35.51s/it]08/16/2023 01:12:50 - INFO - __main__ -   Step: 1513, LR: 5.3259221829206675e-06, Loss: 0.2935338318347931
 74%|███████▍  | 1514/2040 [14:53:20<5:12:19, 35.63s/it]08/16/2023 01:13:26 - INFO - __main__ -   Step: 1514, LR: 5.315816068721577e-06, Loss: 0.28583967685699463
 74%|███████▍  | 1515/2040 [14:53:56<5:11:37, 35.61s/it]08/16/2023 01:14:02 - INFO - __main__ -   Step: 1515, LR: 5.3057099545224865e-06, Loss: 0.36749017238616943
 74%|███████▍  | 1516/2040 [14:54:32<5:11:06, 35.62s/it]08/16/2023 01:14:37 - INFO - __main__ -   Step: 1516, LR: 5.295603840323396e-06, Loss: 0.36518025398254395
 74%|███████▍  | 1517/2040 [14:55:07<5:09:09, 35.47s/it]08/16/2023 01:15:13 - INFO - __main__ -   Step: 1517, LR: 5.2854977261243055e-06, Loss: 0.36907631158828735
 74%|███████▍  | 1518/2040 [14:55:42<5:09:00, 35.52s/it]08/16/2023 01:15:48 - INFO - __main__ -   Step: 1518, LR: 5.275391611925215e-06, Loss: 0.3779115676879883
 74%|███████▍  | 1519/2040 [14:56:18<5:09:48, 35.68s/it]08/16/2023 01:16:24 - INFO - __main__ -   Step: 1519, LR: 5.2652854977261245e-06, Loss: 0.3679131865501404
 75%|███████▍  | 1520/2040 [14:56:54<5:08:48, 35.63s/it]08/16/2023 01:17:00 - INFO - __main__ -   Step: 1520, LR: 5.2551793835270344e-06, Loss: 0.3408331871032715
 75%|███████▍  | 1521/2040 [14:57:29<5:07:57, 35.60s/it]08/16/2023 01:17:35 - INFO - __main__ -   Step: 1521, LR: 5.2450732693279435e-06, Loss: 0.35985279083251953
 75%|███████▍  | 1522/2040 [14:58:05<5:06:30, 35.50s/it]08/16/2023 01:18:11 - INFO - __main__ -   Step: 1522, LR: 5.234967155128854e-06, Loss: 0.43314772844314575
 75%|███████▍  | 1523/2040 [14:58:40<5:05:35, 35.46s/it]08/16/2023 01:18:46 - INFO - __main__ -   Step: 1523, LR: 5.2248610409297625e-06, Loss: 0.3423224687576294
 75%|███████▍  | 1524/2040 [14:59:16<5:05:02, 35.47s/it]08/16/2023 01:19:21 - INFO - __main__ -   Step: 1524, LR: 5.214754926730673e-06, Loss: 0.39411017298698425
 75%|███████▍  | 1525/2040 [14:59:52<5:05:53, 35.64s/it]08/16/2023 01:19:57 - INFO - __main__ -   Step: 1525, LR: 5.204648812531582e-06, Loss: 0.3264666497707367
 75%|███████▍  | 1526/2040 [15:00:27<5:03:54, 35.48s/it]08/16/2023 01:20:33 - INFO - __main__ -   Step: 1526, LR: 5.194542698332492e-06, Loss: 0.34169742465019226
 75%|███████▍  | 1527/2040 [15:01:02<5:02:33, 35.39s/it]08/16/2023 01:21:08 - INFO - __main__ -   Step: 1527, LR: 5.184436584133401e-06, Loss: 0.344940721988678
 75%|███████▍  | 1528/2040 [15:01:37<5:01:36, 35.34s/it]08/16/2023 01:21:43 - INFO - __main__ -   Step: 1528, LR: 5.174330469934311e-06, Loss: 0.36706435680389404
 75%|███████▍  | 1529/2040 [15:02:13<5:01:55, 35.45s/it]08/16/2023 01:22:19 - INFO - __main__ -   Step: 1529, LR: 5.16422435573522e-06, Loss: 0.40840232372283936
 75%|███████▌  | 1530/2040 [15:02:48<5:00:57, 35.41s/it]08/16/2023 01:22:54 - INFO - __main__ -   Step: 1530, LR: 5.15411824153613e-06, Loss: 0.3214932978153229
 75%|███████▌  | 1531/2040 [15:03:25<5:03:26, 35.77s/it]08/16/2023 01:23:31 - INFO - __main__ -   Step: 1531, LR: 5.144012127337039e-06, Loss: 0.34166577458381653
 75%|███████▌  | 1532/2040 [15:04:00<5:00:47, 35.53s/it]08/16/2023 01:24:06 - INFO - __main__ -   Step: 1532, LR: 5.133906013137949e-06, Loss: 0.4249193072319031
 75%|███████▌  | 1533/2040 [15:04:35<4:59:09, 35.40s/it]08/16/2023 01:24:41 - INFO - __main__ -   Step: 1533, LR: 5.123799898938858e-06, Loss: 0.3000280261039734
 75%|███████▌  | 1534/2040 [15:05:10<4:57:39, 35.30s/it]08/16/2023 01:25:16 - INFO - __main__ -   Step: 1534, LR: 5.113693784739768e-06, Loss: 0.3502534329891205
 75%|███████▌  | 1535/2040 [15:05:45<4:57:58, 35.40s/it]08/16/2023 01:25:51 - INFO - __main__ -   Step: 1535, LR: 5.103587670540677e-06, Loss: 0.359761118888855
 75%|███████▌  | 1536/2040 [15:06:21<4:57:50, 35.46s/it]08/16/2023 01:26:27 - INFO - __main__ -   Step: 1536, LR: 5.093481556341587e-06, Loss: 0.3758483827114105
 75%|███████▌  | 1537/2040 [15:06:56<4:55:38, 35.27s/it]08/16/2023 01:27:02 - INFO - __main__ -   Step: 1537, LR: 5.083375442142496e-06, Loss: 0.4096006155014038
 75%|███████▌  | 1538/2040 [15:07:31<4:54:29, 35.20s/it]08/16/2023 01:27:37 - INFO - __main__ -   Step: 1538, LR: 5.073269327943406e-06, Loss: 0.3384566903114319
 75%|███████▌  | 1539/2040 [15:08:06<4:52:49, 35.07s/it]08/16/2023 01:28:12 - INFO - __main__ -   Step: 1539, LR: 5.063163213744315e-06, Loss: 0.33424896001815796
 75%|███████▌  | 1540/2040 [15:08:41<4:53:32, 35.23s/it]08/16/2023 01:28:47 - INFO - __main__ -   Step: 1540, LR: 5.053057099545225e-06, Loss: 0.4180592894554138
 76%|███████▌  | 1541/2040 [15:09:17<4:54:37, 35.43s/it]08/16/2023 01:29:23 - INFO - __main__ -   Step: 1541, LR: 5.042950985346134e-06, Loss: 0.36750102043151855
 76%|███████▌  | 1542/2040 [15:09:53<4:53:56, 35.41s/it]08/16/2023 01:29:58 - INFO - __main__ -   Step: 1542, LR: 5.032844871147044e-06, Loss: 0.41052258014678955
 76%|███████▌  | 1543/2040 [15:10:28<4:52:15, 35.28s/it]08/16/2023 01:30:33 - INFO - __main__ -   Step: 1543, LR: 5.022738756947953e-06, Loss: 0.34709370136260986
 76%|███████▌  | 1544/2040 [15:11:03<4:51:59, 35.32s/it]08/16/2023 01:31:09 - INFO - __main__ -   Step: 1544, LR: 5.012632642748863e-06, Loss: 0.3560500144958496
 76%|███████▌  | 1545/2040 [15:11:39<4:52:46, 35.49s/it]08/16/2023 01:31:45 - INFO - __main__ -   Step: 1545, LR: 5.002526528549772e-06, Loss: 0.35539865493774414
 76%|███████▌  | 1546/2040 [15:12:15<4:52:43, 35.55s/it]08/16/2023 01:32:20 - INFO - __main__ -   Step: 1546, LR: 4.992420414350683e-06, Loss: 0.35230767726898193
 76%|███████▌  | 1547/2040 [15:12:50<4:52:21, 35.58s/it]08/16/2023 01:32:56 - INFO - __main__ -   Step: 1547, LR: 4.982314300151592e-06, Loss: 0.3238731920719147
 76%|███████▌  | 1548/2040 [15:13:25<4:50:37, 35.44s/it]08/16/2023 01:33:31 - INFO - __main__ -   Step: 1548, LR: 4.972208185952502e-06, Loss: 0.3852587640285492
 76%|███████▌  | 1549/2040 [15:14:01<4:49:39, 35.40s/it]08/16/2023 01:34:06 - INFO - __main__ -   Step: 1549, LR: 4.962102071753411e-06, Loss: 0.34506163001060486
 76%|███████▌  | 1550/2040 [15:14:36<4:48:14, 35.30s/it]08/16/2023 01:34:42 - INFO - __main__ -   Step: 1550, LR: 4.951995957554321e-06, Loss: 0.3462633490562439
 76%|███████▌  | 1551/2040 [15:15:11<4:48:34, 35.41s/it]08/16/2023 01:35:17 - INFO - __main__ -   Step: 1551, LR: 4.94188984335523e-06, Loss: 0.33765897154808044
 76%|███████▌  | 1552/2040 [15:15:47<4:49:11, 35.56s/it]08/16/2023 01:35:53 - INFO - __main__ -   Step: 1552, LR: 4.93178372915614e-06, Loss: 0.3480744957923889
 76%|███████▌  | 1553/2040 [15:16:23<4:48:09, 35.50s/it]08/16/2023 01:36:28 - INFO - __main__ -   Step: 1553, LR: 4.921677614957049e-06, Loss: 0.3236774802207947
 76%|███████▌  | 1554/2040 [15:16:59<4:48:57, 35.67s/it]08/16/2023 01:37:05 - INFO - __main__ -   Step: 1554, LR: 4.911571500757959e-06, Loss: 0.3893236517906189
 76%|███████▌  | 1555/2040 [15:17:35<4:51:11, 36.02s/it]08/16/2023 01:37:41 - INFO - __main__ -   Step: 1555, LR: 4.901465386558868e-06, Loss: 0.36109647154808044
 76%|███████▋  | 1556/2040 [15:18:12<4:50:42, 36.04s/it]08/16/2023 01:38:17 - INFO - __main__ -   Step: 1556, LR: 4.891359272359778e-06, Loss: 0.44737741351127625
 76%|███████▋  | 1557/2040 [15:18:47<4:49:03, 35.91s/it]08/16/2023 01:38:53 - INFO - __main__ -   Step: 1557, LR: 4.881253158160688e-06, Loss: 0.33088988065719604
 76%|███████▋  | 1558/2040 [15:19:22<4:46:36, 35.68s/it]08/16/2023 01:39:28 - INFO - __main__ -   Step: 1558, LR: 4.871147043961597e-06, Loss: 0.3855200409889221
 76%|███████▋  | 1559/2040 [15:19:58<4:45:35, 35.62s/it]08/16/2023 01:40:04 - INFO - __main__ -   Step: 1559, LR: 4.861040929762507e-06, Loss: 0.35257142782211304
 76%|███████▋  | 1560/2040 [15:20:33<4:43:29, 35.44s/it]08/16/2023 01:40:39 - INFO - __main__ -   Step: 1560, LR: 4.850934815563416e-06, Loss: 0.32735133171081543
 77%|███████▋  | 1561/2040 [15:21:08<4:42:54, 35.44s/it]08/16/2023 01:41:14 - INFO - __main__ -   Step: 1561, LR: 4.840828701364326e-06, Loss: 0.37242794036865234
 77%|███████▋  | 1562/2040 [15:21:44<4:42:46, 35.50s/it]08/16/2023 01:41:50 - INFO - __main__ -   Step: 1562, LR: 4.830722587165235e-06, Loss: 0.3441541790962219
 77%|███████▋  | 1563/2040 [15:22:20<4:42:46, 35.57s/it]08/16/2023 01:42:26 - INFO - __main__ -   Step: 1563, LR: 4.820616472966145e-06, Loss: 0.3342186212539673
 77%|███████▋  | 1564/2040 [15:22:55<4:41:19, 35.46s/it]08/16/2023 01:43:01 - INFO - __main__ -   Step: 1564, LR: 4.810510358767054e-06, Loss: 0.39652496576309204
 77%|███████▋  | 1565/2040 [15:23:30<4:40:36, 35.45s/it]08/16/2023 01:43:36 - INFO - __main__ -   Step: 1565, LR: 4.800404244567964e-06, Loss: 0.3489515483379364
 77%|███████▋  | 1566/2040 [15:24:06<4:40:31, 35.51s/it]08/16/2023 01:44:12 - INFO - __main__ -   Step: 1566, LR: 4.790298130368873e-06, Loss: 0.32254689931869507
 77%|███████▋  | 1567/2040 [15:24:45<4:48:55, 36.65s/it]08/16/2023 01:44:51 - INFO - __main__ -   Step: 1567, LR: 4.780192016169783e-06, Loss: 0.32648396492004395
 77%|███████▋  | 1568/2040 [15:25:22<4:47:35, 36.56s/it]08/16/2023 01:45:27 - INFO - __main__ -   Step: 1568, LR: 4.770085901970693e-06, Loss: 0.3988223969936371
 77%|███████▋  | 1569/2040 [15:26:00<4:51:45, 37.17s/it]08/16/2023 01:46:06 - INFO - __main__ -   Step: 1569, LR: 4.759979787771602e-06, Loss: 0.3307352066040039
 77%|███████▋  | 1570/2040 [15:26:36<4:47:58, 36.76s/it]08/16/2023 01:46:42 - INFO - __main__ -   Step: 1570, LR: 4.749873673572512e-06, Loss: 0.3773897886276245
 77%|███████▋  | 1571/2040 [15:27:11<4:43:58, 36.33s/it]08/16/2023 01:47:17 - INFO - __main__ -   Step: 1571, LR: 4.739767559373421e-06, Loss: 0.42009231448173523
 77%|███████▋  | 1572/2040 [15:27:47<4:42:30, 36.22s/it]08/16/2023 01:47:53 - INFO - __main__ -   Step: 1572, LR: 4.729661445174331e-06, Loss: 0.3766499161720276
 77%|███████▋  | 1573/2040 [15:28:22<4:38:42, 35.81s/it]08/16/2023 01:48:28 - INFO - __main__ -   Step: 1573, LR: 4.719555330975241e-06, Loss: 0.36199674010276794
 77%|███████▋  | 1574/2040 [15:28:57<4:36:39, 35.62s/it]08/16/2023 01:49:03 - INFO - __main__ -   Step: 1574, LR: 4.70944921677615e-06, Loss: 0.4320213794708252
 77%|███████▋  | 1575/2040 [15:29:33<4:35:44, 35.58s/it]08/16/2023 01:49:39 - INFO - __main__ -   Step: 1575, LR: 4.69934310257706e-06, Loss: 0.37526363134384155
 77%|███████▋  | 1576/2040 [15:30:09<4:35:55, 35.68s/it]08/16/2023 01:50:15 - INFO - __main__ -   Step: 1576, LR: 4.68923698837797e-06, Loss: 0.3906446099281311
 77%|███████▋  | 1577/2040 [15:30:44<4:34:07, 35.52s/it]08/16/2023 01:50:50 - INFO - __main__ -   Step: 1577, LR: 4.679130874178879e-06, Loss: 0.33014434576034546
 77%|███████▋  | 1578/2040 [15:31:19<4:33:13, 35.48s/it]08/16/2023 01:51:25 - INFO - __main__ -   Step: 1578, LR: 4.669024759979789e-06, Loss: 0.36475980281829834
 77%|███████▋  | 1579/2040 [15:31:54<4:31:22, 35.32s/it]08/16/2023 01:52:00 - INFO - __main__ -   Step: 1579, LR: 4.658918645780698e-06, Loss: 0.3803270757198334
 77%|███████▋  | 1580/2040 [15:32:30<4:31:03, 35.36s/it]08/16/2023 01:52:36 - INFO - __main__ -   Step: 1580, LR: 4.648812531581608e-06, Loss: 0.38073527812957764
 78%|███████▊  | 1581/2040 [15:33:05<4:30:26, 35.35s/it]08/16/2023 01:53:11 - INFO - __main__ -   Step: 1581, LR: 4.638706417382517e-06, Loss: 0.4447547197341919
 78%|███████▊  | 1582/2040 [15:33:41<4:31:16, 35.54s/it]08/16/2023 01:53:47 - INFO - __main__ -   Step: 1582, LR: 4.628600303183427e-06, Loss: 0.3086344301700592
 78%|███████▊  | 1583/2040 [15:34:16<4:29:23, 35.37s/it]08/16/2023 01:54:22 - INFO - __main__ -   Step: 1583, LR: 4.618494188984336e-06, Loss: 0.33783501386642456
 78%|███████▊  | 1584/2040 [15:34:52<4:31:33, 35.73s/it]08/16/2023 01:54:58 - INFO - __main__ -   Step: 1584, LR: 4.608388074785246e-06, Loss: 0.30849260091781616
 78%|███████▊  | 1585/2040 [15:35:29<4:32:16, 35.91s/it]08/16/2023 01:55:35 - INFO - __main__ -   Step: 1585, LR: 4.598281960586155e-06, Loss: 0.41814348101615906
 78%|███████▊  | 1586/2040 [15:36:05<4:31:21, 35.86s/it]08/16/2023 01:56:10 - INFO - __main__ -   Step: 1586, LR: 4.588175846387065e-06, Loss: 0.38591161370277405
 78%|███████▊  | 1587/2040 [15:36:40<4:30:35, 35.84s/it]08/16/2023 01:56:46 - INFO - __main__ -   Step: 1587, LR: 4.578069732187974e-06, Loss: 0.42156416177749634
 78%|███████▊  | 1588/2040 [15:37:15<4:27:50, 35.55s/it]08/16/2023 01:57:21 - INFO - __main__ -   Step: 1588, LR: 4.567963617988884e-06, Loss: 0.336673378944397
 78%|███████▊  | 1589/2040 [15:37:50<4:25:47, 35.36s/it]08/16/2023 01:57:56 - INFO - __main__ -   Step: 1589, LR: 4.5578575037897936e-06, Loss: 0.43707937002182007
 78%|███████▊  | 1590/2040 [15:38:26<4:25:25, 35.39s/it]08/16/2023 01:58:31 - INFO - __main__ -   Step: 1590, LR: 4.547751389590703e-06, Loss: 0.36156633496284485
 78%|███████▊  | 1591/2040 [15:39:02<4:26:12, 35.57s/it]08/16/2023 01:59:07 - INFO - __main__ -   Step: 1591, LR: 4.5376452753916125e-06, Loss: 0.4082891345024109
 78%|███████▊  | 1592/2040 [15:39:37<4:25:00, 35.49s/it]08/16/2023 01:59:43 - INFO - __main__ -   Step: 1592, LR: 4.527539161192522e-06, Loss: 0.3623110353946686
 78%|███████▊  | 1593/2040 [15:40:13<4:25:33, 35.65s/it]08/16/2023 02:00:19 - INFO - __main__ -   Step: 1593, LR: 4.5174330469934315e-06, Loss: 0.41820889711380005
 78%|███████▊  | 1594/2040 [15:40:48<4:23:35, 35.46s/it]08/16/2023 02:00:54 - INFO - __main__ -   Step: 1594, LR: 4.507326932794341e-06, Loss: 0.35095542669296265
 78%|███████▊  | 1595/2040 [15:41:23<4:23:13, 35.49s/it]08/16/2023 02:01:29 - INFO - __main__ -   Step: 1595, LR: 4.4972208185952505e-06, Loss: 0.36240091919898987
 78%|███████▊  | 1596/2040 [15:41:59<4:22:29, 35.47s/it]08/16/2023 02:02:05 - INFO - __main__ -   Step: 1596, LR: 4.48711470439616e-06, Loss: 0.3462962806224823
 78%|███████▊  | 1597/2040 [15:42:35<4:23:19, 35.66s/it]08/16/2023 02:02:41 - INFO - __main__ -   Step: 1597, LR: 4.4770085901970695e-06, Loss: 0.3423107862472534
 78%|███████▊  | 1598/2040 [15:43:10<4:22:14, 35.60s/it]08/16/2023 02:03:16 - INFO - __main__ -   Step: 1598, LR: 4.466902475997979e-06, Loss: 0.37844371795654297
 78%|███████▊  | 1599/2040 [15:43:46<4:21:22, 35.56s/it]08/16/2023 02:03:52 - INFO - __main__ -   Step: 1599, LR: 4.4567963617988885e-06, Loss: 0.3341097831726074
 78%|███████▊  | 1600/2040 [15:44:21<4:19:24, 35.37s/it]08/16/2023 02:04:27 - INFO - __main__ -   Step: 1600, LR: 4.4466902475997984e-06, Loss: 0.4934120178222656
08/16/2023 02:04:27 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600
08/16/2023 02:04:27 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-16 02:04:27,287] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-16 02:04:27,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-16 02:04:27,292] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-16 02:04:27,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-16 02:04:27,293] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-16 02:04:27,293] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-16 02:04:27,293] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-16 02:04:27,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-16 02:04:27,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-16 02:04:27,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-16 02:04:27,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-16 02:04:27,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-16 02:04:27,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-16 02:04:27,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-16 02:04:27,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-16 02:04:47,899] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-16 02:04:47,899] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-16 02:04:48,704] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-16 02:04:48,705] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-16 02:04:49,079] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-16 02:04:49,079] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-16 02:04:51,833] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-16 02:04:51,834] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-16 02:04:51,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 02:04:51,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 02:04:51,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 02:04:51,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/16/2023 02:04:51 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/pytorch_model
08/16/2023 02:04:51 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/scheduler.bin
08/16/2023 02:04:51 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1600/random_states_0.pkl
 78%|███████▊  | 1601/2040 [15:45:20<5:11:53, 42.63s/it]08/16/2023 02:05:26 - INFO - __main__ -   Step: 1601, LR: 4.4365841334007075e-06, Loss: 0.43435096740722656
 79%|███████▊  | 1602/2040 [15:45:55<4:54:00, 40.28s/it]08/16/2023 02:06:01 - INFO - __main__ -   Step: 1602, LR: 4.4264780192016174e-06, Loss: 0.38525933027267456
 79%|███████▊  | 1603/2040 [15:46:31<4:43:14, 38.89s/it]08/16/2023 02:06:37 - INFO - __main__ -   Step: 1603, LR: 4.4163719050025265e-06, Loss: 0.3047707676887512
 79%|███████▊  | 1604/2040 [15:47:06<4:34:24, 37.76s/it]08/16/2023 02:07:12 - INFO - __main__ -   Step: 1604, LR: 4.4062657908034364e-06, Loss: 0.3482906222343445
 79%|███████▊  | 1605/2040 [15:47:41<4:28:41, 37.06s/it]08/16/2023 02:07:47 - INFO - __main__ -   Step: 1605, LR: 4.3961596766043455e-06, Loss: 0.36150187253952026
 79%|███████▊  | 1606/2040 [15:48:17<4:25:27, 36.70s/it]08/16/2023 02:08:23 - INFO - __main__ -   Step: 1606, LR: 4.386053562405255e-06, Loss: 0.33068132400512695
 79%|███████▉  | 1607/2040 [15:48:52<4:21:35, 36.25s/it]08/16/2023 02:08:58 - INFO - __main__ -   Step: 1607, LR: 4.375947448206165e-06, Loss: 0.3726603388786316
 79%|███████▉  | 1608/2040 [15:49:28<4:19:42, 36.07s/it]08/16/2023 02:09:34 - INFO - __main__ -   Step: 1608, LR: 4.365841334007075e-06, Loss: 0.4086126685142517
 79%|███████▉  | 1609/2040 [15:50:03<4:16:38, 35.73s/it]08/16/2023 02:10:09 - INFO - __main__ -   Step: 1609, LR: 4.355735219807984e-06, Loss: 0.36646953225135803
 79%|███████▉  | 1610/2040 [15:50:39<4:15:32, 35.66s/it]08/16/2023 02:10:44 - INFO - __main__ -   Step: 1610, LR: 4.345629105608894e-06, Loss: 0.3577207922935486
 79%|███████▉  | 1611/2040 [15:51:14<4:14:41, 35.62s/it]08/16/2023 02:11:20 - INFO - __main__ -   Step: 1611, LR: 4.335522991409803e-06, Loss: 0.32514023780822754
 79%|███████▉  | 1612/2040 [15:51:50<4:13:59, 35.61s/it]08/16/2023 02:11:56 - INFO - __main__ -   Step: 1612, LR: 4.325416877210713e-06, Loss: 0.42833805084228516
 79%|███████▉  | 1613/2040 [15:52:25<4:12:21, 35.46s/it]08/16/2023 02:12:31 - INFO - __main__ -   Step: 1613, LR: 4.315310763011622e-06, Loss: 0.4404023289680481
 79%|███████▉  | 1614/2040 [15:53:01<4:12:22, 35.54s/it]08/16/2023 02:13:06 - INFO - __main__ -   Step: 1614, LR: 4.305204648812532e-06, Loss: 0.38865482807159424
 79%|███████▉  | 1615/2040 [15:53:36<4:10:50, 35.41s/it]08/16/2023 02:13:42 - INFO - __main__ -   Step: 1615, LR: 4.295098534613441e-06, Loss: 0.40938621759414673
 79%|███████▉  | 1616/2040 [15:54:11<4:10:16, 35.42s/it]08/16/2023 02:14:17 - INFO - __main__ -   Step: 1616, LR: 4.284992420414351e-06, Loss: 0.36960306763648987
 79%|███████▉  | 1617/2040 [15:54:47<4:10:56, 35.59s/it]08/16/2023 02:14:53 - INFO - __main__ -   Step: 1617, LR: 4.27488630621526e-06, Loss: 0.2780386805534363
 79%|███████▉  | 1618/2040 [15:55:22<4:09:08, 35.42s/it]08/16/2023 02:15:28 - INFO - __main__ -   Step: 1618, LR: 4.26478019201617e-06, Loss: 0.3448992967605591
 79%|███████▉  | 1619/2040 [15:55:57<4:07:48, 35.32s/it]08/16/2023 02:16:03 - INFO - __main__ -   Step: 1619, LR: 4.25467407781708e-06, Loss: 0.28493764996528625
 79%|███████▉  | 1620/2040 [15:56:32<4:06:51, 35.26s/it]08/16/2023 02:16:38 - INFO - __main__ -   Step: 1620, LR: 4.244567963617989e-06, Loss: 0.3502195477485657
 79%|███████▉  | 1621/2040 [15:57:08<4:06:33, 35.31s/it]08/16/2023 02:17:14 - INFO - __main__ -   Step: 1621, LR: 4.234461849418899e-06, Loss: 0.287608802318573
 80%|███████▉  | 1622/2040 [15:57:44<4:07:37, 35.54s/it]08/16/2023 02:17:50 - INFO - __main__ -   Step: 1622, LR: 4.224355735219808e-06, Loss: 0.3961593508720398
 80%|███████▉  | 1623/2040 [15:58:19<4:06:24, 35.45s/it]08/16/2023 02:18:25 - INFO - __main__ -   Step: 1623, LR: 4.214249621020718e-06, Loss: 0.33492523431777954
 80%|███████▉  | 1624/2040 [15:58:54<4:04:51, 35.32s/it]08/16/2023 02:19:00 - INFO - __main__ -   Step: 1624, LR: 4.204143506821627e-06, Loss: 0.3164759874343872
 80%|███████▉  | 1625/2040 [15:59:29<4:03:31, 35.21s/it]08/16/2023 02:19:35 - INFO - __main__ -   Step: 1625, LR: 4.194037392622537e-06, Loss: 0.3731321096420288
 80%|███████▉  | 1626/2040 [16:00:04<4:03:25, 35.28s/it]08/16/2023 02:20:10 - INFO - __main__ -   Step: 1626, LR: 4.183931278423446e-06, Loss: 0.37405920028686523
 80%|███████▉  | 1627/2040 [16:00:40<4:04:18, 35.49s/it]08/16/2023 02:20:46 - INFO - __main__ -   Step: 1627, LR: 4.173825164224356e-06, Loss: 0.47254425287246704
 80%|███████▉  | 1628/2040 [16:01:16<4:04:21, 35.59s/it]08/16/2023 02:21:22 - INFO - __main__ -   Step: 1628, LR: 4.163719050025265e-06, Loss: 0.3976062536239624
 80%|███████▉  | 1629/2040 [16:01:51<4:02:43, 35.43s/it]08/16/2023 02:21:57 - INFO - __main__ -   Step: 1629, LR: 4.153612935826175e-06, Loss: 0.36393189430236816
 80%|███████▉  | 1630/2040 [16:02:26<4:01:36, 35.36s/it]08/16/2023 02:22:32 - INFO - __main__ -   Step: 1630, LR: 4.143506821627084e-06, Loss: 0.3262733221054077
 80%|███████▉  | 1631/2040 [16:03:02<4:01:05, 35.37s/it]08/16/2023 02:23:08 - INFO - __main__ -   Step: 1631, LR: 4.133400707427994e-06, Loss: 0.38503333926200867
 80%|████████  | 1632/2040 [16:03:38<4:01:48, 35.56s/it]08/16/2023 02:23:44 - INFO - __main__ -   Step: 1632, LR: 4.123294593228904e-06, Loss: 0.4672605097293854
 80%|████████  | 1633/2040 [16:04:13<4:00:12, 35.41s/it]08/16/2023 02:24:19 - INFO - __main__ -   Step: 1633, LR: 4.113188479029813e-06, Loss: 0.4227288067340851
 80%|████████  | 1634/2040 [16:04:49<4:01:15, 35.65s/it]08/16/2023 02:24:55 - INFO - __main__ -   Step: 1634, LR: 4.103082364830723e-06, Loss: 0.3461039066314697
 80%|████████  | 1635/2040 [16:05:24<3:59:52, 35.54s/it]08/16/2023 02:25:30 - INFO - __main__ -   Step: 1635, LR: 4.092976250631632e-06, Loss: 0.4364168047904968
 80%|████████  | 1636/2040 [16:06:00<3:58:34, 35.43s/it]08/16/2023 02:26:06 - INFO - __main__ -   Step: 1636, LR: 4.082870136432542e-06, Loss: 0.32731616497039795
 80%|████████  | 1637/2040 [16:06:35<3:58:17, 35.48s/it]08/16/2023 02:26:41 - INFO - __main__ -   Step: 1637, LR: 4.072764022233451e-06, Loss: 0.3632466197013855
 80%|████████  | 1638/2040 [16:07:10<3:57:16, 35.41s/it]08/16/2023 02:27:16 - INFO - __main__ -   Step: 1638, LR: 4.062657908034361e-06, Loss: 0.34087249636650085
 80%|████████  | 1639/2040 [16:07:45<3:55:49, 35.29s/it]08/16/2023 02:27:51 - INFO - __main__ -   Step: 1639, LR: 4.05255179383527e-06, Loss: 0.33197760581970215
 80%|████████  | 1640/2040 [16:08:20<3:54:22, 35.16s/it]08/16/2023 02:28:26 - INFO - __main__ -   Step: 1640, LR: 4.04244567963618e-06, Loss: 0.36069345474243164
 80%|████████  | 1641/2040 [16:08:55<3:53:31, 35.12s/it]08/16/2023 02:29:01 - INFO - __main__ -   Step: 1641, LR: 4.03233956543709e-06, Loss: 0.37790948152542114
 80%|████████  | 1642/2040 [16:09:30<3:52:44, 35.09s/it]08/16/2023 02:29:36 - INFO - __main__ -   Step: 1642, LR: 4.022233451238e-06, Loss: 0.4001019597053528
 81%|████████  | 1643/2040 [16:10:06<3:53:02, 35.22s/it]08/16/2023 02:30:12 - INFO - __main__ -   Step: 1643, LR: 4.012127337038909e-06, Loss: 0.3331718444824219
 81%|████████  | 1644/2040 [16:10:41<3:52:06, 35.17s/it]08/16/2023 02:30:47 - INFO - __main__ -   Step: 1644, LR: 4.002021222839819e-06, Loss: 0.3670462667942047
 81%|████████  | 1645/2040 [16:11:16<3:51:08, 35.11s/it]08/16/2023 02:31:22 - INFO - __main__ -   Step: 1645, LR: 3.991915108640728e-06, Loss: 0.3312395513057709
 81%|████████  | 1646/2040 [16:11:51<3:51:14, 35.21s/it]08/16/2023 02:31:57 - INFO - __main__ -   Step: 1646, LR: 3.981808994441638e-06, Loss: 0.38123998045921326
 81%|████████  | 1647/2040 [16:12:27<3:50:44, 35.23s/it]08/16/2023 02:32:33 - INFO - __main__ -   Step: 1647, LR: 3.971702880242547e-06, Loss: 0.402241051197052
 81%|████████  | 1648/2040 [16:13:02<3:50:45, 35.32s/it]08/16/2023 02:33:08 - INFO - __main__ -   Step: 1648, LR: 3.961596766043457e-06, Loss: 0.32781994342803955
 81%|████████  | 1649/2040 [16:13:37<3:48:19, 35.04s/it]08/16/2023 02:33:42 - INFO - __main__ -   Step: 1649, LR: 3.951490651844366e-06, Loss: 0.4096364378929138
 81%|████████  | 1650/2040 [16:14:12<3:48:11, 35.11s/it]08/16/2023 02:34:18 - INFO - __main__ -   Step: 1650, LR: 3.941384537645276e-06, Loss: 0.35763099789619446
 81%|████████  | 1651/2040 [16:14:46<3:46:12, 34.89s/it]08/16/2023 02:34:52 - INFO - __main__ -   Step: 1651, LR: 3.931278423446186e-06, Loss: 0.32320982217788696
 81%|████████  | 1652/2040 [16:15:21<3:46:02, 34.95s/it]08/16/2023 02:35:27 - INFO - __main__ -   Step: 1652, LR: 3.921172309247095e-06, Loss: 0.3840486705303192
 81%|████████  | 1653/2040 [16:15:57<3:46:09, 35.06s/it]08/16/2023 02:36:03 - INFO - __main__ -   Step: 1653, LR: 3.911066195048005e-06, Loss: 0.3547465205192566
 81%|████████  | 1654/2040 [16:16:31<3:44:48, 34.95s/it]08/16/2023 02:36:37 - INFO - __main__ -   Step: 1654, LR: 3.900960080848914e-06, Loss: 0.3536868095397949
 81%|████████  | 1655/2040 [16:17:06<3:43:46, 34.87s/it]08/16/2023 02:37:12 - INFO - __main__ -   Step: 1655, LR: 3.890853966649824e-06, Loss: 0.37632694840431213
 81%|████████  | 1656/2040 [16:17:41<3:42:54, 34.83s/it]08/16/2023 02:37:47 - INFO - __main__ -   Step: 1656, LR: 3.880747852450733e-06, Loss: 0.3249814510345459
 81%|████████  | 1657/2040 [16:18:16<3:43:31, 35.02s/it]08/16/2023 02:38:22 - INFO - __main__ -   Step: 1657, LR: 3.870641738251643e-06, Loss: 0.3490201234817505
 81%|████████▏ | 1658/2040 [16:18:52<3:45:23, 35.40s/it]08/16/2023 02:38:58 - INFO - __main__ -   Step: 1658, LR: 3.860535624052552e-06, Loss: 0.37147101759910583
 81%|████████▏ | 1659/2040 [16:19:28<3:44:36, 35.37s/it]08/16/2023 02:39:34 - INFO - __main__ -   Step: 1659, LR: 3.850429509853462e-06, Loss: 0.3070269227027893
 81%|████████▏ | 1660/2040 [16:20:02<3:42:40, 35.16s/it]08/16/2023 02:40:08 - INFO - __main__ -   Step: 1660, LR: 3.840323395654371e-06, Loss: 0.3668399453163147
 81%|████████▏ | 1661/2040 [16:20:37<3:41:48, 35.12s/it]08/16/2023 02:40:43 - INFO - __main__ -   Step: 1661, LR: 3.830217281455281e-06, Loss: 0.3016706705093384
 81%|████████▏ | 1662/2040 [16:21:13<3:41:40, 35.19s/it]08/16/2023 02:41:19 - INFO - __main__ -   Step: 1662, LR: 3.820111167256191e-06, Loss: 0.3568911552429199
 82%|████████▏ | 1663/2040 [16:21:48<3:41:51, 35.31s/it]08/16/2023 02:41:54 - INFO - __main__ -   Step: 1663, LR: 3.8100050530570997e-06, Loss: 0.3549949824810028
 82%|████████▏ | 1664/2040 [16:22:23<3:40:10, 35.14s/it]08/16/2023 02:42:29 - INFO - __main__ -   Step: 1664, LR: 3.7998989388580092e-06, Loss: 0.36855536699295044
 82%|████████▏ | 1665/2040 [16:22:58<3:38:31, 34.96s/it]08/16/2023 02:43:04 - INFO - __main__ -   Step: 1665, LR: 3.7897928246589187e-06, Loss: 0.419709712266922
 82%|████████▏ | 1666/2040 [16:23:33<3:38:18, 35.02s/it]08/16/2023 02:43:39 - INFO - __main__ -   Step: 1666, LR: 3.779686710459828e-06, Loss: 0.3675755262374878
 82%|████████▏ | 1667/2040 [16:24:09<3:39:06, 35.24s/it]08/16/2023 02:44:15 - INFO - __main__ -   Step: 1667, LR: 3.7695805962607377e-06, Loss: 0.31896060705184937
 82%|████████▏ | 1668/2040 [16:24:44<3:39:31, 35.41s/it]08/16/2023 02:44:50 - INFO - __main__ -   Step: 1668, LR: 3.7594744820616476e-06, Loss: 0.2755916714668274
 82%|████████▏ | 1669/2040 [16:25:19<3:38:00, 35.26s/it]08/16/2023 02:45:25 - INFO - __main__ -   Step: 1669, LR: 3.749368367862557e-06, Loss: 0.32782161235809326
 82%|████████▏ | 1670/2040 [16:25:55<3:37:40, 35.30s/it]08/16/2023 02:46:01 - INFO - __main__ -   Step: 1670, LR: 3.7392622536634666e-06, Loss: 0.3382868766784668
 82%|████████▏ | 1671/2040 [16:26:29<3:36:02, 35.13s/it]08/16/2023 02:46:35 - INFO - __main__ -   Step: 1671, LR: 3.729156139464376e-06, Loss: 0.34933900833129883
 82%|████████▏ | 1672/2040 [16:27:04<3:34:52, 35.03s/it]08/16/2023 02:47:10 - INFO - __main__ -   Step: 1672, LR: 3.7190500252652856e-06, Loss: 0.4191958010196686
 82%|████████▏ | 1673/2040 [16:27:40<3:35:54, 35.30s/it]08/16/2023 02:47:46 - INFO - __main__ -   Step: 1673, LR: 3.708943911066195e-06, Loss: 0.33078914880752563
 82%|████████▏ | 1674/2040 [16:28:16<3:36:21, 35.47s/it]08/16/2023 02:48:22 - INFO - __main__ -   Step: 1674, LR: 3.6988377968671046e-06, Loss: 0.38830122351646423
 82%|████████▏ | 1675/2040 [16:28:51<3:34:03, 35.19s/it]08/16/2023 02:48:56 - INFO - __main__ -   Step: 1675, LR: 3.688731682668014e-06, Loss: 0.3027712106704712
 82%|████████▏ | 1676/2040 [16:29:25<3:32:59, 35.11s/it]08/16/2023 02:49:31 - INFO - __main__ -   Step: 1676, LR: 3.6786255684689245e-06, Loss: 0.4368380308151245
 82%|████████▏ | 1677/2040 [16:30:00<3:32:09, 35.07s/it]08/16/2023 02:50:06 - INFO - __main__ -   Step: 1677, LR: 3.668519454269834e-06, Loss: 0.37704357504844666
 82%|████████▏ | 1678/2040 [16:30:36<3:32:01, 35.14s/it]08/16/2023 02:50:42 - INFO - __main__ -   Step: 1678, LR: 3.6584133400707434e-06, Loss: 0.3279559016227722
 82%|████████▏ | 1679/2040 [16:31:11<3:31:56, 35.23s/it]08/16/2023 02:51:17 - INFO - __main__ -   Step: 1679, LR: 3.648307225871653e-06, Loss: 0.3487600088119507
 82%|████████▏ | 1680/2040 [16:31:46<3:31:15, 35.21s/it]08/16/2023 02:51:52 - INFO - __main__ -   Step: 1680, LR: 3.6382011116725624e-06, Loss: 0.3655012249946594
 82%|████████▏ | 1681/2040 [16:32:22<3:31:16, 35.31s/it]08/16/2023 02:52:28 - INFO - __main__ -   Step: 1681, LR: 3.628094997473472e-06, Loss: 0.40521863102912903
 82%|████████▏ | 1682/2040 [16:33:00<3:35:16, 36.08s/it]08/16/2023 02:53:06 - INFO - __main__ -   Step: 1682, LR: 3.6179888832743814e-06, Loss: 0.31884336471557617
 82%|████████▎ | 1683/2040 [16:33:36<3:34:02, 35.97s/it]08/16/2023 02:53:41 - INFO - __main__ -   Step: 1683, LR: 3.607882769075291e-06, Loss: 0.3827498257160187
 83%|████████▎ | 1684/2040 [16:34:13<3:35:23, 36.30s/it]08/16/2023 02:54:18 - INFO - __main__ -   Step: 1684, LR: 3.5977766548762004e-06, Loss: 0.4291139245033264
 83%|████████▎ | 1685/2040 [16:34:48<3:32:56, 35.99s/it]08/16/2023 02:54:54 - INFO - __main__ -   Step: 1685, LR: 3.58767054067711e-06, Loss: 0.3048229217529297
 83%|████████▎ | 1686/2040 [16:35:23<3:30:18, 35.64s/it]08/16/2023 02:55:29 - INFO - __main__ -   Step: 1686, LR: 3.5775644264780194e-06, Loss: 0.36278191208839417
 83%|████████▎ | 1687/2040 [16:35:57<3:27:29, 35.27s/it]08/16/2023 02:56:03 - INFO - __main__ -   Step: 1687, LR: 3.5674583122789293e-06, Loss: 0.3383132815361023
 83%|████████▎ | 1688/2040 [16:36:32<3:26:36, 35.22s/it]08/16/2023 02:56:38 - INFO - __main__ -   Step: 1688, LR: 3.557352198079839e-06, Loss: 0.3376043736934662
 83%|████████▎ | 1689/2040 [16:37:07<3:26:09, 35.24s/it]08/16/2023 02:57:13 - INFO - __main__ -   Step: 1689, LR: 3.5472460838807483e-06, Loss: 0.3355291485786438
 83%|████████▎ | 1690/2040 [16:37:42<3:24:48, 35.11s/it]08/16/2023 02:57:48 - INFO - __main__ -   Step: 1690, LR: 3.537139969681658e-06, Loss: 0.42160964012145996
 83%|████████▎ | 1691/2040 [16:38:17<3:23:23, 34.97s/it]08/16/2023 02:58:23 - INFO - __main__ -   Step: 1691, LR: 3.5270338554825673e-06, Loss: 0.3128756284713745
 83%|████████▎ | 1692/2040 [16:38:52<3:23:46, 35.13s/it]08/16/2023 02:58:58 - INFO - __main__ -   Step: 1692, LR: 3.516927741283477e-06, Loss: 0.3226695656776428
 83%|████████▎ | 1693/2040 [16:39:28<3:23:44, 35.23s/it]08/16/2023 02:59:34 - INFO - __main__ -   Step: 1693, LR: 3.5068216270843863e-06, Loss: 0.322287917137146
 83%|████████▎ | 1694/2040 [16:40:04<3:23:57, 35.37s/it]08/16/2023 03:00:09 - INFO - __main__ -   Step: 1694, LR: 3.496715512885296e-06, Loss: 0.3386102020740509
 83%|████████▎ | 1695/2040 [16:40:38<3:22:15, 35.17s/it]08/16/2023 03:00:44 - INFO - __main__ -   Step: 1695, LR: 3.4866093986862053e-06, Loss: 0.35510966181755066
 83%|████████▎ | 1696/2040 [16:41:13<3:21:38, 35.17s/it]08/16/2023 03:01:19 - INFO - __main__ -   Step: 1696, LR: 3.476503284487115e-06, Loss: 0.3496370315551758
 83%|████████▎ | 1697/2040 [16:41:50<3:23:22, 35.57s/it]08/16/2023 03:01:56 - INFO - __main__ -   Step: 1697, LR: 3.4663971702880243e-06, Loss: 0.3440243899822235
 83%|████████▎ | 1698/2040 [16:42:26<3:24:20, 35.85s/it]08/16/2023 03:02:32 - INFO - __main__ -   Step: 1698, LR: 3.456291056088934e-06, Loss: 0.34266358613967896
 83%|████████▎ | 1699/2040 [16:43:02<3:23:13, 35.76s/it]08/16/2023 03:03:08 - INFO - __main__ -   Step: 1699, LR: 3.4461849418898437e-06, Loss: 0.3207720220088959
 83%|████████▎ | 1700/2040 [16:43:37<3:20:55, 35.46s/it]08/16/2023 03:03:43 - INFO - __main__ -   Step: 1700, LR: 3.4360788276907532e-06, Loss: 0.36168235540390015
08/16/2023 03:03:43 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700
08/16/2023 03:03:43 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-16 03:03:43,171] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-16 03:03:43,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-16 03:03:43,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-16 03:03:43,177] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-16 03:03:43,177] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-16 03:03:43,177] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-16 03:03:43,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-16 03:03:43,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-16 03:03:43,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-16 03:03:43,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-16 03:03:43,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-16 03:03:43,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-16 03:03:43,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-16 03:03:43,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-16 03:03:43,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-16 03:04:03,218] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-16 03:04:03,218] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-16 03:04:04,030] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-16 03:04:04,031] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-16 03:04:04,529] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-16 03:04:04,529] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-16 03:04:07,270] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-16 03:04:07,270] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-16 03:04:07,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 03:04:07,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 03:04:07,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 03:04:07,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/16/2023 03:04:07 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/pytorch_model
08/16/2023 03:04:07 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/scheduler.bin
08/16/2023 03:04:07 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1700/random_states_0.pkl
 83%|████████▎ | 1701/2040 [16:44:37<4:02:16, 42.88s/it]08/16/2023 03:04:43 - INFO - __main__ -   Step: 1701, LR: 3.4259727134916627e-06, Loss: 0.350985586643219
 83%|████████▎ | 1702/2040 [16:45:12<3:49:06, 40.67s/it]08/16/2023 03:05:18 - INFO - __main__ -   Step: 1702, LR: 3.4158665992925722e-06, Loss: 0.3923112750053406
 83%|████████▎ | 1703/2040 [16:45:48<3:39:01, 39.00s/it]08/16/2023 03:05:53 - INFO - __main__ -   Step: 1703, LR: 3.4057604850934817e-06, Loss: 0.3328295946121216
 84%|████████▎ | 1704/2040 [16:46:23<3:33:00, 38.04s/it]08/16/2023 03:06:29 - INFO - __main__ -   Step: 1704, LR: 3.3956543708943912e-06, Loss: 0.31967872381210327
 84%|████████▎ | 1705/2040 [16:47:00<3:29:46, 37.57s/it]08/16/2023 03:07:06 - INFO - __main__ -   Step: 1705, LR: 3.3855482566953007e-06, Loss: 0.29774296283721924
 84%|████████▎ | 1706/2040 [16:47:35<3:25:48, 36.97s/it]08/16/2023 03:07:41 - INFO - __main__ -   Step: 1706, LR: 3.37544214249621e-06, Loss: 0.2854541838169098
 84%|████████▎ | 1707/2040 [16:48:11<3:22:29, 36.49s/it]08/16/2023 03:08:17 - INFO - __main__ -   Step: 1707, LR: 3.3653360282971197e-06, Loss: 0.46063029766082764
 84%|████████▎ | 1708/2040 [16:48:46<3:19:02, 35.97s/it]08/16/2023 03:08:51 - INFO - __main__ -   Step: 1708, LR: 3.355229914098029e-06, Loss: 0.3429097533226013
 84%|████████▍ | 1709/2040 [16:49:21<3:17:52, 35.87s/it]08/16/2023 03:09:27 - INFO - __main__ -   Step: 1709, LR: 3.3451237998989387e-06, Loss: 0.4070078432559967
 84%|████████▍ | 1710/2040 [16:49:58<3:18:25, 36.08s/it]08/16/2023 03:10:04 - INFO - __main__ -   Step: 1710, LR: 3.335017685699848e-06, Loss: 0.40356576442718506
 84%|████████▍ | 1711/2040 [16:50:34<3:17:36, 36.04s/it]08/16/2023 03:10:40 - INFO - __main__ -   Step: 1711, LR: 3.3249115715007585e-06, Loss: 0.30380192399024963
 84%|████████▍ | 1712/2040 [16:51:09<3:15:41, 35.80s/it]08/16/2023 03:11:15 - INFO - __main__ -   Step: 1712, LR: 3.314805457301668e-06, Loss: 0.3121034502983093
 84%|████████▍ | 1713/2040 [16:51:44<3:13:27, 35.50s/it]08/16/2023 03:11:50 - INFO - __main__ -   Step: 1713, LR: 3.3046993431025775e-06, Loss: 0.29964613914489746
 84%|████████▍ | 1714/2040 [16:52:19<3:12:39, 35.46s/it]08/16/2023 03:12:25 - INFO - __main__ -   Step: 1714, LR: 3.294593228903487e-06, Loss: 0.3450361490249634
 84%|████████▍ | 1715/2040 [16:52:55<3:12:01, 35.45s/it]08/16/2023 03:13:00 - INFO - __main__ -   Step: 1715, LR: 3.2844871147043965e-06, Loss: 0.3168236017227173
 84%|████████▍ | 1716/2040 [16:53:31<3:12:21, 35.62s/it]08/16/2023 03:13:36 - INFO - __main__ -   Step: 1716, LR: 3.274381000505306e-06, Loss: 0.39799946546554565
 84%|████████▍ | 1717/2040 [16:54:06<3:11:23, 35.55s/it]08/16/2023 03:14:12 - INFO - __main__ -   Step: 1717, LR: 3.2642748863062155e-06, Loss: 0.3589822053909302
 84%|████████▍ | 1718/2040 [16:54:41<3:10:06, 35.42s/it]08/16/2023 03:14:47 - INFO - __main__ -   Step: 1718, LR: 3.254168772107125e-06, Loss: 0.3486970067024231
 84%|████████▍ | 1719/2040 [16:55:17<3:09:39, 35.45s/it]08/16/2023 03:15:22 - INFO - __main__ -   Step: 1719, LR: 3.244062657908035e-06, Loss: 0.36563950777053833
 84%|████████▍ | 1720/2040 [16:55:52<3:09:12, 35.48s/it]08/16/2023 03:15:58 - INFO - __main__ -   Step: 1720, LR: 3.2339565437089444e-06, Loss: 0.32038605213165283
 84%|████████▍ | 1721/2040 [16:56:28<3:09:09, 35.58s/it]08/16/2023 03:16:34 - INFO - __main__ -   Step: 1721, LR: 3.223850429509854e-06, Loss: 0.2987963855266571
 84%|████████▍ | 1722/2040 [16:57:03<3:07:27, 35.37s/it]08/16/2023 03:17:09 - INFO - __main__ -   Step: 1722, LR: 3.2137443153107634e-06, Loss: 0.3787020146846771
 84%|████████▍ | 1723/2040 [16:57:38<3:06:26, 35.29s/it]08/16/2023 03:17:44 - INFO - __main__ -   Step: 1723, LR: 3.203638201111673e-06, Loss: 0.3857422173023224
 85%|████████▍ | 1724/2040 [16:58:13<3:05:23, 35.20s/it]08/16/2023 03:18:19 - INFO - __main__ -   Step: 1724, LR: 3.1935320869125824e-06, Loss: 0.32885628938674927
 85%|████████▍ | 1725/2040 [16:58:49<3:06:27, 35.52s/it]08/16/2023 03:18:55 - INFO - __main__ -   Step: 1725, LR: 3.183425972713492e-06, Loss: 0.36446645855903625
 85%|████████▍ | 1726/2040 [16:59:25<3:06:09, 35.57s/it]08/16/2023 03:19:31 - INFO - __main__ -   Step: 1726, LR: 3.1733198585144014e-06, Loss: 0.2938677966594696
 85%|████████▍ | 1727/2040 [17:00:00<3:05:38, 35.58s/it]08/16/2023 03:20:06 - INFO - __main__ -   Step: 1727, LR: 3.163213744315311e-06, Loss: 0.3363528251647949
 85%|████████▍ | 1728/2040 [17:00:36<3:04:14, 35.43s/it]08/16/2023 03:20:41 - INFO - __main__ -   Step: 1728, LR: 3.1531076301162204e-06, Loss: 0.41346850991249084
 85%|████████▍ | 1729/2040 [17:01:10<3:02:41, 35.25s/it]08/16/2023 03:21:16 - INFO - __main__ -   Step: 1729, LR: 3.14300151591713e-06, Loss: 0.3558812141418457
 85%|████████▍ | 1730/2040 [17:01:46<3:02:10, 35.26s/it]08/16/2023 03:21:52 - INFO - __main__ -   Step: 1730, LR: 3.13289540171804e-06, Loss: 0.36287808418273926
 85%|████████▍ | 1731/2040 [17:02:21<3:02:17, 35.40s/it]08/16/2023 03:22:27 - INFO - __main__ -   Step: 1731, LR: 3.1227892875189493e-06, Loss: 0.33890777826309204
 85%|████████▍ | 1732/2040 [17:02:57<3:01:49, 35.42s/it]08/16/2023 03:23:03 - INFO - __main__ -   Step: 1732, LR: 3.112683173319859e-06, Loss: 0.39847302436828613
 85%|████████▍ | 1733/2040 [17:03:32<3:01:32, 35.48s/it]08/16/2023 03:23:38 - INFO - __main__ -   Step: 1733, LR: 3.1025770591207683e-06, Loss: 0.3180433511734009
 85%|████████▌ | 1734/2040 [17:04:07<2:59:57, 35.28s/it]08/16/2023 03:24:13 - INFO - __main__ -   Step: 1734, LR: 3.092470944921678e-06, Loss: 0.359931617975235
 85%|████████▌ | 1735/2040 [17:04:43<3:00:21, 35.48s/it]08/16/2023 03:24:49 - INFO - __main__ -   Step: 1735, LR: 3.0823648307225873e-06, Loss: 0.35819655656814575
 85%|████████▌ | 1736/2040 [17:05:19<2:59:43, 35.47s/it]08/16/2023 03:25:25 - INFO - __main__ -   Step: 1736, LR: 3.072258716523497e-06, Loss: 0.28395208716392517
 85%|████████▌ | 1737/2040 [17:05:55<2:59:39, 35.58s/it]08/16/2023 03:26:00 - INFO - __main__ -   Step: 1737, LR: 3.0621526023244063e-06, Loss: 0.34792909026145935
 85%|████████▌ | 1738/2040 [17:06:30<2:59:14, 35.61s/it]08/16/2023 03:26:36 - INFO - __main__ -   Step: 1738, LR: 3.052046488125316e-06, Loss: 0.3360840678215027
 85%|████████▌ | 1739/2040 [17:07:06<2:58:19, 35.54s/it]08/16/2023 03:27:11 - INFO - __main__ -   Step: 1739, LR: 3.0419403739262253e-06, Loss: 0.34075820446014404
 85%|████████▌ | 1740/2040 [17:07:41<2:58:03, 35.61s/it]08/16/2023 03:27:47 - INFO - __main__ -   Step: 1740, LR: 3.031834259727135e-06, Loss: 0.4257625341415405
 85%|████████▌ | 1741/2040 [17:08:16<2:56:29, 35.42s/it]08/16/2023 03:28:22 - INFO - __main__ -   Step: 1741, LR: 3.0217281455280443e-06, Loss: 0.3642973303794861
 85%|████████▌ | 1742/2040 [17:08:52<2:56:08, 35.47s/it]08/16/2023 03:28:58 - INFO - __main__ -   Step: 1742, LR: 3.0116220313289542e-06, Loss: 0.3924594521522522
 85%|████████▌ | 1743/2040 [17:09:28<2:55:48, 35.52s/it]08/16/2023 03:29:33 - INFO - __main__ -   Step: 1743, LR: 3.0015159171298637e-06, Loss: 0.38205963373184204
 85%|████████▌ | 1744/2040 [17:10:03<2:55:45, 35.63s/it]08/16/2023 03:30:09 - INFO - __main__ -   Step: 1744, LR: 2.9914098029307732e-06, Loss: 0.3016113042831421
 86%|████████▌ | 1745/2040 [17:10:39<2:55:06, 35.62s/it]08/16/2023 03:30:45 - INFO - __main__ -   Step: 1745, LR: 2.981303688731683e-06, Loss: 0.29281020164489746
 86%|████████▌ | 1746/2040 [17:11:14<2:53:23, 35.38s/it]08/16/2023 03:31:20 - INFO - __main__ -   Step: 1746, LR: 2.9711975745325926e-06, Loss: 0.29274189472198486
 86%|████████▌ | 1747/2040 [17:11:49<2:52:47, 35.38s/it]08/16/2023 03:31:55 - INFO - __main__ -   Step: 1747, LR: 2.961091460333502e-06, Loss: 0.37319886684417725
 86%|████████▌ | 1748/2040 [17:12:25<2:52:29, 35.44s/it]08/16/2023 03:32:31 - INFO - __main__ -   Step: 1748, LR: 2.9509853461344116e-06, Loss: 0.36695533990859985
 86%|████████▌ | 1749/2040 [17:13:01<2:52:44, 35.62s/it]08/16/2023 03:33:07 - INFO - __main__ -   Step: 1749, LR: 2.940879231935321e-06, Loss: 0.3046680688858032
 86%|████████▌ | 1750/2040 [17:13:36<2:51:29, 35.48s/it]08/16/2023 03:33:42 - INFO - __main__ -   Step: 1750, LR: 2.930773117736231e-06, Loss: 0.379042387008667
 86%|████████▌ | 1751/2040 [17:14:11<2:50:21, 35.37s/it]08/16/2023 03:34:17 - INFO - __main__ -   Step: 1751, LR: 2.9206670035371405e-06, Loss: 0.4294686019420624
 86%|████████▌ | 1752/2040 [17:14:46<2:49:47, 35.37s/it]08/16/2023 03:34:52 - INFO - __main__ -   Step: 1752, LR: 2.91056088933805e-06, Loss: 0.3521721065044403
 86%|████████▌ | 1753/2040 [17:15:22<2:49:29, 35.44s/it]08/16/2023 03:35:28 - INFO - __main__ -   Step: 1753, LR: 2.9004547751389595e-06, Loss: 0.398608922958374
 86%|████████▌ | 1754/2040 [17:15:58<2:49:53, 35.64s/it]08/16/2023 03:36:04 - INFO - __main__ -   Step: 1754, LR: 2.890348660939869e-06, Loss: 0.35382145643234253
 86%|████████▌ | 1755/2040 [17:16:34<2:48:52, 35.55s/it]08/16/2023 03:36:39 - INFO - __main__ -   Step: 1755, LR: 2.8802425467407785e-06, Loss: 0.38008901476860046
 86%|████████▌ | 1756/2040 [17:17:09<2:48:19, 35.56s/it]08/16/2023 03:37:15 - INFO - __main__ -   Step: 1756, LR: 2.870136432541688e-06, Loss: 0.3101675510406494
 86%|████████▌ | 1757/2040 [17:17:44<2:47:13, 35.46s/it]08/16/2023 03:37:50 - INFO - __main__ -   Step: 1757, LR: 2.8600303183425975e-06, Loss: 0.335949569940567
 86%|████████▌ | 1758/2040 [17:18:20<2:47:25, 35.62s/it]08/16/2023 03:38:26 - INFO - __main__ -   Step: 1758, LR: 2.849924204143507e-06, Loss: 0.3614104390144348
 86%|████████▌ | 1759/2040 [17:18:56<2:46:58, 35.65s/it]08/16/2023 03:39:02 - INFO - __main__ -   Step: 1759, LR: 2.8398180899444165e-06, Loss: 0.34485191106796265
 86%|████████▋ | 1760/2040 [17:19:32<2:46:12, 35.62s/it]08/16/2023 03:39:38 - INFO - __main__ -   Step: 1760, LR: 2.829711975745326e-06, Loss: 0.3560473322868347
 86%|████████▋ | 1761/2040 [17:20:07<2:44:57, 35.48s/it]08/16/2023 03:40:13 - INFO - __main__ -   Step: 1761, LR: 2.819605861546236e-06, Loss: 0.350496381521225
 86%|████████▋ | 1762/2040 [17:20:42<2:43:43, 35.33s/it]08/16/2023 03:40:48 - INFO - __main__ -   Step: 1762, LR: 2.8094997473471454e-06, Loss: 0.3677451014518738
 86%|████████▋ | 1763/2040 [17:21:18<2:43:43, 35.46s/it]08/16/2023 03:41:23 - INFO - __main__ -   Step: 1763, LR: 2.799393633148055e-06, Loss: 0.29948222637176514
 86%|████████▋ | 1764/2040 [17:21:53<2:43:23, 35.52s/it]08/16/2023 03:41:59 - INFO - __main__ -   Step: 1764, LR: 2.7892875189489644e-06, Loss: 0.3225950598716736
 87%|████████▋ | 1765/2040 [17:22:29<2:43:15, 35.62s/it]08/16/2023 03:42:35 - INFO - __main__ -   Step: 1765, LR: 2.779181404749874e-06, Loss: 0.37261033058166504
 87%|████████▋ | 1766/2040 [17:23:04<2:41:29, 35.36s/it]08/16/2023 03:43:10 - INFO - __main__ -   Step: 1766, LR: 2.7690752905507834e-06, Loss: 0.35415101051330566
 87%|████████▋ | 1767/2040 [17:23:39<2:40:26, 35.26s/it]08/16/2023 03:43:45 - INFO - __main__ -   Step: 1767, LR: 2.758969176351693e-06, Loss: 0.29573482275009155
 87%|████████▋ | 1768/2040 [17:24:14<2:39:29, 35.18s/it]08/16/2023 03:44:20 - INFO - __main__ -   Step: 1768, LR: 2.7488630621526024e-06, Loss: 0.3230515718460083
 87%|████████▋ | 1769/2040 [17:24:49<2:39:03, 35.22s/it]08/16/2023 03:44:55 - INFO - __main__ -   Step: 1769, LR: 2.738756947953512e-06, Loss: 0.34285324811935425
 87%|████████▋ | 1770/2040 [17:25:25<2:39:22, 35.42s/it]08/16/2023 03:45:31 - INFO - __main__ -   Step: 1770, LR: 2.7286508337544214e-06, Loss: 0.39727702736854553
 87%|████████▋ | 1771/2040 [17:26:00<2:38:12, 35.29s/it]08/16/2023 03:46:06 - INFO - __main__ -   Step: 1771, LR: 2.718544719555331e-06, Loss: 0.31810158491134644
 87%|████████▋ | 1772/2040 [17:26:36<2:38:09, 35.41s/it]08/16/2023 03:46:42 - INFO - __main__ -   Step: 1772, LR: 2.7084386053562404e-06, Loss: 0.33145391941070557
 87%|████████▋ | 1773/2040 [17:27:10<2:36:36, 35.19s/it]08/16/2023 03:47:16 - INFO - __main__ -   Step: 1773, LR: 2.6983324911571503e-06, Loss: 0.4425513744354248
 87%|████████▋ | 1774/2040 [17:27:46<2:36:25, 35.28s/it]08/16/2023 03:47:52 - INFO - __main__ -   Step: 1774, LR: 2.68822637695806e-06, Loss: 0.4414374828338623
 87%|████████▋ | 1775/2040 [17:28:22<2:36:28, 35.43s/it]08/16/2023 03:48:28 - INFO - __main__ -   Step: 1775, LR: 2.6781202627589693e-06, Loss: 0.354347288608551
 87%|████████▋ | 1776/2040 [17:28:57<2:36:08, 35.49s/it]08/16/2023 03:49:03 - INFO - __main__ -   Step: 1776, LR: 2.668014148559879e-06, Loss: 0.3104860782623291
 87%|████████▋ | 1777/2040 [17:29:32<2:35:01, 35.37s/it]08/16/2023 03:49:38 - INFO - __main__ -   Step: 1777, LR: 2.6579080343607883e-06, Loss: 0.33739015460014343
 87%|████████▋ | 1778/2040 [17:30:07<2:33:47, 35.22s/it]08/16/2023 03:50:13 - INFO - __main__ -   Step: 1778, LR: 2.647801920161698e-06, Loss: 0.4160461723804474
 87%|████████▋ | 1779/2040 [17:30:43<2:33:28, 35.28s/it]08/16/2023 03:50:49 - INFO - __main__ -   Step: 1779, LR: 2.6376958059626073e-06, Loss: 0.3512738347053528
 87%|████████▋ | 1780/2040 [17:31:18<2:33:22, 35.39s/it]08/16/2023 03:51:24 - INFO - __main__ -   Step: 1780, LR: 2.6275896917635172e-06, Loss: 0.3144165873527527
 87%|████████▋ | 1781/2040 [17:31:53<2:32:29, 35.33s/it]08/16/2023 03:51:59 - INFO - __main__ -   Step: 1781, LR: 2.617483577564427e-06, Loss: 0.39186131954193115
 87%|████████▋ | 1782/2040 [17:32:28<2:31:18, 35.19s/it]08/16/2023 03:52:34 - INFO - __main__ -   Step: 1782, LR: 2.6073774633653366e-06, Loss: 0.3681271970272064
 87%|████████▋ | 1783/2040 [17:33:04<2:30:55, 35.23s/it]08/16/2023 03:53:10 - INFO - __main__ -   Step: 1783, LR: 2.597271349166246e-06, Loss: 0.3295183479785919
 87%|████████▋ | 1784/2040 [17:33:39<2:30:44, 35.33s/it]08/16/2023 03:53:45 - INFO - __main__ -   Step: 1784, LR: 2.5871652349671556e-06, Loss: 0.31182318925857544
 88%|████████▊ | 1785/2040 [17:34:15<2:30:41, 35.46s/it]08/16/2023 03:54:21 - INFO - __main__ -   Step: 1785, LR: 2.577059120768065e-06, Loss: 0.295301228761673
 88%|████████▊ | 1786/2040 [17:34:51<2:30:26, 35.54s/it]08/16/2023 03:54:57 - INFO - __main__ -   Step: 1786, LR: 2.5669530065689746e-06, Loss: 0.3625675439834595
 88%|████████▊ | 1787/2040 [17:35:27<2:31:10, 35.85s/it]08/16/2023 03:55:33 - INFO - __main__ -   Step: 1787, LR: 2.556846892369884e-06, Loss: 0.3601117730140686
 88%|████████▊ | 1788/2040 [17:36:03<2:30:11, 35.76s/it]08/16/2023 03:56:09 - INFO - __main__ -   Step: 1788, LR: 2.5467407781707936e-06, Loss: 0.3444564640522003
 88%|████████▊ | 1789/2040 [17:36:38<2:29:18, 35.69s/it]08/16/2023 03:56:44 - INFO - __main__ -   Step: 1789, LR: 2.536634663971703e-06, Loss: 0.3598853051662445
 88%|████████▊ | 1790/2040 [17:37:14<2:28:53, 35.73s/it]08/16/2023 03:57:20 - INFO - __main__ -   Step: 1790, LR: 2.5265285497726126e-06, Loss: 0.2822767496109009
 88%|████████▊ | 1791/2040 [17:37:50<2:27:46, 35.61s/it]08/16/2023 03:57:55 - INFO - __main__ -   Step: 1791, LR: 2.516422435573522e-06, Loss: 0.3910975456237793
 88%|████████▊ | 1792/2040 [17:38:25<2:26:44, 35.50s/it]08/16/2023 03:58:31 - INFO - __main__ -   Step: 1792, LR: 2.5063163213744316e-06, Loss: 0.3698570728302002
 88%|████████▊ | 1793/2040 [17:39:00<2:25:32, 35.36s/it]08/16/2023 03:59:06 - INFO - __main__ -   Step: 1793, LR: 2.4962102071753415e-06, Loss: 0.32547542452812195
 88%|████████▊ | 1794/2040 [17:39:37<2:26:50, 35.82s/it]08/16/2023 03:59:43 - INFO - __main__ -   Step: 1794, LR: 2.486104092976251e-06, Loss: 0.3845505118370056
 88%|████████▊ | 1795/2040 [17:40:12<2:26:06, 35.78s/it]08/16/2023 04:00:18 - INFO - __main__ -   Step: 1795, LR: 2.4759979787771605e-06, Loss: 0.34492260217666626
 88%|████████▊ | 1796/2040 [17:40:48<2:25:12, 35.71s/it]08/16/2023 04:00:54 - INFO - __main__ -   Step: 1796, LR: 2.46589186457807e-06, Loss: 0.3361022472381592
 88%|████████▊ | 1797/2040 [17:41:23<2:23:57, 35.55s/it]08/16/2023 04:01:29 - INFO - __main__ -   Step: 1797, LR: 2.4557857503789795e-06, Loss: 0.3157813549041748
 88%|████████▊ | 1798/2040 [17:41:58<2:22:39, 35.37s/it]08/16/2023 04:02:04 - INFO - __main__ -   Step: 1798, LR: 2.445679636179889e-06, Loss: 0.3483349084854126
 88%|████████▊ | 1799/2040 [17:42:33<2:21:49, 35.31s/it]08/16/2023 04:02:39 - INFO - __main__ -   Step: 1799, LR: 2.4355735219807985e-06, Loss: 0.35564813017845154
 88%|████████▊ | 1800/2040 [17:43:09<2:21:52, 35.47s/it]08/16/2023 04:03:15 - INFO - __main__ -   Step: 1800, LR: 2.425467407781708e-06, Loss: 0.3188121020793915
08/16/2023 04:03:15 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800
08/16/2023 04:03:15 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-16 04:03:15,443] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-16 04:03:15,449] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-16 04:03:15,449] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-16 04:03:15,449] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-16 04:03:15,449] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-16 04:03:15,450] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-16 04:03:15,450] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-16 04:03:15,460] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-16 04:03:15,460] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-16 04:03:15,460] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-16 04:03:15,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-16 04:03:15,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-16 04:03:15,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-16 04:03:15,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-16 04:03:15,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-16 04:03:36,397] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-16 04:03:36,398] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-16 04:03:36,963] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-16 04:03:36,963] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-16 04:03:37,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-16 04:03:37,518] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-16 04:03:39,594] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-16 04:03:39,594] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-16 04:03:39,598] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 04:03:39,598] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 04:03:39,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 04:03:39,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/16/2023 04:03:39 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/pytorch_model
08/16/2023 04:03:39 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/scheduler.bin
08/16/2023 04:03:39 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1800/random_states_0.pkl
 88%|████████▊ | 1801/2040 [17:44:08<2:49:37, 42.59s/it]08/16/2023 04:04:14 - INFO - __main__ -   Step: 1801, LR: 2.4153612935826175e-06, Loss: 0.324936181306839
 88%|████████▊ | 1802/2040 [17:44:44<2:40:34, 40.48s/it]08/16/2023 04:04:50 - INFO - __main__ -   Step: 1802, LR: 2.405255179383527e-06, Loss: 0.31900399923324585
 88%|████████▊ | 1803/2040 [17:45:20<2:34:51, 39.21s/it]08/16/2023 04:05:26 - INFO - __main__ -   Step: 1803, LR: 2.3951490651844365e-06, Loss: 0.30202484130859375
 88%|████████▊ | 1804/2040 [17:45:55<2:29:32, 38.02s/it]08/16/2023 04:06:01 - INFO - __main__ -   Step: 1804, LR: 2.3850429509853464e-06, Loss: 0.3389955759048462
 88%|████████▊ | 1805/2040 [17:46:31<2:26:13, 37.34s/it]08/16/2023 04:06:37 - INFO - __main__ -   Step: 1805, LR: 2.374936836786256e-06, Loss: 0.2880725860595703
 89%|████████▊ | 1806/2040 [17:47:06<2:22:45, 36.61s/it]08/16/2023 04:07:12 - INFO - __main__ -   Step: 1806, LR: 2.3648307225871654e-06, Loss: 0.36145609617233276
 89%|████████▊ | 1807/2040 [17:47:42<2:21:32, 36.45s/it]08/16/2023 04:07:48 - INFO - __main__ -   Step: 1807, LR: 2.354724608388075e-06, Loss: 0.36231815814971924
 89%|████████▊ | 1808/2040 [17:48:18<2:20:11, 36.26s/it]08/16/2023 04:08:24 - INFO - __main__ -   Step: 1808, LR: 2.344618494188985e-06, Loss: 0.347732812166214
 89%|████████▊ | 1809/2040 [17:48:54<2:19:18, 36.18s/it]08/16/2023 04:09:00 - INFO - __main__ -   Step: 1809, LR: 2.3345123799898943e-06, Loss: 0.3550356924533844
 89%|████████▊ | 1810/2040 [17:49:29<2:17:44, 35.93s/it]08/16/2023 04:09:35 - INFO - __main__ -   Step: 1810, LR: 2.324406265790804e-06, Loss: 0.36336085200309753
 89%|████████▉ | 1811/2040 [17:50:04<2:16:11, 35.68s/it]08/16/2023 04:10:10 - INFO - __main__ -   Step: 1811, LR: 2.3143001515917133e-06, Loss: 0.338485449552536
 89%|████████▉ | 1812/2040 [17:50:39<2:14:28, 35.39s/it]08/16/2023 04:10:45 - INFO - __main__ -   Step: 1812, LR: 2.304194037392623e-06, Loss: 0.33730757236480713
 89%|████████▉ | 1813/2040 [17:51:15<2:14:16, 35.49s/it]08/16/2023 04:11:21 - INFO - __main__ -   Step: 1813, LR: 2.2940879231935323e-06, Loss: 0.3275666832923889
 89%|████████▉ | 1814/2040 [17:51:51<2:14:41, 35.76s/it]08/16/2023 04:11:57 - INFO - __main__ -   Step: 1814, LR: 2.283981808994442e-06, Loss: 0.31806105375289917
 89%|████████▉ | 1815/2040 [17:52:34<2:21:39, 37.78s/it]08/16/2023 04:12:39 - INFO - __main__ -   Step: 1815, LR: 2.2738756947953513e-06, Loss: 0.43538323044776917
 89%|████████▉ | 1816/2040 [17:53:16<2:26:25, 39.22s/it]08/16/2023 04:13:22 - INFO - __main__ -   Step: 1816, LR: 2.263769580596261e-06, Loss: 0.30036845803260803
 89%|████████▉ | 1817/2040 [17:53:59<2:30:09, 40.40s/it]08/16/2023 04:14:05 - INFO - __main__ -   Step: 1817, LR: 2.2536634663971703e-06, Loss: 0.3746330142021179
 89%|████████▉ | 1818/2040 [17:54:41<2:30:28, 40.67s/it]08/16/2023 04:14:47 - INFO - __main__ -   Step: 1818, LR: 2.24355735219808e-06, Loss: 0.390263170003891
 89%|████████▉ | 1819/2040 [17:55:24<2:33:03, 41.55s/it]08/16/2023 04:15:30 - INFO - __main__ -   Step: 1819, LR: 2.2334512379989893e-06, Loss: 0.3216921091079712
 89%|████████▉ | 1820/2040 [17:56:07<2:34:13, 42.06s/it]08/16/2023 04:16:13 - INFO - __main__ -   Step: 1820, LR: 2.2233451237998992e-06, Loss: 0.3331056833267212
 89%|████████▉ | 1821/2040 [17:56:51<2:35:08, 42.50s/it]08/16/2023 04:16:57 - INFO - __main__ -   Step: 1821, LR: 2.2132390096008087e-06, Loss: 0.32336968183517456
 89%|████████▉ | 1822/2040 [17:57:35<2:35:50, 42.89s/it]08/16/2023 04:17:41 - INFO - __main__ -   Step: 1822, LR: 2.2031328954017182e-06, Loss: 0.2996249198913574
 89%|████████▉ | 1823/2040 [17:58:12<2:28:54, 41.17s/it]08/16/2023 04:18:18 - INFO - __main__ -   Step: 1823, LR: 2.1930267812026277e-06, Loss: 0.3480444550514221
 89%|████████▉ | 1824/2040 [17:58:56<2:31:14, 42.01s/it]08/16/2023 04:19:02 - INFO - __main__ -   Step: 1824, LR: 2.1829206670035376e-06, Loss: 0.394779771566391
 89%|████████▉ | 1825/2040 [17:59:38<2:30:45, 42.07s/it]08/16/2023 04:19:44 - INFO - __main__ -   Step: 1825, LR: 2.172814552804447e-06, Loss: 0.30441346764564514
 90%|████████▉ | 1826/2040 [18:00:22<2:32:03, 42.63s/it]08/16/2023 04:20:28 - INFO - __main__ -   Step: 1826, LR: 2.1627084386053566e-06, Loss: 0.39574316143989563
 90%|████████▉ | 1827/2040 [18:01:05<2:31:10, 42.58s/it]08/16/2023 04:21:10 - INFO - __main__ -   Step: 1827, LR: 2.152602324406266e-06, Loss: 0.37502461671829224
 90%|████████▉ | 1828/2040 [18:01:48<2:31:15, 42.81s/it]08/16/2023 04:21:54 - INFO - __main__ -   Step: 1828, LR: 2.1424962102071756e-06, Loss: 0.30355238914489746
 90%|████████▉ | 1829/2040 [18:02:30<2:30:19, 42.74s/it]08/16/2023 04:22:36 - INFO - __main__ -   Step: 1829, LR: 2.132390096008085e-06, Loss: 0.32677316665649414
 90%|████████▉ | 1830/2040 [18:03:14<2:30:08, 42.90s/it]08/16/2023 04:23:20 - INFO - __main__ -   Step: 1830, LR: 2.1222839818089946e-06, Loss: 0.2793155312538147
 90%|████████▉ | 1831/2040 [18:03:54<2:26:43, 42.12s/it]08/16/2023 04:24:00 - INFO - __main__ -   Step: 1831, LR: 2.112177867609904e-06, Loss: 0.3956298232078552
 90%|████████▉ | 1832/2040 [18:04:34<2:23:59, 41.54s/it]08/16/2023 04:24:40 - INFO - __main__ -   Step: 1832, LR: 2.1020717534108136e-06, Loss: 0.340304434299469
 90%|████████▉ | 1833/2040 [18:05:16<2:23:11, 41.50s/it]08/16/2023 04:25:22 - INFO - __main__ -   Step: 1833, LR: 2.091965639211723e-06, Loss: 0.3144579231739044
 90%|████████▉ | 1834/2040 [18:05:54<2:18:52, 40.45s/it]08/16/2023 04:26:00 - INFO - __main__ -   Step: 1834, LR: 2.0818595250126326e-06, Loss: 0.3012940287590027
 90%|████████▉ | 1835/2040 [18:06:36<2:20:05, 41.00s/it]08/16/2023 04:26:42 - INFO - __main__ -   Step: 1835, LR: 2.071753410813542e-06, Loss: 0.3630775809288025
 90%|█████████ | 1836/2040 [18:07:20<2:22:25, 41.89s/it]08/16/2023 04:27:26 - INFO - __main__ -   Step: 1836, LR: 2.061647296614452e-06, Loss: 0.37566226720809937
 90%|█████████ | 1837/2040 [18:07:56<2:15:45, 40.13s/it]08/16/2023 04:28:02 - INFO - __main__ -   Step: 1837, LR: 2.0515411824153615e-06, Loss: 0.3232959806919098
 90%|█████████ | 1838/2040 [18:08:38<2:16:35, 40.57s/it]08/16/2023 04:28:43 - INFO - __main__ -   Step: 1838, LR: 2.041435068216271e-06, Loss: 0.3215382397174835
 90%|█████████ | 1839/2040 [18:09:20<2:17:56, 41.18s/it]08/16/2023 04:29:26 - INFO - __main__ -   Step: 1839, LR: 2.0313289540171805e-06, Loss: 0.4305925667285919
 90%|█████████ | 1840/2040 [18:09:58<2:13:53, 40.17s/it]08/16/2023 04:30:04 - INFO - __main__ -   Step: 1840, LR: 2.02122283981809e-06, Loss: 0.3208065629005432
 90%|█████████ | 1841/2040 [18:10:41<2:16:34, 41.18s/it]08/16/2023 04:30:47 - INFO - __main__ -   Step: 1841, LR: 2.011116725619e-06, Loss: 0.32570406794548035
 90%|█████████ | 1842/2040 [18:11:18<2:11:20, 39.80s/it]08/16/2023 04:31:24 - INFO - __main__ -   Step: 1842, LR: 2.0010106114199094e-06, Loss: 0.33489638566970825
 90%|█████████ | 1843/2040 [18:12:01<2:14:11, 40.87s/it]08/16/2023 04:32:07 - INFO - __main__ -   Step: 1843, LR: 1.990904497220819e-06, Loss: 0.3501836657524109
 90%|█████████ | 1844/2040 [18:12:45<2:15:55, 41.61s/it]08/16/2023 04:32:51 - INFO - __main__ -   Step: 1844, LR: 1.9807983830217284e-06, Loss: 0.34157872200012207
 90%|█████████ | 1845/2040 [18:13:22<2:10:54, 40.28s/it]08/16/2023 04:33:28 - INFO - __main__ -   Step: 1845, LR: 1.970692268822638e-06, Loss: 0.33647483587265015
 90%|█████████ | 1846/2040 [18:13:59<2:06:59, 39.27s/it]08/16/2023 04:34:05 - INFO - __main__ -   Step: 1846, LR: 1.9605861546235474e-06, Loss: 0.2818124294281006
 91%|█████████ | 1847/2040 [18:14:42<2:09:57, 40.40s/it]08/16/2023 04:34:48 - INFO - __main__ -   Step: 1847, LR: 1.950480040424457e-06, Loss: 0.335759699344635
 91%|█████████ | 1848/2040 [18:15:25<2:11:43, 41.16s/it]08/16/2023 04:35:31 - INFO - __main__ -   Step: 1848, LR: 1.9403739262253664e-06, Loss: 0.32500380277633667
 91%|█████████ | 1849/2040 [18:16:01<2:06:20, 39.69s/it]08/16/2023 04:36:07 - INFO - __main__ -   Step: 1849, LR: 1.930267812026276e-06, Loss: 0.29905712604522705
 91%|█████████ | 1850/2040 [18:16:43<2:08:02, 40.44s/it]08/16/2023 04:36:49 - INFO - __main__ -   Step: 1850, LR: 1.9201616978271854e-06, Loss: 0.40329164266586304
 91%|█████████ | 1851/2040 [18:17:26<2:09:53, 41.23s/it]08/16/2023 04:37:32 - INFO - __main__ -   Step: 1851, LR: 1.9100555836280953e-06, Loss: 0.3581375181674957
 91%|█████████ | 1852/2040 [18:18:03<2:05:18, 39.99s/it]08/16/2023 04:38:09 - INFO - __main__ -   Step: 1852, LR: 1.8999494694290046e-06, Loss: 0.2801906168460846
 91%|█████████ | 1853/2040 [18:18:40<2:01:34, 39.01s/it]08/16/2023 04:38:46 - INFO - __main__ -   Step: 1853, LR: 1.889843355229914e-06, Loss: 0.37716132402420044
 91%|█████████ | 1854/2040 [18:19:23<2:04:02, 40.02s/it]08/16/2023 04:39:28 - INFO - __main__ -   Step: 1854, LR: 1.8797372410308238e-06, Loss: 0.3774917721748352
 91%|█████████ | 1855/2040 [18:20:04<2:04:24, 40.35s/it]08/16/2023 04:40:10 - INFO - __main__ -   Step: 1855, LR: 1.8696311268317333e-06, Loss: 0.2913612127304077
 91%|█████████ | 1856/2040 [18:20:47<2:06:11, 41.15s/it]08/16/2023 04:40:53 - INFO - __main__ -   Step: 1856, LR: 1.8595250126326428e-06, Loss: 0.3108154833316803
 91%|█████████ | 1857/2040 [18:21:30<2:07:04, 41.67s/it]08/16/2023 04:41:35 - INFO - __main__ -   Step: 1857, LR: 1.8494188984335523e-06, Loss: 0.3131052851676941
 91%|█████████ | 1858/2040 [18:22:07<2:02:56, 40.53s/it]08/16/2023 04:42:13 - INFO - __main__ -   Step: 1858, LR: 1.8393127842344622e-06, Loss: 0.3641979694366455
 91%|█████████ | 1859/2040 [18:22:50<2:03:52, 41.07s/it]08/16/2023 04:42:56 - INFO - __main__ -   Step: 1859, LR: 1.8292066700353717e-06, Loss: 0.338001012802124
 91%|█████████ | 1860/2040 [18:23:33<2:05:21, 41.78s/it]08/16/2023 04:43:39 - INFO - __main__ -   Step: 1860, LR: 1.8191005558362812e-06, Loss: 0.3705899715423584
 91%|█████████ | 1861/2040 [18:24:10<2:00:38, 40.44s/it]08/16/2023 04:44:16 - INFO - __main__ -   Step: 1861, LR: 1.8089944416371907e-06, Loss: 0.36309993267059326
 91%|█████████▏| 1862/2040 [18:24:47<1:56:09, 39.15s/it]08/16/2023 04:44:53 - INFO - __main__ -   Step: 1862, LR: 1.7988883274381002e-06, Loss: 0.3456500768661499
 91%|█████████▏| 1863/2040 [18:25:28<1:57:04, 39.68s/it]08/16/2023 04:45:33 - INFO - __main__ -   Step: 1863, LR: 1.7887822132390097e-06, Loss: 0.36691367626190186
 91%|█████████▏| 1864/2040 [18:26:11<1:59:34, 40.76s/it]08/16/2023 04:46:17 - INFO - __main__ -   Step: 1864, LR: 1.7786760990399194e-06, Loss: 0.3667106628417969
 91%|█████████▏| 1865/2040 [18:26:53<2:00:19, 41.25s/it]08/16/2023 04:46:59 - INFO - __main__ -   Step: 1865, LR: 1.768569984840829e-06, Loss: 0.3080874979496002
 91%|█████████▏| 1866/2040 [18:27:36<2:00:57, 41.71s/it]08/16/2023 04:47:42 - INFO - __main__ -   Step: 1866, LR: 1.7584638706417384e-06, Loss: 0.35629910230636597
 92%|█████████▏| 1867/2040 [18:28:19<2:01:36, 42.18s/it]08/16/2023 04:48:25 - INFO - __main__ -   Step: 1867, LR: 1.748357756442648e-06, Loss: 0.3312739431858063
 92%|█████████▏| 1868/2040 [18:29:03<2:01:52, 42.52s/it]08/16/2023 04:49:09 - INFO - __main__ -   Step: 1868, LR: 1.7382516422435574e-06, Loss: 0.3684147894382477
 92%|█████████▏| 1869/2040 [18:29:45<2:01:24, 42.60s/it]08/16/2023 04:49:51 - INFO - __main__ -   Step: 1869, LR: 1.728145528044467e-06, Loss: 0.35061806440353394
 92%|█████████▏| 1870/2040 [18:30:29<2:01:31, 42.89s/it]08/16/2023 04:50:35 - INFO - __main__ -   Step: 1870, LR: 1.7180394138453766e-06, Loss: 0.2665172815322876
 92%|█████████▏| 1871/2040 [18:31:09<1:58:32, 42.08s/it]08/16/2023 04:51:15 - INFO - __main__ -   Step: 1871, LR: 1.7079332996462861e-06, Loss: 0.338903546333313
 92%|█████████▏| 1872/2040 [18:31:45<1:52:17, 40.11s/it]08/16/2023 04:51:51 - INFO - __main__ -   Step: 1872, LR: 1.6978271854471956e-06, Loss: 0.31612080335617065
 92%|█████████▏| 1873/2040 [18:32:19<1:46:35, 38.30s/it]08/16/2023 04:52:25 - INFO - __main__ -   Step: 1873, LR: 1.687721071248105e-06, Loss: 0.366396963596344
 92%|█████████▏| 1874/2040 [18:32:55<1:43:52, 37.55s/it]08/16/2023 04:53:00 - INFO - __main__ -   Step: 1874, LR: 1.6776149570490146e-06, Loss: 0.3217342495918274
 92%|█████████▏| 1875/2040 [18:33:30<1:41:29, 36.90s/it]08/16/2023 04:53:36 - INFO - __main__ -   Step: 1875, LR: 1.667508842849924e-06, Loss: 0.3119703531265259
 92%|█████████▏| 1876/2040 [18:34:05<1:39:13, 36.30s/it]08/16/2023 04:54:11 - INFO - __main__ -   Step: 1876, LR: 1.657402728650834e-06, Loss: 0.3704943060874939
 92%|█████████▏| 1877/2040 [18:34:40<1:37:33, 35.91s/it]08/16/2023 04:54:46 - INFO - __main__ -   Step: 1877, LR: 1.6472966144517435e-06, Loss: 0.2877037227153778
 92%|█████████▏| 1878/2040 [18:35:14<1:35:52, 35.51s/it]08/16/2023 04:55:20 - INFO - __main__ -   Step: 1878, LR: 1.637190500252653e-06, Loss: 0.3325475752353668
 92%|█████████▏| 1879/2040 [18:35:50<1:35:18, 35.52s/it]08/16/2023 04:55:56 - INFO - __main__ -   Step: 1879, LR: 1.6270843860535625e-06, Loss: 0.33800360560417175
 92%|█████████▏| 1880/2040 [18:36:25<1:34:22, 35.39s/it]08/16/2023 04:56:31 - INFO - __main__ -   Step: 1880, LR: 1.6169782718544722e-06, Loss: 0.3853799104690552
 92%|█████████▏| 1881/2040 [18:37:00<1:33:26, 35.26s/it]08/16/2023 04:57:06 - INFO - __main__ -   Step: 1881, LR: 1.6068721576553817e-06, Loss: 0.3545699417591095
 92%|█████████▏| 1882/2040 [18:37:36<1:33:07, 35.36s/it]08/16/2023 04:57:42 - INFO - __main__ -   Step: 1882, LR: 1.5967660434562912e-06, Loss: 0.3549378514289856
 92%|█████████▏| 1883/2040 [18:38:11<1:32:11, 35.24s/it]08/16/2023 04:58:16 - INFO - __main__ -   Step: 1883, LR: 1.5866599292572007e-06, Loss: 0.322123646736145
 92%|█████████▏| 1884/2040 [18:38:45<1:31:00, 35.01s/it]08/16/2023 04:58:51 - INFO - __main__ -   Step: 1884, LR: 1.5765538150581102e-06, Loss: 0.3535512685775757
 92%|█████████▏| 1885/2040 [18:39:20<1:30:25, 35.01s/it]08/16/2023 04:59:26 - INFO - __main__ -   Step: 1885, LR: 1.56644770085902e-06, Loss: 0.36389249563217163
 92%|█████████▏| 1886/2040 [18:39:56<1:30:32, 35.28s/it]08/16/2023 05:00:02 - INFO - __main__ -   Step: 1886, LR: 1.5563415866599294e-06, Loss: 0.31196480989456177
 92%|█████████▎| 1887/2040 [18:40:31<1:29:41, 35.18s/it]08/16/2023 05:00:37 - INFO - __main__ -   Step: 1887, LR: 1.546235472460839e-06, Loss: 0.3582342565059662
 93%|█████████▎| 1888/2040 [18:41:06<1:28:57, 35.11s/it]08/16/2023 05:01:12 - INFO - __main__ -   Step: 1888, LR: 1.5361293582617484e-06, Loss: 0.3716484308242798
 93%|█████████▎| 1889/2040 [18:41:40<1:28:01, 34.98s/it]08/16/2023 05:01:46 - INFO - __main__ -   Step: 1889, LR: 1.526023244062658e-06, Loss: 0.3791157901287079
 93%|█████████▎| 1890/2040 [18:42:16<1:27:34, 35.03s/it]08/16/2023 05:02:22 - INFO - __main__ -   Step: 1890, LR: 1.5159171298635674e-06, Loss: 0.3408730924129486
 93%|█████████▎| 1891/2040 [18:42:51<1:26:53, 34.99s/it]08/16/2023 05:02:56 - INFO - __main__ -   Step: 1891, LR: 1.5058110156644771e-06, Loss: 0.41052693128585815
 93%|█████████▎| 1892/2040 [18:43:26<1:26:19, 34.99s/it]08/16/2023 05:03:31 - INFO - __main__ -   Step: 1892, LR: 1.4957049014653866e-06, Loss: 0.29225867986679077
 93%|█████████▎| 1893/2040 [18:44:00<1:25:05, 34.73s/it]08/16/2023 05:04:06 - INFO - __main__ -   Step: 1893, LR: 1.4855987872662963e-06, Loss: 0.39760398864746094
 93%|█████████▎| 1894/2040 [18:44:35<1:24:42, 34.81s/it]08/16/2023 05:04:41 - INFO - __main__ -   Step: 1894, LR: 1.4754926730672058e-06, Loss: 0.3559148907661438
 93%|█████████▎| 1895/2040 [18:45:10<1:24:29, 34.96s/it]08/16/2023 05:05:16 - INFO - __main__ -   Step: 1895, LR: 1.4653865588681155e-06, Loss: 0.3479951024055481
 93%|█████████▎| 1896/2040 [18:45:45<1:24:00, 35.00s/it]08/16/2023 05:05:51 - INFO - __main__ -   Step: 1896, LR: 1.455280444669025e-06, Loss: 0.34957996010780334
 93%|█████████▎| 1897/2040 [18:46:20<1:23:22, 34.98s/it]08/16/2023 05:06:26 - INFO - __main__ -   Step: 1897, LR: 1.4451743304699345e-06, Loss: 0.33572354912757874
 93%|█████████▎| 1898/2040 [18:46:54<1:22:18, 34.78s/it]08/16/2023 05:07:00 - INFO - __main__ -   Step: 1898, LR: 1.435068216270844e-06, Loss: 0.34022513031959534
 93%|█████████▎| 1899/2040 [18:47:29<1:21:26, 34.66s/it]08/16/2023 05:07:35 - INFO - __main__ -   Step: 1899, LR: 1.4249621020717535e-06, Loss: 0.332633376121521
 93%|█████████▎| 1900/2040 [18:48:04<1:21:13, 34.81s/it]08/16/2023 05:08:10 - INFO - __main__ -   Step: 1900, LR: 1.414855987872663e-06, Loss: 0.30083540081977844
08/16/2023 05:08:10 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900
08/16/2023 05:08:10 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-16 05:08:10,259] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-16 05:08:10,265] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-16 05:08:10,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-16 05:08:10,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-16 05:08:10,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-16 05:08:10,265] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-16 05:08:10,266] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-16 05:08:10,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-16 05:08:10,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-16 05:08:10,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-16 05:08:10,277] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-16 05:08:10,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-16 05:08:10,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-16 05:08:10,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-16 05:08:10,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-16 05:08:31,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-16 05:08:31,006] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-16 05:08:31,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-16 05:08:31,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-16 05:08:33,312] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-16 05:08:33,313] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-16 05:08:33,739] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-16 05:08:33,739] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-16 05:08:33,743] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 05:08:33,744] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 05:08:33,744] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 05:08:33,744] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/16/2023 05:08:33 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/pytorch_model
08/16/2023 05:08:33 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/scheduler.bin
08/16/2023 05:08:33 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_1900/random_states_0.pkl
 93%|█████████▎| 1901/2040 [18:49:02<1:37:03, 41.90s/it]08/16/2023 05:09:08 - INFO - __main__ -   Step: 1901, LR: 1.4047498736735727e-06, Loss: 0.30790382623672485
 93%|█████████▎| 1902/2040 [18:49:39<1:32:31, 40.23s/it]08/16/2023 05:09:45 - INFO - __main__ -   Step: 1902, LR: 1.3946437594744822e-06, Loss: 0.3344639837741852
 93%|█████████▎| 1903/2040 [18:50:13<1:28:11, 38.62s/it]08/16/2023 05:10:19 - INFO - __main__ -   Step: 1903, LR: 1.3845376452753917e-06, Loss: 0.3114214539527893
 93%|█████████▎| 1904/2040 [18:50:49<1:25:41, 37.81s/it]08/16/2023 05:10:55 - INFO - __main__ -   Step: 1904, LR: 1.3744315310763012e-06, Loss: 0.3719673156738281
 93%|█████████▎| 1905/2040 [18:51:24<1:23:02, 36.90s/it]08/16/2023 05:11:30 - INFO - __main__ -   Step: 1905, LR: 1.3643254168772107e-06, Loss: 0.3315703570842743
 93%|█████████▎| 1906/2040 [18:51:59<1:21:18, 36.40s/it]08/16/2023 05:12:05 - INFO - __main__ -   Step: 1906, LR: 1.3542193026781202e-06, Loss: 0.30201053619384766
 93%|█████████▎| 1907/2040 [18:52:35<1:20:05, 36.13s/it]08/16/2023 05:12:41 - INFO - __main__ -   Step: 1907, LR: 1.34411318847903e-06, Loss: 0.33042797446250916
 94%|█████████▎| 1908/2040 [18:53:11<1:19:18, 36.05s/it]08/16/2023 05:13:17 - INFO - __main__ -   Step: 1908, LR: 1.3340070742799394e-06, Loss: 0.41881582140922546
 94%|█████████▎| 1909/2040 [18:53:46<1:18:01, 35.74s/it]08/16/2023 05:13:52 - INFO - __main__ -   Step: 1909, LR: 1.323900960080849e-06, Loss: 0.36050453782081604
 94%|█████████▎| 1910/2040 [18:54:21<1:17:08, 35.60s/it]08/16/2023 05:14:27 - INFO - __main__ -   Step: 1910, LR: 1.3137948458817586e-06, Loss: 0.3415788412094116
 94%|█████████▎| 1911/2040 [18:54:56<1:16:17, 35.48s/it]08/16/2023 05:15:02 - INFO - __main__ -   Step: 1911, LR: 1.3036887316826683e-06, Loss: 0.30598005652427673
 94%|█████████▎| 1912/2040 [18:55:32<1:15:48, 35.53s/it]08/16/2023 05:15:38 - INFO - __main__ -   Step: 1912, LR: 1.2935826174835778e-06, Loss: 0.35620373487472534
 94%|█████████▍| 1913/2040 [18:56:08<1:15:28, 35.66s/it]08/16/2023 05:16:14 - INFO - __main__ -   Step: 1913, LR: 1.2834765032844873e-06, Loss: 0.31905925273895264
 94%|█████████▍| 1914/2040 [18:56:43<1:14:39, 35.55s/it]08/16/2023 05:16:49 - INFO - __main__ -   Step: 1914, LR: 1.2733703890853968e-06, Loss: 0.3264721930027008
 94%|█████████▍| 1915/2040 [18:57:19<1:14:00, 35.53s/it]08/16/2023 05:17:25 - INFO - __main__ -   Step: 1915, LR: 1.2632642748863063e-06, Loss: 0.3833627700805664
 94%|█████████▍| 1916/2040 [18:57:54<1:13:02, 35.34s/it]08/16/2023 05:17:59 - INFO - __main__ -   Step: 1916, LR: 1.2531581606872158e-06, Loss: 0.4061661958694458
 94%|█████████▍| 1917/2040 [18:58:29<1:12:29, 35.36s/it]08/16/2023 05:18:35 - INFO - __main__ -   Step: 1917, LR: 1.2430520464881255e-06, Loss: 0.3640868067741394
 94%|█████████▍| 1918/2040 [18:59:05<1:12:20, 35.58s/it]08/16/2023 05:19:11 - INFO - __main__ -   Step: 1918, LR: 1.232945932289035e-06, Loss: 0.3417882025241852
 94%|█████████▍| 1919/2040 [18:59:40<1:11:21, 35.38s/it]08/16/2023 05:19:46 - INFO - __main__ -   Step: 1919, LR: 1.2228398180899445e-06, Loss: 0.3948550224304199
 94%|█████████▍| 1920/2040 [19:00:15<1:10:40, 35.34s/it]08/16/2023 05:20:21 - INFO - __main__ -   Step: 1920, LR: 1.212733703890854e-06, Loss: 0.36010903120040894
 94%|█████████▍| 1921/2040 [19:00:50<1:09:54, 35.25s/it]08/16/2023 05:20:56 - INFO - __main__ -   Step: 1921, LR: 1.2026275896917635e-06, Loss: 0.37550032138824463
 94%|█████████▍| 1922/2040 [19:01:26<1:09:23, 35.28s/it]08/16/2023 05:21:32 - INFO - __main__ -   Step: 1922, LR: 1.1925214754926732e-06, Loss: 0.32003161311149597
 94%|█████████▍| 1923/2040 [19:02:02<1:09:17, 35.53s/it]08/16/2023 05:22:08 - INFO - __main__ -   Step: 1923, LR: 1.1824153612935827e-06, Loss: 0.33401980996131897
 94%|█████████▍| 1924/2040 [19:02:40<1:10:05, 36.25s/it]08/16/2023 05:22:46 - INFO - __main__ -   Step: 1924, LR: 1.1723092470944924e-06, Loss: 0.4073834717273712
 94%|█████████▍| 1925/2040 [19:03:16<1:09:41, 36.36s/it]08/16/2023 05:23:22 - INFO - __main__ -   Step: 1925, LR: 1.162203132895402e-06, Loss: 0.326355904340744
 94%|█████████▍| 1926/2040 [19:03:54<1:09:41, 36.68s/it]08/16/2023 05:24:00 - INFO - __main__ -   Step: 1926, LR: 1.1520970186963114e-06, Loss: 0.3448368310928345
 94%|█████████▍| 1927/2040 [19:04:29<1:08:20, 36.29s/it]08/16/2023 05:24:35 - INFO - __main__ -   Step: 1927, LR: 1.141990904497221e-06, Loss: 0.35433506965637207
 95%|█████████▍| 1928/2040 [19:05:05<1:07:23, 36.11s/it]08/16/2023 05:25:11 - INFO - __main__ -   Step: 1928, LR: 1.1318847902981304e-06, Loss: 0.37978994846343994
 95%|█████████▍| 1929/2040 [19:05:41<1:06:37, 36.01s/it]08/16/2023 05:25:46 - INFO - __main__ -   Step: 1929, LR: 1.12177867609904e-06, Loss: 0.3727930188179016
 95%|█████████▍| 1930/2040 [19:06:15<1:05:18, 35.62s/it]08/16/2023 05:26:21 - INFO - __main__ -   Step: 1930, LR: 1.1116725618999496e-06, Loss: 0.37627509236335754
 95%|█████████▍| 1931/2040 [19:06:51<1:04:30, 35.51s/it]08/16/2023 05:26:56 - INFO - __main__ -   Step: 1931, LR: 1.1015664477008591e-06, Loss: 0.3591727912425995
 95%|█████████▍| 1932/2040 [19:07:25<1:03:29, 35.27s/it]08/16/2023 05:27:31 - INFO - __main__ -   Step: 1932, LR: 1.0914603335017688e-06, Loss: 0.3129299283027649
 95%|█████████▍| 1933/2040 [19:08:01<1:03:04, 35.36s/it]08/16/2023 05:28:07 - INFO - __main__ -   Step: 1933, LR: 1.0813542193026783e-06, Loss: 0.36209797859191895
 95%|█████████▍| 1934/2040 [19:08:36<1:02:37, 35.45s/it]08/16/2023 05:28:42 - INFO - __main__ -   Step: 1934, LR: 1.0712481051035878e-06, Loss: 0.3428133428096771
 95%|█████████▍| 1935/2040 [19:09:11<1:01:47, 35.31s/it]08/16/2023 05:29:17 - INFO - __main__ -   Step: 1935, LR: 1.0611419909044973e-06, Loss: 0.32125958800315857
 95%|█████████▍| 1936/2040 [19:09:47<1:01:11, 35.30s/it]08/16/2023 05:29:53 - INFO - __main__ -   Step: 1936, LR: 1.0510358767054068e-06, Loss: 0.34460949897766113
 95%|█████████▍| 1937/2040 [19:10:22<1:00:20, 35.15s/it]08/16/2023 05:30:27 - INFO - __main__ -   Step: 1937, LR: 1.0409297625063163e-06, Loss: 0.31453660130500793
 95%|█████████▌| 1938/2040 [19:10:57<59:57, 35.27s/it]  08/16/2023 05:31:03 - INFO - __main__ -   Step: 1938, LR: 1.030823648307226e-06, Loss: 0.32568252086639404
 95%|█████████▌| 1939/2040 [19:11:34<1:00:11, 35.76s/it]08/16/2023 05:31:40 - INFO - __main__ -   Step: 1939, LR: 1.0207175341081355e-06, Loss: 0.3350030779838562
 95%|█████████▌| 1940/2040 [19:12:14<1:01:50, 37.11s/it]08/16/2023 05:32:20 - INFO - __main__ -   Step: 1940, LR: 1.010611419909045e-06, Loss: 0.373996764421463
 95%|█████████▌| 1941/2040 [19:12:52<1:01:20, 37.18s/it]08/16/2023 05:32:57 - INFO - __main__ -   Step: 1941, LR: 1.0005053057099547e-06, Loss: 0.39790624380111694
 95%|█████████▌| 1942/2040 [19:13:29<1:00:41, 37.16s/it]08/16/2023 05:33:35 - INFO - __main__ -   Step: 1942, LR: 9.903991915108642e-07, Loss: 0.3673447370529175
 95%|█████████▌| 1943/2040 [19:14:06<59:54, 37.05s/it]  08/16/2023 05:34:11 - INFO - __main__ -   Step: 1943, LR: 9.802930773117737e-07, Loss: 0.36924922466278076
 95%|█████████▌| 1944/2040 [19:14:42<59:06, 36.95s/it]08/16/2023 05:34:48 - INFO - __main__ -   Step: 1944, LR: 9.701869631126832e-07, Loss: 0.30806398391723633
 95%|█████████▌| 1945/2040 [19:15:21<59:27, 37.55s/it]08/16/2023 05:35:27 - INFO - __main__ -   Step: 1945, LR: 9.600808489135927e-07, Loss: 0.38129785656929016
 95%|█████████▌| 1946/2040 [19:15:57<58:14, 37.18s/it]08/16/2023 05:36:03 - INFO - __main__ -   Step: 1946, LR: 9.499747347145023e-07, Loss: 0.32410523295402527
 95%|█████████▌| 1947/2040 [19:16:34<57:11, 36.89s/it]08/16/2023 05:36:40 - INFO - __main__ -   Step: 1947, LR: 9.398686205154119e-07, Loss: 0.3488609790802002
 95%|█████████▌| 1948/2040 [19:17:09<55:50, 36.42s/it]08/16/2023 05:37:15 - INFO - __main__ -   Step: 1948, LR: 9.297625063163214e-07, Loss: 0.34156036376953125
 96%|█████████▌| 1949/2040 [19:17:45<55:02, 36.29s/it]08/16/2023 05:37:51 - INFO - __main__ -   Step: 1949, LR: 9.196563921172311e-07, Loss: 0.29806646704673767
 96%|█████████▌| 1950/2040 [19:18:21<54:05, 36.06s/it]08/16/2023 05:38:26 - INFO - __main__ -   Step: 1950, LR: 9.095502779181406e-07, Loss: 0.33240950107574463
 96%|█████████▌| 1951/2040 [19:18:56<53:08, 35.82s/it]08/16/2023 05:39:02 - INFO - __main__ -   Step: 1951, LR: 8.994441637190501e-07, Loss: 0.3843550682067871
 96%|█████████▌| 1952/2040 [19:19:31<52:09, 35.56s/it]08/16/2023 05:39:37 - INFO - __main__ -   Step: 1952, LR: 8.893380495199597e-07, Loss: 0.280661940574646
 96%|█████████▌| 1953/2040 [19:20:06<51:27, 35.49s/it]08/16/2023 05:40:12 - INFO - __main__ -   Step: 1953, LR: 8.792319353208692e-07, Loss: 0.29621392488479614
 96%|█████████▌| 1954/2040 [19:20:41<50:44, 35.40s/it]08/16/2023 05:40:47 - INFO - __main__ -   Step: 1954, LR: 8.691258211217787e-07, Loss: 0.2758488953113556
 96%|█████████▌| 1955/2040 [19:21:16<50:05, 35.35s/it]08/16/2023 05:41:22 - INFO - __main__ -   Step: 1955, LR: 8.590197069226883e-07, Loss: 0.2981804609298706
 96%|█████████▌| 1956/2040 [19:21:52<49:28, 35.33s/it]08/16/2023 05:41:58 - INFO - __main__ -   Step: 1956, LR: 8.489135927235978e-07, Loss: 0.4056704342365265
 96%|█████████▌| 1957/2040 [19:22:26<48:34, 35.11s/it]08/16/2023 05:42:32 - INFO - __main__ -   Step: 1957, LR: 8.388074785245073e-07, Loss: 0.30297696590423584
 96%|█████████▌| 1958/2040 [19:23:02<48:02, 35.16s/it]08/16/2023 05:43:08 - INFO - __main__ -   Step: 1958, LR: 8.28701364325417e-07, Loss: 0.3602027893066406
 96%|█████████▌| 1959/2040 [19:23:37<47:41, 35.33s/it]08/16/2023 05:43:43 - INFO - __main__ -   Step: 1959, LR: 8.185952501263265e-07, Loss: 0.31084078550338745
 96%|█████████▌| 1960/2040 [19:24:12<46:56, 35.21s/it]08/16/2023 05:44:18 - INFO - __main__ -   Step: 1960, LR: 8.084891359272361e-07, Loss: 0.29461604356765747
 96%|█████████▌| 1961/2040 [19:24:48<46:25, 35.25s/it]08/16/2023 05:44:54 - INFO - __main__ -   Step: 1961, LR: 7.983830217281456e-07, Loss: 0.3893336057662964
 96%|█████████▌| 1962/2040 [19:25:22<45:31, 35.02s/it]08/16/2023 05:45:28 - INFO - __main__ -   Step: 1962, LR: 7.882769075290551e-07, Loss: 0.41216209530830383
 96%|█████████▌| 1963/2040 [19:25:57<45:03, 35.11s/it]08/16/2023 05:46:03 - INFO - __main__ -   Step: 1963, LR: 7.781707933299647e-07, Loss: 0.35807478427886963
 96%|█████████▋| 1964/2040 [19:26:33<44:34, 35.19s/it]08/16/2023 05:46:39 - INFO - __main__ -   Step: 1964, LR: 7.680646791308742e-07, Loss: 0.33858487010002136
 96%|█████████▋| 1965/2040 [19:27:09<44:13, 35.37s/it]08/16/2023 05:47:15 - INFO - __main__ -   Step: 1965, LR: 7.579585649317837e-07, Loss: 0.27355659008026123
 96%|█████████▋| 1966/2040 [19:27:44<43:29, 35.26s/it]08/16/2023 05:47:50 - INFO - __main__ -   Step: 1966, LR: 7.478524507326933e-07, Loss: 0.301277756690979
 96%|█████████▋| 1967/2040 [19:28:20<43:10, 35.49s/it]08/16/2023 05:48:26 - INFO - __main__ -   Step: 1967, LR: 7.377463365336029e-07, Loss: 0.342621386051178
 96%|█████████▋| 1968/2040 [19:28:55<42:23, 35.33s/it]08/16/2023 05:49:01 - INFO - __main__ -   Step: 1968, LR: 7.276402223345125e-07, Loss: 0.29746323823928833
 97%|█████████▋| 1969/2040 [19:29:33<42:46, 36.15s/it]08/16/2023 05:49:39 - INFO - __main__ -   Step: 1969, LR: 7.17534108135422e-07, Loss: 0.39266592264175415
 97%|█████████▋| 1970/2040 [19:30:09<42:20, 36.29s/it]08/16/2023 05:50:15 - INFO - __main__ -   Step: 1970, LR: 7.074279939363315e-07, Loss: 0.33377575874328613
 97%|█████████▋| 1971/2040 [19:30:47<42:02, 36.56s/it]08/16/2023 05:50:52 - INFO - __main__ -   Step: 1971, LR: 6.973218797372411e-07, Loss: 0.33292102813720703
 97%|█████████▋| 1972/2040 [19:31:22<41:01, 36.20s/it]08/16/2023 05:51:28 - INFO - __main__ -   Step: 1972, LR: 6.872157655381506e-07, Loss: 0.34533339738845825
 97%|█████████▋| 1973/2040 [19:31:57<40:05, 35.90s/it]08/16/2023 05:52:03 - INFO - __main__ -   Step: 1973, LR: 6.771096513390601e-07, Loss: 0.35320138931274414
 97%|█████████▋| 1974/2040 [19:32:32<39:11, 35.62s/it]08/16/2023 05:52:38 - INFO - __main__ -   Step: 1974, LR: 6.670035371399697e-07, Loss: 0.3343128561973572
 97%|█████████▋| 1975/2040 [19:33:07<38:30, 35.55s/it]08/16/2023 05:53:13 - INFO - __main__ -   Step: 1975, LR: 6.568974229408793e-07, Loss: 0.35040032863616943
 97%|█████████▋| 1976/2040 [19:33:43<38:05, 35.71s/it]08/16/2023 05:53:49 - INFO - __main__ -   Step: 1976, LR: 6.467913087417889e-07, Loss: 0.370543509721756
 97%|█████████▋| 1977/2040 [19:34:18<37:16, 35.50s/it]08/16/2023 05:54:24 - INFO - __main__ -   Step: 1977, LR: 6.366851945426984e-07, Loss: 0.3853721618652344
 97%|█████████▋| 1978/2040 [19:34:54<36:34, 35.40s/it]08/16/2023 05:55:00 - INFO - __main__ -   Step: 1978, LR: 6.265790803436079e-07, Loss: 0.3158928453922272
 97%|█████████▋| 1979/2040 [19:35:28<35:44, 35.16s/it]08/16/2023 05:55:34 - INFO - __main__ -   Step: 1979, LR: 6.164729661445175e-07, Loss: 0.3192370533943176
 97%|█████████▋| 1980/2040 [19:36:04<35:17, 35.29s/it]08/16/2023 05:56:10 - INFO - __main__ -   Step: 1980, LR: 6.06366851945427e-07, Loss: 0.3664030134677887
 97%|█████████▋| 1981/2040 [19:36:39<34:45, 35.34s/it]08/16/2023 05:56:45 - INFO - __main__ -   Step: 1981, LR: 5.962607377463366e-07, Loss: 0.32240596413612366
 97%|█████████▋| 1982/2040 [19:37:15<34:19, 35.51s/it]08/16/2023 05:57:21 - INFO - __main__ -   Step: 1982, LR: 5.861546235472462e-07, Loss: 0.3707759380340576
 97%|█████████▋| 1983/2040 [19:37:50<33:32, 35.31s/it]08/16/2023 05:57:56 - INFO - __main__ -   Step: 1983, LR: 5.760485093481557e-07, Loss: 0.34839487075805664
 97%|█████████▋| 1984/2040 [19:38:25<32:52, 35.22s/it]08/16/2023 05:58:31 - INFO - __main__ -   Step: 1984, LR: 5.659423951490652e-07, Loss: 0.3676091432571411
 97%|█████████▋| 1985/2040 [19:39:01<32:32, 35.51s/it]08/16/2023 05:59:07 - INFO - __main__ -   Step: 1985, LR: 5.558362809499748e-07, Loss: 0.2931027412414551
 97%|█████████▋| 1986/2040 [19:39:37<31:53, 35.44s/it]08/16/2023 05:59:42 - INFO - __main__ -   Step: 1986, LR: 5.457301667508844e-07, Loss: 0.37298479676246643
 97%|█████████▋| 1987/2040 [19:40:12<31:22, 35.52s/it]08/16/2023 06:00:18 - INFO - __main__ -   Step: 1987, LR: 5.356240525517939e-07, Loss: 0.3312309980392456
 97%|█████████▋| 1988/2040 [19:40:47<30:42, 35.43s/it]08/16/2023 06:00:53 - INFO - __main__ -   Step: 1988, LR: 5.255179383527034e-07, Loss: 0.3432183861732483
 98%|█████████▊| 1989/2040 [19:41:22<29:59, 35.29s/it]08/16/2023 06:01:28 - INFO - __main__ -   Step: 1989, LR: 5.15411824153613e-07, Loss: 0.2827266454696655
 98%|█████████▊| 1990/2040 [19:41:57<29:17, 35.15s/it]08/16/2023 06:02:03 - INFO - __main__ -   Step: 1990, LR: 5.053057099545225e-07, Loss: 0.28526997566223145
 98%|█████████▊| 1991/2040 [19:42:32<28:41, 35.13s/it]08/16/2023 06:02:38 - INFO - __main__ -   Step: 1991, LR: 4.951995957554321e-07, Loss: 0.32413721084594727
 98%|█████████▊| 1992/2040 [19:43:08<28:16, 35.34s/it]08/16/2023 06:03:14 - INFO - __main__ -   Step: 1992, LR: 4.850934815563416e-07, Loss: 0.27794700860977173
 98%|█████████▊| 1993/2040 [19:43:43<27:39, 35.30s/it]08/16/2023 06:03:49 - INFO - __main__ -   Step: 1993, LR: 4.7498736735725115e-07, Loss: 0.31037139892578125
 98%|█████████▊| 1994/2040 [19:44:19<27:05, 35.34s/it]08/16/2023 06:04:25 - INFO - __main__ -   Step: 1994, LR: 4.648812531581607e-07, Loss: 0.33114326000213623
 98%|█████████▊| 1995/2040 [19:44:54<26:22, 35.17s/it]08/16/2023 06:04:59 - INFO - __main__ -   Step: 1995, LR: 4.547751389590703e-07, Loss: 0.3449026346206665
 98%|█████████▊| 1996/2040 [19:45:29<25:48, 35.19s/it]08/16/2023 06:05:35 - INFO - __main__ -   Step: 1996, LR: 4.4466902475997985e-07, Loss: 0.3400864601135254
 98%|█████████▊| 1997/2040 [19:46:04<25:08, 35.07s/it]08/16/2023 06:06:10 - INFO - __main__ -   Step: 1997, LR: 4.3456291056088935e-07, Loss: 0.4026346504688263
 98%|█████████▊| 1998/2040 [19:46:39<24:35, 35.13s/it]08/16/2023 06:06:45 - INFO - __main__ -   Step: 1998, LR: 4.244567963617989e-07, Loss: 0.35751986503601074
 98%|█████████▊| 1999/2040 [19:47:14<24:05, 35.24s/it]08/16/2023 06:07:20 - INFO - __main__ -   Step: 1999, LR: 4.143506821627085e-07, Loss: 0.36472415924072266
 98%|█████████▊| 2000/2040 [19:47:50<23:28, 35.22s/it]08/16/2023 06:07:55 - INFO - __main__ -   Step: 2000, LR: 4.0424456796361805e-07, Loss: 0.302884042263031
08/16/2023 06:07:55 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000
08/16/2023 06:07:55 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-08-16 06:07:55,955] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2023-08-16 06:07:55,960] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-08-16 06:07:55,960] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-08-16 06:07:55,960] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-08-16 06:07:55,960] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-08-16 06:07:55,961] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-08-16 06:07:55,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-08-16 06:07:55,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-08-16 06:07:55,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-08-16 06:07:55,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-08-16 06:07:55,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-08-16 06:07:55,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-08-16 06:07:55,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-08-16 06:07:55,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-08-16 06:07:55,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-08-16 06:08:16,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-08-16 06:08:16,479] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-08-16 06:08:16,537] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-08-16 06:08:16,538] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-08-16 06:08:16,935] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-08-16 06:08:16,936] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-08-16 06:08:17,834] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-08-16 06:08:17,834] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-08-16 06:08:17,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 06:08:17,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 06:08:17,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-08-16 06:08:17,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
08/16/2023 06:08:17 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/pytorch_model
08/16/2023 06:08:17 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/scheduler.bin
08/16/2023 06:08:17 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/step_2000/random_states_0.pkl
 98%|█████████▊| 2001/2040 [19:48:47<27:11, 41.82s/it]08/16/2023 06:08:53 - INFO - __main__ -   Step: 2001, LR: 3.9413845376452755e-07, Loss: 0.36702361702919006
 98%|█████████▊| 2002/2040 [19:49:22<25:15, 39.88s/it]08/16/2023 06:09:28 - INFO - __main__ -   Step: 2002, LR: 3.840323395654371e-07, Loss: 0.3642008900642395
 98%|█████████▊| 2003/2040 [19:49:57<23:40, 38.38s/it]08/16/2023 06:10:03 - INFO - __main__ -   Step: 2003, LR: 3.7392622536634665e-07, Loss: 0.36697202920913696
 98%|█████████▊| 2004/2040 [19:50:32<22:27, 37.43s/it]08/16/2023 06:10:38 - INFO - __main__ -   Step: 2004, LR: 3.6382011116725625e-07, Loss: 0.30841493606567383
 98%|█████████▊| 2005/2040 [19:51:08<21:27, 36.79s/it]08/16/2023 06:11:13 - INFO - __main__ -   Step: 2005, LR: 3.5371399696816575e-07, Loss: 0.28280121088027954
 98%|█████████▊| 2006/2040 [19:51:43<20:38, 36.42s/it]08/16/2023 06:11:49 - INFO - __main__ -   Step: 2006, LR: 3.436078827690753e-07, Loss: 0.37292808294296265
 98%|█████████▊| 2007/2040 [19:52:18<19:49, 36.06s/it]08/16/2023 06:12:24 - INFO - __main__ -   Step: 2007, LR: 3.3350176856998485e-07, Loss: 0.2918599545955658
 98%|█████████▊| 2008/2040 [19:52:53<19:02, 35.72s/it]08/16/2023 06:12:59 - INFO - __main__ -   Step: 2008, LR: 3.2339565437089445e-07, Loss: 0.32744425535202026
 98%|█████████▊| 2009/2040 [19:53:29<18:26, 35.70s/it]08/16/2023 06:13:35 - INFO - __main__ -   Step: 2009, LR: 3.1328954017180395e-07, Loss: 0.29611802101135254
 99%|█████████▊| 2010/2040 [19:54:04<17:44, 35.48s/it]08/16/2023 06:14:10 - INFO - __main__ -   Step: 2010, LR: 3.031834259727135e-07, Loss: 0.38678237795829773
 99%|█████████▊| 2011/2040 [19:54:39<17:09, 35.49s/it]08/16/2023 06:14:45 - INFO - __main__ -   Step: 2011, LR: 2.930773117736231e-07, Loss: 0.3059794306755066
 99%|█████████▊| 2012/2040 [19:55:14<16:30, 35.39s/it]08/16/2023 06:15:20 - INFO - __main__ -   Step: 2012, LR: 2.829711975745326e-07, Loss: 0.4005323648452759
 99%|█████████▊| 2013/2040 [19:55:50<15:57, 35.48s/it]08/16/2023 06:15:56 - INFO - __main__ -   Step: 2013, LR: 2.728650833754422e-07, Loss: 0.3595210909843445
 99%|█████████▊| 2014/2040 [19:56:25<15:18, 35.33s/it]08/16/2023 06:16:31 - INFO - __main__ -   Step: 2014, LR: 2.627589691763517e-07, Loss: 0.29902368783950806
 99%|█████████▉| 2015/2040 [19:57:01<14:44, 35.36s/it]08/16/2023 06:17:06 - INFO - __main__ -   Step: 2015, LR: 2.5265285497726125e-07, Loss: 0.28218963742256165
 99%|█████████▉| 2016/2040 [19:57:36<14:09, 35.39s/it]08/16/2023 06:17:42 - INFO - __main__ -   Step: 2016, LR: 2.425467407781708e-07, Loss: 0.41297972202301025
 99%|█████████▉| 2017/2040 [19:58:11<13:32, 35.33s/it]08/16/2023 06:18:17 - INFO - __main__ -   Step: 2017, LR: 2.3244062657908035e-07, Loss: 0.4109169840812683
 99%|█████████▉| 2018/2040 [19:58:47<12:57, 35.33s/it]08/16/2023 06:18:52 - INFO - __main__ -   Step: 2018, LR: 2.2233451237998993e-07, Loss: 0.3127637207508087
 99%|█████████▉| 2019/2040 [19:59:21<12:18, 35.17s/it]08/16/2023 06:19:27 - INFO - __main__ -   Step: 2019, LR: 2.1222839818089945e-07, Loss: 0.36412477493286133
 99%|█████████▉| 2020/2040 [19:59:57<11:44, 35.20s/it]08/16/2023 06:20:03 - INFO - __main__ -   Step: 2020, LR: 2.0212228398180903e-07, Loss: 0.3610806465148926
 99%|█████████▉| 2021/2040 [20:00:32<11:09, 35.26s/it]08/16/2023 06:20:38 - INFO - __main__ -   Step: 2021, LR: 1.9201616978271855e-07, Loss: 0.3477720022201538
 99%|█████████▉| 2022/2040 [20:01:08<10:37, 35.42s/it]08/16/2023 06:21:14 - INFO - __main__ -   Step: 2022, LR: 1.8191005558362813e-07, Loss: 0.31185364723205566
 99%|█████████▉| 2023/2040 [20:01:43<10:01, 35.36s/it]08/16/2023 06:21:49 - INFO - __main__ -   Step: 2023, LR: 1.7180394138453765e-07, Loss: 0.3636048436164856
 99%|█████████▉| 2024/2040 [20:02:18<09:23, 35.19s/it]08/16/2023 06:22:24 - INFO - __main__ -   Step: 2024, LR: 1.6169782718544723e-07, Loss: 0.2892524302005768
 99%|█████████▉| 2025/2040 [20:02:53<08:49, 35.29s/it]08/16/2023 06:22:59 - INFO - __main__ -   Step: 2025, LR: 1.5159171298635675e-07, Loss: 0.3945466876029968
 99%|█████████▉| 2026/2040 [20:03:29<08:15, 35.39s/it]08/16/2023 06:23:35 - INFO - __main__ -   Step: 2026, LR: 1.414855987872663e-07, Loss: 0.3747923970222473
 99%|█████████▉| 2027/2040 [20:04:05<07:41, 35.48s/it]08/16/2023 06:24:11 - INFO - __main__ -   Step: 2027, LR: 1.3137948458817585e-07, Loss: 0.3467763662338257
 99%|█████████▉| 2028/2040 [20:04:41<07:07, 35.59s/it]08/16/2023 06:24:46 - INFO - __main__ -   Step: 2028, LR: 1.212733703890854e-07, Loss: 0.32329297065734863
 99%|█████████▉| 2029/2040 [20:05:16<06:32, 35.66s/it]08/16/2023 06:25:22 - INFO - __main__ -   Step: 2029, LR: 1.1116725618999496e-07, Loss: 0.3262259364128113
100%|█████████▉| 2030/2040 [20:05:52<05:56, 35.63s/it]08/16/2023 06:25:58 - INFO - __main__ -   Step: 2030, LR: 1.0106114199090451e-07, Loss: 0.3147108256816864
100%|█████████▉| 2031/2040 [20:06:28<05:21, 35.71s/it]08/16/2023 06:26:34 - INFO - __main__ -   Step: 2031, LR: 9.095502779181406e-08, Loss: 0.3865603804588318
100%|█████████▉| 2032/2040 [20:07:03<04:44, 35.62s/it]08/16/2023 06:27:09 - INFO - __main__ -   Step: 2032, LR: 8.084891359272361e-08, Loss: 0.29179471731185913
100%|█████████▉| 2033/2040 [20:07:38<04:07, 35.42s/it]08/16/2023 06:27:44 - INFO - __main__ -   Step: 2033, LR: 7.074279939363315e-08, Loss: 0.3030816912651062
100%|█████████▉| 2034/2040 [20:08:14<03:32, 35.39s/it]08/16/2023 06:28:19 - INFO - __main__ -   Step: 2034, LR: 6.06366851945427e-08, Loss: 0.37311574816703796
100%|█████████▉| 2035/2040 [20:08:49<02:56, 35.37s/it]08/16/2023 06:28:55 - INFO - __main__ -   Step: 2035, LR: 5.053057099545226e-08, Loss: 0.29043325781822205
100%|█████████▉| 2036/2040 [20:09:25<02:22, 35.52s/it]08/16/2023 06:29:31 - INFO - __main__ -   Step: 2036, LR: 4.042445679636181e-08, Loss: 0.3186744451522827
100%|█████████▉| 2037/2040 [20:10:00<01:46, 35.39s/it]08/16/2023 06:30:06 - INFO - __main__ -   Step: 2037, LR: 3.031834259727135e-08, Loss: 0.34588566422462463
100%|█████████▉| 2038/2040 [20:10:35<01:10, 35.39s/it]08/16/2023 06:30:41 - INFO - __main__ -   Step: 2038, LR: 2.0212228398180903e-08, Loss: 0.3282558023929596
100%|█████████▉| 2039/2040 [20:11:11<00:35, 35.53s/it]08/16/2023 06:31:17 - INFO - __main__ -   Step: 2039, LR: 1.0106114199090452e-08, Loss: 0.2730196714401245
100%|██████████| 2040/2040 [20:11:43<00:00, 34.57s/it]08/16/2023 06:31:49 - INFO - __main__ -   Step: 2040, LR: 0.0, Loss: 0.27711158990859985
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: learning_rate ▃███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:    train_loss █▃▃▂▃▂▃▂▂▃▃▃▃▂▂▂▂▂▂▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb: learning_rate 0.0
wandb:    train_loss 0.27711
wandb: 
wandb: 🚀 View run 2023-08-14-2 at: https://wandb.ai/kidrain61/open_instruct/runs/ygi8r383
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230815_102000-ygi8r383/logs
tokenizer config file saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/tokenizer_config.json
Special tokens file saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/special_tokens_map.json
added tokens file saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/added_tokens.json
Configuration saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/config.json
Configuration saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/generation_config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf/pytorch_model.bin.index.json.
/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2089: UserWarning: Run (ygi8r383) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),
100%|██████████| 2040/2040 [20:12:11<00:00, 35.65s/it]
