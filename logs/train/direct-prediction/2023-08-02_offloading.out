[2023-08-02 13:25:49,746] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-02 13:25:54,410] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 13:25:54,421] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 13:25:54,425] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 13:25:54,427] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 13:25:55,391] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 13:25:55,392] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 13:25:55,392] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 13:25:55,392] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 13:25:55,392] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-08-02 13:25:55,422] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 13:25:55,422] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 13:25:55,471] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 13:25:55,471] [INFO] [comm.py:616:init_distributed] cdb=None
08/02/2023 13:25:55 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/02/2023 13:25:55 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/02/2023 13:25:55 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/02/2023 13:25:55 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

loading configuration file config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

loading file tokenizer.model from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/tokenizer.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/special_tokens_map.json
loading file tokenizer_config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/tokenizer_config.json
loading weights file model.safetensors from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/model.safetensors.index.json
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.31.0"
}

[2023-08-02 13:26:02,074] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.99s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.99s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.30s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.31s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.75s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.9,
  "top_p": 0.6,
  "transformers_version": "4.31.0"
}

Assigning <s> to the bos_token key of the tokenizer
Assigning </s> to the eos_token key of the tokenizer
Assigning <unk> to the unk_token key of the tokenizer
Assigning <pad> to the pad_token key of the tokenizer
Adding <pad> to the vocabulary
08/02/2023 13:26:16 - INFO - __main__ - Sample 69174 of the training set: {'input_ids': tensor([    1,  1152,   920,  1784,  1422,  8178,  1819,   310,   395, 29916,
        29938,   338,   779,  3676, 29912, 29916,   718, 29896, 29945, 29900,
         1042,   263,  6374,  6043, 29973,    13,  6295,   591, 29915,   276,
         3063,   363,  8178,  1819,   310,   395, 29916, 29938,   393,   723,
         1207,   779,  3676, 29912, 29916, 29974, 29896, 29945, 29900,  1042,
          263,  6374,  6043, 29973, 21700, 29901, 21104,    13,  7341, 29889,
        21700, 29901, 21104,    13,  6295,   393,   723,  2099,   393,   395,
        29916, 29974, 29896, 29945, 29900, 29938,   723,   505,   304,   367,
          263,  4922,  6862, 29889, 21700, 29901,  6374,    13,  7341, 29889,
        21700, 29901, 21104,    13,  6295,   591,  1033,  2436,   445,   408,
          395, 29916, 29974, 29896, 29945, 29900, 29922, 29876, 29985, 29906,
         1628,   988,   395, 29876, 29938,   338,   263,  6374,  6043, 29889,
        21700, 29901,  6374,    13,  2855,   591, 29915,   276,  3063,   363,
         8178,  1819,   310,   395, 29916,  1628,   577,   395, 29916, 29922,
        29876, 29985, 29906, 29899, 29896, 29945, 29900,  1504, 21700, 29901,
        21104,    13,  6295,   395, 29876, 29985, 29906, 29899, 29896, 29945,
        29900, 29966, 29900,  1504, 21700, 29901,  6374,    13,  6295,   395,
        29876, 29985, 29906, 29966, 29896, 29945, 29900,  1504, 21700, 29901,
         6374,    13,  6295,   395, 29876, 29966, 29896, 29906,  1504, 21700,
        29901,  8178]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100, 21104,  -100,  -100,  -100,
         -100,  -100, 21104,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,
         -100,  -100, 21104,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
        21104,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
08/02/2023 13:26:16 - INFO - __main__ - Sample 67278 of the training set: {'input_ids': tensor([    1, 10987,   278, 19087,   995, 29901,   259,   395,   320,  3676,
        29912, 29916,   998, 29906,  7517, 29916, 29974, 29896, 10869,  3676,
        29912, 29916,   998, 29906,  7402, 29916, 29974, 29896, 29913,   395,
           13, 29902,  8369,   393,   278,  4603,   338, 18348,   297,   921,
        29892,  6593,   393,  2381, 20304,   921,   322,   448, 29916,  1838,
        29915, 29873,  1735,   372, 29889, 21700, 29901,  6374,    13,  6295,
          306,  4997,   565,   921,   353, 29871, 29900,   338,   263,  1950,
        14020,   363,   278, 19087,   995, 29889, 21700, 29901,  6374,    13,
         3644,   921,   353, 29871, 29900, 29892,   769,   278,  4603,  5466,
        11057,   304,   779,  3676, 29912, 29896, 29913,   718,   320,  3676,
        29912, 29896, 29913,   353, 29871, 29906,  1504, 21700, 29901,  6374,
           13,  1762,  9659,   393,   445,   338,   278, 19087,   995, 29892,
          306,   817,   304,  1510,   393,   278,  4603,   338,  2337,  7621,
         1135,   470,  5186,   304, 29871, 29906,   363,   738,   916,   995,
          310,   921, 29889, 21700, 29901,  6374,    13,  6716,   982,   304,
          437,   393,   338,   304,   671,   278, 13862, 29899, 21576, 14585,
        29892,   607,  4083,   393,   363,   738,  1661, 29899, 22198,  3694,
          263,   322,   289, 29892,   278, 23342,  2099,   313, 29874,   718,
          289, 29897,   847, 29871, 29906,   338,  7621,  1135,   470,  5186,
          304,   278, 26224,  2099,   779,  3676, 29912,   370,  4311, 21700,
        29901,  6374,    13,  3644,   306,  3394,   445, 14585,   304,   278,
         1023,  6862, 16778,   297,   278,  4603, 29892,   306,   679, 29901,
           13,    13, 29938,   320,  1154,   741,  3676, 29912, 29916,   998,
        29906,  7517, 29916, 29974, 29896, 10869,  3676, 29912, 29916,   998,
        29906,  7402, 29916, 29974, 29896,  7585, 29906, 29913,   320,  6279,
          320,  3676,   741,  3676, 29912, 29916,   998, 29906,  7517, 29916,
        29974, 29896, 29913,   320,  3822,   320,  3676, 29912, 29916,   998,
        29906,  7402, 29916, 29974, 29896,   930,   395, 21700, 29901,  6374,
           13,  8942,   572,  9215,   278,  1492, 29899,  3179,  2625, 29892,
          306,   679, 29901,    13,    13, 29938,   320,  1154,   741,  3676,
        29912, 29916,   998, 29906,  7517, 29916, 29974, 29896, 10869,  3676,
        29912, 29916,   998, 29906,  7402, 29916, 29974, 29896,  7585, 29906,
        29913,   320,  6279,   320,  3676, 29912, 29916, 29985, 29906,   718,
        29871, 29896, 29913,   395, 21700, 29901,  8178]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1])}.
08/02/2023 13:26:16 - INFO - __main__ - Sample 31487 of the training set: {'input_ids': tensor([    1,  3118,  1196,   338,  5439,   491,    13, 29905,  7110,   463,
        29912, 12571, 29913, 29871, 29906,  2474, 29871, 29941,  2474, 29871,
        29946,   320,   355, 29912, 12571, 29913,   718,   260,   320,   463,
        29912, 12571, 29913, 29871, 29896,  2474, 29871, 29896,  2474,   448,
        29895,   320,   355, 29912, 12571,  1836, 18899,  2744,  1228,  1196,
          338,  5439,   491,    13, 29905,  7110,   463, 29912, 12571, 29913,
        29871, 29896,  2474, 29871, 29946,  2474, 29871, 29945,   320,   355,
        29912, 12571, 29913,   718,   318,   320,   463, 29912, 12571, 29913,
          413,  2474, 29871, 29906,  2474, 29871, 29896,   320,   355, 29912,
        12571,  1836, 18899,  3644,   278,  3454,   526,  5614,  6468,   279,
          313, 29875, 29889, 29872, 29889,   727,   338,   263, 10694,   393,
         3743,  1716,  3454,   511,   769,  1284,   599,  1950,  1819,   310,
          395, 29895,  7449,    13, 29902,  8369,   393,   278,  3454,   526,
         2183,   297, 25011,  2200,  4608,   883, 29892,   322,   393,   278,
         5305, 12047,   310,   278,  3454,   526,   779,   463, 29912, 12571,
        29913, 29871, 29896,  2474, 29871, 29896,  2474,   448, 29895,   320,
          355, 29912, 12571,  1042,   322,   779,   463, 29912, 12571, 29913,
          413,  2474, 29871, 29906,  2474, 29871, 29896,   320,   355, 29912,
        12571,  1836, 29938, 21700, 29901,  6374,    13, 29902, 17386,   393,
          565,   278,  3454,   526,  5614,  6468,   279, 29892,   769,   278,
         5305, 12047,  1818,   367,  5608,   368, 14278, 29892,  6593,   393,
          727,   338,   777, 17336,   395, 29883, 29938,  1316,   393,   779,
          463, 29912, 12571, 29913, 29871, 29896,  2474, 29871, 29896,  2474,
          448, 29895,   320,   355, 29912, 12571, 29913,   353,   274,   320,
          463, 29912, 12571, 29913,   413,  2474, 29871, 29906,  2474, 29871,
        29896,   320,   355, 29912, 12571,  1836, 29938, 21700, 29901,  8178]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, 8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
08/02/2023 13:26:16 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.
[2023-08-02 13:26:16,526] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
08/02/2023 13:26:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
08/02/2023 13:26:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
08/02/2023 13:26:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
08/02/2023 13:26:16 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/02/2023 13:26:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
08/02/2023 13:26:16 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/02/2023 13:26:16 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/02/2023 13:26:16 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-08-02 13:26:17,976] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-02 13:26:17,977] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-08-02 13:26:17,977] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-08-02 13:26:17,989] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-08-02 13:26:17,989] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-08-02 13:26:17,989] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2023-08-02 13:26:17,989] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2023-08-02 13:26:18,126] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
[2023-08-02 13:26:18,127] [INFO] [utils.py:786:see_memory_usage] MA 3.69 GB         Max_MA 4.27 GB         CA 5.69 GB         Max_CA 6 GB 
[2023-08-02 13:26:18,127] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 174.45 GB, percent = 17.3%
[2023-08-02 13:26:18,129] [INFO] [stage3.py:117:__init__] Reduce bucket size 16777216
[2023-08-02 13:26:18,129] [INFO] [stage3.py:118:__init__] Prefetch bucket size 15099494
[2023-08-02 13:26:18,217] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-02 13:26:18,218] [INFO] [utils.py:786:see_memory_usage] MA 3.69 GB         Max_MA 3.69 GB         CA 5.69 GB         Max_CA 6 GB 
[2023-08-02 13:26:18,218] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 174.45 GB, percent = 17.3%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2023-08-02 13:26:18,340] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-08-02 13:26:18,340] [INFO] [utils.py:786:see_memory_usage] MA 3.32 GB         Max_MA 3.75 GB         CA 5.69 GB         Max_CA 6 GB 
[2023-08-02 13:26:18,340] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 174.45 GB, percent = 17.3%
[2023-08-02 13:26:18,429] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
[2023-08-02 13:26:18,429] [INFO] [utils.py:786:see_memory_usage] MA 3.32 GB         Max_MA 3.32 GB         CA 5.69 GB         Max_CA 6 GB 
[2023-08-02 13:26:18,430] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 174.44 GB, percent = 17.3%
[2023-08-02 13:26:21,463] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 2
[2023-08-02 13:26:21,463] [INFO] [utils.py:786:see_memory_usage] MA 3.32 GB         Max_MA 3.32 GB         CA 3.78 GB         Max_CA 6 GB 
[2023-08-02 13:26:21,463] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 183.82 GB, percent = 18.2%
[2023-08-02 13:26:21,554] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
[2023-08-02 13:26:21,555] [INFO] [utils.py:786:see_memory_usage] MA 3.32 GB         Max_MA 3.32 GB         CA 3.78 GB         Max_CA 4 GB 
[2023-08-02 13:26:21,555] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 183.82 GB, percent = 18.2%
[2023-08-02 13:26:21,657] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
[2023-08-02 13:26:21,658] [INFO] [utils.py:786:see_memory_usage] MA 9.6 GB         Max_MA 10.87 GB         CA 11.92 GB         Max_CA 12 GB 
[2023-08-02 13:26:21,658] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 183.67 GB, percent = 18.2%
[2023-08-02 13:26:22,524] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-08-02 13:26:22,525] [INFO] [utils.py:786:see_memory_usage] MA 9.6 GB         Max_MA 9.6 GB         CA 11.92 GB         Max_CA 12 GB 
[2023-08-02 13:26:22,525] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 174.36 GB, percent = 17.3%
[2023-08-02 13:26:22,637] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-08-02 13:26:22,637] [INFO] [utils.py:786:see_memory_usage] MA 22.15 GB         Max_MA 30.97 GB         CA 35.69 GB         Max_CA 36 GB 
[2023-08-02 13:26:22,637] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 174.37 GB, percent = 17.3%
[2023-08-02 13:26:22,638] [INFO] [stage3.py:424:_setup_for_real_optimizer] optimizer state initialized
[2023-08-02 13:26:22,819] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-08-02 13:26:22,820] [INFO] [utils.py:786:see_memory_usage] MA 25.32 GB         Max_MA 25.81 GB         CA 35.69 GB         Max_CA 36 GB 
[2023-08-02 13:26:22,820] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 174.39 GB, percent = 17.3%
[2023-08-02 13:26:22,820] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-08-02 13:26:22,820] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-02 13:26:22,820] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-02 13:26:22,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-08-02 13:26:22,821] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb0d539e050>
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-02 13:26:22,821] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 16
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   train_batch_size ............. 128
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-02 13:26:22,822] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-02 13:26:22,823] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-02 13:26:22,823] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-08-02 13:26:22,823] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-02 13:26:22,823] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3
[2023-08-02 13:26:22,823] [INFO] [config.py:950:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
08/02/2023 13:26:22 - INFO - __main__ - ***** Running training *****
08/02/2023 13:26:22 - INFO - __main__ -   Num examples = 87012
08/02/2023 13:26:22 - INFO - __main__ -   Num Epochs = 3
08/02/2023 13:26:22 - INFO - __main__ -   Instantaneous batch size per device = 2
08/02/2023 13:26:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128
08/02/2023 13:26:22 - INFO - __main__ -   Gradient Accumulation steps = 16
08/02/2023 13:26:22 - INFO - __main__ -   Total optimization steps = 2040
  0%|          | 0/2040 [00:00<?, ?it/s]  0%|          | 1/2040 [00:12<6:53:09, 12.16s/it]08/02/2023 13:26:34 - INFO - __main__ -   Step: 1, LR: 3.278688524590164e-07, Loss: 2.68446683883667
  0%|          | 2/2040 [00:24<6:56:34, 12.26s/it]08/02/2023 13:26:47 - INFO - __main__ -   Step: 2, LR: 6.557377049180328e-07, Loss: 2.6965115070343018
  0%|          | 3/2040 [00:35<6:34:33, 11.62s/it]08/02/2023 13:26:58 - INFO - __main__ -   Step: 3, LR: 9.836065573770493e-07, Loss: 2.552764892578125
  0%|          | 4/2040 [00:46<6:27:19, 11.41s/it]08/02/2023 13:27:09 - INFO - __main__ -   Step: 4, LR: 1.3114754098360657e-06, Loss: 2.869081735610962
  0%|          | 5/2040 [00:59<6:41:36, 11.84s/it]08/02/2023 13:27:21 - INFO - __main__ -   Step: 5, LR: 1.6393442622950819e-06, Loss: 2.3962655067443848
  0%|          | 6/2040 [01:08<6:16:47, 11.11s/it]08/02/2023 13:27:31 - INFO - __main__ -   Step: 6, LR: 1.9672131147540985e-06, Loss: 2.4182069301605225
  0%|          | 7/2040 [01:20<6:28:19, 11.46s/it]08/02/2023 13:27:43 - INFO - __main__ -   Step: 7, LR: 2.295081967213115e-06, Loss: 1.867387294769287
  0%|          | 8/2040 [01:31<6:18:22, 11.17s/it]08/02/2023 13:27:54 - INFO - __main__ -   Step: 8, LR: 2.6229508196721314e-06, Loss: 1.8669624328613281
  0%|          | 9/2040 [01:42<6:13:38, 11.04s/it]08/02/2023 13:28:05 - INFO - __main__ -   Step: 9, LR: 2.9508196721311478e-06, Loss: 0.794930636882782
  0%|          | 10/2040 [01:54<6:21:59, 11.29s/it]08/02/2023 13:28:16 - INFO - __main__ -   Step: 10, LR: 3.2786885245901638e-06, Loss: 0.8344652652740479
  1%|          | 11/2040 [02:03<5:57:19, 10.57s/it]08/02/2023 13:28:25 - INFO - __main__ -   Step: 11, LR: 3.6065573770491806e-06, Loss: 0.8019883036613464
  1%|          | 12/2040 [02:11<5:33:46,  9.88s/it]08/02/2023 13:28:34 - INFO - __main__ -   Step: 12, LR: 3.934426229508197e-06, Loss: 0.798531174659729
  1%|          | 13/2040 [02:19<5:20:01,  9.47s/it]08/02/2023 13:28:42 - INFO - __main__ -   Step: 13, LR: 4.2622950819672135e-06, Loss: 0.6699624061584473
  1%|          | 14/2040 [02:28<5:10:16,  9.19s/it]08/02/2023 13:28:51 - INFO - __main__ -   Step: 14, LR: 4.59016393442623e-06, Loss: 0.7817422747612
  1%|          | 15/2040 [02:36<5:03:54,  9.00s/it]08/02/2023 13:28:59 - INFO - __main__ -   Step: 15, LR: 4.918032786885246e-06, Loss: 0.7861524820327759
  1%|          | 16/2040 [02:45<5:01:32,  8.94s/it]08/02/2023 13:29:08 - INFO - __main__ -   Step: 16, LR: 5.245901639344263e-06, Loss: 0.6694778203964233
  1%|          | 17/2040 [02:54<4:57:28,  8.82s/it]08/02/2023 13:29:17 - INFO - __main__ -   Step: 17, LR: 5.573770491803278e-06, Loss: 0.7145964503288269
  1%|          | 18/2040 [03:02<4:50:30,  8.62s/it]08/02/2023 13:29:25 - INFO - __main__ -   Step: 18, LR: 5.9016393442622956e-06, Loss: 0.7774875164031982
  1%|          | 19/2040 [03:11<4:50:40,  8.63s/it]08/02/2023 13:29:33 - INFO - __main__ -   Step: 19, LR: 6.229508196721312e-06, Loss: 0.9998372197151184
  1%|          | 20/2040 [03:19<4:46:37,  8.51s/it]08/02/2023 13:29:42 - INFO - __main__ -   Step: 20, LR: 6.5573770491803276e-06, Loss: 0.7271252870559692
  1%|          | 21/2040 [03:28<4:48:33,  8.58s/it]08/02/2023 13:29:50 - INFO - __main__ -   Step: 21, LR: 6.885245901639345e-06, Loss: 0.7732232809066772
  1%|          | 22/2040 [03:38<5:06:15,  9.11s/it]08/02/2023 13:30:01 - INFO - __main__ -   Step: 22, LR: 7.213114754098361e-06, Loss: 0.8937124609947205
  1%|          | 23/2040 [03:50<5:32:59,  9.91s/it]08/02/2023 13:30:13 - INFO - __main__ -   Step: 23, LR: 7.540983606557377e-06, Loss: 0.8518272638320923
  1%|          | 24/2040 [03:58<5:20:29,  9.54s/it]08/02/2023 13:30:21 - INFO - __main__ -   Step: 24, LR: 7.868852459016394e-06, Loss: 0.6980329751968384
  1%|          | 25/2040 [04:10<5:45:11, 10.28s/it]08/02/2023 13:30:33 - INFO - __main__ -   Step: 25, LR: 8.19672131147541e-06, Loss: 0.6779614090919495
  1%|▏         | 26/2040 [04:21<5:53:14, 10.52s/it]08/02/2023 13:30:44 - INFO - __main__ -   Step: 26, LR: 8.524590163934427e-06, Loss: 0.6490184664726257
  1%|▏         | 27/2040 [04:31<5:43:13, 10.23s/it]08/02/2023 13:30:54 - INFO - __main__ -   Step: 27, LR: 8.852459016393443e-06, Loss: 0.7459166049957275
  1%|▏         | 28/2040 [04:43<6:00:55, 10.76s/it]08/02/2023 13:31:06 - INFO - __main__ -   Step: 28, LR: 9.18032786885246e-06, Loss: 0.7411936521530151
  1%|▏         | 29/2040 [04:53<5:53:00, 10.53s/it]08/02/2023 13:31:16 - INFO - __main__ -   Step: 29, LR: 9.508196721311476e-06, Loss: 0.732094407081604
  1%|▏         | 30/2040 [05:02<5:35:06, 10.00s/it]08/02/2023 13:31:25 - INFO - __main__ -   Step: 30, LR: 9.836065573770493e-06, Loss: 0.7044163942337036
  2%|▏         | 31/2040 [05:14<5:57:01, 10.66s/it]08/02/2023 13:31:37 - INFO - __main__ -   Step: 31, LR: 1.0163934426229509e-05, Loss: 0.7752400040626526
  2%|▏         | 32/2040 [05:25<6:03:10, 10.85s/it]08/02/2023 13:31:48 - INFO - __main__ -   Step: 32, LR: 1.0491803278688525e-05, Loss: 0.7703897953033447
  2%|▏         | 33/2040 [05:33<5:36:06, 10.05s/it]08/02/2023 13:31:56 - INFO - __main__ -   Step: 33, LR: 1.0819672131147544e-05, Loss: 0.695734441280365
  2%|▏         | 34/2040 [05:42<5:24:20,  9.70s/it]08/02/2023 13:32:05 - INFO - __main__ -   Step: 34, LR: 1.1147540983606557e-05, Loss: 0.6005837321281433
  2%|▏         | 35/2040 [05:51<5:12:55,  9.36s/it]08/02/2023 13:32:14 - INFO - __main__ -   Step: 35, LR: 1.1475409836065575e-05, Loss: 0.6203810572624207
  2%|▏         | 36/2040 [05:59<5:03:28,  9.09s/it]08/02/2023 13:32:22 - INFO - __main__ -   Step: 36, LR: 1.1803278688524591e-05, Loss: 0.5729089975357056
  2%|▏         | 37/2040 [06:08<5:00:19,  9.00s/it]08/02/2023 13:32:31 - INFO - __main__ -   Step: 37, LR: 1.2131147540983608e-05, Loss: 0.5871245861053467
  2%|▏         | 38/2040 [06:17<4:54:24,  8.82s/it]08/02/2023 13:32:39 - INFO - __main__ -   Step: 38, LR: 1.2459016393442624e-05, Loss: 0.721016526222229
  2%|▏         | 39/2040 [06:25<4:52:35,  8.77s/it]08/02/2023 13:32:48 - INFO - __main__ -   Step: 39, LR: 1.2786885245901642e-05, Loss: 0.6865798234939575
  2%|▏         | 40/2040 [06:36<5:14:19,  9.43s/it]08/02/2023 13:32:59 - INFO - __main__ -   Step: 40, LR: 1.3114754098360655e-05, Loss: 0.7364456057548523
  2%|▏         | 41/2040 [06:48<5:39:56, 10.20s/it]08/02/2023 13:33:11 - INFO - __main__ -   Step: 41, LR: 1.3442622950819673e-05, Loss: 0.6181575059890747
  2%|▏         | 42/2040 [06:57<5:25:42,  9.78s/it]08/02/2023 13:33:20 - INFO - __main__ -   Step: 42, LR: 1.377049180327869e-05, Loss: 0.6101142168045044
  2%|▏         | 43/2040 [07:08<5:34:52, 10.06s/it]08/02/2023 13:33:31 - INFO - __main__ -   Step: 43, LR: 1.4098360655737706e-05, Loss: 0.6015572547912598
  2%|▏         | 44/2040 [07:19<5:42:56, 10.31s/it]08/02/2023 13:33:41 - INFO - __main__ -   Step: 44, LR: 1.4426229508196722e-05, Loss: 0.5588524341583252
  2%|▏         | 45/2040 [07:28<5:38:17, 10.17s/it]08/02/2023 13:33:51 - INFO - __main__ -   Step: 45, LR: 1.4754098360655739e-05, Loss: 0.6347209215164185
  2%|▏         | 46/2040 [07:37<5:25:50,  9.80s/it]08/02/2023 13:34:00 - INFO - __main__ -   Step: 46, LR: 1.5081967213114754e-05, Loss: 0.599070131778717
  2%|▏         | 47/2040 [07:46<5:11:18,  9.37s/it]08/02/2023 13:34:09 - INFO - __main__ -   Step: 47, LR: 1.5409836065573772e-05, Loss: 0.6240242123603821
  2%|▏         | 48/2040 [07:55<5:13:42,  9.45s/it]08/02/2023 13:34:18 - INFO - __main__ -   Step: 48, LR: 1.5737704918032788e-05, Loss: 0.6170439720153809
  2%|▏         | 49/2040 [08:07<5:37:59, 10.19s/it]08/02/2023 13:34:30 - INFO - __main__ -   Step: 49, LR: 1.6065573770491805e-05, Loss: 0.5719351172447205
  2%|▏         | 50/2040 [08:19<5:49:00, 10.52s/it]08/02/2023 13:34:41 - INFO - __main__ -   Step: 50, LR: 1.639344262295082e-05, Loss: 0.611649751663208
  2%|▎         | 51/2040 [08:31<6:04:26, 10.99s/it]08/02/2023 13:34:54 - INFO - __main__ -   Step: 51, LR: 1.6721311475409837e-05, Loss: 0.6517969369888306
  3%|▎         | 52/2040 [08:43<6:14:26, 11.30s/it]08/02/2023 13:35:06 - INFO - __main__ -   Step: 52, LR: 1.7049180327868854e-05, Loss: 0.5759242177009583
  3%|▎         | 53/2040 [08:52<5:53:02, 10.66s/it]08/02/2023 13:35:15 - INFO - __main__ -   Step: 53, LR: 1.737704918032787e-05, Loss: 0.6329075694084167
  3%|▎         | 54/2040 [09:00<5:30:31,  9.99s/it]08/02/2023 13:35:23 - INFO - __main__ -   Step: 54, LR: 1.7704918032786887e-05, Loss: 0.6635950803756714
  3%|▎         | 55/2040 [09:09<5:14:58,  9.52s/it]08/02/2023 13:35:32 - INFO - __main__ -   Step: 55, LR: 1.8032786885245903e-05, Loss: 0.6552144885063171
  3%|▎         | 56/2040 [09:17<5:04:46,  9.22s/it]08/02/2023 13:35:40 - INFO - __main__ -   Step: 56, LR: 1.836065573770492e-05, Loss: 0.5634685754776001
  3%|▎         | 57/2040 [09:26<4:56:17,  8.96s/it]08/02/2023 13:35:48 - INFO - __main__ -   Step: 57, LR: 1.8688524590163936e-05, Loss: 0.5991988182067871
  3%|▎         | 58/2040 [09:35<4:57:19,  9.00s/it]08/02/2023 13:35:58 - INFO - __main__ -   Step: 58, LR: 1.9016393442622952e-05, Loss: 0.575473427772522
  3%|▎         | 59/2040 [09:46<5:22:28,  9.77s/it]08/02/2023 13:36:09 - INFO - __main__ -   Step: 59, LR: 1.934426229508197e-05, Loss: 0.5968295335769653
  3%|▎         | 60/2040 [09:58<5:38:35, 10.26s/it]08/02/2023 13:36:20 - INFO - __main__ -   Step: 60, LR: 1.9672131147540985e-05, Loss: 0.6416441202163696
  3%|▎         | 61/2040 [10:07<5:31:24, 10.05s/it]08/02/2023 13:36:30 - INFO - __main__ -   Step: 61, LR: 2e-05, Loss: 0.6168190240859985
  3%|▎         | 62/2040 [10:20<6:03:19, 11.02s/it]08/02/2023 13:36:43 - INFO - __main__ -   Step: 62, LR: 1.998989388580091e-05, Loss: 0.5276597738265991
  3%|▎         | 63/2040 [10:31<5:56:01, 10.80s/it]08/02/2023 13:36:54 - INFO - __main__ -   Step: 63, LR: 1.9979787771601823e-05, Loss: 0.5640673637390137
  3%|▎         | 64/2040 [10:42<6:02:04, 10.99s/it]08/02/2023 13:37:05 - INFO - __main__ -   Step: 64, LR: 1.9969681657402732e-05, Loss: 0.6424100995063782
  3%|▎         | 65/2040 [10:54<6:14:01, 11.36s/it]08/02/2023 13:37:17 - INFO - __main__ -   Step: 65, LR: 1.995957554320364e-05, Loss: 0.5176931023597717
  3%|▎         | 66/2040 [11:04<5:57:19, 10.86s/it]08/02/2023 13:37:27 - INFO - __main__ -   Step: 66, LR: 1.994946942900455e-05, Loss: 0.5891685485839844
  3%|▎         | 67/2040 [11:16<6:09:10, 11.23s/it]08/02/2023 13:37:39 - INFO - __main__ -   Step: 67, LR: 1.993936331480546e-05, Loss: 0.6035135984420776
  3%|▎         | 68/2040 [11:29<6:24:26, 11.70s/it]08/02/2023 13:37:52 - INFO - __main__ -   Step: 68, LR: 1.992925720060637e-05, Loss: 0.5746884346008301
  3%|▎         | 69/2040 [11:38<5:57:19, 10.88s/it]08/02/2023 13:38:01 - INFO - __main__ -   Step: 69, LR: 1.9919151086407278e-05, Loss: 0.5273374319076538
  3%|▎         | 70/2040 [11:47<5:34:51, 10.20s/it]08/02/2023 13:38:09 - INFO - __main__ -   Step: 70, LR: 1.9909044972208187e-05, Loss: 0.6324030756950378
  3%|▎         | 71/2040 [11:55<5:17:22,  9.67s/it]08/02/2023 13:38:18 - INFO - __main__ -   Step: 71, LR: 1.98989388580091e-05, Loss: 0.5478195548057556
  4%|▎         | 72/2040 [12:04<5:06:27,  9.34s/it]08/02/2023 13:38:26 - INFO - __main__ -   Step: 72, LR: 1.9888832743810008e-05, Loss: 0.5837578177452087
  4%|▎         | 73/2040 [12:12<4:59:09,  9.13s/it]08/02/2023 13:38:35 - INFO - __main__ -   Step: 73, LR: 1.9878726629610917e-05, Loss: 0.5914157629013062
  4%|▎         | 74/2040 [12:21<4:53:52,  8.97s/it]08/02/2023 13:38:44 - INFO - __main__ -   Step: 74, LR: 1.9868620515411826e-05, Loss: 0.5477758646011353
  4%|▎         | 75/2040 [12:29<4:48:42,  8.82s/it]08/02/2023 13:38:52 - INFO - __main__ -   Step: 75, LR: 1.9858514401212735e-05, Loss: 0.6289517879486084
  4%|▎         | 76/2040 [12:40<5:08:53,  9.44s/it]08/02/2023 13:39:03 - INFO - __main__ -   Step: 76, LR: 1.9848408287013645e-05, Loss: 0.6154997944831848
  4%|▍         | 77/2040 [12:53<5:37:10, 10.31s/it]08/02/2023 13:39:15 - INFO - __main__ -   Step: 77, LR: 1.9838302172814554e-05, Loss: 0.6089640855789185
  4%|▍         | 78/2040 [13:02<5:26:36,  9.99s/it]08/02/2023 13:39:25 - INFO - __main__ -   Step: 78, LR: 1.9828196058615463e-05, Loss: 0.5570052862167358
  4%|▍         | 79/2040 [13:10<5:13:03,  9.58s/it]08/02/2023 13:39:33 - INFO - __main__ -   Step: 79, LR: 1.9818089944416375e-05, Loss: 0.5549280047416687
  4%|▍         | 80/2040 [13:20<5:09:03,  9.46s/it]08/02/2023 13:39:42 - INFO - __main__ -   Step: 80, LR: 1.9807983830217284e-05, Loss: 0.6294866800308228
  4%|▍         | 81/2040 [13:31<5:26:02,  9.99s/it]08/02/2023 13:39:54 - INFO - __main__ -   Step: 81, LR: 1.9797877716018193e-05, Loss: 0.5736628174781799
  4%|▍         | 82/2040 [13:44<5:53:15, 10.82s/it]08/02/2023 13:40:06 - INFO - __main__ -   Step: 82, LR: 1.9787771601819102e-05, Loss: 0.5318223237991333
  4%|▍         | 83/2040 [13:53<5:36:13, 10.31s/it]08/02/2023 13:40:15 - INFO - __main__ -   Step: 83, LR: 1.977766548762001e-05, Loss: 0.5249055624008179
  4%|▍         | 84/2040 [14:01<5:16:09,  9.70s/it]08/02/2023 13:40:24 - INFO - __main__ -   Step: 84, LR: 1.976755937342092e-05, Loss: 0.5555919408798218
  4%|▍         | 85/2040 [14:09<5:03:45,  9.32s/it]08/02/2023 13:40:32 - INFO - __main__ -   Step: 85, LR: 1.975745325922183e-05, Loss: 0.5823304653167725
  4%|▍         | 86/2040 [14:18<4:58:37,  9.17s/it]08/02/2023 13:40:41 - INFO - __main__ -   Step: 86, LR: 1.974734714502274e-05, Loss: 0.5753628611564636
  4%|▍         | 87/2040 [14:27<4:54:54,  9.06s/it]08/02/2023 13:40:50 - INFO - __main__ -   Step: 87, LR: 1.973724103082365e-05, Loss: 0.5549045205116272
  4%|▍         | 88/2040 [14:35<4:48:08,  8.86s/it]08/02/2023 13:40:58 - INFO - __main__ -   Step: 88, LR: 1.972713491662456e-05, Loss: 0.5101054906845093
  4%|▍         | 89/2040 [14:44<4:47:31,  8.84s/it]08/02/2023 13:41:07 - INFO - __main__ -   Step: 89, LR: 1.971702880242547e-05, Loss: 0.590018630027771
  4%|▍         | 90/2040 [14:59<5:49:46, 10.76s/it]08/02/2023 13:41:22 - INFO - __main__ -   Step: 90, LR: 1.970692268822638e-05, Loss: 0.5261881351470947
  4%|▍         | 91/2040 [15:10<5:47:48, 10.71s/it]08/02/2023 13:41:33 - INFO - __main__ -   Step: 91, LR: 1.9696816574027287e-05, Loss: 0.5112925171852112
  5%|▍         | 92/2040 [15:19<5:30:00, 10.16s/it]08/02/2023 13:41:42 - INFO - __main__ -   Step: 92, LR: 1.9686710459828196e-05, Loss: 0.5337891578674316
  5%|▍         | 93/2040 [15:27<5:13:07,  9.65s/it]08/02/2023 13:41:50 - INFO - __main__ -   Step: 93, LR: 1.9676604345629106e-05, Loss: 0.5408031940460205
  5%|▍         | 94/2040 [15:36<5:03:45,  9.37s/it]08/02/2023 13:41:59 - INFO - __main__ -   Step: 94, LR: 1.9666498231430015e-05, Loss: 0.5287519693374634
  5%|▍         | 95/2040 [15:44<4:54:33,  9.09s/it]08/02/2023 13:42:07 - INFO - __main__ -   Step: 95, LR: 1.9656392117230927e-05, Loss: 0.5172260999679565
  5%|▍         | 96/2040 [15:53<4:51:11,  8.99s/it]08/02/2023 13:42:16 - INFO - __main__ -   Step: 96, LR: 1.9646286003031836e-05, Loss: 0.6416217684745789
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 988, in <module>
    main()
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 912, in main
    outputs = model(**batch, use_cache=False)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1769, in forward
    loss = self.module(*inputs, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 806, in forward
    outputs = self.model(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 693, in forward
    layer_outputs = decoder_layer(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 421, in forward
    hidden_states = self.mlp(hidden_states)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 216, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/activations.py", line 150, in forward
    return nn.functional.silu(input)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/functional.py", line 2059, in silu
    return torch._C._nn.silu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 79.20 GiB total capacity; 75.33 GiB already allocated; 120.31 MiB free; 76.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  5%|▍         | 96/2040 [16:01<5:24:23, 10.01s/it]
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3065177 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3065178 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3065179 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3065176) of binary: /data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/python
Traceback (most recent call last):
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 45, in main
    args.func(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 964, in launch_command
    deepspeed_launcher(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 687, in deepspeed_launcher
    distrib_run.run(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-02_13:42:27
  host      : a100
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3065176)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[2023-08-02 13:46:48,049] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-02 13:46:53,392] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 13:46:53,401] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 13:46:53,405] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 13:46:53,405] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 13:46:54,441] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 13:46:54,441] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 13:46:54,498] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 13:46:54,498] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 13:46:54,498] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-08-02 13:46:54,585] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 13:46:54,585] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 13:46:54,688] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 13:46:54,688] [INFO] [comm.py:616:init_distributed] cdb=None
08/02/2023 13:46:54 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/02/2023 13:46:54 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/02/2023 13:46:54 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/02/2023 13:46:54 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

loading configuration file config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

loading file tokenizer.model from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/tokenizer.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/special_tokens_map.json
loading file tokenizer_config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/tokenizer_config.json
loading weights file model.safetensors from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/model.safetensors.index.json
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.31.0"
}

[2023-08-02 13:47:03,073] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.97s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.9,
  "top_p": 0.6,
  "transformers_version": "4.31.0"
}

Assigning <s> to the bos_token key of the tokenizer
Assigning </s> to the eos_token key of the tokenizer
Assigning <unk> to the unk_token key of the tokenizer
Assigning <pad> to the pad_token key of the tokenizer
Adding <pad> to the vocabulary
08/02/2023 13:47:15 - INFO - __main__ - Sample 14222 of the training set: {'input_ids': tensor([    1,   512,   779, 26701, 16417,  1628,   395,  2882, 29922, 29871,
        29946, 29906, 29945,  1628,   395,  5371, 29922, 29946, 29945, 29900,
         1628,   322,   395,  2477, 29922, 29945, 29896, 29900,  1504,   530,
        13290,  1298,   395, 29925, 29938,   338,   769, 12061, 29892,   322,
        24611,   526, 12061,  1549,   395, 29925, 29938,  8943,   304,   278,
        11192,   310,   278, 17205, 29889,   960,  1438,  2211, 24611,   526,
          310,   385,  5186,  3309,   395, 29881,  1628,  1284,   395, 29881,
         1504,    13, 29902,  8369,   393,   278, 24611, 12061,  1549,   395,
        29925, 29938,  8943,   304,   278, 11192,   310,   278, 17205,   883,
         2211,  7968,  3367, 19536,   393,   526,  2788,   304,   278,  2441,
        17205, 29892,  1951,   896,   505,   278,  1021, 23619, 29889, 21700,
        29901, 21104,    13, 29902,   884,  8369,   393,   278, 11959,   310,
          278, 11192,   310,  1438,  7968,  3367, 19536,   304,   278, 11192,
          310,   278,  2441, 17205,  1818,   367,  4868, 29892,  1951,   896,
          526,  2788,  3367, 19536, 29889, 21700, 29901, 21104,    13, 12024,
          592,  1246,   445, 11959,   395, 29878,  1504, 21700, 29901, 21104,
           13, 11760,   306,   505,   393,   395, 29878,   353,   320,  1154,
        29912, 29881,  1157,  2882, 29913,   353,   320,  1154, 29912, 29881,
         1157,  5371, 29913,   353,   320,  1154, 29912, 29881,  1157,  2477,
         4311, 21700, 29901,  6374,    13,  6857,   666,  5890,  1716, 11192,
          491,   395,  2882,  1628,   306,   679,   393,   395, 29881,   353,
          364,  2882,  1504, 21700, 29901,  6374,    13,  8942,  2327,   368,
        29892,   306,   679,   393,   395, 29881,   353,   364,  5371, 29938,
          322,   395, 29881,   353,   364,  2477,  1504, 21700, 29901,  6374,
           13,  4013,  2794,   393,   395, 29878,  2882,   353,   364,  5371,
          353,   364,  2477,   353,   270,  1504, 21700, 29901,  6374,    13,
         2528,   292,  1438,  2211, 10693,  4208, 29892,   306,   679,   393,
          395, 29941,  5499,   353, 17571,   718, 17403,   718, 14614,  1504,
        21700, 29901,  8178]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100, 21104,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100, 21104,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 21104,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
08/02/2023 13:47:15 - INFO - __main__ - Sample 50190 of the training set: {'input_ids': tensor([    1,  8602,  2521,   395, 19658, 29938,   756,  2625, 27497,   395,
         2882, 29922, 29945,  1628,   395,  5371, 29922, 29953,  1628,   322,
          395,  2477, 29922, 29955,  1504,  7803, 24557,  1369, 21699,   515,
          395, 29909, 29938,   322, 29349, 29880,  3412,   278,   639, 14772,
          310,   278, 17205,   297, 11564, 18112,   472,   278,  1021,  6210,
        29889,  2688,  5870,   472,  1298,   395, 29928,  1504,  1724,   338,
          395, 29121, 15485,    13, 29902,   864,   304,  1284,   278,  3309,
          310,   395, 29121,  1628,   607,   338,   760,   310,   278,   639,
        14772,   310,   278, 17205, 29889, 21700, 29901, 21104,    13, 29902,
         8369,   393,   278, 24557,  9850,   472,   278,  1021,  6210, 29892,
          577,   278,   931,   896,  2125,   304,  6159,   395, 29928, 29938,
          338,   278,  1021, 29889, 21700, 29901, 21104,    13, 29902,   884,
         8369,   393,   278,  5418,   896,  9850,   304,  6159,   395, 29928,
        29938,   338, 29839,   304,   278, 10696,   896, 12059,   355,   472,
          395, 29909,  1504, 21700, 29901, 21104,    13, 29902, 17386,   393,
          278, 10696,  2652,  3019,   310,   263, 17205,  1933,  2247,   278,
        11564,  2625,   297,   278,  1021, 11959,   408,   278, 20114, 11192,
        29889, 21700, 29901, 21104,    13, 29902,  4997,   565,   395,  3035,
        29938,   338,   278, 10696,  2652,  3019,   310,   779,  2521,   319,
         1504, 21700, 29901,  8178]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100, 21104,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100, 21104,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100, 21104,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100, 21104,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1])}.
08/02/2023 13:47:15 - INFO - __main__ - Sample 78563 of the training set: {'input_ids': tensor([    1,   450,   931,  1492,  1286,   338, 29871, 29929,   288, 29915,
        13058, 29889,  1551,   263, 29871, 29896, 29906, 29899, 18721, 12006,
        29892,   825,   931,   674,   372,   367, 29871, 29906, 29900, 29900,
        29946,  6199,   515,  1286, 29973,    13,  4013,  1108, 20789,   777,
          878,  1070, 23342, 29892,   607,  2794,   591,   817,   304,  1348,
         1048,   825,  5930,   746,   591,   748,  4940, 29871, 29896, 29906,
          288, 29915, 13058,   373,   278, 12006, 29889, 21700, 29901,  6374,
           13,  3644,   591,   788, 29871, 29896, 29906,  6199,   304,   738,
          931, 29892,   591,   679,  1250,   304,   278,  1021,   931, 29889,
        21700, 29901,  6374,    13,  2831,  1342, 29892, 29871, 29929,   718,
        29871, 29896, 29906,   353, 29871, 29906, 29896, 29892,   607,   338,
          278,  1021,   408, 29871, 29929,   288, 29915, 13058,   373,   263,
        29871, 29896, 29906, 29899, 18721, 12006, 29889, 21700, 29901,  6374,
           13,  6295,  4417, 29871, 29896, 29906,  6199,   947,   451,  1735,
          278,   931,   878,  7207, 29871, 29896, 29906, 29889, 21700, 29901,
         6374,    13,  8439,  1079, 29892,   304,  1284,   278,   931, 29871,
        29906, 29900, 29900, 29946,  6199,   515,  1286, 29892,   591,   508,
        11455,   738,  2473,  2701,   310, 29871, 29896, 29906,   297, 29871,
        29906, 29900, 29900, 29946, 29892,  1951,   896,   674,   451,  6602,
          278,  1234, 29889, 21700, 29901,  8178]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, 8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])}.
08/02/2023 13:47:15 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.
[2023-08-02 13:47:15,352] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
08/02/2023 13:47:15 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
08/02/2023 13:47:15 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
08/02/2023 13:47:15 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
08/02/2023 13:47:15 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/02/2023 13:47:15 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/02/2023 13:47:15 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
08/02/2023 13:47:15 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/02/2023 13:47:15 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-08-02 13:47:16,350] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-02 13:47:16,351] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-08-02 13:47:16,351] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-08-02 13:47:16,361] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-08-02 13:47:16,362] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-08-02 13:47:16,362] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2023-08-02 13:47:16,362] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2023-08-02 13:47:16,491] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
[2023-08-02 13:47:16,492] [INFO] [utils.py:786:see_memory_usage] MA 3.69 GB         Max_MA 4.27 GB         CA 5.62 GB         Max_CA 6 GB 
[2023-08-02 13:47:16,492] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 168.65 GB, percent = 16.7%
[2023-08-02 13:47:16,493] [INFO] [stage3.py:117:__init__] Reduce bucket size 16777216
[2023-08-02 13:47:16,493] [INFO] [stage3.py:118:__init__] Prefetch bucket size 15099494
[2023-08-02 13:47:16,584] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-02 13:47:16,584] [INFO] [utils.py:786:see_memory_usage] MA 3.69 GB         Max_MA 3.69 GB         CA 5.62 GB         Max_CA 6 GB 
[2023-08-02 13:47:16,585] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 168.49 GB, percent = 16.7%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2023-08-02 13:47:16,690] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-08-02 13:47:16,691] [INFO] [utils.py:786:see_memory_usage] MA 3.32 GB         Max_MA 3.75 GB         CA 5.62 GB         Max_CA 6 GB 
[2023-08-02 13:47:16,691] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 168.5 GB, percent = 16.7%
[2023-08-02 13:47:16,780] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
[2023-08-02 13:47:16,781] [INFO] [utils.py:786:see_memory_usage] MA 3.32 GB         Max_MA 3.32 GB         CA 5.62 GB         Max_CA 6 GB 
[2023-08-02 13:47:16,781] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 168.49 GB, percent = 16.7%
[2023-08-02 13:47:19,291] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 2
[2023-08-02 13:47:19,292] [INFO] [utils.py:786:see_memory_usage] MA 3.32 GB         Max_MA 3.32 GB         CA 3.86 GB         Max_CA 6 GB 
[2023-08-02 13:47:19,292] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 178.0 GB, percent = 17.7%
[2023-08-02 13:47:19,393] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
[2023-08-02 13:47:19,394] [INFO] [utils.py:786:see_memory_usage] MA 3.32 GB         Max_MA 3.32 GB         CA 3.86 GB         Max_CA 4 GB 
[2023-08-02 13:47:19,394] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 175.02 GB, percent = 17.4%
[2023-08-02 13:47:19,548] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
[2023-08-02 13:47:19,548] [INFO] [utils.py:786:see_memory_usage] MA 9.6 GB         Max_MA 10.87 GB         CA 12.01 GB         Max_CA 12 GB 
[2023-08-02 13:47:19,548] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 174.89 GB, percent = 17.4%
[2023-08-02 13:47:20,047] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-08-02 13:47:20,047] [INFO] [utils.py:786:see_memory_usage] MA 9.6 GB         Max_MA 9.6 GB         CA 12.01 GB         Max_CA 12 GB 
[2023-08-02 13:47:20,047] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 168.66 GB, percent = 16.7%
[2023-08-02 13:47:20,174] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-08-02 13:47:20,175] [INFO] [utils.py:786:see_memory_usage] MA 22.15 GB         Max_MA 30.97 GB         CA 35.78 GB         Max_CA 36 GB 
[2023-08-02 13:47:20,175] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 168.67 GB, percent = 16.7%
[2023-08-02 13:47:20,175] [INFO] [stage3.py:424:_setup_for_real_optimizer] optimizer state initialized
[2023-08-02 13:47:20,372] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-08-02 13:47:20,373] [INFO] [utils.py:786:see_memory_usage] MA 25.32 GB         Max_MA 25.81 GB         CA 35.78 GB         Max_CA 36 GB 
[2023-08-02 13:47:20,373] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 168.59 GB, percent = 16.7%
[2023-08-02 13:47:20,373] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-08-02 13:47:20,373] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-02 13:47:20,373] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-02 13:47:20,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-08-02 13:47:20,374] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-02 13:47:20,374] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-02 13:47:20,374] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-02 13:47:20,374] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-02 13:47:20,374] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-02 13:47:20,374] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-02 13:47:20,374] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3fde35df60>
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 16
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-02 13:47:20,375] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   train_batch_size ............. 128
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-02 13:47:20,376] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3
[2023-08-02 13:47:20,376] [INFO] [config.py:950:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
08/02/2023 13:47:20 - INFO - __main__ - ***** Running training *****
08/02/2023 13:47:20 - INFO - __main__ -   Num examples = 87012
08/02/2023 13:47:20 - INFO - __main__ -   Num Epochs = 3
08/02/2023 13:47:20 - INFO - __main__ -   Instantaneous batch size per device = 2
08/02/2023 13:47:20 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128
08/02/2023 13:47:20 - INFO - __main__ -   Gradient Accumulation steps = 16
08/02/2023 13:47:20 - INFO - __main__ -   Total optimization steps = 2040
  0%|          | 0/2040 [00:00<?, ?it/s]  0%|          | 1/2040 [00:12<6:56:26, 12.25s/it]08/02/2023 13:47:32 - INFO - __main__ -   Step: 1, LR: 3.278688524590164e-07, Loss: 2.684464693069458
  0%|          | 2/2040 [00:22<6:10:43, 10.91s/it]08/02/2023 13:47:42 - INFO - __main__ -   Step: 2, LR: 6.557377049180328e-07, Loss: 2.696509599685669
  0%|          | 3/2040 [00:31<5:43:41, 10.12s/it]08/02/2023 13:47:51 - INFO - __main__ -   Step: 3, LR: 9.836065573770493e-07, Loss: 2.5509374141693115
  0%|          | 4/2040 [00:39<5:20:13,  9.44s/it]08/02/2023 13:48:00 - INFO - __main__ -   Step: 4, LR: 1.3114754098360657e-06, Loss: 2.868264675140381
  0%|          | 5/2040 [00:48<5:16:52,  9.34s/it]08/02/2023 13:48:09 - INFO - __main__ -   Step: 5, LR: 1.6393442622950819e-06, Loss: 2.3977675437927246
  0%|          | 6/2040 [00:59<5:28:54,  9.70s/it]08/02/2023 13:48:19 - INFO - __main__ -   Step: 6, LR: 1.9672131147540985e-06, Loss: 2.418497085571289
  0%|          | 7/2040 [01:08<5:19:58,  9.44s/it]08/02/2023 13:48:28 - INFO - __main__ -   Step: 7, LR: 2.295081967213115e-06, Loss: 1.866814374923706
  0%|          | 8/2040 [01:17<5:15:10,  9.31s/it]08/02/2023 13:48:37 - INFO - __main__ -   Step: 8, LR: 2.6229508196721314e-06, Loss: 1.8660255670547485
  0%|          | 9/2040 [01:25<5:07:57,  9.10s/it]08/02/2023 13:48:46 - INFO - __main__ -   Step: 9, LR: 2.9508196721311478e-06, Loss: 0.7940928339958191
  0%|          | 10/2040 [01:36<5:22:55,  9.54s/it]08/02/2023 13:48:56 - INFO - __main__ -   Step: 10, LR: 3.2786885245901638e-06, Loss: 0.8350439667701721
  1%|          | 11/2040 [01:45<5:15:24,  9.33s/it]08/02/2023 13:49:05 - INFO - __main__ -   Step: 11, LR: 3.6065573770491806e-06, Loss: 0.8017418384552002
  1%|          | 12/2040 [01:54<5:08:52,  9.14s/it]08/02/2023 13:49:14 - INFO - __main__ -   Step: 12, LR: 3.934426229508197e-06, Loss: 0.8007638454437256
  1%|          | 13/2040 [02:04<5:23:12,  9.57s/it]08/02/2023 13:49:24 - INFO - __main__ -   Step: 13, LR: 4.2622950819672135e-06, Loss: 0.66983962059021
  1%|          | 14/2040 [02:13<5:14:11,  9.30s/it]08/02/2023 13:49:33 - INFO - __main__ -   Step: 14, LR: 4.59016393442623e-06, Loss: 0.7870163321495056
  1%|          | 15/2040 [02:21<5:05:59,  9.07s/it]08/02/2023 13:49:42 - INFO - __main__ -   Step: 15, LR: 4.918032786885246e-06, Loss: 0.796875
  1%|          | 16/2040 [02:30<5:01:06,  8.93s/it]08/02/2023 13:49:50 - INFO - __main__ -   Step: 16, LR: 5.245901639344263e-06, Loss: 0.6692696809768677
  1%|          | 17/2040 [02:39<4:57:56,  8.84s/it]08/02/2023 13:49:59 - INFO - __main__ -   Step: 17, LR: 5.573770491803278e-06, Loss: 0.7145531177520752
  1%|          | 18/2040 [02:47<4:57:49,  8.84s/it]08/02/2023 13:50:08 - INFO - __main__ -   Step: 18, LR: 5.9016393442622956e-06, Loss: 0.7782735824584961
  1%|          | 19/2040 [02:58<5:16:52,  9.41s/it]08/02/2023 13:50:18 - INFO - __main__ -   Step: 19, LR: 6.229508196721312e-06, Loss: 0.9926843047142029
  1%|          | 20/2040 [03:06<5:06:27,  9.10s/it]08/02/2023 13:50:27 - INFO - __main__ -   Step: 20, LR: 6.5573770491803276e-06, Loss: 0.7255967259407043
  1%|          | 21/2040 [03:15<5:02:48,  9.00s/it]08/02/2023 13:50:36 - INFO - __main__ -   Step: 21, LR: 6.885245901639345e-06, Loss: 0.7805290222167969
  1%|          | 22/2040 [03:24<4:55:15,  8.78s/it]08/02/2023 13:50:44 - INFO - __main__ -   Step: 22, LR: 7.213114754098361e-06, Loss: 0.890674352645874
  1%|          | 23/2040 [03:35<5:17:59,  9.46s/it]08/02/2023 13:50:55 - INFO - __main__ -   Step: 23, LR: 7.540983606557377e-06, Loss: 0.8592324256896973
  1%|          | 24/2040 [03:43<5:06:46,  9.13s/it]08/02/2023 13:51:03 - INFO - __main__ -   Step: 24, LR: 7.868852459016394e-06, Loss: 0.7046875953674316
  1%|          | 25/2040 [03:51<5:00:42,  8.95s/it]08/02/2023 13:51:12 - INFO - __main__ -   Step: 25, LR: 8.19672131147541e-06, Loss: 0.682477593421936
  1%|▏         | 26/2040 [04:02<5:18:25,  9.49s/it]08/02/2023 13:51:23 - INFO - __main__ -   Step: 26, LR: 8.524590163934427e-06, Loss: 0.6463273167610168
  1%|▏         | 27/2040 [04:11<5:10:16,  9.25s/it]08/02/2023 13:51:31 - INFO - __main__ -   Step: 27, LR: 8.852459016393443e-06, Loss: 0.7453844547271729
  1%|▏         | 28/2040 [04:19<5:02:57,  9.03s/it]08/02/2023 13:51:40 - INFO - __main__ -   Step: 28, LR: 9.18032786885246e-06, Loss: 0.7451558113098145
  1%|▏         | 29/2040 [04:28<4:53:44,  8.76s/it]08/02/2023 13:51:48 - INFO - __main__ -   Step: 29, LR: 9.508196721311476e-06, Loss: 0.7368232607841492
  1%|▏         | 30/2040 [04:36<4:52:46,  8.74s/it]08/02/2023 13:51:57 - INFO - __main__ -   Step: 30, LR: 9.836065573770493e-06, Loss: 0.698320746421814
  2%|▏         | 31/2040 [04:45<4:50:52,  8.69s/it]08/02/2023 13:52:05 - INFO - __main__ -   Step: 31, LR: 1.0163934426229509e-05, Loss: 0.7651066780090332
  2%|▏         | 32/2040 [04:53<4:50:48,  8.69s/it]08/02/2023 13:52:14 - INFO - __main__ -   Step: 32, LR: 1.0491803278688525e-05, Loss: 0.7649437189102173
  2%|▏         | 33/2040 [05:05<5:14:34,  9.40s/it]08/02/2023 13:52:25 - INFO - __main__ -   Step: 33, LR: 1.0819672131147544e-05, Loss: 0.6922568082809448
  2%|▏         | 34/2040 [05:13<5:08:30,  9.23s/it]08/02/2023 13:52:34 - INFO - __main__ -   Step: 34, LR: 1.1147540983606557e-05, Loss: 0.60248863697052
  2%|▏         | 35/2040 [05:22<5:01:37,  9.03s/it]08/02/2023 13:52:42 - INFO - __main__ -   Step: 35, LR: 1.1475409836065575e-05, Loss: 0.6134982109069824
  2%|▏         | 36/2040 [05:31<5:04:16,  9.11s/it]08/02/2023 13:52:52 - INFO - __main__ -   Step: 36, LR: 1.1803278688524591e-05, Loss: 0.5721144080162048
  2%|▏         | 37/2040 [05:42<5:17:30,  9.51s/it]08/02/2023 13:53:02 - INFO - __main__ -   Step: 37, LR: 1.2131147540983608e-05, Loss: 0.5942730903625488
  2%|▏         | 38/2040 [05:50<5:07:33,  9.22s/it]08/02/2023 13:53:11 - INFO - __main__ -   Step: 38, LR: 1.2459016393442624e-05, Loss: 0.7209913730621338
  2%|▏         | 39/2040 [06:02<5:30:47,  9.92s/it]08/02/2023 13:53:22 - INFO - __main__ -   Step: 39, LR: 1.2786885245901642e-05, Loss: 0.6940816640853882
  2%|▏         | 40/2040 [06:11<5:20:14,  9.61s/it]08/02/2023 13:53:31 - INFO - __main__ -   Step: 40, LR: 1.3114754098360655e-05, Loss: 0.7474638819694519
  2%|▏         | 41/2040 [06:19<5:11:04,  9.34s/it]08/02/2023 13:53:40 - INFO - __main__ -   Step: 41, LR: 1.3442622950819673e-05, Loss: 0.6159259080886841
  2%|▏         | 42/2040 [06:28<5:00:29,  9.02s/it]08/02/2023 13:53:48 - INFO - __main__ -   Step: 42, LR: 1.377049180327869e-05, Loss: 0.5970968008041382
  2%|▏         | 43/2040 [06:36<4:56:17,  8.90s/it]08/02/2023 13:53:57 - INFO - __main__ -   Step: 43, LR: 1.4098360655737706e-05, Loss: 0.5859916806221008
  2%|▏         | 44/2040 [06:47<5:11:50,  9.37s/it]08/02/2023 13:54:07 - INFO - __main__ -   Step: 44, LR: 1.4426229508196722e-05, Loss: 0.5443733930587769
  2%|▏         | 45/2040 [06:56<5:11:41,  9.37s/it]08/02/2023 13:54:17 - INFO - __main__ -   Step: 45, LR: 1.4754098360655739e-05, Loss: 0.6094152331352234
  2%|▏         | 46/2040 [07:05<5:06:32,  9.22s/it]08/02/2023 13:54:25 - INFO - __main__ -   Step: 46, LR: 1.5081967213114754e-05, Loss: 0.6172674894332886
  2%|▏         | 47/2040 [07:13<4:56:44,  8.93s/it]08/02/2023 13:54:34 - INFO - __main__ -   Step: 47, LR: 1.5409836065573772e-05, Loss: 0.6420528292655945
  2%|▏         | 48/2040 [07:22<4:50:42,  8.76s/it]08/02/2023 13:54:42 - INFO - __main__ -   Step: 48, LR: 1.5737704918032788e-05, Loss: 0.6241690516471863
  2%|▏         | 49/2040 [07:31<4:53:55,  8.86s/it]08/02/2023 13:54:51 - INFO - __main__ -   Step: 49, LR: 1.6065573770491805e-05, Loss: 0.5834583044052124
  2%|▏         | 50/2040 [07:42<5:21:01,  9.68s/it]08/02/2023 13:55:03 - INFO - __main__ -   Step: 50, LR: 1.639344262295082e-05, Loss: 0.6283618211746216
  2%|▎         | 51/2040 [07:54<5:45:23, 10.42s/it]08/02/2023 13:55:15 - INFO - __main__ -   Step: 51, LR: 1.6721311475409837e-05, Loss: 0.6363444328308105
  3%|▎         | 52/2040 [08:04<5:38:10, 10.21s/it]08/02/2023 13:55:25 - INFO - __main__ -   Step: 52, LR: 1.7049180327868854e-05, Loss: 0.5755599737167358
  3%|▎         | 53/2040 [08:13<5:23:48,  9.78s/it]08/02/2023 13:55:33 - INFO - __main__ -   Step: 53, LR: 1.737704918032787e-05, Loss: 0.6062796115875244
  3%|▎         | 54/2040 [08:21<5:10:17,  9.37s/it]08/02/2023 13:55:42 - INFO - __main__ -   Step: 54, LR: 1.7704918032786887e-05, Loss: 0.6517060399055481
  3%|▎         | 55/2040 [08:30<5:01:10,  9.10s/it]08/02/2023 13:55:50 - INFO - __main__ -   Step: 55, LR: 1.8032786885245903e-05, Loss: 0.6599660515785217
  3%|▎         | 56/2040 [08:38<4:55:18,  8.93s/it]08/02/2023 13:55:59 - INFO - __main__ -   Step: 56, LR: 1.836065573770492e-05, Loss: 0.6266295313835144
  3%|▎         | 57/2040 [08:48<5:02:04,  9.14s/it]08/02/2023 13:56:08 - INFO - __main__ -   Step: 57, LR: 1.8688524590163936e-05, Loss: 0.5993560552597046
  3%|▎         | 58/2040 [08:59<5:22:56,  9.78s/it]08/02/2023 13:56:20 - INFO - __main__ -   Step: 58, LR: 1.9016393442622952e-05, Loss: 0.539494514465332
  3%|▎         | 59/2040 [09:08<5:13:50,  9.51s/it]08/02/2023 13:56:28 - INFO - __main__ -   Step: 59, LR: 1.934426229508197e-05, Loss: 0.5823203325271606
  3%|▎         | 60/2040 [09:16<5:00:15,  9.10s/it]08/02/2023 13:56:37 - INFO - __main__ -   Step: 60, LR: 1.9672131147540985e-05, Loss: 0.6539753079414368
  3%|▎         | 61/2040 [09:25<4:57:35,  9.02s/it]08/02/2023 13:56:45 - INFO - __main__ -   Step: 61, LR: 2e-05, Loss: 0.5772789716720581
  3%|▎         | 62/2040 [09:34<4:57:32,  9.03s/it]08/02/2023 13:56:55 - INFO - __main__ -   Step: 62, LR: 1.998989388580091e-05, Loss: 0.5133668184280396
  3%|▎         | 63/2040 [09:46<5:27:59,  9.95s/it]08/02/2023 13:57:07 - INFO - __main__ -   Step: 63, LR: 1.9979787771601823e-05, Loss: 0.5795709490776062
  3%|▎         | 64/2040 [09:56<5:25:17,  9.88s/it]08/02/2023 13:57:16 - INFO - __main__ -   Step: 64, LR: 1.9969681657402732e-05, Loss: 0.6194790005683899
  3%|▎         | 65/2040 [10:07<5:36:31, 10.22s/it]08/02/2023 13:57:27 - INFO - __main__ -   Step: 65, LR: 1.995957554320364e-05, Loss: 0.5203596353530884
  3%|▎         | 66/2040 [10:17<5:36:59, 10.24s/it]08/02/2023 13:57:38 - INFO - __main__ -   Step: 66, LR: 1.994946942900455e-05, Loss: 0.5754356980323792
  3%|▎         | 67/2040 [10:26<5:22:23,  9.80s/it]08/02/2023 13:57:46 - INFO - __main__ -   Step: 67, LR: 1.993936331480546e-05, Loss: 0.5695369839668274
  3%|▎         | 68/2040 [10:34<5:07:22,  9.35s/it]08/02/2023 13:57:55 - INFO - __main__ -   Step: 68, LR: 1.992925720060637e-05, Loss: 0.574154257774353
  3%|▎         | 69/2040 [10:43<5:02:39,  9.21s/it]08/02/2023 13:58:04 - INFO - __main__ -   Step: 69, LR: 1.9919151086407278e-05, Loss: 0.5105563402175903
  3%|▎         | 70/2040 [10:52<4:55:26,  9.00s/it]08/02/2023 13:58:12 - INFO - __main__ -   Step: 70, LR: 1.9909044972208187e-05, Loss: 0.5942286252975464
  3%|▎         | 71/2040 [11:00<4:48:29,  8.79s/it]08/02/2023 13:58:20 - INFO - __main__ -   Step: 71, LR: 1.98989388580091e-05, Loss: 0.5339772701263428
  4%|▎         | 72/2040 [11:09<4:46:45,  8.74s/it]08/02/2023 13:58:29 - INFO - __main__ -   Step: 72, LR: 1.9888832743810008e-05, Loss: 0.5878759622573853
  4%|▎         | 73/2040 [11:20<5:08:33,  9.41s/it]08/02/2023 13:58:40 - INFO - __main__ -   Step: 73, LR: 1.9878726629610917e-05, Loss: 0.5801035165786743
  4%|▎         | 74/2040 [11:30<5:22:01,  9.83s/it]08/02/2023 13:58:51 - INFO - __main__ -   Step: 74, LR: 1.9868620515411826e-05, Loss: 0.5316660404205322
  4%|▎         | 75/2040 [11:39<5:09:47,  9.46s/it]08/02/2023 13:58:59 - INFO - __main__ -   Step: 75, LR: 1.9858514401212735e-05, Loss: 0.5701408386230469
  4%|▎         | 76/2040 [11:48<5:00:05,  9.17s/it]08/02/2023 13:59:08 - INFO - __main__ -   Step: 76, LR: 1.9848408287013645e-05, Loss: 0.5911610126495361
  4%|▍         | 77/2040 [11:57<5:00:02,  9.17s/it]08/02/2023 13:59:17 - INFO - __main__ -   Step: 77, LR: 1.9838302172814554e-05, Loss: 0.6315464973449707
  4%|▍         | 78/2040 [12:09<5:29:57, 10.09s/it]08/02/2023 13:59:29 - INFO - __main__ -   Step: 78, LR: 1.9828196058615463e-05, Loss: 0.6070796251296997
  4%|▍         | 79/2040 [12:18<5:21:47,  9.85s/it]08/02/2023 13:59:39 - INFO - __main__ -   Step: 79, LR: 1.9818089944416375e-05, Loss: 0.5996927618980408
  4%|▍         | 80/2040 [12:30<5:43:28, 10.51s/it]08/02/2023 13:59:51 - INFO - __main__ -   Step: 80, LR: 1.9807983830217284e-05, Loss: 0.693169116973877
  4%|▍         | 81/2040 [12:40<5:38:16, 10.36s/it]08/02/2023 14:00:01 - INFO - __main__ -   Step: 81, LR: 1.9797877716018193e-05, Loss: 0.615075945854187
  4%|▍         | 82/2040 [12:49<5:25:44,  9.98s/it]08/02/2023 14:00:10 - INFO - __main__ -   Step: 82, LR: 1.9787771601819102e-05, Loss: 0.5411192178726196
  4%|▍         | 83/2040 [12:58<5:10:58,  9.53s/it]08/02/2023 14:00:18 - INFO - __main__ -   Step: 83, LR: 1.977766548762001e-05, Loss: 0.541241466999054
  4%|▍         | 84/2040 [13:07<5:06:32,  9.40s/it]08/02/2023 14:00:27 - INFO - __main__ -   Step: 84, LR: 1.976755937342092e-05, Loss: 0.56519615650177
  4%|▍         | 85/2040 [13:19<5:34:25, 10.26s/it]08/02/2023 14:00:40 - INFO - __main__ -   Step: 85, LR: 1.975745325922183e-05, Loss: 0.6176100373268127
  4%|▍         | 86/2040 [13:29<5:26:23, 10.02s/it]08/02/2023 14:00:49 - INFO - __main__ -   Step: 86, LR: 1.974734714502274e-05, Loss: 0.583034873008728
  4%|▍         | 87/2040 [13:37<5:13:36,  9.63s/it]08/02/2023 14:00:58 - INFO - __main__ -   Step: 87, LR: 1.973724103082365e-05, Loss: 0.5789176821708679
  4%|▍         | 88/2040 [13:46<5:01:19,  9.26s/it]08/02/2023 14:01:06 - INFO - __main__ -   Step: 88, LR: 1.972713491662456e-05, Loss: 0.4946923553943634
  4%|▍         | 89/2040 [13:55<4:57:19,  9.14s/it]08/02/2023 14:01:15 - INFO - __main__ -   Step: 89, LR: 1.971702880242547e-05, Loss: 0.5809049010276794
  4%|▍         | 90/2040 [14:03<4:52:51,  9.01s/it]08/02/2023 14:01:24 - INFO - __main__ -   Step: 90, LR: 1.970692268822638e-05, Loss: 0.5178766846656799
  4%|▍         | 91/2040 [14:12<4:47:15,  8.84s/it]08/02/2023 14:01:32 - INFO - __main__ -   Step: 91, LR: 1.9696816574027287e-05, Loss: 0.5367722511291504
  5%|▍         | 92/2040 [14:20<4:44:06,  8.75s/it]08/02/2023 14:01:41 - INFO - __main__ -   Step: 92, LR: 1.9686710459828196e-05, Loss: 0.5590341687202454
  5%|▍         | 93/2040 [14:30<4:56:46,  9.15s/it]08/02/2023 14:01:51 - INFO - __main__ -   Step: 93, LR: 1.9676604345629106e-05, Loss: 0.5115368366241455
  5%|▍         | 94/2040 [14:43<5:27:05, 10.09s/it]08/02/2023 14:02:03 - INFO - __main__ -   Step: 94, LR: 1.9666498231430015e-05, Loss: 0.5396482944488525
  5%|▍         | 95/2040 [14:52<5:15:27,  9.73s/it]08/02/2023 14:02:12 - INFO - __main__ -   Step: 95, LR: 1.9656392117230927e-05, Loss: 0.5535770654678345
  5%|▍         | 96/2040 [15:03<5:34:52, 10.34s/it]08/02/2023 14:02:24 - INFO - __main__ -   Step: 96, LR: 1.9646286003031836e-05, Loss: 0.6421617865562439
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 988, in <module>
    main()
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 912, in main
    outputs = model(**batch, use_cache=False)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1769, in forward
    loss = self.module(*inputs, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 806, in forward
    outputs = self.model(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 693, in forward
    layer_outputs = decoder_layer(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 421, in forward
    hidden_states = self.mlp(hidden_states)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 216, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/activations.py", line 150, in forward
    return nn.functional.silu(input)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/functional.py", line 2059, in silu
    return torch._C._nn.silu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 79.20 GiB total capacity; 75.33 GiB already allocated; 36.31 MiB free; 76.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  5%|▍         | 96/2040 [15:14<5:08:30,  9.52s/it]
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3120768 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3120769 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3120770 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3120767) of binary: /data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/python
Traceback (most recent call last):
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 45, in main
    args.func(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 964, in launch_command
    deepspeed_launcher(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 687, in deepspeed_launcher
    distrib_run.run(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-02_14:02:40
  host      : a100
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3120767)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[2023-08-02 14:23:59,419] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-02 14:24:07,878] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 14:24:07,901] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 14:24:07,909] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 14:24:07,954] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-02 14:24:09,397] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 14:24:09,397] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 14:24:09,399] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 14:24:09,399] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 14:24:09,410] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 14:24:09,410] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 14:24:09,460] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-02 14:24:09,460] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-02 14:24:09,460] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
08/02/2023 14:24:11 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'offload_param': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/02/2023 14:24:11 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'offload_param': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/02/2023 14:24:11 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'offload_param': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

08/02/2023 14:24:11 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'offload_param': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

loading configuration file config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

loading file tokenizer.model from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/tokenizer.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/special_tokens_map.json
loading file tokenizer_config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/tokenizer_config.json
loading weights file model.safetensors from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/model.safetensors.index.json
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.31.0"
}

[2023-08-02 14:24:20,886] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.30s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.06s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/351b2c357c69b4779bde72c0e7f7da639443d904/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.9,
  "top_p": 0.6,
  "transformers_version": "4.31.0"
}

Assigning <s> to the bos_token key of the tokenizer
Assigning </s> to the eos_token key of the tokenizer
Assigning <unk> to the unk_token key of the tokenizer
Assigning <pad> to the pad_token key of the tokenizer
Adding <pad> to the vocabulary
08/02/2023 14:24:33 - INFO - __main__ - Sample 78664 of the training set: {'input_ids': tensor([    1,  1932,  1269,  7636,   310,   263, 28704,   338, 11664,   491,
          395, 29945, 29900,  8958,  1628,   491,   825, 10151,   338,   278,
         7101,  4038,   310,   278, 28704, 11664, 29973,    13,  6295,   278,
         7101,  4038,   310,   278,  2441, 28704,   338, 29871, 29953,  3064,
          278,  4038,   310,  1269,  3700, 29889, 21700, 29901,  6374,    13,
         2855,  1269,  3700,   338,   263,  6862, 29892,   577,   967,  4038,
          338,   278,  3309,   310,  1269,  7636, 10674,  1965, 29889, 21700,
        29901, 21104,    13, 12024, 29915, 29879,  1246,   278,  3309,   310,
         1269,  7636,   921, 29889,  1105,   278,  7101,  4038,   310,   278,
         2441, 28704,   338, 29871, 29953, 29394, 29916, 29985, 29906,  1504,
        21700, 29901,  6374,    13,  2855,   565,   591,  7910,   278,  3309,
          310,  1269,  7636,   491, 29871, 29945, 29900, 13667,   591,   679,
        29871, 29896, 29889, 29945, 29930, 29916, 29889, 21700, 29901,  8178]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100, 21104,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
08/02/2023 14:24:33 - INFO - __main__ - Sample 77870 of the training set: {'input_ids': tensor([    1,  1619,  1302, 29899, 24602, 26977,   871,  4188,   267,  3694,
          393,   526,  8572,  1821,   491, 29871, 29946, 29892,  1316,   408,
        29871, 29906, 29900, 29892,   470, 29871, 29946, 29892, 29900, 29900,
        29946, 29889, 29871,  1128,  1784,  1422,  6743, 13340,   526,  1950,
          297,  3694,   393, 26977,  4188,   267, 29973,    13,  1762,  4505,
          445,  1108, 29892,   306,   817,   304,  1348,  1048,   825,   372,
         2794,   363,   263,  1353,   304,   367,  8572,  1821,   491, 29871,
        29946, 29889, 21700, 29901,  6374,    13, 29909,  1353,   338,  8572,
         1821,   491, 29871, 29946,   565,   278,  1833,  1023, 13340,   310,
          278,  1353,   526,  8572,  1821,   491, 29871, 29946, 29889, 21700,
        29901,  6374,    13,  2831,  1342, 29892, 29871, 29906, 29900,   338,
         8572,  1821,   491, 29871, 29946,  1363, 29871, 29906, 29900,   847,
        29871, 29946,   353, 29871, 29945, 29892,   541, 29871, 29906, 29896,
          338,   451,  8572,  1821,   491, 29871, 29946,  1363, 29871, 29906,
        29896,   847, 29871, 29946,   353, 29871, 29945, 29889, 29906, 29945,
        29889, 21700, 29901,  6374,    13,  6295, 29892,   304,  1284,   278,
         1950,  6743, 13340,   363,  3694,   393, 26977,  4188,   267, 29892,
          306,   817,   304,  1284,   278,  1950, 11000,   310,  1833,  1023,
        13340,   393,   526,  8572,  1821,   491, 29871, 29946, 29889, 21700,
        29901,  6374,    13,  6716,   982,   304,   437,   445,   338,   304,
         1051,   599,   278,  2473,  2701,   310, 29871, 29946,   515, 29871,
        29900,   304, 29871, 29929, 29929,   322,  1106,   472,  1009,  1833,
         1023, 13340, 29889, 21700, 29901,  6374,    13, 15597,   526, 29871,
        29900, 29900, 29892, 29871, 29900, 29946, 29892, 29871, 29900, 29947,
        29892, 29871, 29896, 29906, 29892, 29871, 29896, 29953, 29892, 29871,
        29906, 29900, 29892, 29871, 29906, 29946, 29892, 29871, 29906, 29947,
        29892, 29871, 29941, 29906, 29892, 29871, 29941, 29953, 29892, 29871,
        29946, 29900, 29892, 29871, 29946, 29946, 29892, 29871, 29946, 29947,
        29892, 29871, 29945, 29906, 29892, 29871, 29945, 29953, 29892, 29871,
        29953, 29900, 29892, 29871, 29953, 29946, 29892, 29871, 29953, 29947,
        29892, 29871, 29955, 29906, 29892, 29871, 29955, 29953, 29892, 29871,
        29947, 29900, 29892, 29871, 29947, 29946, 29892, 29871, 29947, 29947,
        29892,   322, 29871, 29929, 29906, 29889, 21700, 29901,  6374,    13,
         2744,  1228,   982,   304,   437,   445,   338,   304,   671,   263,
         4766, 29889, 21700, 29901, 21104,    13, 29902,  8369,   393,  1432,
        11582,  1353,   338,  8572,  1821,   491, 29871, 29946, 29892,   577,
          306,   508,  1369,   411, 29871, 29900, 29900,   322,   788, 29871,
        29946,   304,   679,   278,  2446,  2999,   310, 29871, 29946, 29892,
          322,   577,   373, 29889, 21700, 29901, 21104,    13,  4013,   982,
        29892,   306,   679,   278,  1021,  1051,   408,  1434, 29901, 29871,
        29900, 29900, 29892, 29871, 29900, 29946, 29892, 29871, 29900, 29947,
        29892, 29871, 29896, 29906, 29892, 29871, 29896, 29953, 29892, 29871,
        29906, 29900, 29892, 29871, 29906, 29946, 29892, 29871, 29906, 29947,
        29892, 29871, 29941, 29906, 29892, 29871, 29941, 29953, 29892, 29871,
        29946, 29900, 29892, 29871, 29946, 29946, 29892, 29871, 29946, 29947,
        29892, 29871, 29945, 29906, 29892, 29871, 29945, 29953, 29892, 29871,
        29953, 29900, 29892, 29871, 29953, 29946, 29892, 29871, 29953, 29947,
        29892, 29871, 29955, 29906, 29892, 29871, 29955, 29953, 29892, 29871,
        29947, 29900, 29892, 29871, 29947, 29946, 29892, 29871, 29947, 29947,
        29892,   322, 29871, 29929, 29906, 29889, 21700, 29901, 21104,    13,
        10454, 29892,   306,   508,  1106,   472,   278,  6743, 13340,   310,
         1438,  3694,   322,  1074,   920,  1784,  1422,  6743, 13340,   727,
          526, 29889, 21700, 29901,  6374,    13, 15597,   526, 29871, 29900,
        29892, 29871, 29946, 29892, 29871, 29947, 29892,   322, 29871, 29906,
        29889, 21700, 29901,  8178]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100, 21104,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100, 21104,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 21104,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
08/02/2023 14:24:33 - INFO - __main__ - Sample 83134 of the training set: {'input_ids': tensor([    1,  1724,   338,   278, 14176,  1950,  2211, 29899, 26204,  1353,
          393,   338,  8572,  1821,   491, 29871, 29941,   322,  8572,  1821,
          491, 29871, 29953, 29973,    13,  1576,  1353,   756,   304,   367,
         8572,  1821,   491, 29871, 29941,   322, 29871, 29953, 29892,   577,
          372,  1818,   367,  8572,  1821,   491,  1009,  3203,  3619,  2999,
        29889, 21700, 29901,  6374,    13,  6295,   372,  1818,   367,  8572,
         1821,   491,   395, 29941, 29930, 29953, 29922, 29896, 29947,  1504,
        21700, 29901,  8178]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        8178]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1])}.
08/02/2023 14:24:33 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Creating extension directory /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117/cpu_adam...
Detected CUDA files, patching ldflags
Emitting ninja build file /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /data/users/zhangjunlei/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[1/3] /usr/local/cuda-11.3/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda-11.3/include -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/include -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/include/TH -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.3/include -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o 
[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda-11.3/include -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/include -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/include/TH -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.3/include -isystem /data/users/zhangjunlei/anaconda3/envs/open-instruct/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda-11.3/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-11.3/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 30.84830093383789 seconds
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 30.86901545524597 secondsTime to load cpu_adam op: 30.859716415405273 seconds

Loading extension module cpu_adam...
Time to load cpu_adam op: 30.869680643081665 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
08/02/2023 14:25:08 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2023-08-02 14:25:09,013] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
08/02/2023 14:25:09 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
08/02/2023 14:25:09 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
08/02/2023 14:25:09 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
08/02/2023 14:25:09 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/02/2023 14:25:09 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/02/2023 14:25:09 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
08/02/2023 14:25:09 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-08-02 14:25:10,493] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-02 14:25:10,495] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-08-02 14:25:10,495] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-08-02 14:25:10,512] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-08-02 14:25:10,512] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-08-02 14:25:10,512] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2023-08-02 14:25:10,512] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2023-08-02 14:25:10,721] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
[2023-08-02 14:25:10,722] [INFO] [utils.py:786:see_memory_usage] MA 0.55 GB         Max_MA 1.1 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-02 14:25:10,722] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 203.69 GB, percent = 20.2%
[2023-08-02 14:25:10,725] [INFO] [stage3.py:117:__init__] Reduce bucket size 16777216
[2023-08-02 14:25:10,725] [INFO] [stage3.py:118:__init__] Prefetch bucket size 15099494
[2023-08-02 14:25:10,886] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-02 14:25:10,886] [INFO] [utils.py:786:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-02 14:25:10,886] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 203.7 GB, percent = 20.2%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2023-08-02 14:25:11,239] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-08-02 14:25:11,240] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.55 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-02 14:25:11,241] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 203.84 GB, percent = 20.2%
[2023-08-02 14:25:11,401] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
[2023-08-02 14:25:11,402] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-02 14:25:11,402] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 203.84 GB, percent = 20.2%
[2023-08-02 14:25:16,934] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 2
[2023-08-02 14:25:16,934] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-02 14:25:16,935] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 221.2 GB, percent = 22.0%
[2023-08-02 14:25:17,042] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
[2023-08-02 14:25:17,043] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-02 14:25:17,043] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 221.98 GB, percent = 22.0%
[2023-08-02 14:25:21,640] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
[2023-08-02 14:25:21,641] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-02 14:25:21,641] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 243.16 GB, percent = 24.1%
[2023-08-02 14:25:21,747] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-08-02 14:25:21,748] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-02 14:25:21,748] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 244.27 GB, percent = 24.3%
[2023-08-02 14:25:40,150] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-08-02 14:25:40,151] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB 
[2023-08-02 14:25:40,152] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 327.6 GB, percent = 32.5%
[2023-08-02 14:25:45,725] [INFO] [stage3.py:424:_setup_for_real_optimizer] optimizer state initialized
[2023-08-02 14:25:53,292] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-08-02 14:25:53,293] [INFO] [utils.py:786:see_memory_usage] MA 0.09 GB         Max_MA 0.58 GB         CA 2.02 GB         Max_CA 2 GB 
[2023-08-02 14:25:53,293] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 343.94 GB, percent = 34.1%
[2023-08-02 14:25:53,293] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-08-02 14:25:53,293] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-02 14:25:53,293] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-02 14:25:53,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-08-02 14:25:53,298] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f15c7be1300>
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-02 14:25:53,299] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 16
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-02 14:25:53,300] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   train_batch_size ............. 128
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-02 14:25:53,301] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3
[2023-08-02 14:25:53,301] [INFO] [config.py:950:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
08/02/2023 14:25:53 - INFO - __main__ - ***** Running training *****
08/02/2023 14:25:53 - INFO - __main__ -   Num examples = 87012
08/02/2023 14:25:53 - INFO - __main__ -   Num Epochs = 3
08/02/2023 14:25:53 - INFO - __main__ -   Instantaneous batch size per device = 2
08/02/2023 14:25:53 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128
08/02/2023 14:25:53 - INFO - __main__ -   Gradient Accumulation steps = 16
08/02/2023 14:25:53 - INFO - __main__ -   Total optimization steps = 2040
  0%|          | 0/2040 [00:00<?, ?it/s]  0%|          | 1/2040 [00:44<25:06:09, 44.32s/it]08/02/2023 14:26:37 - INFO - __main__ -   Step: 1, LR: 3.278688524590164e-07, Loss: 2.6844687461853027
  0%|          | 2/2040 [01:35<27:30:42, 48.60s/it]08/02/2023 14:27:29 - INFO - __main__ -   Step: 2, LR: 6.557377049180328e-07, Loss: 2.6965110301971436
  0%|          | 3/2040 [02:23<27:14:58, 48.16s/it]08/02/2023 14:28:16 - INFO - __main__ -   Step: 3, LR: 9.836065573770493e-07, Loss: 2.5504233837127686
  0%|          | 4/2040 [03:08<26:33:37, 46.96s/it]08/02/2023 14:29:01 - INFO - __main__ -   Step: 4, LR: 1.3114754098360657e-06, Loss: 2.8675057888031006
  0%|          | 5/2040 [03:51<25:41:17, 45.44s/it]08/02/2023 14:29:44 - INFO - __main__ -   Step: 5, LR: 1.6393442622950819e-06, Loss: 2.3983354568481445
  0%|          | 6/2040 [04:37<25:49:11, 45.70s/it]08/02/2023 14:30:30 - INFO - __main__ -   Step: 6, LR: 1.9672131147540985e-06, Loss: 2.4168100357055664
  0%|          | 7/2040 [05:24<26:04:57, 46.19s/it]08/02/2023 14:31:18 - INFO - __main__ -   Step: 7, LR: 2.295081967213115e-06, Loss: 1.8638466596603394
  0%|          | 8/2040 [06:07<25:26:20, 45.07s/it]08/02/2023 14:32:00 - INFO - __main__ -   Step: 8, LR: 2.6229508196721314e-06, Loss: 1.8660740852355957
  0%|          | 9/2040 [06:53<25:36:46, 45.40s/it]08/02/2023 14:32:46 - INFO - __main__ -   Step: 9, LR: 2.9508196721311478e-06, Loss: 0.7956141233444214
  0%|          | 10/2040 [07:39<25:45:36, 45.68s/it]08/02/2023 14:33:33 - INFO - __main__ -   Step: 10, LR: 3.2786885245901638e-06, Loss: 0.8364054560661316
  1%|          | 11/2040 [08:25<25:48:26, 45.79s/it]08/02/2023 14:34:19 - INFO - __main__ -   Step: 11, LR: 3.6065573770491806e-06, Loss: 0.8020920157432556
  1%|          | 12/2040 [09:11<25:45:13, 45.72s/it]08/02/2023 14:35:04 - INFO - __main__ -   Step: 12, LR: 3.934426229508197e-06, Loss: 0.7988476753234863
  1%|          | 13/2040 [09:59<26:11:52, 46.53s/it]08/02/2023 14:35:53 - INFO - __main__ -   Step: 13, LR: 4.2622950819672135e-06, Loss: 0.6682884693145752
  1%|          | 14/2040 [10:47<26:21:58, 46.85s/it]08/02/2023 14:36:40 - INFO - __main__ -   Step: 14, LR: 4.59016393442623e-06, Loss: 0.7868562936782837
  1%|          | 15/2040 [11:24<24:36:52, 43.76s/it]08/02/2023 14:37:17 - INFO - __main__ -   Step: 15, LR: 4.918032786885246e-06, Loss: 0.7985577583312988
  1%|          | 16/2040 [12:10<25:01:15, 44.50s/it]08/02/2023 14:38:03 - INFO - __main__ -   Step: 16, LR: 5.245901639344263e-06, Loss: 0.6718986630439758
  1%|          | 17/2040 [12:57<25:28:10, 45.32s/it]08/02/2023 14:38:50 - INFO - __main__ -   Step: 17, LR: 5.573770491803278e-06, Loss: 0.7178782224655151
  1%|          | 18/2040 [13:44<25:41:58, 45.76s/it]08/02/2023 14:39:37 - INFO - __main__ -   Step: 18, LR: 5.9016393442622956e-06, Loss: 0.7795816659927368
  1%|          | 19/2040 [14:31<25:53:33, 46.12s/it]08/02/2023 14:40:24 - INFO - __main__ -   Step: 19, LR: 6.229508196721312e-06, Loss: 0.9965021014213562
  1%|          | 20/2040 [15:18<25:59:48, 46.33s/it]08/02/2023 14:41:11 - INFO - __main__ -   Step: 20, LR: 6.5573770491803276e-06, Loss: 0.7282640337944031
  1%|          | 21/2040 [16:05<26:05:13, 46.51s/it]08/02/2023 14:41:58 - INFO - __main__ -   Step: 21, LR: 6.885245901639345e-06, Loss: 0.7785192728042603
  1%|          | 22/2040 [16:52<26:09:37, 46.67s/it]08/02/2023 14:42:45 - INFO - __main__ -   Step: 22, LR: 7.213114754098361e-06, Loss: 0.8958659172058105
  1%|          | 23/2040 [17:28<24:23:00, 43.52s/it]08/02/2023 14:43:21 - INFO - __main__ -   Step: 23, LR: 7.540983606557377e-06, Loss: 0.856602668762207
  1%|          | 24/2040 [18:03<23:02:53, 41.16s/it]08/02/2023 14:43:57 - INFO - __main__ -   Step: 24, LR: 7.868852459016394e-06, Loss: 0.6979832649230957
  1%|          | 25/2040 [18:39<22:07:50, 39.54s/it]08/02/2023 14:44:32 - INFO - __main__ -   Step: 25, LR: 8.19672131147541e-06, Loss: 0.6781283617019653
  1%|▏         | 26/2040 [19:15<21:31:59, 38.49s/it]08/02/2023 14:45:09 - INFO - __main__ -   Step: 26, LR: 8.524590163934427e-06, Loss: 0.6479731798171997
  1%|▏         | 27/2040 [19:51<21:01:08, 37.59s/it]08/02/2023 14:45:44 - INFO - __main__ -   Step: 27, LR: 8.852459016393443e-06, Loss: 0.7436823844909668
  1%|▏         | 28/2040 [20:27<20:43:31, 37.08s/it]08/02/2023 14:46:20 - INFO - __main__ -   Step: 28, LR: 9.18032786885246e-06, Loss: 0.7426460981369019
  1%|▏         | 29/2040 [21:02<20:28:53, 36.67s/it]08/02/2023 14:46:56 - INFO - __main__ -   Step: 29, LR: 9.508196721311476e-06, Loss: 0.7322081327438354
  1%|▏         | 30/2040 [21:38<20:23:30, 36.52s/it]08/02/2023 14:47:32 - INFO - __main__ -   Step: 30, LR: 9.836065573770493e-06, Loss: 0.7014961242675781
  2%|▏         | 31/2040 [22:15<20:20:45, 36.46s/it]08/02/2023 14:48:08 - INFO - __main__ -   Step: 31, LR: 1.0163934426229509e-05, Loss: 0.7703148126602173
  2%|▏         | 32/2040 [22:51<20:15:02, 36.31s/it]08/02/2023 14:48:44 - INFO - __main__ -   Step: 32, LR: 1.0491803278688525e-05, Loss: 0.7692250609397888
  2%|▏         | 33/2040 [23:27<20:11:30, 36.22s/it]08/02/2023 14:49:20 - INFO - __main__ -   Step: 33, LR: 1.0819672131147544e-05, Loss: 0.6882354021072388
  2%|▏         | 34/2040 [24:04<20:22:18, 36.56s/it]08/02/2023 14:49:57 - INFO - __main__ -   Step: 34, LR: 1.1147540983606557e-05, Loss: 0.601020097732544
  2%|▏         | 35/2040 [24:40<20:18:23, 36.46s/it]08/02/2023 14:50:34 - INFO - __main__ -   Step: 35, LR: 1.1475409836065575e-05, Loss: 0.6174235343933105
  2%|▏         | 36/2040 [25:16<20:12:04, 36.29s/it]08/02/2023 14:51:10 - INFO - __main__ -   Step: 36, LR: 1.1803278688524591e-05, Loss: 0.5651701092720032
  2%|▏         | 37/2040 [25:52<20:08:13, 36.19s/it]08/02/2023 14:51:46 - INFO - __main__ -   Step: 37, LR: 1.2131147540983608e-05, Loss: 0.5898739695549011
  2%|▏         | 38/2040 [26:28<20:01:34, 36.01s/it]08/02/2023 14:52:21 - INFO - __main__ -   Step: 38, LR: 1.2459016393442624e-05, Loss: 0.7355668544769287
  2%|▏         | 39/2040 [27:04<20:00:42, 36.00s/it]08/02/2023 14:52:57 - INFO - __main__ -   Step: 39, LR: 1.2786885245901642e-05, Loss: 0.7109918594360352
  2%|▏         | 40/2040 [27:40<20:01:26, 36.04s/it]08/02/2023 14:53:33 - INFO - __main__ -   Step: 40, LR: 1.3114754098360655e-05, Loss: 0.7511187791824341
  2%|▏         | 41/2040 [28:17<20:08:28, 36.27s/it]08/02/2023 14:54:10 - INFO - __main__ -   Step: 41, LR: 1.3442622950819673e-05, Loss: 0.6177752017974854
  2%|▏         | 42/2040 [28:52<20:02:52, 36.12s/it]08/02/2023 14:54:46 - INFO - __main__ -   Step: 42, LR: 1.377049180327869e-05, Loss: 0.6431053876876831
  2%|▏         | 43/2040 [29:28<19:58:56, 36.02s/it]08/02/2023 14:55:22 - INFO - __main__ -   Step: 43, LR: 1.4098360655737706e-05, Loss: 0.7236130237579346
  2%|▏         | 44/2040 [30:04<19:57:07, 35.99s/it]08/02/2023 14:55:57 - INFO - __main__ -   Step: 44, LR: 1.4426229508196722e-05, Loss: 0.6056182980537415
  2%|▏         | 45/2040 [30:40<19:56:43, 35.99s/it]08/02/2023 14:56:33 - INFO - __main__ -   Step: 45, LR: 1.4754098360655739e-05, Loss: 0.6892223358154297
  2%|▏         | 46/2040 [31:16<19:56:24, 36.00s/it]08/02/2023 14:57:10 - INFO - __main__ -   Step: 46, LR: 1.5081967213114754e-05, Loss: 0.6466222405433655
  2%|▏         | 47/2040 [31:52<19:55:00, 35.98s/it]08/02/2023 14:57:45 - INFO - __main__ -   Step: 47, LR: 1.5409836065573772e-05, Loss: 0.6376018524169922
  2%|▏         | 48/2040 [32:28<19:54:45, 35.99s/it]08/02/2023 14:58:21 - INFO - __main__ -   Step: 48, LR: 1.5737704918032788e-05, Loss: 0.6638344526290894
  2%|▏         | 49/2040 [33:04<19:53:29, 35.97s/it]08/02/2023 14:58:57 - INFO - __main__ -   Step: 49, LR: 1.6065573770491805e-05, Loss: 0.6157346963882446
  2%|▏         | 50/2040 [33:40<19:50:08, 35.88s/it]08/02/2023 14:59:33 - INFO - __main__ -   Step: 50, LR: 1.639344262295082e-05, Loss: 0.584430992603302
  2%|▎         | 51/2040 [34:16<19:52:04, 35.96s/it]08/02/2023 15:00:09 - INFO - __main__ -   Step: 51, LR: 1.6721311475409837e-05, Loss: 0.669208824634552
  3%|▎         | 52/2040 [34:52<19:51:51, 35.97s/it]08/02/2023 15:00:45 - INFO - __main__ -   Step: 52, LR: 1.7049180327868854e-05, Loss: 0.6143934726715088
  3%|▎         | 53/2040 [35:27<19:46:52, 35.84s/it]08/02/2023 15:01:21 - INFO - __main__ -   Step: 53, LR: 1.737704918032787e-05, Loss: 0.6037999391555786
  3%|▎         | 54/2040 [36:03<19:48:02, 35.89s/it]08/02/2023 15:01:57 - INFO - __main__ -   Step: 54, LR: 1.7704918032786887e-05, Loss: 0.6504687070846558
  3%|▎         | 55/2040 [36:39<19:41:27, 35.71s/it]08/02/2023 15:02:32 - INFO - __main__ -   Step: 55, LR: 1.8032786885245903e-05, Loss: 0.6657581329345703
  3%|▎         | 56/2040 [37:15<19:44:35, 35.82s/it]08/02/2023 15:03:08 - INFO - __main__ -   Step: 56, LR: 1.836065573770492e-05, Loss: 0.5393447875976562
  3%|▎         | 57/2040 [37:51<19:45:03, 35.86s/it]08/02/2023 15:03:44 - INFO - __main__ -   Step: 57, LR: 1.8688524590163936e-05, Loss: 0.5666634440422058
  3%|▎         | 58/2040 [38:26<19:41:20, 35.76s/it]08/02/2023 15:04:20 - INFO - __main__ -   Step: 58, LR: 1.9016393442622952e-05, Loss: 0.5460385084152222
  3%|▎         | 59/2040 [39:02<19:44:56, 35.89s/it]08/02/2023 15:04:56 - INFO - __main__ -   Step: 59, LR: 1.934426229508197e-05, Loss: 0.5486800670623779
  3%|▎         | 60/2040 [39:39<19:48:23, 36.01s/it]08/02/2023 15:05:32 - INFO - __main__ -   Step: 60, LR: 1.9672131147540985e-05, Loss: 0.5880271792411804
  3%|▎         | 61/2040 [40:14<19:43:21, 35.88s/it]08/02/2023 15:06:08 - INFO - __main__ -   Step: 61, LR: 2e-05, Loss: 0.6242791414260864
  3%|▎         | 62/2040 [40:50<19:37:47, 35.73s/it]08/02/2023 15:06:43 - INFO - __main__ -   Step: 62, LR: 1.998989388580091e-05, Loss: 0.5319575071334839
  3%|▎         | 63/2040 [41:26<19:40:37, 35.83s/it]08/02/2023 15:07:19 - INFO - __main__ -   Step: 63, LR: 1.9979787771601823e-05, Loss: 0.536857008934021
  3%|▎         | 64/2040 [42:02<19:41:25, 35.87s/it]08/02/2023 15:07:55 - INFO - __main__ -   Step: 64, LR: 1.9969681657402732e-05, Loss: 0.5871528387069702
  3%|▎         | 65/2040 [42:38<19:42:58, 35.94s/it]08/02/2023 15:08:31 - INFO - __main__ -   Step: 65, LR: 1.995957554320364e-05, Loss: 0.5383655428886414
  3%|▎         | 66/2040 [43:14<19:45:54, 36.05s/it]08/02/2023 15:09:07 - INFO - __main__ -   Step: 66, LR: 1.994946942900455e-05, Loss: 0.5720247626304626
  3%|▎         | 67/2040 [43:50<19:44:26, 36.02s/it]08/02/2023 15:09:43 - INFO - __main__ -   Step: 67, LR: 1.993936331480546e-05, Loss: 0.6025667190551758
  3%|▎         | 68/2040 [44:26<19:38:19, 35.85s/it]08/02/2023 15:10:19 - INFO - __main__ -   Step: 68, LR: 1.992925720060637e-05, Loss: 0.6089844703674316
  3%|▎         | 69/2040 [45:02<19:39:49, 35.92s/it]08/02/2023 15:10:55 - INFO - __main__ -   Step: 69, LR: 1.9919151086407278e-05, Loss: 0.5054820775985718
  3%|▎         | 70/2040 [45:38<19:40:08, 35.94s/it]08/02/2023 15:11:31 - INFO - __main__ -   Step: 70, LR: 1.9909044972208187e-05, Loss: 0.6329590082168579
  3%|▎         | 71/2040 [46:13<19:36:11, 35.84s/it]08/02/2023 15:12:07 - INFO - __main__ -   Step: 71, LR: 1.98989388580091e-05, Loss: 0.5748989582061768
  4%|▎         | 72/2040 [46:49<19:36:35, 35.87s/it]08/02/2023 15:12:42 - INFO - __main__ -   Step: 72, LR: 1.9888832743810008e-05, Loss: 0.6107404232025146
  4%|▎         | 73/2040 [47:25<19:38:25, 35.95s/it]08/02/2023 15:13:19 - INFO - __main__ -   Step: 73, LR: 1.9878726629610917e-05, Loss: 0.5775394439697266
  4%|▎         | 74/2040 [48:01<19:38:08, 35.96s/it]08/02/2023 15:13:55 - INFO - __main__ -   Step: 74, LR: 1.9868620515411826e-05, Loss: 0.5513409376144409
  4%|▎         | 75/2040 [48:39<19:50:27, 36.35s/it]08/02/2023 15:14:32 - INFO - __main__ -   Step: 75, LR: 1.9858514401212735e-05, Loss: 0.559345543384552
  4%|▎         | 76/2040 [49:15<19:48:12, 36.30s/it]08/02/2023 15:15:08 - INFO - __main__ -   Step: 76, LR: 1.9848408287013645e-05, Loss: 0.5980710387229919
  4%|▍         | 77/2040 [49:51<19:44:36, 36.21s/it]08/02/2023 15:15:44 - INFO - __main__ -   Step: 77, LR: 1.9838302172814554e-05, Loss: 0.6034659147262573
  4%|▍         | 78/2040 [50:27<19:44:20, 36.22s/it]08/02/2023 15:16:20 - INFO - __main__ -   Step: 78, LR: 1.9828196058615463e-05, Loss: 0.5841064453125
  4%|▍         | 79/2040 [51:04<19:47:56, 36.35s/it]08/02/2023 15:16:57 - INFO - __main__ -   Step: 79, LR: 1.9818089944416375e-05, Loss: 0.603764533996582
  4%|▍         | 80/2040 [51:40<19:48:54, 36.40s/it]08/02/2023 15:17:33 - INFO - __main__ -   Step: 80, LR: 1.9807983830217284e-05, Loss: 0.6432958841323853
  4%|▍         | 81/2040 [52:16<19:43:29, 36.25s/it]08/02/2023 15:18:09 - INFO - __main__ -   Step: 81, LR: 1.9797877716018193e-05, Loss: 0.6093057990074158
  4%|▍         | 82/2040 [52:52<19:44:17, 36.29s/it]08/02/2023 15:18:46 - INFO - __main__ -   Step: 82, LR: 1.9787771601819102e-05, Loss: 0.5628376603126526
  4%|▍         | 83/2040 [53:29<19:42:12, 36.25s/it]08/02/2023 15:19:22 - INFO - __main__ -   Step: 83, LR: 1.977766548762001e-05, Loss: 0.5397884845733643
  4%|▍         | 84/2040 [54:06<19:51:13, 36.54s/it]08/02/2023 15:19:59 - INFO - __main__ -   Step: 84, LR: 1.976755937342092e-05, Loss: 0.5767823457717896
  4%|▍         | 85/2040 [54:42<19:52:15, 36.59s/it]08/02/2023 15:20:36 - INFO - __main__ -   Step: 85, LR: 1.975745325922183e-05, Loss: 0.5818297863006592
  4%|▍         | 86/2040 [55:19<19:50:41, 36.56s/it]08/02/2023 15:21:12 - INFO - __main__ -   Step: 86, LR: 1.974734714502274e-05, Loss: 0.5693263411521912
  4%|▍         | 87/2040 [55:56<19:49:44, 36.55s/it]08/02/2023 15:21:49 - INFO - __main__ -   Step: 87, LR: 1.973724103082365e-05, Loss: 0.5549736022949219
  4%|▍         | 88/2040 [56:32<19:48:11, 36.52s/it]08/02/2023 15:22:25 - INFO - __main__ -   Step: 88, LR: 1.972713491662456e-05, Loss: 0.5153616666793823
  4%|▍         | 89/2040 [57:08<19:47:40, 36.52s/it]08/02/2023 15:23:02 - INFO - __main__ -   Step: 89, LR: 1.971702880242547e-05, Loss: 0.583228349685669
  4%|▍         | 90/2040 [57:45<19:47:41, 36.54s/it]08/02/2023 15:23:38 - INFO - __main__ -   Step: 90, LR: 1.970692268822638e-05, Loss: 0.5281962752342224
  4%|▍         | 91/2040 [58:22<19:48:51, 36.60s/it]08/02/2023 15:24:15 - INFO - __main__ -   Step: 91, LR: 1.9696816574027287e-05, Loss: 0.5206869840621948
  5%|▍         | 92/2040 [58:58<19:46:00, 36.53s/it]08/02/2023 15:24:51 - INFO - __main__ -   Step: 92, LR: 1.9686710459828196e-05, Loss: 0.5614491701126099
  5%|▍         | 93/2040 [59:34<19:40:57, 36.39s/it]08/02/2023 15:25:28 - INFO - __main__ -   Step: 93, LR: 1.9676604345629106e-05, Loss: 0.5462719798088074
  5%|▍         | 94/2040 [1:00:11<19:45:27, 36.55s/it]08/02/2023 15:26:04 - INFO - __main__ -   Step: 94, LR: 1.9666498231430015e-05, Loss: 0.5256832242012024
  5%|▍         | 95/2040 [1:00:48<19:47:23, 36.63s/it]08/02/2023 15:26:41 - INFO - __main__ -   Step: 95, LR: 1.9656392117230927e-05, Loss: 0.5047392845153809
  5%|▍         | 96/2040 [1:01:24<19:44:15, 36.55s/it]08/02/2023 15:27:18 - INFO - __main__ -   Step: 96, LR: 1.9646286003031836e-05, Loss: 0.6144801378250122
[2023-08-02 15:27:57,048] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 97/2040 [1:02:03<20:06:21, 37.25s/it]08/02/2023 15:27:57 - INFO - __main__ -   Step: 97, LR: 1.9636179888832745e-05, Loss: 0.507914125919342
  5%|▍         | 98/2040 [1:02:39<19:56:04, 36.95s/it]08/02/2023 15:28:33 - INFO - __main__ -   Step: 98, LR: 1.9626073774633658e-05, Loss: 0.5714437365531921
  5%|▍         | 99/2040 [1:03:15<19:44:07, 36.60s/it]08/02/2023 15:29:09 - INFO - __main__ -   Step: 99, LR: 1.9615967660434563e-05, Loss: 0.6638860702514648
  5%|▍         | 100/2040 [1:03:52<19:41:26, 36.54s/it]08/02/2023 15:29:45 - INFO - __main__ -   Step: 100, LR: 1.9605861546235472e-05, Loss: 0.5359514951705933
  5%|▍         | 101/2040 [1:04:27<19:33:41, 36.32s/it]08/02/2023 15:30:21 - INFO - __main__ -   Step: 101, LR: 1.959575543203638e-05, Loss: 0.5900585651397705
  5%|▌         | 102/2040 [1:05:03<19:26:33, 36.12s/it]08/02/2023 15:30:56 - INFO - __main__ -   Step: 102, LR: 1.9585649317837294e-05, Loss: 0.5426716804504395
  5%|▌         | 103/2040 [1:05:38<19:17:15, 35.85s/it]08/02/2023 15:31:32 - INFO - __main__ -   Step: 103, LR: 1.9575543203638203e-05, Loss: 0.6249011158943176
  5%|▌         | 104/2040 [1:06:14<19:14:40, 35.79s/it]08/02/2023 15:32:07 - INFO - __main__ -   Step: 104, LR: 1.9565437089439112e-05, Loss: 0.559319019317627
  5%|▌         | 105/2040 [1:06:50<19:14:47, 35.81s/it]08/02/2023 15:32:43 - INFO - __main__ -   Step: 105, LR: 1.955533097524002e-05, Loss: 0.5656523108482361
  5%|▌         | 106/2040 [1:07:26<19:17:04, 35.90s/it]08/02/2023 15:33:19 - INFO - __main__ -   Step: 106, LR: 1.9545224861040934e-05, Loss: 0.5285893082618713
  5%|▌         | 107/2040 [1:08:02<19:20:13, 36.01s/it]08/02/2023 15:33:56 - INFO - __main__ -   Step: 107, LR: 1.9535118746841843e-05, Loss: 0.5714559555053711
  5%|▌         | 108/2040 [1:08:38<19:20:22, 36.04s/it]08/02/2023 15:34:32 - INFO - __main__ -   Step: 108, LR: 1.9525012632642752e-05, Loss: 0.5352626442909241
  5%|▌         | 109/2040 [1:09:14<19:17:23, 35.96s/it]08/02/2023 15:35:07 - INFO - __main__ -   Step: 109, LR: 1.951490651844366e-05, Loss: 0.6195685863494873
  5%|▌         | 110/2040 [1:09:51<19:22:53, 36.15s/it]08/02/2023 15:35:44 - INFO - __main__ -   Step: 110, LR: 1.950480040424457e-05, Loss: 0.5661271810531616
  5%|▌         | 111/2040 [1:10:27<19:22:44, 36.17s/it]08/02/2023 15:36:20 - INFO - __main__ -   Step: 111, LR: 1.949469429004548e-05, Loss: 0.5244104266166687
  5%|▌         | 112/2040 [1:11:03<19:23:15, 36.20s/it]08/02/2023 15:36:56 - INFO - __main__ -   Step: 112, LR: 1.9484588175846388e-05, Loss: 0.5415230989456177
  6%|▌         | 113/2040 [1:11:40<19:25:39, 36.29s/it]08/02/2023 15:37:33 - INFO - __main__ -   Step: 113, LR: 1.9474482061647297e-05, Loss: 0.6134365200996399
  6%|▌         | 114/2040 [1:12:16<19:28:11, 36.39s/it]08/02/2023 15:38:10 - INFO - __main__ -   Step: 114, LR: 1.946437594744821e-05, Loss: 0.6521571278572083
  6%|▌         | 115/2040 [1:12:52<19:24:47, 36.30s/it]08/02/2023 15:38:46 - INFO - __main__ -   Step: 115, LR: 1.945426983324912e-05, Loss: 0.5764209032058716
  6%|▌         | 116/2040 [1:13:29<19:27:55, 36.42s/it]08/02/2023 15:39:22 - INFO - __main__ -   Step: 116, LR: 1.9444163719050028e-05, Loss: 0.577460527420044
  6%|▌         | 117/2040 [1:14:06<19:28:59, 36.47s/it]08/02/2023 15:39:59 - INFO - __main__ -   Step: 117, LR: 1.9434057604850937e-05, Loss: 0.6110316514968872
  6%|▌         | 118/2040 [1:14:42<19:25:15, 36.38s/it]08/02/2023 15:40:35 - INFO - __main__ -   Step: 118, LR: 1.9423951490651846e-05, Loss: 0.5809605121612549
  6%|▌         | 119/2040 [1:15:18<19:24:24, 36.37s/it]08/02/2023 15:41:12 - INFO - __main__ -   Step: 119, LR: 1.9413845376452755e-05, Loss: 0.6125751733779907
  6%|▌         | 120/2040 [1:15:55<19:25:15, 36.41s/it]08/02/2023 15:41:48 - INFO - __main__ -   Step: 120, LR: 1.9403739262253664e-05, Loss: 0.5957412123680115
  6%|▌         | 121/2040 [1:16:31<19:25:58, 36.46s/it]08/02/2023 15:42:25 - INFO - __main__ -   Step: 121, LR: 1.9393633148054573e-05, Loss: 0.5499727725982666
  6%|▌         | 122/2040 [1:17:07<19:19:04, 36.26s/it]08/02/2023 15:43:00 - INFO - __main__ -   Step: 122, LR: 1.9383527033855486e-05, Loss: 0.541816234588623
  6%|▌         | 123/2040 [1:17:44<19:21:58, 36.37s/it]08/02/2023 15:43:37 - INFO - __main__ -   Step: 123, LR: 1.9373420919656395e-05, Loss: 0.6326624155044556
  6%|▌         | 124/2040 [1:18:21<19:31:44, 36.69s/it]08/02/2023 15:44:14 - INFO - __main__ -   Step: 124, LR: 1.9363314805457304e-05, Loss: 0.5823113322257996
  6%|▌         | 125/2040 [1:18:58<19:30:58, 36.69s/it]08/02/2023 15:44:51 - INFO - __main__ -   Step: 125, LR: 1.9353208691258213e-05, Loss: 0.5210229158401489
  6%|▌         | 126/2040 [1:19:34<19:30:02, 36.68s/it]08/02/2023 15:45:28 - INFO - __main__ -   Step: 126, LR: 1.9343102577059122e-05, Loss: 0.5951948165893555
  6%|▌         | 127/2040 [1:20:10<19:20:21, 36.39s/it]08/02/2023 15:46:04 - INFO - __main__ -   Step: 127, LR: 1.933299646286003e-05, Loss: 0.6369181871414185
  6%|▋         | 128/2040 [1:20:46<19:14:39, 36.23s/it]08/02/2023 15:46:39 - INFO - __main__ -   Step: 128, LR: 1.932289034866094e-05, Loss: 0.5473933219909668
  6%|▋         | 129/2040 [1:21:22<19:11:15, 36.15s/it]08/02/2023 15:47:15 - INFO - __main__ -   Step: 129, LR: 1.931278423446185e-05, Loss: 0.5515263080596924
  6%|▋         | 130/2040 [1:21:58<19:07:53, 36.06s/it]08/02/2023 15:47:51 - INFO - __main__ -   Step: 130, LR: 1.930267812026276e-05, Loss: 0.5246175527572632
  6%|▋         | 131/2040 [1:22:33<19:02:17, 35.90s/it]08/02/2023 15:48:27 - INFO - __main__ -   Step: 131, LR: 1.929257200606367e-05, Loss: 0.5627108216285706
  6%|▋         | 132/2040 [1:23:10<19:04:15, 35.98s/it]08/02/2023 15:49:03 - INFO - __main__ -   Step: 132, LR: 1.928246589186458e-05, Loss: 0.5314888954162598
  7%|▋         | 133/2040 [1:23:45<18:57:18, 35.78s/it]08/02/2023 15:49:38 - INFO - __main__ -   Step: 133, LR: 1.927235977766549e-05, Loss: 0.5478113889694214
  7%|▋         | 134/2040 [1:24:21<19:00:37, 35.91s/it]08/02/2023 15:50:14 - INFO - __main__ -   Step: 134, LR: 1.9262253663466398e-05, Loss: 0.6113250851631165
  7%|▋         | 135/2040 [1:24:57<18:57:54, 35.84s/it]08/02/2023 15:50:50 - INFO - __main__ -   Step: 135, LR: 1.9252147549267307e-05, Loss: 0.48512017726898193
  7%|▋         | 136/2040 [1:25:33<18:58:48, 35.89s/it]08/02/2023 15:51:26 - INFO - __main__ -   Step: 136, LR: 1.9242041435068216e-05, Loss: 0.567250669002533
  7%|▋         | 137/2040 [1:26:09<18:56:39, 35.84s/it]08/02/2023 15:52:02 - INFO - __main__ -   Step: 137, LR: 1.923193532086913e-05, Loss: 0.5228073596954346
  7%|▋         | 138/2040 [1:26:44<18:55:36, 35.82s/it]08/02/2023 15:52:38 - INFO - __main__ -   Step: 138, LR: 1.9221829206670038e-05, Loss: 0.5667391419410706
  7%|▋         | 139/2040 [1:27:20<18:57:47, 35.91s/it]08/02/2023 15:53:14 - INFO - __main__ -   Step: 139, LR: 1.9211723092470947e-05, Loss: 0.6263107061386108
  7%|▋         | 140/2040 [1:27:56<18:57:41, 35.93s/it]08/02/2023 15:53:50 - INFO - __main__ -   Step: 140, LR: 1.9201616978271856e-05, Loss: 0.5737597346305847
  7%|▋         | 141/2040 [1:28:32<18:55:50, 35.89s/it]08/02/2023 15:54:25 - INFO - __main__ -   Step: 141, LR: 1.9191510864072768e-05, Loss: 0.6334637403488159
  7%|▋         | 142/2040 [1:29:08<18:52:12, 35.79s/it]08/02/2023 15:55:01 - INFO - __main__ -   Step: 142, LR: 1.9181404749873674e-05, Loss: 0.6618592739105225
  7%|▋         | 143/2040 [1:29:44<18:59:03, 36.03s/it]08/02/2023 15:55:38 - INFO - __main__ -   Step: 143, LR: 1.9171298635674583e-05, Loss: 0.7196277379989624
  7%|▋         | 144/2040 [1:30:20<18:57:54, 36.01s/it]08/02/2023 15:56:14 - INFO - __main__ -   Step: 144, LR: 1.9161192521475492e-05, Loss: 0.7170101404190063
  7%|▋         | 145/2040 [1:30:56<18:56:05, 35.97s/it]08/02/2023 15:56:49 - INFO - __main__ -   Step: 145, LR: 1.9151086407276404e-05, Loss: 0.6730432510375977
  7%|▋         | 146/2040 [1:31:32<18:55:40, 35.98s/it]08/02/2023 15:57:25 - INFO - __main__ -   Step: 146, LR: 1.9140980293077314e-05, Loss: 0.5453881621360779
  7%|▋         | 147/2040 [1:32:08<18:55:21, 35.99s/it]08/02/2023 15:58:01 - INFO - __main__ -   Step: 147, LR: 1.9130874178878223e-05, Loss: 0.6263546943664551
  7%|▋         | 148/2040 [1:32:44<18:53:13, 35.94s/it]08/02/2023 15:58:37 - INFO - __main__ -   Step: 148, LR: 1.912076806467913e-05, Loss: 0.5930819511413574
  7%|▋         | 149/2040 [1:33:20<18:50:50, 35.88s/it]08/02/2023 15:59:13 - INFO - __main__ -   Step: 149, LR: 1.9110661950480044e-05, Loss: 0.645607590675354
  7%|▋         | 150/2040 [1:33:56<18:51:40, 35.93s/it]08/02/2023 15:59:49 - INFO - __main__ -   Step: 150, LR: 1.9100555836280953e-05, Loss: 0.6011492609977722
  7%|▋         | 151/2040 [1:34:32<18:51:02, 35.93s/it]08/02/2023 16:00:25 - INFO - __main__ -   Step: 151, LR: 1.9090449722081862e-05, Loss: 0.5759309530258179
  7%|▋         | 152/2040 [1:35:07<18:44:01, 35.72s/it]08/02/2023 16:01:00 - INFO - __main__ -   Step: 152, LR: 1.908034360788277e-05, Loss: 0.5889195203781128
  8%|▊         | 153/2040 [1:35:44<18:57:23, 36.17s/it]08/02/2023 16:01:37 - INFO - __main__ -   Step: 153, LR: 1.907023749368368e-05, Loss: 0.5657157897949219
  8%|▊         | 154/2040 [1:36:21<18:58:43, 36.23s/it]08/02/2023 16:02:14 - INFO - __main__ -   Step: 154, LR: 1.906013137948459e-05, Loss: 0.6097052097320557
  8%|▊         | 155/2040 [1:36:56<18:50:54, 36.00s/it]08/02/2023 16:02:49 - INFO - __main__ -   Step: 155, LR: 1.90500252652855e-05, Loss: 0.6895627975463867
  8%|▊         | 156/2040 [1:37:32<18:52:42, 36.07s/it]08/02/2023 16:03:26 - INFO - __main__ -   Step: 156, LR: 1.9039919151086408e-05, Loss: 0.6257858872413635
  8%|▊         | 157/2040 [1:38:08<18:49:20, 35.99s/it]08/02/2023 16:04:01 - INFO - __main__ -   Step: 157, LR: 1.902981303688732e-05, Loss: 0.5523933172225952
  8%|▊         | 158/2040 [1:38:44<18:44:47, 35.86s/it]08/02/2023 16:04:37 - INFO - __main__ -   Step: 158, LR: 1.901970692268823e-05, Loss: 0.5528103709220886
  8%|▊         | 159/2040 [1:39:20<18:46:12, 35.92s/it]08/02/2023 16:05:13 - INFO - __main__ -   Step: 159, LR: 1.9009600808489138e-05, Loss: 0.5706379413604736
  8%|▊         | 160/2040 [1:39:55<18:41:50, 35.80s/it]08/02/2023 16:05:49 - INFO - __main__ -   Step: 160, LR: 1.8999494694290047e-05, Loss: 0.5273656845092773
  8%|▊         | 161/2040 [1:40:31<18:41:28, 35.81s/it]08/02/2023 16:06:24 - INFO - __main__ -   Step: 161, LR: 1.8989388580090956e-05, Loss: 0.5819308161735535
  8%|▊         | 162/2040 [1:41:07<18:40:02, 35.78s/it]08/02/2023 16:07:00 - INFO - __main__ -   Step: 162, LR: 1.8979282465891865e-05, Loss: 0.5490535497665405
  8%|▊         | 163/2040 [1:41:42<18:38:27, 35.75s/it]08/02/2023 16:07:36 - INFO - __main__ -   Step: 163, LR: 1.8969176351692775e-05, Loss: 0.5854284167289734
  8%|▊         | 164/2040 [1:42:18<18:37:06, 35.73s/it]08/02/2023 16:08:11 - INFO - __main__ -   Step: 164, LR: 1.8959070237493684e-05, Loss: 0.5603685975074768
  8%|▊         | 165/2040 [1:42:54<18:36:27, 35.73s/it]08/02/2023 16:08:47 - INFO - __main__ -   Step: 165, LR: 1.8948964123294596e-05, Loss: 0.5905390977859497
  8%|▊         | 166/2040 [1:43:29<18:33:08, 35.64s/it]08/02/2023 16:09:23 - INFO - __main__ -   Step: 166, LR: 1.8938858009095505e-05, Loss: 0.5393276214599609
  8%|▊         | 167/2040 [1:44:05<18:30:45, 35.58s/it]08/02/2023 16:09:58 - INFO - __main__ -   Step: 167, LR: 1.8928751894896414e-05, Loss: 0.5437478423118591
  8%|▊         | 168/2040 [1:44:40<18:31:52, 35.64s/it]08/02/2023 16:10:34 - INFO - __main__ -   Step: 168, LR: 1.8918645780697323e-05, Loss: 0.57923424243927
  8%|▊         | 169/2040 [1:45:16<18:29:24, 35.58s/it]08/02/2023 16:11:09 - INFO - __main__ -   Step: 169, LR: 1.8908539666498232e-05, Loss: 0.5774281024932861
  8%|▊         | 170/2040 [1:45:52<18:34:55, 35.77s/it]08/02/2023 16:11:45 - INFO - __main__ -   Step: 170, LR: 1.889843355229914e-05, Loss: 0.662076473236084
  8%|▊         | 171/2040 [1:46:28<18:31:17, 35.68s/it]08/02/2023 16:12:21 - INFO - __main__ -   Step: 171, LR: 1.888832743810005e-05, Loss: 0.5324463844299316
  8%|▊         | 172/2040 [1:47:03<18:27:20, 35.57s/it]08/02/2023 16:12:56 - INFO - __main__ -   Step: 172, LR: 1.8878221323900963e-05, Loss: 0.5177180767059326
  8%|▊         | 173/2040 [1:47:39<18:29:14, 35.65s/it]08/02/2023 16:13:32 - INFO - __main__ -   Step: 173, LR: 1.8868115209701872e-05, Loss: 0.5000617504119873
  9%|▊         | 174/2040 [1:48:14<18:26:48, 35.59s/it]08/02/2023 16:14:07 - INFO - __main__ -   Step: 174, LR: 1.885800909550278e-05, Loss: 0.5311659574508667
  9%|▊         | 175/2040 [1:48:50<18:24:05, 35.52s/it]08/02/2023 16:14:43 - INFO - __main__ -   Step: 175, LR: 1.884790298130369e-05, Loss: 0.5539866089820862
  9%|▊         | 176/2040 [1:49:25<18:26:09, 35.61s/it]08/02/2023 16:15:19 - INFO - __main__ -   Step: 176, LR: 1.88377968671046e-05, Loss: 0.562525749206543
  9%|▊         | 177/2040 [1:50:01<18:23:07, 35.53s/it]08/02/2023 16:15:54 - INFO - __main__ -   Step: 177, LR: 1.882769075290551e-05, Loss: 0.5108635425567627
  9%|▊         | 178/2040 [1:50:37<18:26:03, 35.64s/it]08/02/2023 16:16:30 - INFO - __main__ -   Step: 178, LR: 1.8817584638706417e-05, Loss: 0.42968595027923584
  9%|▉         | 179/2040 [1:51:12<18:27:18, 35.70s/it]08/02/2023 16:17:06 - INFO - __main__ -   Step: 179, LR: 1.8807478524507327e-05, Loss: 0.5473276376724243
  9%|▉         | 180/2040 [1:51:48<18:23:54, 35.61s/it]08/02/2023 16:17:41 - INFO - __main__ -   Step: 180, LR: 1.879737241030824e-05, Loss: 0.591621994972229
  9%|▉         | 181/2040 [1:52:24<18:32:38, 35.91s/it]08/02/2023 16:18:18 - INFO - __main__ -   Step: 181, LR: 1.8787266296109148e-05, Loss: 0.5973696708679199
  9%|▉         | 182/2040 [1:52:59<18:22:34, 35.60s/it]08/02/2023 16:18:53 - INFO - __main__ -   Step: 182, LR: 1.8777160181910057e-05, Loss: 0.5445139408111572
  9%|▉         | 183/2040 [1:53:36<18:33:05, 35.96s/it]08/02/2023 16:19:29 - INFO - __main__ -   Step: 183, LR: 1.8767054067710966e-05, Loss: 0.5851340293884277
  9%|▉         | 184/2040 [1:54:12<18:33:04, 35.98s/it]08/02/2023 16:20:05 - INFO - __main__ -   Step: 184, LR: 1.875694795351188e-05, Loss: 0.5625202655792236
  9%|▉         | 185/2040 [1:54:48<18:29:46, 35.90s/it]08/02/2023 16:20:41 - INFO - __main__ -   Step: 185, LR: 1.8746841839312784e-05, Loss: 0.5912195444107056
  9%|▉         | 186/2040 [1:55:24<18:30:18, 35.93s/it]08/02/2023 16:21:17 - INFO - __main__ -   Step: 186, LR: 1.8736735725113693e-05, Loss: 0.5706886649131775
  9%|▉         | 187/2040 [1:56:00<18:27:24, 35.86s/it]08/02/2023 16:21:53 - INFO - __main__ -   Step: 187, LR: 1.8726629610914602e-05, Loss: 0.6232553720474243
  9%|▉         | 188/2040 [1:56:35<18:21:37, 35.69s/it]08/02/2023 16:22:28 - INFO - __main__ -   Step: 188, LR: 1.8716523496715515e-05, Loss: 0.6397424936294556
  9%|▉         | 189/2040 [1:57:11<18:23:46, 35.78s/it]08/02/2023 16:23:04 - INFO - __main__ -   Step: 189, LR: 1.8706417382516424e-05, Loss: 0.589594304561615
  9%|▉         | 190/2040 [1:57:46<18:21:16, 35.72s/it]08/02/2023 16:23:40 - INFO - __main__ -   Step: 190, LR: 1.8696311268317333e-05, Loss: 0.613387942314148
  9%|▉         | 191/2040 [1:58:22<18:17:55, 35.63s/it]08/02/2023 16:24:15 - INFO - __main__ -   Step: 191, LR: 1.8686205154118242e-05, Loss: 0.6132488250732422
  9%|▉         | 192/2040 [1:58:57<18:15:48, 35.58s/it]08/02/2023 16:24:51 - INFO - __main__ -   Step: 192, LR: 1.8676099039919155e-05, Loss: 0.5823543667793274
  9%|▉         | 193/2040 [1:59:33<18:18:03, 35.67s/it]08/02/2023 16:25:26 - INFO - __main__ -   Step: 193, LR: 1.8665992925720064e-05, Loss: 0.552059531211853
 10%|▉         | 194/2040 [2:00:09<18:19:44, 35.74s/it]08/02/2023 16:26:02 - INFO - __main__ -   Step: 194, LR: 1.8655886811520973e-05, Loss: 0.5149555802345276
 10%|▉         | 195/2040 [2:00:44<18:15:56, 35.64s/it]08/02/2023 16:26:38 - INFO - __main__ -   Step: 195, LR: 1.8645780697321882e-05, Loss: 0.5236244201660156
 10%|▉         | 196/2040 [2:01:20<18:18:48, 35.75s/it]08/02/2023 16:27:14 - INFO - __main__ -   Step: 196, LR: 1.863567458312279e-05, Loss: 0.5913203954696655
 10%|▉         | 197/2040 [2:01:56<18:18:38, 35.77s/it]08/02/2023 16:27:50 - INFO - __main__ -   Step: 197, LR: 1.86255684689237e-05, Loss: 0.6818006038665771
 10%|▉         | 198/2040 [2:02:32<18:17:32, 35.75s/it]08/02/2023 16:28:25 - INFO - __main__ -   Step: 198, LR: 1.861546235472461e-05, Loss: 0.5844528079032898
 10%|▉         | 199/2040 [2:03:08<18:18:23, 35.80s/it]08/02/2023 16:29:01 - INFO - __main__ -   Step: 199, LR: 1.8605356240525518e-05, Loss: 0.5700194835662842
 10%|▉         | 200/2040 [2:03:44<18:19:38, 35.86s/it]08/02/2023 16:29:37 - INFO - __main__ -   Step: 200, LR: 1.859525012632643e-05, Loss: 0.5140135884284973
 10%|▉         | 201/2040 [2:04:20<18:17:59, 35.82s/it]08/02/2023 16:30:13 - INFO - __main__ -   Step: 201, LR: 1.858514401212734e-05, Loss: 0.582208514213562
 10%|▉         | 202/2040 [2:04:55<18:13:44, 35.70s/it]08/02/2023 16:30:48 - INFO - __main__ -   Step: 202, LR: 1.857503789792825e-05, Loss: 0.5618607997894287
 10%|▉         | 203/2040 [2:05:31<18:12:44, 35.69s/it]08/02/2023 16:31:24 - INFO - __main__ -   Step: 203, LR: 1.8564931783729158e-05, Loss: 0.5844689011573792
 10%|█         | 204/2040 [2:06:07<18:14:29, 35.77s/it]08/02/2023 16:32:00 - INFO - __main__ -   Step: 204, LR: 1.8554825669530067e-05, Loss: 0.5705406069755554
 10%|█         | 205/2040 [2:06:42<18:09:21, 35.62s/it]08/02/2023 16:32:35 - INFO - __main__ -   Step: 205, LR: 1.8544719555330976e-05, Loss: 0.536220133304596
 10%|█         | 206/2040 [2:07:18<18:09:54, 35.66s/it]08/02/2023 16:33:11 - INFO - __main__ -   Step: 206, LR: 1.8534613441131885e-05, Loss: 0.559636116027832
 10%|█         | 207/2040 [2:07:53<18:05:11, 35.52s/it]08/02/2023 16:33:46 - INFO - __main__ -   Step: 207, LR: 1.8524507326932798e-05, Loss: 0.6949095726013184
 10%|█         | 208/2040 [2:08:28<18:04:35, 35.52s/it]08/02/2023 16:34:22 - INFO - __main__ -   Step: 208, LR: 1.8514401212733707e-05, Loss: 0.6103140115737915
 10%|█         | 209/2040 [2:09:04<18:03:15, 35.50s/it]08/02/2023 16:34:57 - INFO - __main__ -   Step: 209, LR: 1.8504295098534616e-05, Loss: 0.6000699996948242
 10%|█         | 210/2040 [2:09:40<18:06:03, 35.61s/it]08/02/2023 16:35:33 - INFO - __main__ -   Step: 210, LR: 1.8494188984335525e-05, Loss: 0.5700951814651489
 10%|█         | 211/2040 [2:10:15<18:03:35, 35.55s/it]08/02/2023 16:36:08 - INFO - __main__ -   Step: 211, LR: 1.8484082870136434e-05, Loss: 0.4628788232803345
 10%|█         | 212/2040 [2:10:51<18:04:51, 35.61s/it]08/02/2023 16:36:44 - INFO - __main__ -   Step: 212, LR: 1.8473976755937343e-05, Loss: 0.5377495884895325
 10%|█         | 213/2040 [2:11:27<18:06:58, 35.70s/it]08/02/2023 16:37:20 - INFO - __main__ -   Step: 213, LR: 1.8463870641738252e-05, Loss: 0.5083096027374268
 10%|█         | 214/2040 [2:12:03<18:06:55, 35.71s/it]08/02/2023 16:37:56 - INFO - __main__ -   Step: 214, LR: 1.845376452753916e-05, Loss: 0.6143921613693237
 11%|█         | 215/2040 [2:12:38<18:01:13, 35.55s/it]08/02/2023 16:38:31 - INFO - __main__ -   Step: 215, LR: 1.8443658413340073e-05, Loss: 0.5922056436538696
 11%|█         | 216/2040 [2:13:13<17:58:44, 35.48s/it]08/02/2023 16:39:06 - INFO - __main__ -   Step: 216, LR: 1.8433552299140983e-05, Loss: 0.5701218247413635
 11%|█         | 217/2040 [2:13:49<18:04:03, 35.68s/it]08/02/2023 16:39:42 - INFO - __main__ -   Step: 217, LR: 1.842344618494189e-05, Loss: 0.5655741691589355
 11%|█         | 218/2040 [2:14:25<18:05:17, 35.74s/it]08/02/2023 16:40:18 - INFO - __main__ -   Step: 218, LR: 1.84133400707428e-05, Loss: 0.5129438042640686
 11%|█         | 219/2040 [2:15:01<18:04:03, 35.72s/it]08/02/2023 16:40:54 - INFO - __main__ -   Step: 219, LR: 1.840323395654371e-05, Loss: 0.5780366063117981
 11%|█         | 220/2040 [2:15:37<18:07:21, 35.85s/it]08/02/2023 16:41:30 - INFO - __main__ -   Step: 220, LR: 1.839312784234462e-05, Loss: 0.5726332664489746
 11%|█         | 221/2040 [2:16:13<18:11:23, 36.00s/it]08/02/2023 16:42:07 - INFO - __main__ -   Step: 221, LR: 1.8383021728145528e-05, Loss: 0.5087660551071167
 11%|█         | 222/2040 [2:16:49<18:09:37, 35.96s/it]08/02/2023 16:42:42 - INFO - __main__ -   Step: 222, LR: 1.8372915613946437e-05, Loss: 0.5620032548904419
 11%|█         | 223/2040 [2:17:25<18:12:41, 36.08s/it]08/02/2023 16:43:19 - INFO - __main__ -   Step: 223, LR: 1.836280949974735e-05, Loss: 0.5392246842384338
 11%|█         | 224/2040 [2:18:01<18:09:17, 35.99s/it]08/02/2023 16:43:55 - INFO - __main__ -   Step: 224, LR: 1.835270338554826e-05, Loss: 0.5167534947395325
 11%|█         | 225/2040 [2:18:37<18:05:10, 35.87s/it]08/02/2023 16:44:30 - INFO - __main__ -   Step: 225, LR: 1.8342597271349168e-05, Loss: 0.5948965549468994
 11%|█         | 226/2040 [2:19:12<18:00:48, 35.75s/it]08/02/2023 16:45:06 - INFO - __main__ -   Step: 226, LR: 1.8332491157150077e-05, Loss: 0.5495425462722778
 11%|█         | 227/2040 [2:19:48<18:00:31, 35.76s/it]08/02/2023 16:45:41 - INFO - __main__ -   Step: 227, LR: 1.832238504295099e-05, Loss: 0.5354808568954468
 11%|█         | 228/2040 [2:20:24<18:04:17, 35.90s/it]08/02/2023 16:46:18 - INFO - __main__ -   Step: 228, LR: 1.8312278928751895e-05, Loss: 0.5470439791679382
 11%|█         | 229/2040 [2:21:01<18:06:14, 35.99s/it]08/02/2023 16:46:54 - INFO - __main__ -   Step: 229, LR: 1.8302172814552804e-05, Loss: 0.5151453018188477
 11%|█▏        | 230/2040 [2:21:37<18:06:54, 36.03s/it]08/02/2023 16:47:30 - INFO - __main__ -   Step: 230, LR: 1.8292066700353713e-05, Loss: 0.5864348411560059
 11%|█▏        | 231/2040 [2:22:13<18:04:51, 35.98s/it]08/02/2023 16:48:06 - INFO - __main__ -   Step: 231, LR: 1.8281960586154625e-05, Loss: 0.5449047684669495
 11%|█▏        | 232/2040 [2:22:48<18:01:43, 35.90s/it]08/02/2023 16:48:42 - INFO - __main__ -   Step: 232, LR: 1.8271854471955535e-05, Loss: 0.5059670805931091
 11%|█▏        | 233/2040 [2:23:24<18:02:05, 35.93s/it]08/02/2023 16:49:18 - INFO - __main__ -   Step: 233, LR: 1.8261748357756444e-05, Loss: 0.540160596370697
 11%|█▏        | 234/2040 [2:24:00<17:59:30, 35.86s/it]08/02/2023 16:49:53 - INFO - __main__ -   Step: 234, LR: 1.8251642243557353e-05, Loss: 0.5245369672775269
 12%|█▏        | 235/2040 [2:24:36<18:02:41, 35.99s/it]08/02/2023 16:50:30 - INFO - __main__ -   Step: 235, LR: 1.8241536129358265e-05, Loss: 0.5544309616088867
 12%|█▏        | 236/2040 [2:25:12<18:02:40, 36.01s/it]08/02/2023 16:51:06 - INFO - __main__ -   Step: 236, LR: 1.8231430015159174e-05, Loss: 0.5667901039123535
 12%|█▏        | 237/2040 [2:25:48<18:01:48, 36.00s/it]08/02/2023 16:51:42 - INFO - __main__ -   Step: 237, LR: 1.8221323900960083e-05, Loss: 0.5047111511230469
 12%|█▏        | 238/2040 [2:26:24<17:59:21, 35.94s/it]08/02/2023 16:52:17 - INFO - __main__ -   Step: 238, LR: 1.8211217786760992e-05, Loss: 0.47775498032569885
 12%|█▏        | 239/2040 [2:27:00<18:01:42, 36.04s/it]08/02/2023 16:52:54 - INFO - __main__ -   Step: 239, LR: 1.82011116725619e-05, Loss: 0.5586894750595093
 12%|█▏        | 240/2040 [2:27:36<17:59:50, 35.99s/it]08/02/2023 16:53:30 - INFO - __main__ -   Step: 240, LR: 1.819100555836281e-05, Loss: 0.5090004801750183
 12%|█▏        | 241/2040 [2:28:12<18:00:33, 36.04s/it]08/02/2023 16:54:06 - INFO - __main__ -   Step: 241, LR: 1.818089944416372e-05, Loss: 0.607895016670227
 12%|█▏        | 242/2040 [2:28:49<18:01:13, 36.08s/it]08/02/2023 16:54:42 - INFO - __main__ -   Step: 242, LR: 1.8170793329964632e-05, Loss: 0.5168696045875549
 12%|█▏        | 243/2040 [2:29:25<18:02:21, 36.14s/it]08/02/2023 16:55:18 - INFO - __main__ -   Step: 243, LR: 1.816068721576554e-05, Loss: 0.5535027980804443
 12%|█▏        | 244/2040 [2:30:01<18:02:00, 36.15s/it]08/02/2023 16:55:54 - INFO - __main__ -   Step: 244, LR: 1.815058110156645e-05, Loss: 0.5923190116882324
 12%|█▏        | 245/2040 [2:30:37<18:01:18, 36.14s/it]08/02/2023 16:56:30 - INFO - __main__ -   Step: 245, LR: 1.814047498736736e-05, Loss: 0.5564236640930176
 12%|█▏        | 246/2040 [2:31:13<18:02:19, 36.20s/it]08/02/2023 16:57:07 - INFO - __main__ -   Step: 246, LR: 1.813036887316827e-05, Loss: 0.5526841878890991
 12%|█▏        | 247/2040 [2:31:49<17:57:20, 36.05s/it]08/02/2023 16:57:42 - INFO - __main__ -   Step: 247, LR: 1.8120262758969177e-05, Loss: 0.5896822214126587
 12%|█▏        | 248/2040 [2:32:25<17:59:17, 36.14s/it]08/02/2023 16:58:19 - INFO - __main__ -   Step: 248, LR: 1.8110156644770086e-05, Loss: 0.6062687635421753
 12%|█▏        | 249/2040 [2:33:01<17:56:21, 36.06s/it]08/02/2023 16:58:55 - INFO - __main__ -   Step: 249, LR: 1.8100050530570996e-05, Loss: 0.5242213010787964
 12%|█▏        | 250/2040 [2:33:38<17:58:36, 36.15s/it]08/02/2023 16:59:31 - INFO - __main__ -   Step: 250, LR: 1.8089944416371908e-05, Loss: 0.5205854177474976
 12%|█▏        | 251/2040 [2:34:14<18:00:34, 36.24s/it]08/02/2023 17:00:07 - INFO - __main__ -   Step: 251, LR: 1.8079838302172817e-05, Loss: 0.47524791955947876
 12%|█▏        | 252/2040 [2:34:50<17:52:26, 35.99s/it]08/02/2023 17:00:43 - INFO - __main__ -   Step: 252, LR: 1.8069732187973726e-05, Loss: 0.5052614212036133
 12%|█▏        | 253/2040 [2:35:26<17:52:21, 36.01s/it]08/02/2023 17:01:19 - INFO - __main__ -   Step: 253, LR: 1.8059626073774635e-05, Loss: 0.5340516567230225
 12%|█▏        | 254/2040 [2:36:01<17:50:33, 35.97s/it]08/02/2023 17:01:55 - INFO - __main__ -   Step: 254, LR: 1.8049519959575544e-05, Loss: 0.4878338575363159
 12%|█▎        | 255/2040 [2:36:38<17:52:14, 36.04s/it]08/02/2023 17:02:31 - INFO - __main__ -   Step: 255, LR: 1.8039413845376453e-05, Loss: 0.5441575050354004
 13%|█▎        | 256/2040 [2:37:14<17:54:28, 36.14s/it]08/02/2023 17:03:07 - INFO - __main__ -   Step: 256, LR: 1.8029307731177362e-05, Loss: 0.5778534412384033
 13%|█▎        | 257/2040 [2:37:50<17:55:27, 36.19s/it]08/02/2023 17:03:44 - INFO - __main__ -   Step: 257, LR: 1.801920161697827e-05, Loss: 0.5226104259490967
 13%|█▎        | 258/2040 [2:38:27<17:54:25, 36.18s/it]08/02/2023 17:04:20 - INFO - __main__ -   Step: 258, LR: 1.8009095502779184e-05, Loss: 0.5593931674957275
 13%|█▎        | 259/2040 [2:39:02<17:51:10, 36.09s/it]08/02/2023 17:04:56 - INFO - __main__ -   Step: 259, LR: 1.7998989388580093e-05, Loss: 0.640270471572876
 13%|█▎        | 260/2040 [2:39:38<17:50:36, 36.09s/it]08/02/2023 17:05:32 - INFO - __main__ -   Step: 260, LR: 1.7988883274381002e-05, Loss: 0.5798195600509644
 13%|█▎        | 261/2040 [2:40:14<17:47:57, 36.02s/it]08/02/2023 17:06:08 - INFO - __main__ -   Step: 261, LR: 1.797877716018191e-05, Loss: 0.537666916847229
 13%|█▎        | 262/2040 [2:40:51<17:56:55, 36.34s/it]08/02/2023 17:06:45 - INFO - __main__ -   Step: 262, LR: 1.796867104598282e-05, Loss: 0.5582400560379028
 13%|█▎        | 263/2040 [2:41:27<17:52:52, 36.23s/it]08/02/2023 17:07:21 - INFO - __main__ -   Step: 263, LR: 1.795856493178373e-05, Loss: 0.5114749670028687
 13%|█▎        | 264/2040 [2:42:03<17:47:19, 36.06s/it]08/02/2023 17:07:56 - INFO - __main__ -   Step: 264, LR: 1.794845881758464e-05, Loss: 0.5640991926193237
 13%|█▎        | 265/2040 [2:42:39<17:45:16, 36.01s/it]08/02/2023 17:08:32 - INFO - __main__ -   Step: 265, LR: 1.7938352703385547e-05, Loss: 0.5635794401168823
 13%|█▎        | 266/2040 [2:43:15<17:47:19, 36.10s/it]08/02/2023 17:09:09 - INFO - __main__ -   Step: 266, LR: 1.792824658918646e-05, Loss: 0.5919675230979919
 13%|█▎        | 267/2040 [2:43:51<17:40:17, 35.88s/it]08/02/2023 17:09:44 - INFO - __main__ -   Step: 267, LR: 1.791814047498737e-05, Loss: 0.5695741772651672
 13%|█▎        | 268/2040 [2:44:27<17:42:21, 35.97s/it]08/02/2023 17:10:20 - INFO - __main__ -   Step: 268, LR: 1.7908034360788278e-05, Loss: 0.4753091037273407
 13%|█▎        | 269/2040 [2:45:03<17:41:38, 35.97s/it]08/02/2023 17:10:56 - INFO - __main__ -   Step: 269, LR: 1.7897928246589187e-05, Loss: 0.4989546239376068
 13%|█▎        | 270/2040 [2:45:39<17:40:28, 35.95s/it]08/02/2023 17:11:32 - INFO - __main__ -   Step: 270, LR: 1.78878221323901e-05, Loss: 0.5545241236686707
 13%|█▎        | 271/2040 [2:46:14<17:35:33, 35.80s/it]08/02/2023 17:12:07 - INFO - __main__ -   Step: 271, LR: 1.7877716018191005e-05, Loss: 0.5295135974884033
 13%|█▎        | 272/2040 [2:46:50<17:33:05, 35.74s/it]08/02/2023 17:12:43 - INFO - __main__ -   Step: 272, LR: 1.7867609903991914e-05, Loss: 0.513099193572998
 13%|█▎        | 273/2040 [2:47:25<17:32:28, 35.74s/it]08/02/2023 17:13:19 - INFO - __main__ -   Step: 273, LR: 1.7857503789792827e-05, Loss: 0.5184012651443481
 13%|█▎        | 274/2040 [2:48:01<17:32:59, 35.78s/it]08/02/2023 17:13:55 - INFO - __main__ -   Step: 274, LR: 1.7847397675593736e-05, Loss: 0.5548656582832336
 13%|█▎        | 275/2040 [2:48:37<17:32:06, 35.77s/it]08/02/2023 17:14:30 - INFO - __main__ -   Step: 275, LR: 1.7837291561394645e-05, Loss: 0.5395370721817017
 14%|█▎        | 276/2040 [2:49:13<17:30:00, 35.71s/it]08/02/2023 17:15:06 - INFO - __main__ -   Step: 276, LR: 1.7827185447195554e-05, Loss: 0.5634480118751526
 14%|█▎        | 277/2040 [2:49:48<17:27:48, 35.66s/it]08/02/2023 17:15:42 - INFO - __main__ -   Step: 277, LR: 1.7817079332996467e-05, Loss: 0.5658432841300964
 14%|█▎        | 278/2040 [2:50:24<17:26:27, 35.63s/it]08/02/2023 17:16:17 - INFO - __main__ -   Step: 278, LR: 1.7806973218797376e-05, Loss: 0.5561304092407227
 14%|█▎        | 279/2040 [2:51:00<17:29:03, 35.74s/it]08/02/2023 17:16:53 - INFO - __main__ -   Step: 279, LR: 1.7796867104598285e-05, Loss: 0.5151993036270142
 14%|█▎        | 280/2040 [2:51:36<17:29:40, 35.78s/it]08/02/2023 17:17:29 - INFO - __main__ -   Step: 280, LR: 1.7786760990399194e-05, Loss: 0.5058329105377197
 14%|█▍        | 281/2040 [2:52:12<17:32:16, 35.89s/it]08/02/2023 17:18:05 - INFO - __main__ -   Step: 281, LR: 1.7776654876200103e-05, Loss: 0.5126330256462097
 14%|█▍        | 282/2040 [2:52:48<17:31:31, 35.89s/it]08/02/2023 17:18:41 - INFO - __main__ -   Step: 282, LR: 1.7766548762001012e-05, Loss: 0.5639636516571045
 14%|█▍        | 283/2040 [2:53:23<17:29:07, 35.83s/it]08/02/2023 17:19:17 - INFO - __main__ -   Step: 283, LR: 1.775644264780192e-05, Loss: 0.5204917192459106
 14%|█▍        | 284/2040 [2:53:59<17:31:08, 35.92s/it]08/02/2023 17:19:53 - INFO - __main__ -   Step: 284, LR: 1.774633653360283e-05, Loss: 0.5219789743423462
 14%|█▍        | 285/2040 [2:54:36<17:31:28, 35.95s/it]08/02/2023 17:20:29 - INFO - __main__ -   Step: 285, LR: 1.7736230419403743e-05, Loss: 0.5590347051620483
 14%|█▍        | 286/2040 [2:55:12<17:33:12, 36.03s/it]08/02/2023 17:21:05 - INFO - __main__ -   Step: 286, LR: 1.772612430520465e-05, Loss: 0.5800707936286926
 14%|█▍        | 287/2040 [2:55:47<17:28:07, 35.87s/it]08/02/2023 17:21:41 - INFO - __main__ -   Step: 287, LR: 1.771601819100556e-05, Loss: 0.49722132086753845
 14%|█▍        | 288/2040 [2:56:23<17:26:37, 35.84s/it]08/02/2023 17:22:16 - INFO - __main__ -   Step: 288, LR: 1.770591207680647e-05, Loss: 0.500056803226471
 14%|█▍        | 289/2040 [2:56:59<17:27:02, 35.88s/it]08/02/2023 17:22:52 - INFO - __main__ -   Step: 289, LR: 1.769580596260738e-05, Loss: 0.5480746626853943
 14%|█▍        | 290/2040 [2:57:35<17:24:21, 35.81s/it]08/02/2023 17:23:28 - INFO - __main__ -   Step: 290, LR: 1.7685699848408288e-05, Loss: 0.6473960876464844
 14%|█▍        | 291/2040 [2:58:10<17:22:43, 35.77s/it]08/02/2023 17:24:04 - INFO - __main__ -   Step: 291, LR: 1.7675593734209197e-05, Loss: 0.5394644141197205
 14%|█▍        | 292/2040 [2:58:46<17:19:51, 35.69s/it]08/02/2023 17:24:39 - INFO - __main__ -   Step: 292, LR: 1.7665487620010106e-05, Loss: 0.5823927521705627
 14%|█▍        | 293/2040 [2:59:22<17:22:35, 35.81s/it]08/02/2023 17:25:15 - INFO - __main__ -   Step: 293, LR: 1.765538150581102e-05, Loss: 0.5089878439903259
 14%|█▍        | 294/2040 [2:59:58<17:21:42, 35.80s/it]08/02/2023 17:25:51 - INFO - __main__ -   Step: 294, LR: 1.7645275391611928e-05, Loss: 0.6039243340492249
 14%|█▍        | 295/2040 [3:00:34<17:22:23, 35.84s/it]08/02/2023 17:26:27 - INFO - __main__ -   Step: 295, LR: 1.7635169277412837e-05, Loss: 0.5731582641601562
 15%|█▍        | 296/2040 [3:01:09<17:21:57, 35.85s/it]08/02/2023 17:27:03 - INFO - __main__ -   Step: 296, LR: 1.7625063163213746e-05, Loss: 0.5519264340400696
 15%|█▍        | 297/2040 [3:01:45<17:19:26, 35.78s/it]08/02/2023 17:27:38 - INFO - __main__ -   Step: 297, LR: 1.7614957049014655e-05, Loss: 0.4454140067100525
 15%|█▍        | 298/2040 [3:02:20<17:13:38, 35.60s/it]08/02/2023 17:28:14 - INFO - __main__ -   Step: 298, LR: 1.7604850934815564e-05, Loss: 0.506893515586853
 15%|█▍        | 299/2040 [3:02:56<17:12:29, 35.58s/it]08/02/2023 17:28:49 - INFO - __main__ -   Step: 299, LR: 1.7594744820616473e-05, Loss: 0.577859103679657
 15%|█▍        | 300/2040 [3:03:31<17:10:44, 35.54s/it]08/02/2023 17:29:25 - INFO - __main__ -   Step: 300, LR: 1.7584638706417382e-05, Loss: 0.5913258790969849
 15%|█▍        | 301/2040 [3:04:07<17:14:41, 35.70s/it]08/02/2023 17:30:01 - INFO - __main__ -   Step: 301, LR: 1.7574532592218294e-05, Loss: 0.48383957147598267
 15%|█▍        | 302/2040 [3:04:43<17:16:58, 35.80s/it]08/02/2023 17:30:37 - INFO - __main__ -   Step: 302, LR: 1.7564426478019204e-05, Loss: 0.5134258270263672
 15%|█▍        | 303/2040 [3:05:19<17:15:28, 35.77s/it]08/02/2023 17:31:12 - INFO - __main__ -   Step: 303, LR: 1.7554320363820113e-05, Loss: 0.5659385919570923
 15%|█▍        | 304/2040 [3:05:55<17:15:48, 35.80s/it]08/02/2023 17:31:48 - INFO - __main__ -   Step: 304, LR: 1.754421424962102e-05, Loss: 0.5350317358970642
 15%|█▍        | 305/2040 [3:06:31<17:15:06, 35.80s/it]08/02/2023 17:32:24 - INFO - __main__ -   Step: 305, LR: 1.753410813542193e-05, Loss: 0.5695666074752808
 15%|█▌        | 306/2040 [3:07:07<17:22:18, 36.07s/it]08/02/2023 17:33:01 - INFO - __main__ -   Step: 306, LR: 1.752400202122284e-05, Loss: 0.527302622795105
 15%|█▌        | 307/2040 [3:07:43<17:17:20, 35.92s/it]08/02/2023 17:33:36 - INFO - __main__ -   Step: 307, LR: 1.751389590702375e-05, Loss: 0.612221896648407
 15%|█▌        | 308/2040 [3:08:19<17:13:31, 35.80s/it]08/02/2023 17:34:12 - INFO - __main__ -   Step: 308, LR: 1.750378979282466e-05, Loss: 0.5601335167884827
 15%|█▌        | 309/2040 [3:08:55<17:15:16, 35.88s/it]08/02/2023 17:34:48 - INFO - __main__ -   Step: 309, LR: 1.749368367862557e-05, Loss: 0.5111132264137268
 15%|█▌        | 310/2040 [3:09:31<17:15:47, 35.92s/it]08/02/2023 17:35:24 - INFO - __main__ -   Step: 310, LR: 1.748357756442648e-05, Loss: 0.5277402997016907
 15%|█▌        | 311/2040 [3:10:06<17:14:55, 35.91s/it]08/02/2023 17:36:00 - INFO - __main__ -   Step: 311, LR: 1.747347145022739e-05, Loss: 0.528745710849762
 15%|█▌        | 312/2040 [3:10:42<17:14:41, 35.93s/it]08/02/2023 17:36:36 - INFO - __main__ -   Step: 312, LR: 1.74633653360283e-05, Loss: 0.518218994140625
 15%|█▌        | 313/2040 [3:11:18<17:12:08, 35.86s/it]08/02/2023 17:37:11 - INFO - __main__ -   Step: 313, LR: 1.745325922182921e-05, Loss: 0.5333733558654785
 15%|█▌        | 314/2040 [3:11:54<17:12:19, 35.89s/it]08/02/2023 17:37:47 - INFO - __main__ -   Step: 314, LR: 1.7443153107630116e-05, Loss: 0.6189619302749634
 15%|█▌        | 315/2040 [3:12:30<17:14:18, 35.98s/it]08/02/2023 17:38:24 - INFO - __main__ -   Step: 315, LR: 1.7433046993431025e-05, Loss: 0.5773731470108032
 15%|█▌        | 316/2040 [3:13:06<17:12:11, 35.92s/it]08/02/2023 17:38:59 - INFO - __main__ -   Step: 316, LR: 1.7422940879231937e-05, Loss: 0.47820740938186646
 16%|█▌        | 317/2040 [3:13:42<17:14:35, 36.03s/it]08/02/2023 17:39:36 - INFO - __main__ -   Step: 317, LR: 1.7412834765032846e-05, Loss: 0.5399821996688843
 16%|█▌        | 318/2040 [3:14:18<17:12:38, 35.98s/it]08/02/2023 17:40:12 - INFO - __main__ -   Step: 318, LR: 1.7402728650833755e-05, Loss: 0.5783225297927856
 16%|█▌        | 319/2040 [3:14:55<17:15:45, 36.11s/it]08/02/2023 17:40:48 - INFO - __main__ -   Step: 319, LR: 1.7392622536634665e-05, Loss: 0.5267994999885559
 16%|█▌        | 320/2040 [3:15:31<17:15:10, 36.11s/it]08/02/2023 17:41:24 - INFO - __main__ -   Step: 320, LR: 1.7382516422435577e-05, Loss: 0.5267935991287231
 16%|█▌        | 321/2040 [3:16:07<17:13:09, 36.06s/it]08/02/2023 17:42:00 - INFO - __main__ -   Step: 321, LR: 1.7372410308236486e-05, Loss: 0.5575776100158691
 16%|█▌        | 322/2040 [3:16:43<17:13:15, 36.09s/it]08/02/2023 17:42:36 - INFO - __main__ -   Step: 322, LR: 1.7362304194037395e-05, Loss: 0.5638183951377869
 16%|█▌        | 323/2040 [3:17:19<17:16:32, 36.22s/it]08/02/2023 17:43:13 - INFO - __main__ -   Step: 323, LR: 1.7352198079838304e-05, Loss: 0.5741367340087891
 16%|█▌        | 324/2040 [3:17:55<17:14:21, 36.17s/it]08/02/2023 17:43:49 - INFO - __main__ -   Step: 324, LR: 1.7342091965639213e-05, Loss: 0.5038597583770752
 16%|█▌        | 325/2040 [3:18:31<17:12:19, 36.12s/it]08/02/2023 17:44:25 - INFO - __main__ -   Step: 325, LR: 1.7331985851440122e-05, Loss: 0.6056185960769653
 16%|█▌        | 326/2040 [3:19:08<17:11:56, 36.12s/it]08/02/2023 17:45:01 - INFO - __main__ -   Step: 326, LR: 1.732187973724103e-05, Loss: 0.502158522605896
 16%|█▌        | 327/2040 [3:19:44<17:11:29, 36.13s/it]08/02/2023 17:45:37 - INFO - __main__ -   Step: 327, LR: 1.731177362304194e-05, Loss: 0.6414576768875122
 16%|█▌        | 328/2040 [3:20:20<17:08:12, 36.04s/it]08/02/2023 17:46:13 - INFO - __main__ -   Step: 328, LR: 1.7301667508842853e-05, Loss: 0.5878174901008606
 16%|█▌        | 329/2040 [3:20:55<17:07:01, 36.02s/it]08/02/2023 17:46:49 - INFO - __main__ -   Step: 329, LR: 1.7291561394643762e-05, Loss: 0.6002011299133301
 16%|█▌        | 330/2040 [3:21:31<17:05:05, 35.97s/it]08/02/2023 17:47:25 - INFO - __main__ -   Step: 330, LR: 1.728145528044467e-05, Loss: 0.5212676525115967
 16%|█▌        | 331/2040 [3:22:07<17:02:58, 35.91s/it]08/02/2023 17:48:00 - INFO - __main__ -   Step: 331, LR: 1.727134916624558e-05, Loss: 0.5714102387428284
 16%|█▋        | 332/2040 [3:22:43<17:01:20, 35.88s/it]08/02/2023 17:48:36 - INFO - __main__ -   Step: 332, LR: 1.726124305204649e-05, Loss: 0.5518746376037598
 16%|█▋        | 333/2040 [3:23:19<17:03:38, 35.98s/it]08/02/2023 17:49:12 - INFO - __main__ -   Step: 333, LR: 1.72511369378474e-05, Loss: 0.5178078413009644
 16%|█▋        | 334/2040 [3:23:55<16:58:50, 35.83s/it]08/02/2023 17:49:48 - INFO - __main__ -   Step: 334, LR: 1.7241030823648307e-05, Loss: 0.5267322659492493
 16%|█▋        | 335/2040 [3:24:30<16:58:23, 35.84s/it]08/02/2023 17:50:24 - INFO - __main__ -   Step: 335, LR: 1.7230924709449217e-05, Loss: 0.5520725846290588
 16%|█▋        | 336/2040 [3:25:07<16:59:42, 35.91s/it]08/02/2023 17:51:00 - INFO - __main__ -   Step: 336, LR: 1.722081859525013e-05, Loss: 0.5104751586914062
 17%|█▋        | 337/2040 [3:25:43<17:02:45, 36.03s/it]08/02/2023 17:51:36 - INFO - __main__ -   Step: 337, LR: 1.7210712481051038e-05, Loss: 0.5134638547897339
 17%|█▋        | 338/2040 [3:26:19<17:00:51, 35.99s/it]08/02/2023 17:52:12 - INFO - __main__ -   Step: 338, LR: 1.7200606366851947e-05, Loss: 0.5017284750938416
 17%|█▋        | 339/2040 [3:26:54<16:57:51, 35.90s/it]08/02/2023 17:52:48 - INFO - __main__ -   Step: 339, LR: 1.7190500252652856e-05, Loss: 0.5879416465759277
 17%|█▋        | 340/2040 [3:27:30<16:55:12, 35.83s/it]08/02/2023 17:53:23 - INFO - __main__ -   Step: 340, LR: 1.7180394138453765e-05, Loss: 0.5547769665718079
 17%|█▋        | 341/2040 [3:28:07<17:04:06, 36.17s/it]08/02/2023 17:54:00 - INFO - __main__ -   Step: 341, LR: 1.7170288024254674e-05, Loss: 0.5504776835441589
 17%|█▋        | 342/2040 [3:28:43<16:59:45, 36.03s/it]08/02/2023 17:54:36 - INFO - __main__ -   Step: 342, LR: 1.7160181910055583e-05, Loss: 0.5263643264770508
 17%|█▋        | 343/2040 [3:29:19<16:56:57, 35.96s/it]08/02/2023 17:55:12 - INFO - __main__ -   Step: 343, LR: 1.7150075795856496e-05, Loss: 0.5500292778015137
 17%|█▋        | 344/2040 [3:29:54<16:55:22, 35.92s/it]08/02/2023 17:55:48 - INFO - __main__ -   Step: 344, LR: 1.7139969681657405e-05, Loss: 0.589463472366333
 17%|█▋        | 345/2040 [3:30:30<16:48:42, 35.71s/it]08/02/2023 17:56:23 - INFO - __main__ -   Step: 345, LR: 1.7129863567458314e-05, Loss: 0.5408276915550232
 17%|█▋        | 346/2040 [3:31:05<16:46:29, 35.65s/it]08/02/2023 17:56:58 - INFO - __main__ -   Step: 346, LR: 1.7119757453259223e-05, Loss: 0.5831919312477112
 17%|█▋        | 347/2040 [3:31:40<16:41:07, 35.48s/it]08/02/2023 17:57:34 - INFO - __main__ -   Step: 347, LR: 1.7109651339060132e-05, Loss: 0.5629944205284119
 17%|█▋        | 348/2040 [3:32:16<16:41:43, 35.52s/it]08/02/2023 17:58:09 - INFO - __main__ -   Step: 348, LR: 1.709954522486104e-05, Loss: 0.5333575010299683
 17%|█▋        | 349/2040 [3:32:52<16:44:25, 35.64s/it]08/02/2023 17:58:45 - INFO - __main__ -   Step: 349, LR: 1.708943911066195e-05, Loss: 0.5374212265014648
 17%|█▋        | 350/2040 [3:33:28<16:48:47, 35.82s/it]08/02/2023 17:59:21 - INFO - __main__ -   Step: 350, LR: 1.707933299646286e-05, Loss: 0.5422738790512085
 17%|█▋        | 351/2040 [3:34:04<16:50:08, 35.88s/it]08/02/2023 17:59:57 - INFO - __main__ -   Step: 351, LR: 1.7069226882263772e-05, Loss: 0.5048474073410034
 17%|█▋        | 352/2040 [3:34:40<16:47:22, 35.81s/it]08/02/2023 18:00:33 - INFO - __main__ -   Step: 352, LR: 1.705912076806468e-05, Loss: 0.5758603811264038
 17%|█▋        | 353/2040 [3:35:16<16:47:19, 35.83s/it]08/02/2023 18:01:09 - INFO - __main__ -   Step: 353, LR: 1.704901465386559e-05, Loss: 0.5681732296943665
 17%|█▋        | 354/2040 [3:35:51<16:45:16, 35.78s/it]08/02/2023 18:01:44 - INFO - __main__ -   Step: 354, LR: 1.70389085396665e-05, Loss: 0.5627561807632446
 17%|█▋        | 355/2040 [3:36:27<16:44:05, 35.75s/it]08/02/2023 18:02:20 - INFO - __main__ -   Step: 355, LR: 1.702880242546741e-05, Loss: 0.5010818243026733
 17%|█▋        | 356/2040 [3:37:03<16:44:21, 35.78s/it]08/02/2023 18:02:56 - INFO - __main__ -   Step: 356, LR: 1.701869631126832e-05, Loss: 0.5822076797485352
 18%|█▊        | 357/2040 [3:37:38<16:41:34, 35.71s/it]08/02/2023 18:03:32 - INFO - __main__ -   Step: 357, LR: 1.7008590197069226e-05, Loss: 0.6008237600326538
 18%|█▊        | 358/2040 [3:38:14<16:41:48, 35.74s/it]08/02/2023 18:04:07 - INFO - __main__ -   Step: 358, LR: 1.6998484082870135e-05, Loss: 0.5633557438850403
 18%|█▊        | 359/2040 [3:38:49<16:37:52, 35.62s/it]08/02/2023 18:04:43 - INFO - __main__ -   Step: 359, LR: 1.6988377968671048e-05, Loss: 0.4867579936981201
 18%|█▊        | 360/2040 [3:39:25<16:40:34, 35.73s/it]08/02/2023 18:05:19 - INFO - __main__ -   Step: 360, LR: 1.6978271854471957e-05, Loss: 0.6011210680007935
 18%|█▊        | 361/2040 [3:40:01<16:39:54, 35.73s/it]08/02/2023 18:05:54 - INFO - __main__ -   Step: 361, LR: 1.6968165740272866e-05, Loss: 0.5171486139297485
 18%|█▊        | 362/2040 [3:40:37<16:39:09, 35.73s/it]08/02/2023 18:06:30 - INFO - __main__ -   Step: 362, LR: 1.6958059626073775e-05, Loss: 0.45611000061035156
 18%|█▊        | 363/2040 [3:41:13<16:39:33, 35.76s/it]08/02/2023 18:07:06 - INFO - __main__ -   Step: 363, LR: 1.6947953511874688e-05, Loss: 0.4769743084907532
 18%|█▊        | 364/2040 [3:41:49<16:41:45, 35.86s/it]08/02/2023 18:07:42 - INFO - __main__ -   Step: 364, LR: 1.6937847397675597e-05, Loss: 0.52747642993927
 18%|█▊        | 365/2040 [3:42:24<16:37:56, 35.75s/it]08/02/2023 18:08:18 - INFO - __main__ -   Step: 365, LR: 1.6927741283476506e-05, Loss: 0.6330235004425049
 18%|█▊        | 366/2040 [3:43:00<16:38:25, 35.79s/it]08/02/2023 18:08:53 - INFO - __main__ -   Step: 366, LR: 1.6917635169277415e-05, Loss: 0.5510877370834351
 18%|█▊        | 367/2040 [3:43:36<16:34:25, 35.66s/it]08/02/2023 18:09:29 - INFO - __main__ -   Step: 367, LR: 1.6907529055078324e-05, Loss: 0.5981243848800659
 18%|█▊        | 368/2040 [3:44:11<16:33:54, 35.67s/it]08/02/2023 18:10:05 - INFO - __main__ -   Step: 368, LR: 1.6897422940879233e-05, Loss: 0.5246368646621704
 18%|█▊        | 369/2040 [3:44:47<16:34:39, 35.71s/it]08/02/2023 18:10:40 - INFO - __main__ -   Step: 369, LR: 1.6887316826680142e-05, Loss: 0.5511863231658936
 18%|█▊        | 370/2040 [3:45:23<16:37:13, 35.83s/it]08/02/2023 18:11:16 - INFO - __main__ -   Step: 370, LR: 1.687721071248105e-05, Loss: 0.5809783935546875
 18%|█▊        | 371/2040 [3:45:59<16:34:35, 35.76s/it]08/02/2023 18:11:52 - INFO - __main__ -   Step: 371, LR: 1.6867104598281963e-05, Loss: 0.5772632360458374
 18%|█▊        | 372/2040 [3:46:35<16:35:50, 35.82s/it]08/02/2023 18:12:28 - INFO - __main__ -   Step: 372, LR: 1.6856998484082873e-05, Loss: 0.5448481440544128
 18%|█▊        | 373/2040 [3:47:11<16:42:15, 36.07s/it]08/02/2023 18:13:05 - INFO - __main__ -   Step: 373, LR: 1.684689236988378e-05, Loss: 0.5337681770324707
 18%|█▊        | 374/2040 [3:47:47<16:38:26, 35.96s/it]08/02/2023 18:13:40 - INFO - __main__ -   Step: 374, LR: 1.683678625568469e-05, Loss: 0.5589643716812134
 18%|█▊        | 375/2040 [3:48:23<16:36:37, 35.91s/it]08/02/2023 18:14:16 - INFO - __main__ -   Step: 375, LR: 1.68266801414856e-05, Loss: 0.5568692684173584
 18%|█▊        | 376/2040 [3:48:59<16:35:33, 35.90s/it]08/02/2023 18:14:52 - INFO - __main__ -   Step: 376, LR: 1.681657402728651e-05, Loss: 0.631447970867157
 18%|█▊        | 377/2040 [3:49:34<16:33:08, 35.83s/it]08/02/2023 18:15:28 - INFO - __main__ -   Step: 377, LR: 1.6806467913087418e-05, Loss: 0.5333044528961182
 19%|█▊        | 378/2040 [3:50:11<16:36:13, 35.96s/it]08/02/2023 18:16:04 - INFO - __main__ -   Step: 378, LR: 1.679636179888833e-05, Loss: 0.5037937164306641
 19%|█▊        | 379/2040 [3:50:46<16:30:55, 35.79s/it]08/02/2023 18:16:39 - INFO - __main__ -   Step: 379, LR: 1.678625568468924e-05, Loss: 0.6056177616119385
 19%|█▊        | 380/2040 [3:51:23<16:36:06, 36.00s/it]08/02/2023 18:17:16 - INFO - __main__ -   Step: 380, LR: 1.677614957049015e-05, Loss: 0.5446105599403381
 19%|█▊        | 381/2040 [3:52:00<16:44:18, 36.32s/it]08/02/2023 18:17:53 - INFO - __main__ -   Step: 381, LR: 1.6766043456291058e-05, Loss: 0.550117552280426
 19%|█▊        | 382/2040 [3:52:35<16:39:14, 36.16s/it]08/02/2023 18:18:29 - INFO - __main__ -   Step: 382, LR: 1.6755937342091967e-05, Loss: 0.526736855506897
 19%|█▉        | 383/2040 [3:53:12<16:38:30, 36.16s/it]08/02/2023 18:19:05 - INFO - __main__ -   Step: 383, LR: 1.6745831227892876e-05, Loss: 0.524164080619812
 19%|█▉        | 384/2040 [3:53:48<16:38:41, 36.18s/it]08/02/2023 18:19:41 - INFO - __main__ -   Step: 384, LR: 1.6735725113693785e-05, Loss: 0.5504719018936157
 19%|█▉        | 385/2040 [3:54:24<16:35:52, 36.10s/it]08/02/2023 18:20:17 - INFO - __main__ -   Step: 385, LR: 1.6725618999494694e-05, Loss: 0.5477624535560608
 19%|█▉        | 386/2040 [3:54:59<16:31:46, 35.98s/it]08/02/2023 18:20:53 - INFO - __main__ -   Step: 386, LR: 1.6715512885295606e-05, Loss: 0.5239492654800415
 19%|█▉        | 387/2040 [3:55:35<16:29:35, 35.92s/it]08/02/2023 18:21:28 - INFO - __main__ -   Step: 387, LR: 1.6705406771096515e-05, Loss: 0.5516554117202759
 19%|█▉        | 388/2040 [3:56:11<16:26:05, 35.81s/it]08/02/2023 18:22:04 - INFO - __main__ -   Step: 388, LR: 1.6695300656897425e-05, Loss: 0.4851915240287781
 19%|█▉        | 389/2040 [3:56:47<16:26:10, 35.84s/it]08/02/2023 18:22:40 - INFO - __main__ -   Step: 389, LR: 1.6685194542698334e-05, Loss: 0.4934197962284088
 19%|█▉        | 390/2040 [3:57:22<16:19:25, 35.62s/it]08/02/2023 18:23:15 - INFO - __main__ -   Step: 390, LR: 1.6675088428499243e-05, Loss: 0.5035901069641113
 19%|█▉        | 391/2040 [3:57:58<16:23:30, 35.79s/it]08/02/2023 18:23:51 - INFO - __main__ -   Step: 391, LR: 1.6664982314300152e-05, Loss: 0.5215291976928711
 19%|█▉        | 392/2040 [3:58:34<16:21:19, 35.73s/it]08/02/2023 18:24:27 - INFO - __main__ -   Step: 392, LR: 1.665487620010106e-05, Loss: 0.5584509968757629
 19%|█▉        | 393/2040 [3:59:09<16:20:53, 35.73s/it]08/02/2023 18:25:03 - INFO - __main__ -   Step: 393, LR: 1.664477008590197e-05, Loss: 0.5331132411956787
 19%|█▉        | 394/2040 [3:59:46<16:29:03, 36.05s/it]08/02/2023 18:25:39 - INFO - __main__ -   Step: 394, LR: 1.6634663971702882e-05, Loss: 0.5335323810577393
 19%|█▉        | 395/2040 [4:00:22<16:26:37, 35.99s/it]08/02/2023 18:26:15 - INFO - __main__ -   Step: 395, LR: 1.662455785750379e-05, Loss: 0.586656391620636
 19%|█▉        | 396/2040 [4:00:57<16:21:21, 35.82s/it]08/02/2023 18:26:51 - INFO - __main__ -   Step: 396, LR: 1.66144517433047e-05, Loss: 0.4988761246204376
 19%|█▉        | 397/2040 [4:01:32<16:15:09, 35.61s/it]08/02/2023 18:27:26 - INFO - __main__ -   Step: 397, LR: 1.660434562910561e-05, Loss: 0.48630502820014954
 20%|█▉        | 398/2040 [4:02:09<16:19:18, 35.78s/it]08/02/2023 18:28:02 - INFO - __main__ -   Step: 398, LR: 1.6594239514906522e-05, Loss: 0.4808281660079956
 20%|█▉        | 399/2040 [4:02:45<16:21:22, 35.88s/it]08/02/2023 18:28:38 - INFO - __main__ -   Step: 399, LR: 1.658413340070743e-05, Loss: 0.600774347782135
 20%|█▉        | 400/2040 [4:03:21<16:26:36, 36.10s/it]08/02/2023 18:29:15 - INFO - __main__ -   Step: 400, LR: 1.6574027286508337e-05, Loss: 0.5895165205001831
 20%|█▉        | 401/2040 [4:03:57<16:23:58, 36.02s/it]08/02/2023 18:29:50 - INFO - __main__ -   Step: 401, LR: 1.6563921172309246e-05, Loss: 0.6177030801773071
 20%|█▉        | 402/2040 [4:04:33<16:22:28, 35.99s/it]08/02/2023 18:30:26 - INFO - __main__ -   Step: 402, LR: 1.655381505811016e-05, Loss: 0.5974192023277283
 20%|█▉        | 403/2040 [4:05:09<16:24:33, 36.09s/it]08/02/2023 18:31:03 - INFO - __main__ -   Step: 403, LR: 1.6543708943911067e-05, Loss: 0.5773597955703735
 20%|█▉        | 404/2040 [4:05:45<16:22:44, 36.04s/it]08/02/2023 18:31:39 - INFO - __main__ -   Step: 404, LR: 1.6533602829711976e-05, Loss: 0.5302024483680725
 20%|█▉        | 405/2040 [4:06:21<16:21:29, 36.02s/it]08/02/2023 18:32:15 - INFO - __main__ -   Step: 405, LR: 1.6523496715512886e-05, Loss: 0.48305314779281616
 20%|█▉        | 406/2040 [4:06:57<16:22:00, 36.06s/it]08/02/2023 18:32:51 - INFO - __main__ -   Step: 406, LR: 1.6513390601313798e-05, Loss: 0.5191657543182373
 20%|█▉        | 407/2040 [4:07:33<16:15:57, 35.86s/it]08/02/2023 18:33:26 - INFO - __main__ -   Step: 407, LR: 1.6503284487114707e-05, Loss: 0.6009211540222168
 20%|██        | 408/2040 [4:08:09<16:18:01, 35.96s/it]08/02/2023 18:34:02 - INFO - __main__ -   Step: 408, LR: 1.6493178372915616e-05, Loss: 0.584017276763916
 20%|██        | 409/2040 [4:08:45<16:16:31, 35.92s/it]08/02/2023 18:34:38 - INFO - __main__ -   Step: 409, LR: 1.6483072258716525e-05, Loss: 0.5461252331733704
 20%|██        | 410/2040 [4:09:20<16:12:23, 35.79s/it]08/02/2023 18:35:14 - INFO - __main__ -   Step: 410, LR: 1.6472966144517434e-05, Loss: 0.5015692710876465
 20%|██        | 411/2040 [4:09:57<16:17:53, 36.02s/it]08/02/2023 18:35:50 - INFO - __main__ -   Step: 411, LR: 1.6462860030318343e-05, Loss: 0.5838008522987366
 20%|██        | 412/2040 [4:10:33<16:18:36, 36.07s/it]08/02/2023 18:36:26 - INFO - __main__ -   Step: 412, LR: 1.6452753916119252e-05, Loss: 0.5403555035591125
 20%|██        | 413/2040 [4:11:09<16:16:33, 36.01s/it]08/02/2023 18:37:02 - INFO - __main__ -   Step: 413, LR: 1.6442647801920165e-05, Loss: 0.5093239545822144
 20%|██        | 414/2040 [4:11:45<16:13:37, 35.93s/it]08/02/2023 18:37:38 - INFO - __main__ -   Step: 414, LR: 1.6432541687721074e-05, Loss: 0.5114195346832275
 20%|██        | 415/2040 [4:12:20<16:11:21, 35.87s/it]08/02/2023 18:38:14 - INFO - __main__ -   Step: 415, LR: 1.6422435573521983e-05, Loss: 0.5635946989059448
 20%|██        | 416/2040 [4:12:57<16:15:59, 36.06s/it]08/02/2023 18:38:50 - INFO - __main__ -   Step: 416, LR: 1.6412329459322892e-05, Loss: 0.5254143476486206
 20%|██        | 417/2040 [4:13:33<16:11:54, 35.93s/it]08/02/2023 18:39:26 - INFO - __main__ -   Step: 417, LR: 1.64022233451238e-05, Loss: 0.4920138120651245
 20%|██        | 418/2040 [4:14:09<16:13:22, 36.01s/it]08/02/2023 18:40:02 - INFO - __main__ -   Step: 418, LR: 1.639211723092471e-05, Loss: 0.5601463317871094
 21%|██        | 419/2040 [4:14:45<16:15:30, 36.11s/it]08/02/2023 18:40:38 - INFO - __main__ -   Step: 419, LR: 1.638201111672562e-05, Loss: 0.5300294756889343
 21%|██        | 420/2040 [4:15:21<16:13:28, 36.05s/it]08/02/2023 18:41:14 - INFO - __main__ -   Step: 420, LR: 1.637190500252653e-05, Loss: 0.50110924243927
 21%|██        | 421/2040 [4:15:57<16:09:30, 35.93s/it]08/02/2023 18:41:50 - INFO - __main__ -   Step: 421, LR: 1.636179888832744e-05, Loss: 0.561982274055481
 21%|██        | 422/2040 [4:16:33<16:09:42, 35.96s/it]08/02/2023 18:42:26 - INFO - __main__ -   Step: 422, LR: 1.635169277412835e-05, Loss: 0.5158237218856812
 21%|██        | 423/2040 [4:17:09<16:10:00, 35.99s/it]08/02/2023 18:43:02 - INFO - __main__ -   Step: 423, LR: 1.634158665992926e-05, Loss: 0.5453685522079468
 21%|██        | 424/2040 [4:17:45<16:14:33, 36.18s/it]08/02/2023 18:43:39 - INFO - __main__ -   Step: 424, LR: 1.6331480545730168e-05, Loss: 0.5797758102416992
 21%|██        | 425/2040 [4:18:22<16:13:34, 36.17s/it]08/02/2023 18:44:15 - INFO - __main__ -   Step: 425, LR: 1.6321374431531077e-05, Loss: 0.5435242652893066
 21%|██        | 426/2040 [4:18:57<16:11:10, 36.10s/it]08/02/2023 18:44:51 - INFO - __main__ -   Step: 426, LR: 1.6311268317331986e-05, Loss: 0.5388373136520386
 21%|██        | 427/2040 [4:19:33<16:08:11, 36.01s/it]08/02/2023 18:45:27 - INFO - __main__ -   Step: 427, LR: 1.6301162203132895e-05, Loss: 0.5117439031600952
 21%|██        | 428/2040 [4:20:09<16:08:31, 36.05s/it]08/02/2023 18:46:03 - INFO - __main__ -   Step: 428, LR: 1.6291056088933804e-05, Loss: 0.5397491455078125
 21%|██        | 429/2040 [4:20:46<16:08:15, 36.06s/it]08/02/2023 18:46:39 - INFO - __main__ -   Step: 429, LR: 1.6280949974734717e-05, Loss: 0.5429189205169678
 21%|██        | 430/2040 [4:21:21<16:05:34, 35.98s/it]08/02/2023 18:47:15 - INFO - __main__ -   Step: 430, LR: 1.6270843860535626e-05, Loss: 0.5803380608558655
 21%|██        | 431/2040 [4:21:57<16:05:23, 36.00s/it]08/02/2023 18:47:51 - INFO - __main__ -   Step: 431, LR: 1.6260737746336535e-05, Loss: 0.48691070079803467
 21%|██        | 432/2040 [4:22:33<16:02:56, 35.93s/it]08/02/2023 18:48:26 - INFO - __main__ -   Step: 432, LR: 1.6250631632137444e-05, Loss: 0.6396098136901855
 21%|██        | 433/2040 [4:23:09<16:04:10, 36.00s/it]08/02/2023 18:49:03 - INFO - __main__ -   Step: 433, LR: 1.6240525517938353e-05, Loss: 0.5924361944198608
 21%|██▏       | 434/2040 [4:23:46<16:08:17, 36.18s/it]08/02/2023 18:49:39 - INFO - __main__ -   Step: 434, LR: 1.6230419403739262e-05, Loss: 0.5724314451217651
 21%|██▏       | 435/2040 [4:24:22<16:03:29, 36.02s/it]08/02/2023 18:50:15 - INFO - __main__ -   Step: 435, LR: 1.622031328954017e-05, Loss: 0.5101486444473267
 21%|██▏       | 436/2040 [4:24:57<16:01:54, 35.98s/it]08/02/2023 18:50:51 - INFO - __main__ -   Step: 436, LR: 1.621020717534108e-05, Loss: 0.6176261901855469
 21%|██▏       | 437/2040 [4:25:32<15:52:51, 35.67s/it]08/02/2023 18:51:26 - INFO - __main__ -   Step: 437, LR: 1.6200101061141993e-05, Loss: 0.5330592393875122
 21%|██▏       | 438/2040 [4:26:08<15:55:49, 35.80s/it]08/02/2023 18:52:02 - INFO - __main__ -   Step: 438, LR: 1.6189994946942902e-05, Loss: 0.45612210035324097
 22%|██▏       | 439/2040 [4:26:44<15:52:11, 35.68s/it]08/02/2023 18:52:37 - INFO - __main__ -   Step: 439, LR: 1.617988883274381e-05, Loss: 0.5210763216018677
 22%|██▏       | 440/2040 [4:27:20<15:52:45, 35.73s/it]08/02/2023 18:53:13 - INFO - __main__ -   Step: 440, LR: 1.616978271854472e-05, Loss: 0.5031147003173828
 22%|██▏       | 441/2040 [4:27:55<15:51:50, 35.72s/it]08/02/2023 18:53:49 - INFO - __main__ -   Step: 441, LR: 1.6159676604345633e-05, Loss: 0.5376830101013184
 22%|██▏       | 442/2040 [4:28:31<15:53:12, 35.79s/it]08/02/2023 18:54:25 - INFO - __main__ -   Step: 442, LR: 1.614957049014654e-05, Loss: 0.550643265247345
 22%|██▏       | 443/2040 [4:29:07<15:54:13, 35.85s/it]08/02/2023 18:55:01 - INFO - __main__ -   Step: 443, LR: 1.6139464375947447e-05, Loss: 0.48125600814819336
 22%|██▏       | 444/2040 [4:29:43<15:54:05, 35.87s/it]08/02/2023 18:55:37 - INFO - __main__ -   Step: 444, LR: 1.612935826174836e-05, Loss: 0.5376683473587036
 22%|██▏       | 445/2040 [4:30:19<15:51:15, 35.78s/it]08/02/2023 18:56:12 - INFO - __main__ -   Step: 445, LR: 1.611925214754927e-05, Loss: 0.5240195989608765
 22%|██▏       | 446/2040 [4:30:54<15:42:46, 35.49s/it]08/02/2023 18:56:47 - INFO - __main__ -   Step: 446, LR: 1.6109146033350178e-05, Loss: 0.6375395059585571
 22%|██▏       | 447/2040 [4:31:30<15:49:51, 35.78s/it]08/02/2023 18:57:23 - INFO - __main__ -   Step: 447, LR: 1.6099039919151087e-05, Loss: 0.5503056049346924
 22%|██▏       | 448/2040 [4:32:07<15:57:37, 36.09s/it]08/02/2023 18:58:00 - INFO - __main__ -   Step: 448, LR: 1.6088933804952e-05, Loss: 0.5060019493103027
 22%|██▏       | 449/2040 [4:32:43<15:54:44, 36.01s/it]08/02/2023 18:58:36 - INFO - __main__ -   Step: 449, LR: 1.607882769075291e-05, Loss: 0.46040987968444824
 22%|██▏       | 450/2040 [4:33:19<15:53:53, 36.00s/it]08/02/2023 18:59:12 - INFO - __main__ -   Step: 450, LR: 1.6068721576553818e-05, Loss: 0.506894588470459
 22%|██▏       | 451/2040 [4:33:54<15:50:43, 35.90s/it]08/02/2023 18:59:48 - INFO - __main__ -   Step: 451, LR: 1.6058615462354727e-05, Loss: 0.4452749490737915
 22%|██▏       | 452/2040 [4:34:31<15:58:08, 36.20s/it]08/02/2023 19:00:25 - INFO - __main__ -   Step: 452, LR: 1.6048509348155636e-05, Loss: 0.48639604449272156
 22%|██▏       | 453/2040 [4:35:07<15:55:05, 36.11s/it]08/02/2023 19:01:00 - INFO - __main__ -   Step: 453, LR: 1.6038403233956545e-05, Loss: 0.535652756690979
 22%|██▏       | 454/2040 [4:35:43<15:53:51, 36.09s/it]08/02/2023 19:01:37 - INFO - __main__ -   Step: 454, LR: 1.6028297119757454e-05, Loss: 0.5040499567985535
 22%|██▏       | 455/2040 [4:36:19<15:48:01, 35.89s/it]08/02/2023 19:02:12 - INFO - __main__ -   Step: 455, LR: 1.6018191005558363e-05, Loss: 0.4977625012397766
 22%|██▏       | 456/2040 [4:36:55<15:49:52, 35.98s/it]08/02/2023 19:02:48 - INFO - __main__ -   Step: 456, LR: 1.6008084891359275e-05, Loss: 0.561920166015625
 22%|██▏       | 457/2040 [4:37:31<15:51:05, 36.05s/it]08/02/2023 19:03:24 - INFO - __main__ -   Step: 457, LR: 1.5997978777160184e-05, Loss: 0.5117143392562866
 22%|██▏       | 458/2040 [4:38:08<16:00:18, 36.42s/it]08/02/2023 19:04:02 - INFO - __main__ -   Step: 458, LR: 1.5987872662961094e-05, Loss: 0.5322094559669495
 22%|██▎       | 459/2040 [4:38:44<15:57:26, 36.34s/it]08/02/2023 19:04:38 - INFO - __main__ -   Step: 459, LR: 1.5977766548762003e-05, Loss: 0.5688965320587158
 23%|██▎       | 460/2040 [4:39:21<15:55:54, 36.30s/it]08/02/2023 19:05:14 - INFO - __main__ -   Step: 460, LR: 1.5967660434562912e-05, Loss: 0.5677710175514221
 23%|██▎       | 461/2040 [4:39:57<15:56:53, 36.36s/it]08/02/2023 19:05:50 - INFO - __main__ -   Step: 461, LR: 1.595755432036382e-05, Loss: 0.5497771501541138
 23%|██▎       | 462/2040 [4:40:33<15:51:06, 36.16s/it]08/02/2023 19:06:26 - INFO - __main__ -   Step: 462, LR: 1.594744820616473e-05, Loss: 0.5352270603179932
 23%|██▎       | 463/2040 [4:41:09<15:50:10, 36.15s/it]08/02/2023 19:07:02 - INFO - __main__ -   Step: 463, LR: 1.593734209196564e-05, Loss: 0.5032039880752563
 23%|██▎       | 464/2040 [4:41:45<15:49:40, 36.16s/it]08/02/2023 19:07:38 - INFO - __main__ -   Step: 464, LR: 1.592723597776655e-05, Loss: 0.5099266767501831
 23%|██▎       | 465/2040 [4:42:21<15:46:05, 36.04s/it]08/02/2023 19:08:14 - INFO - __main__ -   Step: 465, LR: 1.591712986356746e-05, Loss: 0.48107975721359253
 23%|██▎       | 466/2040 [4:42:57<15:43:21, 35.96s/it]08/02/2023 19:08:50 - INFO - __main__ -   Step: 466, LR: 1.590702374936837e-05, Loss: 0.5026458501815796
 23%|██▎       | 467/2040 [4:43:33<15:42:35, 35.95s/it]08/02/2023 19:09:26 - INFO - __main__ -   Step: 467, LR: 1.589691763516928e-05, Loss: 0.5339796543121338
 23%|██▎       | 468/2040 [4:44:09<15:44:53, 36.06s/it]08/02/2023 19:10:02 - INFO - __main__ -   Step: 468, LR: 1.5886811520970188e-05, Loss: 0.5580930709838867
 23%|██▎       | 469/2040 [4:44:45<15:43:46, 36.04s/it]08/02/2023 19:10:38 - INFO - __main__ -   Step: 469, LR: 1.5876705406771097e-05, Loss: 0.5301614999771118
 23%|██▎       | 470/2040 [4:45:21<15:43:50, 36.07s/it]08/02/2023 19:11:14 - INFO - __main__ -   Step: 470, LR: 1.5866599292572006e-05, Loss: 0.5197610855102539
 23%|██▎       | 471/2040 [4:45:57<15:44:32, 36.12s/it]08/02/2023 19:11:51 - INFO - __main__ -   Step: 471, LR: 1.5856493178372915e-05, Loss: 0.5210745334625244
 23%|██▎       | 472/2040 [4:46:35<15:54:15, 36.51s/it]08/02/2023 19:12:28 - INFO - __main__ -   Step: 472, LR: 1.5846387064173827e-05, Loss: 0.5476281642913818
 23%|██▎       | 473/2040 [4:47:11<15:49:34, 36.36s/it]08/02/2023 19:13:04 - INFO - __main__ -   Step: 473, LR: 1.5836280949974736e-05, Loss: 0.5310810804367065
 23%|██▎       | 474/2040 [4:47:47<15:44:58, 36.21s/it]08/02/2023 19:13:40 - INFO - __main__ -   Step: 474, LR: 1.5826174835775646e-05, Loss: 0.5201725959777832
 23%|██▎       | 475/2040 [4:48:23<15:42:06, 36.12s/it]08/02/2023 19:14:16 - INFO - __main__ -   Step: 475, LR: 1.5816068721576555e-05, Loss: 0.47992846369743347
 23%|██▎       | 476/2040 [4:48:59<15:44:34, 36.24s/it]08/02/2023 19:14:52 - INFO - __main__ -   Step: 476, LR: 1.5805962607377464e-05, Loss: 0.4876185357570648
 23%|██▎       | 477/2040 [4:49:35<15:44:14, 36.25s/it]08/02/2023 19:15:29 - INFO - __main__ -   Step: 477, LR: 1.5795856493178373e-05, Loss: 0.5273634791374207
 23%|██▎       | 478/2040 [4:50:11<15:42:29, 36.20s/it]08/02/2023 19:16:05 - INFO - __main__ -   Step: 478, LR: 1.5785750378979282e-05, Loss: 0.5340812802314758
 23%|██▎       | 479/2040 [4:50:48<15:41:51, 36.20s/it]08/02/2023 19:16:41 - INFO - __main__ -   Step: 479, LR: 1.5775644264780194e-05, Loss: 0.5216848254203796
 24%|██▎       | 480/2040 [4:51:24<15:43:25, 36.29s/it]08/02/2023 19:17:17 - INFO - __main__ -   Step: 480, LR: 1.5765538150581103e-05, Loss: 0.504016637802124
 24%|██▎       | 481/2040 [4:52:00<15:40:18, 36.19s/it]08/02/2023 19:17:53 - INFO - __main__ -   Step: 481, LR: 1.5755432036382012e-05, Loss: 0.5237142443656921
 24%|██▎       | 482/2040 [4:52:36<15:41:01, 36.24s/it]08/02/2023 19:18:30 - INFO - __main__ -   Step: 482, LR: 1.574532592218292e-05, Loss: 0.4313637316226959
 24%|██▎       | 483/2040 [4:53:13<15:42:02, 36.30s/it]08/02/2023 19:19:06 - INFO - __main__ -   Step: 483, LR: 1.5735219807983834e-05, Loss: 0.5406831502914429
 24%|██▎       | 484/2040 [4:53:49<15:42:34, 36.35s/it]08/02/2023 19:19:43 - INFO - __main__ -   Step: 484, LR: 1.5725113693784743e-05, Loss: 0.5804468989372253
 24%|██▍       | 485/2040 [4:54:25<15:40:19, 36.28s/it]08/02/2023 19:20:19 - INFO - __main__ -   Step: 485, LR: 1.5715007579585652e-05, Loss: 0.5216877460479736
 24%|██▍       | 486/2040 [4:55:01<15:37:19, 36.19s/it]08/02/2023 19:20:55 - INFO - __main__ -   Step: 486, LR: 1.5704901465386558e-05, Loss: 0.4669714570045471
 24%|██▍       | 487/2040 [4:55:37<15:31:38, 35.99s/it]08/02/2023 19:21:30 - INFO - __main__ -   Step: 487, LR: 1.569479535118747e-05, Loss: 0.534604549407959
 24%|██▍       | 488/2040 [4:56:14<15:35:49, 36.18s/it]08/02/2023 19:22:07 - INFO - __main__ -   Step: 488, LR: 1.568468923698838e-05, Loss: 0.5380333662033081
 24%|██▍       | 489/2040 [4:56:50<15:39:08, 36.33s/it]08/02/2023 19:22:44 - INFO - __main__ -   Step: 489, LR: 1.567458312278929e-05, Loss: 0.5329833030700684
 24%|██▍       | 490/2040 [4:57:27<15:38:16, 36.32s/it]08/02/2023 19:23:20 - INFO - __main__ -   Step: 490, LR: 1.5664477008590197e-05, Loss: 0.5180879831314087
 24%|██▍       | 491/2040 [4:58:04<15:42:46, 36.52s/it]08/02/2023 19:23:57 - INFO - __main__ -   Step: 491, LR: 1.565437089439111e-05, Loss: 0.5674787759780884
 24%|██▍       | 492/2040 [4:58:40<15:38:46, 36.39s/it]08/02/2023 19:24:33 - INFO - __main__ -   Step: 492, LR: 1.564426478019202e-05, Loss: 0.5241912603378296
 24%|██▍       | 493/2040 [4:59:16<15:38:18, 36.39s/it]08/02/2023 19:25:09 - INFO - __main__ -   Step: 493, LR: 1.5634158665992928e-05, Loss: 0.5460104942321777
 24%|██▍       | 494/2040 [4:59:53<15:40:00, 36.48s/it]08/02/2023 19:25:46 - INFO - __main__ -   Step: 494, LR: 1.5624052551793837e-05, Loss: 0.5625299215316772
 24%|██▍       | 495/2040 [5:00:29<15:40:23, 36.52s/it]08/02/2023 19:26:23 - INFO - __main__ -   Step: 495, LR: 1.5613946437594746e-05, Loss: 0.5745311975479126
 24%|██▍       | 496/2040 [5:01:07<15:45:26, 36.74s/it]08/02/2023 19:27:00 - INFO - __main__ -   Step: 496, LR: 1.5603840323395655e-05, Loss: 0.5370246767997742
 24%|██▍       | 497/2040 [5:01:43<15:43:14, 36.68s/it]08/02/2023 19:27:36 - INFO - __main__ -   Step: 497, LR: 1.5593734209196564e-05, Loss: 0.5488090515136719
 24%|██▍       | 498/2040 [5:02:20<15:44:31, 36.75s/it]08/02/2023 19:28:13 - INFO - __main__ -   Step: 498, LR: 1.5583628094997473e-05, Loss: 0.4782477021217346
 24%|██▍       | 499/2040 [5:02:57<15:44:20, 36.77s/it]08/02/2023 19:28:50 - INFO - __main__ -   Step: 499, LR: 1.5573521980798386e-05, Loss: 0.5523321032524109
 25%|██▍       | 500/2040 [5:03:34<15:44:23, 36.79s/it]08/02/2023 19:29:27 - INFO - __main__ -   Step: 500, LR: 1.5563415866599295e-05, Loss: 0.5015342235565186
 25%|██▍       | 501/2040 [5:04:10<15:42:50, 36.76s/it]08/02/2023 19:30:04 - INFO - __main__ -   Step: 501, LR: 1.5553309752400204e-05, Loss: 0.5675680637359619
 25%|██▍       | 502/2040 [5:04:47<15:40:56, 36.71s/it]08/02/2023 19:30:40 - INFO - __main__ -   Step: 502, LR: 1.5543203638201113e-05, Loss: 0.5461674332618713
 25%|██▍       | 503/2040 [5:05:24<15:44:32, 36.87s/it]08/02/2023 19:31:18 - INFO - __main__ -   Step: 503, LR: 1.5533097524002022e-05, Loss: 0.48959052562713623
 25%|██▍       | 504/2040 [5:06:01<15:42:52, 36.83s/it]08/02/2023 19:31:54 - INFO - __main__ -   Step: 504, LR: 1.552299140980293e-05, Loss: 0.5108311176300049
 25%|██▍       | 505/2040 [5:06:38<15:43:08, 36.87s/it]08/02/2023 19:32:31 - INFO - __main__ -   Step: 505, LR: 1.551288529560384e-05, Loss: 0.5043842196464539
 25%|██▍       | 506/2040 [5:07:15<15:41:11, 36.81s/it]08/02/2023 19:33:08 - INFO - __main__ -   Step: 506, LR: 1.550277918140475e-05, Loss: 0.485515296459198
 25%|██▍       | 507/2040 [5:07:52<15:43:54, 36.94s/it]08/02/2023 19:33:45 - INFO - __main__ -   Step: 507, LR: 1.5492673067205662e-05, Loss: 0.5632295608520508
 25%|██▍       | 508/2040 [5:08:29<15:41:15, 36.86s/it]08/02/2023 19:34:22 - INFO - __main__ -   Step: 508, LR: 1.548256695300657e-05, Loss: 0.490814745426178
 25%|██▍       | 509/2040 [5:09:05<15:38:45, 36.79s/it]08/02/2023 19:34:58 - INFO - __main__ -   Step: 509, LR: 1.547246083880748e-05, Loss: 0.49708321690559387
 25%|██▌       | 510/2040 [5:09:42<15:37:56, 36.78s/it]08/02/2023 19:35:35 - INFO - __main__ -   Step: 510, LR: 1.546235472460839e-05, Loss: 0.5666601657867432
 25%|██▌       | 511/2040 [5:10:18<15:31:57, 36.57s/it]08/02/2023 19:36:11 - INFO - __main__ -   Step: 511, LR: 1.5452248610409298e-05, Loss: 0.47070756554603577
 25%|██▌       | 512/2040 [5:10:55<15:31:06, 36.56s/it]08/02/2023 19:36:48 - INFO - __main__ -   Step: 512, LR: 1.5442142496210207e-05, Loss: 0.49215713143348694
 25%|██▌       | 513/2040 [5:11:32<15:33:56, 36.70s/it]08/02/2023 19:37:25 - INFO - __main__ -   Step: 513, LR: 1.5432036382011116e-05, Loss: 0.4975471794605255
 25%|██▌       | 514/2040 [5:12:08<15:33:18, 36.70s/it]08/02/2023 19:38:02 - INFO - __main__ -   Step: 514, LR: 1.542193026781203e-05, Loss: 0.5596926212310791
 25%|██▌       | 515/2040 [5:12:45<15:32:43, 36.70s/it]08/02/2023 19:38:38 - INFO - __main__ -   Step: 515, LR: 1.5411824153612938e-05, Loss: 0.5518606901168823
 25%|██▌       | 516/2040 [5:13:22<15:33:02, 36.73s/it]08/02/2023 19:39:15 - INFO - __main__ -   Step: 516, LR: 1.5401718039413847e-05, Loss: 0.49814367294311523
 25%|██▌       | 517/2040 [5:13:59<15:32:48, 36.75s/it]08/02/2023 19:39:52 - INFO - __main__ -   Step: 517, LR: 1.5391611925214756e-05, Loss: 0.524078369140625
 25%|██▌       | 518/2040 [5:14:35<15:33:28, 36.80s/it]08/02/2023 19:40:29 - INFO - __main__ -   Step: 518, LR: 1.5381505811015665e-05, Loss: 0.5269439220428467
 25%|██▌       | 519/2040 [5:15:12<15:31:42, 36.75s/it]08/02/2023 19:41:05 - INFO - __main__ -   Step: 519, LR: 1.5371399696816574e-05, Loss: 0.464188814163208
 25%|██▌       | 520/2040 [5:15:50<15:40:53, 37.14s/it]08/02/2023 19:41:43 - INFO - __main__ -   Step: 520, LR: 1.5361293582617483e-05, Loss: 0.5274795889854431
 26%|██▌       | 521/2040 [5:16:27<15:38:12, 37.06s/it]08/02/2023 19:42:20 - INFO - __main__ -   Step: 521, LR: 1.5351187468418392e-05, Loss: 0.5547748804092407
 26%|██▌       | 522/2040 [5:17:04<15:34:44, 36.95s/it]08/02/2023 19:42:57 - INFO - __main__ -   Step: 522, LR: 1.5341081354219305e-05, Loss: 0.4798017144203186
 26%|██▌       | 523/2040 [5:17:41<15:38:31, 37.12s/it]08/02/2023 19:43:35 - INFO - __main__ -   Step: 523, LR: 1.5330975240020214e-05, Loss: 0.5369144678115845
 26%|██▌       | 524/2040 [5:18:19<15:40:09, 37.21s/it]08/02/2023 19:44:12 - INFO - __main__ -   Step: 524, LR: 1.5320869125821123e-05, Loss: 0.5867041945457458
 26%|██▌       | 525/2040 [5:18:56<15:42:04, 37.31s/it]08/02/2023 19:44:49 - INFO - __main__ -   Step: 525, LR: 1.5310763011622032e-05, Loss: 0.5191028714179993
 26%|██▌       | 526/2040 [5:19:35<15:52:52, 37.76s/it]08/02/2023 19:45:28 - INFO - __main__ -   Step: 526, LR: 1.5300656897422944e-05, Loss: 0.5206924676895142
 26%|██▌       | 527/2040 [5:20:13<15:51:24, 37.73s/it]08/02/2023 19:46:06 - INFO - __main__ -   Step: 527, LR: 1.5290550783223854e-05, Loss: 0.513966977596283
 26%|██▌       | 528/2040 [5:20:49<15:40:55, 37.34s/it]08/02/2023 19:46:42 - INFO - __main__ -   Step: 528, LR: 1.5280444669024763e-05, Loss: 0.5425279140472412
 26%|██▌       | 529/2040 [5:21:26<15:36:06, 37.17s/it]08/02/2023 19:47:19 - INFO - __main__ -   Step: 529, LR: 1.5270338554825668e-05, Loss: 0.5252037644386292
 26%|██▌       | 530/2040 [5:22:03<15:32:15, 37.04s/it]08/02/2023 19:47:56 - INFO - __main__ -   Step: 530, LR: 1.526023244062658e-05, Loss: 0.5217541456222534
 26%|██▌       | 531/2040 [5:22:39<15:27:25, 36.88s/it]08/02/2023 19:48:32 - INFO - __main__ -   Step: 531, LR: 1.525012632642749e-05, Loss: 0.5494065284729004
 26%|██▌       | 532/2040 [5:23:16<15:24:44, 36.79s/it]08/02/2023 19:49:09 - INFO - __main__ -   Step: 532, LR: 1.5240020212228399e-05, Loss: 0.5075395703315735
 26%|██▌       | 533/2040 [5:23:52<15:23:21, 36.76s/it]08/02/2023 19:49:46 - INFO - __main__ -   Step: 533, LR: 1.5229914098029308e-05, Loss: 0.5269129872322083
 26%|██▌       | 534/2040 [5:24:29<15:21:16, 36.70s/it]08/02/2023 19:50:22 - INFO - __main__ -   Step: 534, LR: 1.5219807983830219e-05, Loss: 0.5071173906326294
 26%|██▌       | 535/2040 [5:25:06<15:23:41, 36.83s/it]08/02/2023 19:50:59 - INFO - __main__ -   Step: 535, LR: 1.5209701869631128e-05, Loss: 0.5324844121932983
 26%|██▋       | 536/2040 [5:25:43<15:22:11, 36.79s/it]08/02/2023 19:51:36 - INFO - __main__ -   Step: 536, LR: 1.5199595755432037e-05, Loss: 0.5036787986755371
 26%|██▋       | 537/2040 [5:26:19<15:20:18, 36.74s/it]08/02/2023 19:52:13 - INFO - __main__ -   Step: 537, LR: 1.5189489641232946e-05, Loss: 0.6159147620201111
 26%|██▋       | 538/2040 [5:26:57<15:22:59, 36.87s/it]08/02/2023 19:52:50 - INFO - __main__ -   Step: 538, LR: 1.5179383527033857e-05, Loss: 0.564384400844574
 26%|██▋       | 539/2040 [5:27:33<15:20:05, 36.78s/it]08/02/2023 19:53:26 - INFO - __main__ -   Step: 539, LR: 1.5169277412834766e-05, Loss: 0.5861126184463501
 26%|██▋       | 540/2040 [5:28:09<15:15:40, 36.63s/it]08/02/2023 19:54:03 - INFO - __main__ -   Step: 540, LR: 1.5159171298635675e-05, Loss: 0.46601036190986633
 27%|██▋       | 541/2040 [5:28:46<15:11:17, 36.48s/it]08/02/2023 19:54:39 - INFO - __main__ -   Step: 541, LR: 1.5149065184436584e-05, Loss: 0.508349597454071
 27%|██▋       | 542/2040 [5:29:22<15:07:11, 36.34s/it]08/02/2023 19:55:15 - INFO - __main__ -   Step: 542, LR: 1.5138959070237496e-05, Loss: 0.5724393129348755
 27%|██▋       | 543/2040 [5:29:58<15:09:57, 36.47s/it]08/02/2023 19:55:52 - INFO - __main__ -   Step: 543, LR: 1.5128852956038404e-05, Loss: 0.501434326171875
 27%|██▋       | 544/2040 [5:30:35<15:11:04, 36.54s/it]08/02/2023 19:56:28 - INFO - __main__ -   Step: 544, LR: 1.5118746841839313e-05, Loss: 0.5878075361251831
 27%|██▋       | 545/2040 [5:31:13<15:19:09, 36.89s/it]08/02/2023 19:57:06 - INFO - __main__ -   Step: 545, LR: 1.5108640727640222e-05, Loss: 0.4943011403083801
 27%|██▋       | 546/2040 [5:31:50<15:20:29, 36.97s/it]08/02/2023 19:57:43 - INFO - __main__ -   Step: 546, LR: 1.5098534613441134e-05, Loss: 0.6401739120483398
 27%|██▋       | 547/2040 [5:32:26<15:17:17, 36.86s/it]08/02/2023 19:58:20 - INFO - __main__ -   Step: 547, LR: 1.5088428499242043e-05, Loss: 0.6488962173461914
 27%|██▋       | 548/2040 [5:33:03<15:13:46, 36.75s/it]08/02/2023 19:58:56 - INFO - __main__ -   Step: 548, LR: 1.507832238504295e-05, Loss: 0.6019582748413086
 27%|██▋       | 549/2040 [5:33:40<15:15:41, 36.85s/it]08/02/2023 19:59:33 - INFO - __main__ -   Step: 549, LR: 1.5068216270843863e-05, Loss: 0.5627715587615967
 27%|██▋       | 550/2040 [5:34:17<15:14:33, 36.83s/it]08/02/2023 20:00:10 - INFO - __main__ -   Step: 550, LR: 1.5058110156644772e-05, Loss: 0.576216459274292
 27%|██▋       | 551/2040 [5:34:53<15:09:20, 36.64s/it]08/02/2023 20:00:46 - INFO - __main__ -   Step: 551, LR: 1.5048004042445681e-05, Loss: 0.5108535289764404
 27%|██▋       | 552/2040 [5:35:29<15:02:36, 36.40s/it]08/02/2023 20:01:22 - INFO - __main__ -   Step: 552, LR: 1.503789792824659e-05, Loss: 0.5034937858581543
 27%|██▋       | 553/2040 [5:36:05<15:02:14, 36.40s/it]08/02/2023 20:01:59 - INFO - __main__ -   Step: 553, LR: 1.5027791814047501e-05, Loss: 0.5244028568267822
 27%|██▋       | 554/2040 [5:36:42<15:03:46, 36.49s/it]08/02/2023 20:02:35 - INFO - __main__ -   Step: 554, LR: 1.501768569984841e-05, Loss: 0.5218879580497742
 27%|██▋       | 555/2040 [5:37:19<15:04:20, 36.54s/it]08/02/2023 20:03:12 - INFO - __main__ -   Step: 555, LR: 1.500757958564932e-05, Loss: 0.5055592060089111
 27%|██▋       | 556/2040 [5:37:55<15:03:32, 36.53s/it]08/02/2023 20:03:48 - INFO - __main__ -   Step: 556, LR: 1.4997473471450228e-05, Loss: 0.5997515320777893
 27%|██▋       | 557/2040 [5:38:31<15:00:42, 36.44s/it]08/02/2023 20:04:25 - INFO - __main__ -   Step: 557, LR: 1.498736735725114e-05, Loss: 0.49296706914901733
 27%|██▋       | 558/2040 [5:39:08<15:01:26, 36.50s/it]08/02/2023 20:05:01 - INFO - __main__ -   Step: 558, LR: 1.4977261243052048e-05, Loss: 0.4633411169052124
 27%|██▋       | 559/2040 [5:39:45<15:03:03, 36.59s/it]08/02/2023 20:05:38 - INFO - __main__ -   Step: 559, LR: 1.4967155128852957e-05, Loss: 0.5405983924865723
 27%|██▋       | 560/2040 [5:40:23<15:11:54, 36.97s/it]08/02/2023 20:06:16 - INFO - __main__ -   Step: 560, LR: 1.4957049014653866e-05, Loss: 0.5145421028137207
 28%|██▊       | 561/2040 [5:40:59<15:05:07, 36.72s/it]08/02/2023 20:06:52 - INFO - __main__ -   Step: 561, LR: 1.4946942900454777e-05, Loss: 0.5167992115020752
 28%|██▊       | 562/2040 [5:41:35<15:01:17, 36.59s/it]08/02/2023 20:07:28 - INFO - __main__ -   Step: 562, LR: 1.4936836786255686e-05, Loss: 0.6230171322822571
 28%|██▊       | 563/2040 [5:42:12<15:01:00, 36.60s/it]08/02/2023 20:08:05 - INFO - __main__ -   Step: 563, LR: 1.4926730672056595e-05, Loss: 0.5561270713806152
 28%|██▊       | 564/2040 [5:42:48<15:00:18, 36.60s/it]08/02/2023 20:08:42 - INFO - __main__ -   Step: 564, LR: 1.4916624557857504e-05, Loss: 0.5095922946929932
 28%|██▊       | 565/2040 [5:43:25<14:58:08, 36.53s/it]08/02/2023 20:09:18 - INFO - __main__ -   Step: 565, LR: 1.4906518443658415e-05, Loss: 0.5488067865371704
 28%|██▊       | 566/2040 [5:44:02<15:00:09, 36.64s/it]08/02/2023 20:09:55 - INFO - __main__ -   Step: 566, LR: 1.4896412329459324e-05, Loss: 0.5014281868934631
 28%|██▊       | 567/2040 [5:44:38<14:58:31, 36.60s/it]08/02/2023 20:10:31 - INFO - __main__ -   Step: 567, LR: 1.4886306215260233e-05, Loss: 0.5173662900924683
 28%|██▊       | 568/2040 [5:45:14<14:56:05, 36.53s/it]08/02/2023 20:11:08 - INFO - __main__ -   Step: 568, LR: 1.4876200101061142e-05, Loss: 0.5127018690109253
 28%|██▊       | 569/2040 [5:45:50<14:51:17, 36.35s/it]08/02/2023 20:11:44 - INFO - __main__ -   Step: 569, LR: 1.4866093986862053e-05, Loss: 0.5458431243896484
 28%|██▊       | 570/2040 [5:46:28<14:56:33, 36.59s/it]08/02/2023 20:12:21 - INFO - __main__ -   Step: 570, LR: 1.4855987872662962e-05, Loss: 0.47558340430259705
 28%|██▊       | 571/2040 [5:47:04<14:54:25, 36.53s/it]08/02/2023 20:12:57 - INFO - __main__ -   Step: 571, LR: 1.4845881758463871e-05, Loss: 0.4913886785507202
 28%|██▊       | 572/2040 [5:47:40<14:50:58, 36.42s/it]08/02/2023 20:13:33 - INFO - __main__ -   Step: 572, LR: 1.483577564426478e-05, Loss: 0.5123259425163269
 28%|██▊       | 573/2040 [5:48:16<14:46:58, 36.28s/it]08/02/2023 20:14:09 - INFO - __main__ -   Step: 573, LR: 1.4825669530065691e-05, Loss: 0.47740185260772705
 28%|██▊       | 574/2040 [5:48:52<14:44:04, 36.18s/it]08/02/2023 20:14:45 - INFO - __main__ -   Step: 574, LR: 1.48155634158666e-05, Loss: 0.4912581443786621
 28%|██▊       | 575/2040 [5:49:28<14:45:28, 36.27s/it]08/02/2023 20:15:22 - INFO - __main__ -   Step: 575, LR: 1.480545730166751e-05, Loss: 0.5275480151176453
 28%|██▊       | 576/2040 [5:50:04<14:41:07, 36.11s/it]08/02/2023 20:15:58 - INFO - __main__ -   Step: 576, LR: 1.4795351187468418e-05, Loss: 0.5509892106056213
 28%|██▊       | 577/2040 [5:50:40<14:40:11, 36.10s/it]08/02/2023 20:16:34 - INFO - __main__ -   Step: 577, LR: 1.478524507326933e-05, Loss: 0.5117501616477966
 28%|██▊       | 578/2040 [5:51:16<14:39:01, 36.07s/it]08/02/2023 20:17:10 - INFO - __main__ -   Step: 578, LR: 1.4775138959070238e-05, Loss: 0.5822256803512573
 28%|██▊       | 579/2040 [5:51:53<14:44:19, 36.32s/it]08/02/2023 20:17:46 - INFO - __main__ -   Step: 579, LR: 1.4765032844871147e-05, Loss: 0.5402004718780518
 28%|██▊       | 580/2040 [5:52:30<14:46:48, 36.44s/it]08/02/2023 20:18:23 - INFO - __main__ -   Step: 580, LR: 1.4754926730672056e-05, Loss: 0.5915746688842773
 28%|██▊       | 581/2040 [5:53:06<14:43:29, 36.33s/it]08/02/2023 20:18:59 - INFO - __main__ -   Step: 581, LR: 1.4744820616472967e-05, Loss: 0.5352896451950073
 29%|██▊       | 582/2040 [5:53:42<14:41:59, 36.30s/it]08/02/2023 20:19:36 - INFO - __main__ -   Step: 582, LR: 1.4734714502273876e-05, Loss: 0.4764244854450226
 29%|██▊       | 583/2040 [5:54:19<14:44:36, 36.43s/it]08/02/2023 20:20:12 - INFO - __main__ -   Step: 583, LR: 1.4724608388074785e-05, Loss: 0.49570804834365845
 29%|██▊       | 584/2040 [5:54:56<14:48:42, 36.62s/it]08/02/2023 20:20:49 - INFO - __main__ -   Step: 584, LR: 1.4714502273875698e-05, Loss: 0.535094141960144
 29%|██▊       | 585/2040 [5:55:33<14:52:16, 36.79s/it]08/02/2023 20:21:27 - INFO - __main__ -   Step: 585, LR: 1.4704396159676607e-05, Loss: 0.5020885467529297
 29%|██▊       | 586/2040 [5:56:10<14:54:42, 36.92s/it]08/02/2023 20:22:04 - INFO - __main__ -   Step: 586, LR: 1.4694290045477514e-05, Loss: 0.5704808235168457
 29%|██▉       | 587/2040 [5:56:47<14:52:22, 36.85s/it]08/02/2023 20:22:40 - INFO - __main__ -   Step: 587, LR: 1.4684183931278423e-05, Loss: 0.5809012651443481
 29%|██▉       | 588/2040 [5:57:24<14:55:41, 37.01s/it]08/02/2023 20:23:18 - INFO - __main__ -   Step: 588, LR: 1.4674077817079336e-05, Loss: 0.4893917143344879
 29%|██▉       | 589/2040 [5:58:02<14:55:09, 37.02s/it]08/02/2023 20:23:55 - INFO - __main__ -   Step: 589, LR: 1.4663971702880245e-05, Loss: 0.4996805191040039
 29%|██▉       | 590/2040 [5:58:39<14:57:03, 37.12s/it]08/02/2023 20:24:32 - INFO - __main__ -   Step: 590, LR: 1.4653865588681154e-05, Loss: 0.4791443347930908
 29%|██▉       | 591/2040 [5:59:15<14:52:04, 36.94s/it]08/02/2023 20:25:09 - INFO - __main__ -   Step: 591, LR: 1.4643759474482061e-05, Loss: 0.5269147157669067
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3186565 closing signal SIGHUP
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3186566 closing signal SIGHUP
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3186567 closing signal SIGHUP
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3186568 closing signal SIGHUP
Traceback (most recent call last):
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 45, in main
    args.func(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 964, in launch_command
    deepspeed_launcher(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 687, in deepspeed_launcher
    distrib_run.run(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 241, in launch_agent
    result = agent.run()
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 723, in run
    result = self._invoke_run(role)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    time.sleep(monitor_interval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3186320 got signal: 1
