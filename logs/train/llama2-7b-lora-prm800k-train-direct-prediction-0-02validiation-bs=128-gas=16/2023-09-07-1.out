nohup: ignoring input
[2023-09-07 05:44:56,041] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-07 05:44:57,345] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-07 05:44:57,346] [INFO] [runner.py:555:main] cmd = /data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMywgNF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py --model_name_or_path /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9 --data_path /data/users/zhangjunlei/tyx/reward-by-prm800k/datasets/prm800k-train-direct-prediction-0-02validiation-encoded-datasets --output_dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/llama2-7b-lora-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16 --model_max_length 1024 --per_device_train_batch_size 2 --per_device_eval_batch_size 4 --gradient_accumulation_steps 16 --num_train_epochs 100 --evaluation_strategy steps --eval_steps 100 --save_strategy steps --save_steps 100 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --lora_r 256 --lora_alpha 256 --lora_dropout 0.05 --lora_target_modules all_linear --q_lora True --bf16 True --tf32 True --use_accelerate_lib flash-attn-v2 --logging_strategy steps --logging_steps 1
[2023-09-07 05:44:58,798] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-07 05:45:00,219] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 3, 4]}
[2023-09-07 05:45:00,219] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-09-07 05:45:00,219] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2023-09-07 05:45:00,219] [INFO] [launch.py:163:main] dist_world_size=4
[2023-09-07 05:45:00,219] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,3,4
[2023-09-07 05:45:02,208] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-07 05:45:02,246] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-07 05:45:02,272] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-07 05:45:02,272] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.31s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]
[DEBUG] os.environ['LOCAL_RANK'] = 0
[DEBUG] lora_args.lora_target_modules = ['all_linear']
[DEBUG] os.environ['LOCAL_RANK'] = 2
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.06s/it]
[DEBUG] os.environ['LOCAL_RANK'] = 1
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]
[DEBUG] os.environ['LOCAL_RANK'] = 3
trainable params: 639,631,360 || all params: 4,140,044,288 || trainable%: 15.449867573977025
wandb: Currently logged in as: kidrain61. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.15.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.9
wandb: Run data is saved locally in /data/users/zhangjunlei/tyx/wandb/wandb/run-20230907_054556-jq6n8xng
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2-7b-lora-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16-2023-09-07-1
wandb: ⭐️ View project at https://wandb.ai/kidrain61/step-reward
wandb: 🚀 View run at https://wandb.ai/kidrain61/step-reward/runs/jq6n8xng
  0%|          | 0/66500 [00:00<?, ?it/s][W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/66500 [00:06<127:42:25,  6.91s/it]                                                     {'loss': 18.7352, 'learning_rate': 1.0025062656641605e-08, 'epoch': 0.0}
  0%|          | 1/66500 [00:06<127:42:25,  6.91s/it]  0%|          | 2/66500 [00:13<121:31:14,  6.58s/it]                                                     {'loss': 18.3574, 'learning_rate': 2.005012531328321e-08, 'epoch': 0.0}
  0%|          | 2/66500 [00:13<121:31:14,  6.58s/it]  0%|          | 3/66500 [00:19<120:17:12,  6.51s/it]                                                     {'loss': 18.8399, 'learning_rate': 3.0075187969924815e-08, 'epoch': 0.0}
  0%|          | 3/66500 [00:19<120:17:12,  6.51s/it]  0%|          | 4/66500 [00:25<116:14:54,  6.29s/it]                                                     {'loss': 18.4501, 'learning_rate': 4.010025062656642e-08, 'epoch': 0.01}
  0%|          | 4/66500 [00:25<116:14:54,  6.29s/it]  0%|          | 5/66500 [00:32<117:04:07,  6.34s/it]                                                     {'loss': 18.7088, 'learning_rate': 5.0125313283208025e-08, 'epoch': 0.01}
  0%|          | 5/66500 [00:32<117:04:07,  6.34s/it]  0%|          | 6/66500 [00:38<115:54:41,  6.28s/it]                                                     {'loss': 18.7364, 'learning_rate': 6.015037593984963e-08, 'epoch': 0.01}
  0%|          | 6/66500 [00:38<115:54:41,  6.28s/it]  0%|          | 7/66500 [00:44<115:46:58,  6.27s/it]                                                     {'loss': 18.7131, 'learning_rate': 7.017543859649123e-08, 'epoch': 0.01}
  0%|          | 7/66500 [00:44<115:46:58,  6.27s/it]  0%|          | 8/66500 [00:50<113:33:06,  6.15s/it]                                                     {'loss': 18.6523, 'learning_rate': 8.020050125313284e-08, 'epoch': 0.01}
  0%|          | 8/66500 [00:50<113:33:06,  6.15s/it]  0%|          | 9/66500 [00:56<116:16:47,  6.30s/it]                                                     {'loss': 18.8946, 'learning_rate': 9.022556390977444e-08, 'epoch': 0.01}
  0%|          | 9/66500 [00:57<116:16:47,  6.30s/it]  0%|          | 10/66500 [01:03<115:16:02,  6.24s/it]                                                      {'loss': 18.7815, 'learning_rate': 1.0025062656641605e-07, 'epoch': 0.02}
  0%|          | 10/66500 [01:03<115:16:02,  6.24s/it]  0%|          | 11/66500 [01:09<115:38:50,  6.26s/it]                                                      {'loss': 18.7006, 'learning_rate': 1.1027568922305765e-07, 'epoch': 0.02}
  0%|          | 11/66500 [01:09<115:38:50,  6.26s/it]  0%|          | 12/66500 [01:15<115:37:37,  6.26s/it]                                                      {'loss': 18.5575, 'learning_rate': 1.2030075187969926e-07, 'epoch': 0.02}
  0%|          | 12/66500 [01:15<115:37:37,  6.26s/it]  0%|          | 13/66500 [01:22<116:32:26,  6.31s/it]                                                      {'loss': 18.639, 'learning_rate': 1.3032581453634085e-07, 'epoch': 0.02}
  0%|          | 13/66500 [01:22<116:32:26,  6.31s/it]  0%|          | 14/66500 [01:28<116:40:09,  6.32s/it]                                                      {'loss': 18.8123, 'learning_rate': 1.4035087719298247e-07, 'epoch': 0.02}
  0%|          | 14/66500 [01:28<116:40:09,  6.32s/it]  0%|          | 15/66500 [01:34<117:07:49,  6.34s/it]                                                      {'loss': 18.6297, 'learning_rate': 1.5037593984962406e-07, 'epoch': 0.02}
  0%|          | 15/66500 [01:34<117:07:49,  6.34s/it]  0%|          | 16/66500 [01:41<116:32:21,  6.31s/it]                                                      {'loss': 18.8104, 'learning_rate': 1.6040100250626568e-07, 'epoch': 0.02}
  0%|          | 16/66500 [01:41<116:32:21,  6.31s/it]  0%|          | 17/66500 [01:47<116:25:25,  6.30s/it]                                                      {'loss': 18.7513, 'learning_rate': 1.704260651629073e-07, 'epoch': 0.03}
  0%|          | 17/66500 [01:47<116:25:25,  6.30s/it]  0%|          | 18/66500 [01:53<117:39:45,  6.37s/it]                                                      {'loss': 18.785, 'learning_rate': 1.804511278195489e-07, 'epoch': 0.03}
  0%|          | 18/66500 [01:53<117:39:45,  6.37s/it]  0%|          | 19/66500 [02:00<121:03:59,  6.56s/it]                                                      {'loss': 19.0536, 'learning_rate': 1.904761904761905e-07, 'epoch': 0.03}
  0%|          | 19/66500 [02:00<121:03:59,  6.56s/it]  0%|          | 20/66500 [02:07<119:28:53,  6.47s/it]                                                      {'loss': 18.503, 'learning_rate': 2.005012531328321e-07, 'epoch': 0.03}
  0%|          | 20/66500 [02:07<119:28:53,  6.47s/it]  0%|          | 21/66500 [02:13<119:45:57,  6.49s/it]                                                      {'loss': 19.0702, 'learning_rate': 2.105263157894737e-07, 'epoch': 0.03}
  0%|          | 21/66500 [02:13<119:45:57,  6.49s/it]  0%|          | 22/66500 [02:19<118:00:43,  6.39s/it]                                                      {'loss': 18.6783, 'learning_rate': 2.205513784461153e-07, 'epoch': 0.03}
  0%|          | 22/66500 [02:19<118:00:43,  6.39s/it]  0%|          | 23/66500 [02:26<119:05:28,  6.45s/it]                                                      {'loss': 18.7181, 'learning_rate': 2.3057644110275693e-07, 'epoch': 0.03}
  0%|          | 23/66500 [02:26<119:05:28,  6.45s/it]  0%|          | 24/66500 [02:32<118:53:35,  6.44s/it]                                                      {'loss': 18.526, 'learning_rate': 2.406015037593985e-07, 'epoch': 0.04}
  0%|          | 24/66500 [02:32<118:53:35,  6.44s/it]  0%|          | 25/66500 [02:39<118:56:12,  6.44s/it]                                                      {'loss': 19.0005, 'learning_rate': 2.506265664160401e-07, 'epoch': 0.04}
  0%|          | 25/66500 [02:39<118:56:12,  6.44s/it]  0%|          | 26/66500 [02:45<118:44:19,  6.43s/it]                                                      {'loss': 18.8686, 'learning_rate': 2.606516290726817e-07, 'epoch': 0.04}
  0%|          | 26/66500 [02:45<118:44:19,  6.43s/it]  0%|          | 27/66500 [02:52<120:09:53,  6.51s/it]                                                      {'loss': 18.698, 'learning_rate': 2.706766917293233e-07, 'epoch': 0.04}
  0%|          | 27/66500 [02:52<120:09:53,  6.51s/it]  0%|          | 28/66500 [02:58<119:05:39,  6.45s/it]                                                      {'loss': 18.6504, 'learning_rate': 2.8070175438596494e-07, 'epoch': 0.04}
  0%|          | 28/66500 [02:58<119:05:39,  6.45s/it]  0%|          | 29/66500 [03:04<117:20:23,  6.36s/it]                                                      {'loss': 18.7415, 'learning_rate': 2.9072681704260656e-07, 'epoch': 0.04}
  0%|          | 29/66500 [03:04<117:20:23,  6.36s/it]  0%|          | 30/66500 [03:11<116:36:57,  6.32s/it]                                                      {'loss': 18.7525, 'learning_rate': 3.007518796992481e-07, 'epoch': 0.05}
  0%|          | 30/66500 [03:11<116:36:57,  6.32s/it]  0%|          | 31/66500 [03:17<115:41:38,  6.27s/it]                                                      {'loss': 18.5515, 'learning_rate': 3.1077694235588974e-07, 'epoch': 0.05}
  0%|          | 31/66500 [03:17<115:41:38,  6.27s/it]  0%|          | 32/66500 [03:23<114:17:56,  6.19s/it]                                                      {'loss': 18.7138, 'learning_rate': 3.2080200501253136e-07, 'epoch': 0.05}
  0%|          | 32/66500 [03:23<114:17:56,  6.19s/it]  0%|          | 33/66500 [03:29<116:10:50,  6.29s/it]                                                      {'loss': 18.9575, 'learning_rate': 3.308270676691729e-07, 'epoch': 0.05}
  0%|          | 33/66500 [03:29<116:10:50,  6.29s/it]  0%|          | 34/66500 [03:35<115:27:00,  6.25s/it]                                                      {'loss': 18.6932, 'learning_rate': 3.408521303258146e-07, 'epoch': 0.05}
  0%|          | 34/66500 [03:35<115:27:00,  6.25s/it]  0%|          | 35/66500 [03:42<115:35:34,  6.26s/it]                                                      {'loss': 18.3857, 'learning_rate': 3.5087719298245616e-07, 'epoch': 0.05}
  0%|          | 35/66500 [03:42<115:35:34,  6.26s/it]  0%|          | 36/66500 [03:48<115:26:22,  6.25s/it]                                                      {'loss': 18.5655, 'learning_rate': 3.609022556390978e-07, 'epoch': 0.05}
  0%|          | 36/66500 [03:48<115:26:22,  6.25s/it]  0%|          | 37/66500 [03:54<113:34:00,  6.15s/it]                                                      {'loss': 18.1561, 'learning_rate': 3.709273182957394e-07, 'epoch': 0.06}
  0%|          | 37/66500 [03:54<113:34:00,  6.15s/it]  0%|          | 38/66500 [04:00<116:23:34,  6.30s/it]                                                      {'loss': 18.6335, 'learning_rate': 3.80952380952381e-07, 'epoch': 0.06}
  0%|          | 38/66500 [04:01<116:23:34,  6.30s/it]  0%|          | 39/66500 [04:07<118:25:53,  6.42s/it]                                                      {'loss': 18.8984, 'learning_rate': 3.909774436090226e-07, 'epoch': 0.06}
  0%|          | 39/66500 [04:07<118:25:53,  6.42s/it]  0%|          | 40/66500 [04:14<122:05:02,  6.61s/it]                                                      {'loss': 18.7414, 'learning_rate': 4.010025062656642e-07, 'epoch': 0.06}
  0%|          | 40/66500 [04:14<122:05:02,  6.61s/it]  0%|          | 41/66500 [04:20<119:16:18,  6.46s/it]                                                      {'loss': 18.4119, 'learning_rate': 4.110275689223058e-07, 'epoch': 0.06}
  0%|          | 41/66500 [04:20<119:16:18,  6.46s/it]  0%|          | 42/66500 [04:26<117:26:27,  6.36s/it]                                                      {'loss': 18.3886, 'learning_rate': 4.210526315789474e-07, 'epoch': 0.06}
  0%|          | 42/66500 [04:27<117:26:27,  6.36s/it]  0%|          | 43/66500 [04:33<117:38:31,  6.37s/it]                                                      {'loss': 18.5755, 'learning_rate': 4.3107769423558905e-07, 'epoch': 0.06}
  0%|          | 43/66500 [04:33<117:38:31,  6.37s/it]  0%|          | 44/66500 [04:39<116:59:54,  6.34s/it]                                                      {'loss': 18.5466, 'learning_rate': 4.411027568922306e-07, 'epoch': 0.07}
  0%|          | 44/66500 [04:39<116:59:54,  6.34s/it]  0%|          | 45/66500 [04:45<116:40:13,  6.32s/it]                                                      {'loss': 18.4487, 'learning_rate': 4.511278195488722e-07, 'epoch': 0.07}
  0%|          | 45/66500 [04:45<116:40:13,  6.32s/it]  0%|          | 46/66500 [04:52<118:35:06,  6.42s/it]                                                      {'loss': 18.3134, 'learning_rate': 4.6115288220551385e-07, 'epoch': 0.07}
  0%|          | 46/66500 [04:52<118:35:06,  6.42s/it]  0%|          | 47/66500 [04:59<118:39:00,  6.43s/it]                                                      {'loss': 18.3021, 'learning_rate': 4.711779448621554e-07, 'epoch': 0.07}
  0%|          | 47/66500 [04:59<118:39:00,  6.43s/it]  0%|          | 48/66500 [05:05<117:09:16,  6.35s/it]                                                      {'loss': 18.037, 'learning_rate': 4.81203007518797e-07, 'epoch': 0.07}
  0%|          | 48/66500 [05:05<117:09:16,  6.35s/it]  0%|          | 49/66500 [05:11<118:51:51,  6.44s/it]                                                      {'loss': 18.5201, 'learning_rate': 4.912280701754387e-07, 'epoch': 0.07}
  0%|          | 49/66500 [05:11<118:51:51,  6.44s/it]  0%|          | 50/66500 [05:17<116:18:46,  6.30s/it]                                                      {'loss': 18.2254, 'learning_rate': 5.012531328320802e-07, 'epoch': 0.08}
  0%|          | 50/66500 [05:17<116:18:46,  6.30s/it]  0%|          | 51/66500 [05:24<116:21:14,  6.30s/it]                                                      {'loss': 18.2053, 'learning_rate': 5.112781954887219e-07, 'epoch': 0.08}
  0%|          | 51/66500 [05:24<116:21:14,  6.30s/it]  0%|          | 52/66500 [05:31<119:44:03,  6.49s/it]                                                      {'loss': 18.4057, 'learning_rate': 5.213032581453634e-07, 'epoch': 0.08}
  0%|          | 52/66500 [05:31<119:44:03,  6.49s/it]  0%|          | 53/66500 [05:37<118:11:38,  6.40s/it]                                                      {'loss': 18.2097, 'learning_rate': 5.313283208020051e-07, 'epoch': 0.08}
  0%|          | 53/66500 [05:37<118:11:38,  6.40s/it]  0%|          | 54/66500 [05:43<117:45:11,  6.38s/it]                                                      {'loss': 18.1122, 'learning_rate': 5.413533834586466e-07, 'epoch': 0.08}
  0%|          | 54/66500 [05:43<117:45:11,  6.38s/it]  0%|          | 55/66500 [05:49<114:40:23,  6.21s/it]                                                      {'loss': 17.9415, 'learning_rate': 5.513784461152883e-07, 'epoch': 0.08}
  0%|          | 55/66500 [05:49<114:40:23,  6.21s/it]  0%|          | 56/66500 [05:55<116:06:25,  6.29s/it]                                                      {'loss': 18.1772, 'learning_rate': 5.614035087719299e-07, 'epoch': 0.08}
  0%|          | 56/66500 [05:55<116:06:25,  6.29s/it]  0%|          | 57/66500 [06:02<115:46:03,  6.27s/it]                                                      {'loss': 18.2301, 'learning_rate': 5.714285714285715e-07, 'epoch': 0.09}
  0%|          | 57/66500 [06:02<115:46:03,  6.27s/it]  0%|          | 58/66500 [06:08<114:14:35,  6.19s/it]                                                      {'loss': 17.9019, 'learning_rate': 5.814536340852131e-07, 'epoch': 0.09}
  0%|          | 58/66500 [06:08<114:14:35,  6.19s/it]  0%|          | 59/66500 [06:14<115:51:11,  6.28s/it]                                                      {'loss': 17.8935, 'learning_rate': 5.914786967418547e-07, 'epoch': 0.09}
  0%|          | 59/66500 [06:14<115:51:11,  6.28s/it]  0%|          | 60/66500 [06:21<117:57:07,  6.39s/it]                                                      {'loss': 18.1981, 'learning_rate': 6.015037593984962e-07, 'epoch': 0.09}
  0%|          | 60/66500 [06:21<117:57:07,  6.39s/it]  0%|          | 61/66500 [06:27<116:39:50,  6.32s/it]                                                      {'loss': 17.6569, 'learning_rate': 6.115288220551379e-07, 'epoch': 0.09}
  0%|          | 61/66500 [06:27<116:39:50,  6.32s/it]  0%|          | 62/66500 [06:34<119:41:22,  6.49s/it]                                                      {'loss': 18.1562, 'learning_rate': 6.215538847117795e-07, 'epoch': 0.09}
  0%|          | 62/66500 [06:34<119:41:22,  6.49s/it]  0%|          | 63/66500 [06:40<117:12:04,  6.35s/it]                                                      {'loss': 17.4432, 'learning_rate': 6.315789473684211e-07, 'epoch': 0.09}
  0%|          | 63/66500 [06:40<117:12:04,  6.35s/it]  0%|          | 64/66500 [06:46<117:03:38,  6.34s/it]                                                      {'loss': 17.7312, 'learning_rate': 6.416040100250627e-07, 'epoch': 0.1}
  0%|          | 64/66500 [06:46<117:03:38,  6.34s/it]  0%|          | 65/66500 [06:52<115:23:12,  6.25s/it]                                                      {'loss': 17.6903, 'learning_rate': 6.516290726817042e-07, 'epoch': 0.1}
  0%|          | 65/66500 [06:52<115:23:12,  6.25s/it]  0%|          | 66/66500 [06:59<116:41:31,  6.32s/it]                                                      {'loss': 17.6095, 'learning_rate': 6.616541353383458e-07, 'epoch': 0.1}
  0%|          | 66/66500 [06:59<116:41:31,  6.32s/it]  0%|          | 67/66500 [07:05<115:57:46,  6.28s/it]                                                      {'loss': 17.5341, 'learning_rate': 6.716791979949876e-07, 'epoch': 0.1}
  0%|          | 67/66500 [07:05<115:57:46,  6.28s/it]  0%|          | 68/66500 [07:11<116:59:33,  6.34s/it]                                                      {'loss': 17.4148, 'learning_rate': 6.817042606516292e-07, 'epoch': 0.1}
  0%|          | 68/66500 [07:11<116:59:33,  6.34s/it]  0%|          | 69/66500 [07:18<116:12:45,  6.30s/it]                                                      {'loss': 17.158, 'learning_rate': 6.917293233082707e-07, 'epoch': 0.1}
  0%|          | 69/66500 [07:18<116:12:45,  6.30s/it]  0%|          | 70/66500 [07:24<117:57:15,  6.39s/it]                                                      {'loss': 17.1986, 'learning_rate': 7.017543859649123e-07, 'epoch': 0.11}
  0%|          | 70/66500 [07:24<117:57:15,  6.39s/it]  0%|          | 71/66500 [07:30<116:42:52,  6.33s/it]                                                      {'loss': 17.1908, 'learning_rate': 7.117794486215538e-07, 'epoch': 0.11}
  0%|          | 71/66500 [07:30<116:42:52,  6.33s/it]  0%|          | 72/66500 [07:36<115:50:26,  6.28s/it]                                                      {'loss': 17.0523, 'learning_rate': 7.218045112781956e-07, 'epoch': 0.11}
  0%|          | 72/66500 [07:36<115:50:26,  6.28s/it]  0%|          | 73/66500 [07:43<115:46:02,  6.27s/it]                                                      {'loss': 17.1253, 'learning_rate': 7.318295739348372e-07, 'epoch': 0.11}
  0%|          | 73/66500 [07:43<115:46:02,  6.27s/it]  0%|          | 74/66500 [07:49<118:27:31,  6.42s/it]                                                      {'loss': 16.8168, 'learning_rate': 7.418546365914788e-07, 'epoch': 0.11}
  0%|          | 74/66500 [07:50<118:27:31,  6.42s/it]  0%|          | 75/66500 [07:56<120:14:19,  6.52s/it]                                                      {'loss': 16.6301, 'learning_rate': 7.518796992481203e-07, 'epoch': 0.11}
  0%|          | 75/66500 [07:56<120:14:19,  6.52s/it]  0%|          | 76/66500 [08:02<117:32:18,  6.37s/it]                                                      {'loss': 16.6458, 'learning_rate': 7.61904761904762e-07, 'epoch': 0.11}
  0%|          | 76/66500 [08:02<117:32:18,  6.37s/it]  0%|          | 77/66500 [08:09<117:58:32,  6.39s/it]                                                      {'loss': 16.886, 'learning_rate': 7.719298245614036e-07, 'epoch': 0.12}
  0%|          | 77/66500 [08:09<117:58:32,  6.39s/it]  0%|          | 78/66500 [08:15<116:55:20,  6.34s/it]                                                      {'loss': 16.5452, 'learning_rate': 7.819548872180452e-07, 'epoch': 0.12}
  0%|          | 78/66500 [08:15<116:55:20,  6.34s/it]  0%|          | 79/66500 [08:21<118:20:32,  6.41s/it]                                                      {'loss': 16.3462, 'learning_rate': 7.919799498746868e-07, 'epoch': 0.12}
  0%|          | 79/66500 [08:22<118:20:32,  6.41s/it]  0%|          | 80/66500 [08:28<117:56:29,  6.39s/it]                                                      {'loss': 16.1041, 'learning_rate': 8.020050125313284e-07, 'epoch': 0.12}
  0%|          | 80/66500 [08:28<117:56:29,  6.39s/it]  0%|          | 81/66500 [08:34<117:54:49,  6.39s/it]                                                      {'loss': 16.1475, 'learning_rate': 8.1203007518797e-07, 'epoch': 0.12}
  0%|          | 81/66500 [08:34<117:54:49,  6.39s/it]  0%|          | 82/66500 [08:40<116:33:17,  6.32s/it]                                                      {'loss': 16.0004, 'learning_rate': 8.220551378446116e-07, 'epoch': 0.12}
  0%|          | 82/66500 [08:40<116:33:17,  6.32s/it]  0%|          | 83/66500 [08:47<116:53:53,  6.34s/it]                                                      {'loss': 15.5953, 'learning_rate': 8.320802005012532e-07, 'epoch': 0.12}
  0%|          | 83/66500 [08:47<116:53:53,  6.34s/it]  0%|          | 84/66500 [08:53<117:41:25,  6.38s/it]                                                      {'loss': 15.6409, 'learning_rate': 8.421052631578948e-07, 'epoch': 0.13}
  0%|          | 84/66500 [08:53<117:41:25,  6.38s/it]  0%|          | 85/66500 [08:59<115:34:42,  6.26s/it]                                                      {'loss': 15.4123, 'learning_rate': 8.521303258145364e-07, 'epoch': 0.13}
  0%|          | 85/66500 [08:59<115:34:42,  6.26s/it]  0%|          | 86/66500 [09:05<115:30:21,  6.26s/it]                                                      {'loss': 15.227, 'learning_rate': 8.621553884711781e-07, 'epoch': 0.13}
  0%|          | 86/66500 [09:06<115:30:21,  6.26s/it]  0%|          | 87/66500 [09:12<116:48:27,  6.33s/it]                                                      {'loss': 15.2641, 'learning_rate': 8.721804511278196e-07, 'epoch': 0.13}
  0%|          | 87/66500 [09:12<116:48:27,  6.33s/it]  0%|          | 88/66500 [09:19<118:06:39,  6.40s/it]                                                      {'loss': 15.181, 'learning_rate': 8.822055137844612e-07, 'epoch': 0.13}
  0%|          | 88/66500 [09:19<118:06:39,  6.40s/it]  0%|          | 89/66500 [09:25<118:16:00,  6.41s/it]                                                      {'loss': 14.6405, 'learning_rate': 8.922305764411029e-07, 'epoch': 0.13}
  0%|          | 89/66500 [09:25<118:16:00,  6.41s/it]  0%|          | 90/66500 [09:31<116:32:13,  6.32s/it]                                                      {'loss': 14.4542, 'learning_rate': 9.022556390977444e-07, 'epoch': 0.14}
  0%|          | 90/66500 [09:31<116:32:13,  6.32s/it]  0%|          | 91/66500 [09:37<116:00:11,  6.29s/it]                                                      {'loss': 14.3496, 'learning_rate': 9.122807017543861e-07, 'epoch': 0.14}
  0%|          | 91/66500 [09:37<116:00:11,  6.29s/it]  0%|          | 92/66500 [09:43<114:09:30,  6.19s/it]                                                      {'loss': 14.1474, 'learning_rate': 9.223057644110277e-07, 'epoch': 0.14}
  0%|          | 92/66500 [09:43<114:09:30,  6.19s/it]  0%|          | 93/66500 [09:49<113:25:35,  6.15s/it]                                                      {'loss': 14.0423, 'learning_rate': 9.323308270676692e-07, 'epoch': 0.14}
  0%|          | 93/66500 [09:49<113:25:35,  6.15s/it]  0%|          | 94/66500 [09:56<115:17:35,  6.25s/it]                                                      {'loss': 13.7944, 'learning_rate': 9.423558897243108e-07, 'epoch': 0.14}
  0%|          | 94/66500 [09:56<115:17:35,  6.25s/it]  0%|          | 95/66500 [10:02<115:32:17,  6.26s/it]                                                      {'loss': 13.3164, 'learning_rate': 9.523809523809525e-07, 'epoch': 0.14}
  0%|          | 95/66500 [10:02<115:32:17,  6.26s/it]  0%|          | 96/66500 [10:08<115:23:04,  6.26s/it]                                                      {'loss': 13.2152, 'learning_rate': 9.62406015037594e-07, 'epoch': 0.14}
  0%|          | 96/66500 [10:08<115:23:04,  6.26s/it]  0%|          | 97/66500 [10:15<115:06:28,  6.24s/it]                                                      {'loss': 12.8422, 'learning_rate': 9.724310776942358e-07, 'epoch': 0.15}
  0%|          | 97/66500 [10:15<115:06:28,  6.24s/it]  0%|          | 98/66500 [10:21<115:22:10,  6.25s/it]                                                      {'loss': 12.6732, 'learning_rate': 9.824561403508773e-07, 'epoch': 0.15}
  0%|          | 98/66500 [10:21<115:22:10,  6.25s/it]  0%|          | 99/66500 [10:28<118:56:09,  6.45s/it]                                                      {'loss': 12.5125, 'learning_rate': 9.924812030075188e-07, 'epoch': 0.15}
  0%|          | 99/66500 [10:28<118:56:09,  6.45s/it]  0%|          | 100/66500 [10:34<119:10:41,  6.46s/it]                                                       {'loss': 12.1743, 'learning_rate': 1.0025062656641603e-06, 'epoch': 0.15}
  0%|          | 100/66500 [10:34<119:10:41,  6.46s/it]Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 256, in <module>
    train()
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 232, in train
    trainer.train()
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1901, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2226, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2934, in evaluate
    output = eval_loop(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 3147, in evaluation_loop
    logits = self.accelerator.gather_for_metrics((logits))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 2012, in gather_for_metrics
    tensor = self.gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 1985, in gather
    return gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 289, in gather
    return _gpu_gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 269, in _gpu_gather
    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 130, in recursively_apply
    raise TypeError(
TypeError: Unsupported types (<class 'transformers.trainer_utils.EvalPrediction'>) passed to `_gpu_gather_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 256, in <module>
    train()
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 232, in train
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 256, in <module>
    trainer.train()
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    train()
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 232, in train
    trainer.train()
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1901, in _inner_training_loop
    return inner_training_loop(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1901, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2226, in _maybe_log_save_evaluate
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2226, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2934, in evaluate
Traceback (most recent call last):
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2934, in evaluate
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 256, in <module>
    train()
    output = eval_loop(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 3147, in evaluation_loop
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 232, in train
    trainer.train()
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
      File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1901, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
output = eval_loop(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 3147, in evaluation_loop
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2226, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2934, in evaluate
    output = eval_loop(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 3147, in evaluation_loop
    logits = self.accelerator.gather_for_metrics((logits))
    logits = self.accelerator.gather_for_metrics((logits))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 2012, in gather_for_metrics
    tensor = self.gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 2012, in gather_for_metrics
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 1985, in gather
    return gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 289, in gather
    return _gpu_gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 269, in _gpu_gather
    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
      File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 130, in recursively_apply
    raise TypeError(
logits = self.accelerator.gather_for_metrics((logits))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 2012, in gather_for_metrics
TypeError: Unsupported types (<class 'transformers.trainer_utils.EvalPrediction'>) passed to `_gpu_gather_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
    tensor = self.gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 1985, in gather
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 256, in <module>
        tensor = self.gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 1985, in gather
train()
  File "/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py", line 232, in train
    return gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 289, in gather
    return _gpu_gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 269, in _gpu_gather
    trainer.train()    
return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 130, in recursively_apply
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 289, in gather
    raise TypeError(
TypeError: Unsupported types (<class 'transformers.trainer_utils.EvalPrediction'>) passed to `_gpu_gather_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
    return _gpu_gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 269, in _gpu_gather
    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 130, in recursively_apply
    raise TypeError(
TypeError: Unsupported types (<class 'transformers.trainer_utils.EvalPrediction'>) passed to `_gpu_gather_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
    return inner_training_loop(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 1901, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2226, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 2934, in evaluate
    output = eval_loop(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py", line 3147, in evaluation_loop
    logits = self.accelerator.gather_for_metrics((logits))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 2012, in gather_for_metrics
    tensor = self.gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 1985, in gather
    return gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 289, in gather
    return _gpu_gather(tensor)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 269, in _gpu_gather
    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py", line 130, in recursively_apply
    raise TypeError(
TypeError: Unsupported types (<class 'transformers.trainer_utils.EvalPrediction'>) passed to `_gpu_gather_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.016 MB of 0.016 MB uploaded (0.000 MB deduped)[2023-09-07 05:56:43,950] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2210804
Exception ignored in atexit callback: <function _Manager._atexit_setup.<locals>.<lambda> at 0x7fb3082f7be0>
Traceback (most recent call last):
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 165, in <lambda>
    self._atexit_lambda = lambda: self._atexit_teardown()
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 174, in _atexit_teardown
    self._teardown(exit_code)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 185, in _teardown
    result = self._service.join()
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/service/service.py", line 241, in join
    ret = self._internal_proc.wait()
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/ray/_private/worker.py", line 1723, in sigterm_handler
    sys.exit(signum)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/lib/exit_hooks.py", line 36, in exit
    self._orig_exit(orig_code)  # type: ignore
SystemExit: 15
[2023-09-07 05:56:47,048] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2210805
[2023-09-07 05:56:47,073] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2210806
[2023-09-07 05:56:47,096] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2210807
[2023-09-07 05:56:47,096] [ERROR] [launch.py:321:sigkill_handler] ['/data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/python', '-u', '/data/users/zhangjunlei/tyx/FastChat/fastchat/train/train_all_linear_modules_with_qlora.py', '--local_rank=3', '--model_name_or_path', '/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9', '--data_path', '/data/users/zhangjunlei/tyx/reward-by-prm800k/datasets/prm800k-train-direct-prediction-0-02validiation-encoded-datasets', '--output_dir', '/data/users/zhangjunlei/tyx/reward-by-prm800k/models/llama2-7b-lora-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16', '--model_max_length', '1024', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '16', '--num_train_epochs', '100', '--evaluation_strategy', 'steps', '--eval_steps', '100', '--save_strategy', 'steps', '--save_steps', '100', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--lora_r', '256', '--lora_alpha', '256', '--lora_dropout', '0.05', '--lora_target_modules', 'all_linear', '--q_lora', 'True', '--bf16', 'True', '--tf32', 'True', '--use_accelerate_lib', 'flash-attn-v2', '--logging_strategy', 'steps', '--logging_steps', '1'] exits with return code = 1
