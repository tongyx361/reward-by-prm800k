nohup: ignoring input
[2023-08-25 22:07:31,641] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-25 22:07:37,057] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-25 22:07:37,089] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-25 22:07:37,145] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-25 22:07:37,149] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 1022, in <module>
    main()
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 489, in main
    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/llama_flash_attn_monkey_patch.py", line 11, in <module>
    from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
ImportError: cannot import name 'flash_attn_unpadded_qkvpacked_func' from 'flash_attn.flash_attn_interface' (/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py)
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 1022, in <module>
    main()
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 489, in main
    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/llama_flash_attn_monkey_patch.py", line 11, in <module>
    from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
ImportError: cannot import name 'flash_attn_unpadded_qkvpacked_func' from 'flash_attn.flash_attn_interface' (/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py)
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 1022, in <module>
    Traceback (most recent call last):
main()
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 1022, in <module>
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 489, in main
    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/llama_flash_attn_monkey_patch.py", line 11, in <module>
    from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
ImportError: cannot import name 'flash_attn_unpadded_qkvpacked_func' from 'flash_attn.flash_attn_interface' (/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py)
    main()
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py", line 489, in main
    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/llama_flash_attn_monkey_patch.py", line 11, in <module>
    from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
ImportError: cannot import name 'flash_attn_unpadded_qkvpacked_func' from 'flash_attn.flash_attn_interface' (/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2756318) of binary: /data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/python
Traceback (most recent call last):
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 45, in main
    args.func(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 964, in launch_command
    deepspeed_launcher(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 687, in deepspeed_launcher
    distrib_run.run(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/data/users/zhangjunlei/tyx/reward-by-prm800k/open-instruct/open_instruct/finetune.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-08-25_22:07:38
  host      : a100
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2756319)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-08-25_22:07:38
  host      : a100
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2756320)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-08-25_22:07:38
  host      : a100
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2756321)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_22:07:38
  host      : a100
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2756318)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
