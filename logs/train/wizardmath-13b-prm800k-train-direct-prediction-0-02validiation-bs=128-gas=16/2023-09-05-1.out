nohup: ignoring input
[2023-09-05 13:59:50,745] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-09-05 13:59:55,407] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-05 13:59:55,493] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-05 13:59:55,565] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-05 13:59:55,708] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using xformers
Initializing accelerator...
[2023-09-05 13:59:56,636] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-05 13:59:56,636] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-05 13:59:56,636] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using xformers
Initializing accelerator...
[2023-09-05 13:59:56,802] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-05 13:59:56,802] [INFO] [comm.py:616:init_distributed] cdb=None
Using xformers
Initializing accelerator...
[2023-09-05 13:59:56,930] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-05 13:59:56,930] [INFO] [comm.py:616:init_distributed] cdb=None
Using xformers
Initializing accelerator...
[2023-09-05 13:59:57,006] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-05 13:59:57,006] [INFO] [comm.py:616:init_distributed] cdb=None
09/05/2023 13:59:57 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

accelerator.state = Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

09/05/2023 13:59:57 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

09/05/2023 13:59:57 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

09/05/2023 13:59:57 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

loading configuration file /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-13B-V1.0/snapshots/7ef412d2c680ef0fbdcd88d0df31b396d8d3049c/config.json
Model config LlamaConfig {
  "_name_or_path": "/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-13B-V1.0/snapshots/7ef412d2c680ef0fbdcd88d0df31b396d8d3049c",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": false,
  "vocab_size": 32001
}

loading file tokenizer.model
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
loading weights file /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-13B-V1.0/snapshots/7ef412d2c680ef0fbdcd88d0df31b396d8d3049c/pytorch_model.bin.index.json
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.31.0",
  "use_cache": false
}

[2023-09-05 14:00:48,273] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 13.02B parameters
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:41,  8.28s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:41,  8.40s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:42,  8.42s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:10, 14.16s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:15<00:31,  7.85s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:15<00:31,  7.84s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:15<00:31,  7.87s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:23<00:23,  7.94s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:23<00:23,  7.93s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:23<00:23,  7.94s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:24<00:47, 11.78s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:31<00:15,  7.86s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:31<00:15,  7.86s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:31<00:15,  7.89s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:36<00:36, 12.15s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:39<00:07,  7.87s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:39<00:07,  7.86s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:39<00:07,  7.87s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:41<00:00,  5.89s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:41<00:00,  6.93s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:41<00:00,  5.89s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:41<00:00,  6.93s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:41<00:00,  5.91s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:41<00:00,  6.94s/it]
09/05/2023 14:01:31 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
09/05/2023 14:01:31 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
09/05/2023 14:01:31 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
Loading checkpoint shards:  67%|██████▋   | 4/6 [00:46<00:22, 11.22s/it]09/05/2023 14:01:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:2 (world_size=4, worker_count=3, timeout=0:30:00)
09/05/2023 14:01:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:2 (world_size=4, worker_count=3, timeout=0:30:00)
09/05/2023 14:01:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:2 (world_size=4, worker_count=3, timeout=0:30:00)
Loading checkpoint shards:  83%|████████▎ | 5/6 [00:58<00:11, 11.28s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:00<00:00,  8.33s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:00<00:00, 10.11s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-13B-V1.0/snapshots/7ef412d2c680ef0fbdcd88d0df31b396d8d3049c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-13B-V1.0/snapshots/7ef412d2c680ef0fbdcd88d0df31b396d8d3049c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.9,
  "top_p": 0.6,
  "transformers_version": "4.31.0"
}

Assigning <s> to the bos_token key of the tokenizer
Assigning </s> to the eos_token key of the tokenizer
Assigning <unk> to the unk_token key of the tokenizer
Assigning <pad> to the pad_token key of the tokenizer
Adding <pad> to the vocabulary
09/05/2023 14:01:49 - INFO - __main__ - Sample 75698 of the training set: {'input_ids': tensor([    1,   530,  1592,   309,  1008,   284, 17205,   756, 11192,   310,
         3309, 29871, 29906, 10340, 29889,   319,  1473,  1592,   309,  1008,
          284, 17205,   338,  8429,  2534, 11192,   393,   526,   395, 29896,
        29945, 29900,  8958, 29938,   310,   278,  3309,   310,   278, 11192,
          310,   278,   937, 17205, 29889,   319,  4654,  1592,   309,  1008,
          284, 17205,   338,  8429,  2534, 11192,   393,   526,   395, 29896,
        29945, 29900,  8958, 29938,   310,   278,  3309,   310,   278, 11192,
          310,   278,  1473, 17205, 29889,   450,  1889,   338,  7572,  2745,
         3023,  1592,   309,  1008,   284,  3367, 19536,  1863, 29889,  1724,
          674,   367,   278, 10151,  7910,   297,   278,   639, 14772,   515,
          278,   937, 17205,   304,   278, 11582, 17205, 29973, 14657,   596,
         1234,   304,   278, 20471,   260,  9097, 29889,    13,  4806,  1073,
          393,   278,   639, 14772,   310,   278,   937, 17205,   338,   395,
        29906, 29974, 29906, 29974, 29906,   353, 29871, 29953,  1504,    13,
         2855,   393,   278,   639, 14772,   310,   278,  1473, 17205,   338,
          395, 29941, 29974, 29941, 29974, 29941,   353, 29871, 29929,  1504,
           13,  7341, 29889,  1105,   278, 10151,  7910,   297,   278,   639,
        14772,   515,   278,   937, 17205,   304,   278,  1473, 17205,   338,
          779,  1154, 29912, 29929, 29899, 29953,  1157, 29953, 29913, 29930,
        29896, 29900, 29900,   353, 29871, 29945, 29900, 29938, 10151, 29889,
           13, 12024, 29915, 29879,  1284,   278,   639, 14772,   310,   278,
         4654, 17205, 29889,    13,  1576, 11192,   310,   278,  4654, 17205,
          526,   395, 29896, 29945, 29900,  8958, 29938,   310,   278, 11192,
          310,   278,  1473, 17205, 29892,   577,   896,   526,   395, 29896,
        29889, 29945, 29930, 29941,   353, 29871, 29946, 29889, 29945, 29938,
        10340,  1472, 29889,    13,  7058,  2794,   278,   639, 14772,   310,
          278,  4654, 17205,   338,   395, 29946, 29889, 29945, 29974, 29946,
        29889, 29945, 29974, 29946, 29889, 29945,   353, 29871, 29896, 29941,
        29889, 29945, 29938, 10340, 29889,    13,  6295,   278, 10151,  7910,
          297,   278,   639, 14772,   515,   278,  1473, 17205,   304,   278,
         4654, 17205,   338,   779,  1154, 29912, 29896, 29941, 29889, 29945,
        29899, 29929,  1157, 29929, 29913, 29930, 29896, 29900, 29900,   353,
        29871, 29945, 29900, 29938, 10151, 29889,    13, 12024, 29915, 29879,
         1284,   278,   639, 14772,   310,   278, 11582, 17205, 29889,    13,
         1576, 11192,   310,   278, 11582, 17205,   526,   395, 29896, 29945,
        29900,  8958, 29938,   310,   278, 11192,   310,   278,  4654, 17205,
        29892,   577,   896,   526,   395, 29896, 29889, 29945, 29930, 29946,
        29889, 29945,   353, 29871, 29953, 29889, 29955, 29945, 29938, 10340,
         1472, 29889,    13,  7058,  2794,   278,   639, 14772,   310,   278,
        11582, 17205,   338,   395, 29953, 29889, 29955, 29945, 29974, 29953,
        29889, 29955, 29945, 29974, 29953, 29889, 29955, 29945,   353, 29871,
        29906, 29900, 29889, 29906, 29945, 29938, 10340, 29889,    13,  6295,
          278, 10151,  7910,   297,   278,   639, 14772,   515,   278,  4654,
        17205,   304,   278, 11582, 17205,   338,   779,  1154, 29912, 29906,
        29900, 29889, 29906, 29945, 29899, 29896, 29941, 29889, 29945,  1157,
        29896, 29941, 29889, 29945, 29913, 29930, 29896, 29900, 29900,   353,
        29871, 29945, 29900, 29938, 10151, 29889,    13,  6295,   278,  3001,
        10151,  7910,   297,   278,   639, 14772,   515,   278,   937, 17205,
          304,   278, 11582, 17205,   338,   395, 29945, 29900, 29974, 29945,
        29900, 29974, 29945, 29900,   353, 29871, 29896, 29945, 29900, 29938,
        10151, 29889,    13,  7341, 29889,  1105,   278,  1234,   338,   395,
        29896, 29945, 29900,  8958, 29938,  7910, 29889,    13,    13, 29937,
          673,    13,    13, 29896, 29945, 29900,    13]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100, 21104,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  8178])}.
09/05/2023 14:01:49 - INFO - __main__ - Sample 43915 of the training set: {'input_ids': tensor([    1,  8449,   310,   278,  1494,  1033,  6058,   367,   278, 27497,
          310,   278,  7029,  7936,   265,  1338,   310,   263,  1492,  4943,
          544,  1608,   518, 29874,   376,  1884,  3108, 29973,   313,  2744,
          779,  7746, 29912, 23176, 19640,  1042,   338,   263, 19640,   310,
          697,   310,   278,  7705,  6825, 17240,   310,   278,  3800,  1846,
           13,  4535,   726,  8001, 29909, 29897,   500, 10045, 29946, 29892,
        29945, 29892, 29953, 18105,   320,  3425,   320,   726,  8001, 29933,
        29897,   500,  9991, 29946, 29892, 29945, 29892, 29955, 18105, 29871,
          320,  3425,   320,   726,  8001, 29907, 29897,   500,  9991, 29946,
        29892, 29953, 29892, 29955, 18105,   320,  3425,   320,   726,  8001,
        29928, 29897,   500,  9991, 29945, 29892, 29953, 29892, 29955, 18105,
          320,  3425,   320,   726,  8001, 29923, 29897,   500,  9991, 29945,
        29892, 29955, 29892, 29947, 11854,    13, 29902,  1073,   393,   263,
         1492,  4943,   544,  1608,   338,   263,  3800,   411,   378,  7108,
          296,  7705,  6825, 17240, 29892,   322,   393,   278,  7029,  7936,
          265,  1338,   526,   278,  7936,   265,  1338,   310,  1906, 17240,
        29889,    13, 29902,   884,  1073,   393,   278,   349,  1541,   351,
          487,   273,  9185,  1104,  1078,   278, 27497,   310,   278, 11192,
          322,   278, 19640,   310,   263,  1492, 17205, 29892,   577,   306,
          508,   671,   393,   304,  1423,   565,   278,  2183, 27497,   526,
         1950,   363,   385,  7029, 19640, 29889,    13,  2831,  1342, 29892,
          565,   306,   505,   263,  3700,   411, 11192, 29871, 29941,   322,
        29871, 29946, 29892,   769,   278, 19640,   338, 29871, 29945,   491,
          278,   349,  1541,   351,   487,   273,  9185, 29892,   577, 12564,
        29941, 29892, 29946, 29892, 29945, 11854,   338,   263,  1950,   731,
          310, 27497,   363,   278,  7029,  7936,   265,  1338, 29889,    13,
        29902,   508,   884,   671,   278,   378,  3901,   310,   278,   349,
         1541,   351,   487,   273,  9185, 29892,   607,  4083,   393,   565,
          278,  2533,   310,   278, 25256,   310,  1023, 11192, 15743,   278,
         6862,   310,   278, 19640, 29892,   769,   278, 17205,   338,  1492,
        29889,    13,  6295, 29892,   306,   508, 27399,   738,  2984,   393,
          947,   451, 15523,   445,  4195,   363,   738,   310,   278,  2211,
        11000,   310, 11192, 29889,    13, 12024, 29915, 29879,  1369,   411,
         2984,   319, 29892, 12564, 29946, 29892, 29945, 29892, 29953, 23699,
           13,  3644,   306,  2125, 29871, 29946,   322, 29871, 29945,   408,
          278, 11192, 29892,   769,   278, 19640,   881,   367,   779,  3676,
        29912, 29946, 29985, 29906, 29974, 29945, 29985, 29906,  8738,  3676,
        29912, 29946, 29896,  4429,   607,   338,   451, 29871, 29953, 29892,
          577,   445,  2984,   338,   451,  1950, 29889,    13]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        6374, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, 8178])}.
09/05/2023 14:01:49 - INFO - __main__ - Sample 1437 of the training set: {'input_ids': tensor([    1, 12458,  2305,  7845,  2820,   263, 19308,  1591, 29892,   322,
         1269,  2022,   674,  9679,   263,  3918,  4832, 29899, 29879,  2618,
          762, 29889,  1724,   338,   278,  6976,   393,   694,  1023,  2305,
        16246,  2446,   304,  1269,   916,   674,  9679,   278,  1021,  1353,
         1156,   896,  1269,  9679,   278,   762,  2748, 29973, 14657,   596,
         1234,   408,   263,  3619, 15958, 29889,    13,  1762,  1369, 29892,
          306,   674,  3858,   278,  3023,  2305,   408,   319, 29892,   350,
        29892,   315, 29892,   322,   360, 29892,   322,  5251,   393,   896,
          526, 16246,   297, 12006,  3538,  1797, 29889,    13, 29902,   674,
          884,  5251,   393,   278,  1797,   297,   607,   896,  9679,   278,
        17629,   947,   451,  4383, 29892,  1951,   278, 19308,  1591,   338,
        18348, 29889,    13, 29902,   864,   304,  1284,   278,  6976,   393,
          319,   322,   350,  9679,  1422,  3694, 29892,   350,   322,   315,
         9679,  1422,  3694, 29892,   315,   322,   360,  9679,  1422,  3694,
        29892,   322,   360,   322,   319,  9679,  1422,  3694, 29889,    13]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, 8178])}.
09/05/2023 14:01:49 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (2).
09/05/2023 14:01:49 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.
[2023-09-05 14:01:49,170] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
09/05/2023 14:01:49 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
09/05/2023 14:01:49 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-09-05 14:01:49,189] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
09/05/2023 14:01:49 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-09-05 14:01:49,190] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-05 14:01:49,190] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
09/05/2023 14:01:49 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
09/05/2023 14:01:49 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-09-05 14:01:49,205] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-05 14:01:49,205] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-05 14:01:49,205] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2023-09-05 14:01:49,205] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2023-09-05 14:01:49,312] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
[2023-09-05 14:01:49,313] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 7.06 GB         CA 8.9 GB         Max_CA 9 GB 
[2023-09-05 14:01:49,313] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 119.24 GB, percent = 11.8%
[2023-09-05 14:01:49,314] [INFO] [stage3.py:117:__init__] Reduce bucket size 26214400
[2023-09-05 14:01:49,314] [INFO] [stage3.py:118:__init__] Prefetch bucket size 23592960
[2023-09-05 14:01:49,427] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-09-05 14:01:49,428] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 8.9 GB         Max_CA 9 GB 
[2023-09-05 14:01:49,428] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 119.24 GB, percent = 11.8%
Parameter Offload: Total persistent parameters: 414720 in 81 params
[2023-09-05 14:01:49,562] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-09-05 14:01:49,563] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 8.9 GB         Max_CA 9 GB 
[2023-09-05 14:01:49,563] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 119.23 GB, percent = 11.8%
[2023-09-05 14:01:49,688] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
[2023-09-05 14:01:49,689] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 8.9 GB         Max_CA 9 GB 
[2023-09-05 14:01:49,689] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 119.23 GB, percent = 11.8%
[2023-09-05 14:01:54,244] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 4
[2023-09-05 14:01:54,245] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 6.14 GB         Max_CA 9 GB 
[2023-09-05 14:01:54,245] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 137.45 GB, percent = 13.6%
[2023-09-05 14:01:54,355] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
[2023-09-05 14:01:54,356] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 6.14 GB         Max_CA 6 GB 
[2023-09-05 14:01:54,356] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 137.45 GB, percent = 13.6%
[2023-09-05 14:01:54,498] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
[2023-09-05 14:01:54,499] [INFO] [utils.py:786:see_memory_usage] MA 18.26 GB         Max_MA 19.27 GB         CA 21.16 GB         Max_CA 21 GB 
[2023-09-05 14:01:54,499] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 137.45 GB, percent = 13.6%
[2023-09-05 14:01:55,221] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-05 14:01:55,221] [INFO] [utils.py:786:see_memory_usage] MA 18.26 GB         Max_MA 18.26 GB         CA 21.16 GB         Max_CA 21 GB 
[2023-09-05 14:01:55,221] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 119.24 GB, percent = 11.8%
[2023-09-05 14:01:55,440] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-05 14:01:55,440] [INFO] [utils.py:786:see_memory_usage] MA 42.51 GB         Max_MA 52.08 GB         CA 62.48 GB         Max_CA 62 GB 
[2023-09-05 14:01:55,441] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 119.24 GB, percent = 11.8%
[2023-09-05 14:01:55,441] [INFO] [stage3.py:424:_setup_for_real_optimizer] optimizer state initialized
[2023-09-05 14:01:55,590] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-09-05 14:01:55,593] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-09-05 14:01:55,594] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-09-05 14:01:55,599] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-09-05 14:01:55,599] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-09-05 14:01:55,601] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-09-05 14:01:55,602] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-09-05 14:01:55,603] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-09-05 14:01:55,603] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-09-05 14:01:55,607] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-09-05 14:01:55,609] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-09-05 14:01:55,611] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-09-05 14:01:55,676] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-09-05 14:01:55,676] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-09-05 14:01:55,676] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-09-05 14:01:55,718] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-05 14:01:55,719] [INFO] [utils.py:786:see_memory_usage] MA 48.62 GB         Max_MA 49.23 GB         CA 69.51 GB         Max_CA 70 GB 
[2023-09-05 14:01:55,719] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 119.51 GB, percent = 11.9%
[2023-09-05 14:01:55,719] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-05 14:01:55,719] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-05 14:01:55,719] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-05 14:01:55,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-09-05 14:01:55,720] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdb2c079de0>
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-05 14:01:55,720] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 16
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   train_batch_size ............. 128
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-05 14:01:55,721] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-05 14:01:55,722] [INFO] [config.py:964:print]   world_size ................... 4
[2023-09-05 14:01:55,722] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-05 14:01:55,722] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=26214400 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=23592960 param_persistence_threshold=51200 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-05 14:01:55,722] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-05 14:01:55,722] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-05 14:01:55,722] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3
[2023-09-05 14:01:55,722] [INFO] [config.py:950:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 2.621440e+07, 
        "stage3_prefetch_bucket_size": 2.359296e+07, 
        "stage3_param_persistence_threshold": 5.120000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
wandb: Currently logged in as: kidrain61. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.15.9
wandb: Run data is saved locally in /data/users/zhangjunlei/tyx/wandb/wandb/run-20230905_140200-2r9crs86
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16-2023-09-05-1
wandb: ⭐️ View project at https://wandb.ai/kidrain61/step-reward
wandb: 🚀 View run at https://wandb.ai/kidrain61/step-reward/runs/2r9crs86
09/05/2023 14:02:10 - INFO - __main__ - ***** Running training *****
09/05/2023 14:02:10 - INFO - __main__ -   Num examples = 85194
09/05/2023 14:02:10 - INFO - __main__ -   Num Epochs = 100
09/05/2023 14:02:10 - INFO - __main__ -   Instantaneous batch size per device = 2
09/05/2023 14:02:10 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128
09/05/2023 14:02:10 - INFO - __main__ -   Gradient Accumulation steps = 16
09/05/2023 14:02:10 - INFO - __main__ -   Total optimization steps = 66600
09/05/2023 14:02:10 - INFO - __main__ - Resuming from checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5
09/05/2023 14:02:10 - INFO - accelerate.accelerator - Loading states from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5
09/05/2023 14:02:10 - INFO - accelerate.accelerator - Loading DeepSpeed Model and Optimizer
[2023-09-05 14:02:10,335] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-09-05 14:02:10,344] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-09-05 14:02:10,344] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-09-05 14:02:10,352] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-09-05 14:02:10,361] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-09-05 14:02:20,742] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-09-05 14:02:20,742] [INFO] [engine.py:2865:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 3
[2023-09-05 14:02:22,594] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-09-05 14:02:22,595] [INFO] [engine.py:2865:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 2
[2023-09-05 14:02:23,220] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-09-05 14:02:23,220] [INFO] [engine.py:2865:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 1
[2023-09-05 14:02:38,687] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-09-05 14:02:38,688] [INFO] [engine.py:2865:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 0
[2023-09-05 14:02:52,901] [INFO] [engine.py:2815:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 0
[2023-09-05 14:02:52,902] [INFO] [engine.py:2815:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 3
[2023-09-05 14:02:52,902] [INFO] [engine.py:2815:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 2
[2023-09-05 14:02:52,902] [INFO] [engine.py:2815:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 1
09/05/2023 14:02:55 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer loaded from input dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_5/pytorch_model
09/05/2023 14:02:55 - INFO - accelerate.checkpointing - All model weights loaded successfully
09/05/2023 14:02:55 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
09/05/2023 14:02:55 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
09/05/2023 14:02:55 - INFO - accelerate.checkpointing - All random states loaded successfully
09/05/2023 14:02:55 - INFO - accelerate.accelerator - Loading in 0 custom states
09/05/2023 14:02:55 - INFO - __main__ - Resuming from epoch 6
09/05/2023 14:02:55 - INFO - __main__ - ***** Running Validation *****
Evaluating:   0%|          | 0/228 [00:00<?, ?it/s]step: 0
extend+tolist() time: 0.0021555423736572266
Evaluating:   0%|          | 1/228 [00:01<06:08,  1.62s/it]step: 1
extend+tolist() time: 0.0014729499816894531
Evaluating:   1%|          | 2/228 [00:02<03:50,  1.02s/it]step: 2
extend+tolist() time: 0.0022039413452148438
Evaluating:   1%|▏         | 3/228 [00:02<02:39,  1.41it/s]step: 3
extend+tolist() time: 0.0017704963684082031
Evaluating:   2%|▏         | 4/228 [00:02<02:03,  1.81it/s]step: 4
extend+tolist() time: 0.0010187625885009766
Evaluating:   2%|▏         | 5/228 [00:03<01:39,  2.24it/s]step: 5
extend+tolist() time: 0.002127408981323242
Evaluating:   3%|▎         | 6/228 [00:03<01:30,  2.47it/s]step: 6
extend+tolist() time: 0.00213623046875
Evaluating:   3%|▎         | 7/228 [00:03<01:24,  2.61it/s]step: 7
extend+tolist() time: 0.0014014244079589844
Evaluating:   4%|▎         | 8/228 [00:04<01:15,  2.93it/s]step: 8
extend+tolist() time: 0.0008835792541503906
Evaluating:   4%|▍         | 9/228 [00:04<01:07,  3.27it/s]step: 9
extend+tolist() time: 0.0013523101806640625
Evaluating:   4%|▍         | 10/228 [00:04<01:01,  3.52it/s]step: 10
extend+tolist() time: 0.0010101795196533203
Evaluating:   5%|▍         | 11/228 [00:04<00:58,  3.70it/s]step: 11
extend+tolist() time: 0.00103759765625
Evaluating:   5%|▌         | 12/228 [00:04<00:55,  3.90it/s]step: 12
extend+tolist() time: 0.0007495880126953125
Evaluating:   6%|▌         | 13/228 [00:05<00:52,  4.07it/s]step: 13
extend+tolist() time: 0.0006883144378662109
Evaluating:   6%|▌         | 14/228 [00:05<00:51,  4.20it/s]step: 14
extend+tolist() time: 0.0010776519775390625
Evaluating:   7%|▋         | 15/228 [00:05<00:49,  4.29it/s]step: 15
extend+tolist() time: 0.0006566047668457031
Evaluating:   7%|▋         | 16/228 [00:05<00:48,  4.36it/s]step: 16
extend+tolist() time: 0.0006725788116455078
Evaluating:   7%|▋         | 17/228 [00:06<00:48,  4.38it/s]step: 17
extend+tolist() time: 0.0014612674713134766
Evaluating:   8%|▊         | 18/228 [00:06<00:48,  4.32it/s]step: 18
extend+tolist() time: 0.0016663074493408203
Evaluating:   8%|▊         | 19/228 [00:06<00:51,  4.07it/s]step: 19
extend+tolist() time: 0.0010848045349121094
Evaluating:   9%|▉         | 20/228 [00:06<00:52,  3.96it/s]step: 20
extend+tolist() time: 0.0012445449829101562
Evaluating:   9%|▉         | 21/228 [00:07<00:50,  4.08it/s]step: 21
extend+tolist() time: 0.0007860660552978516
Evaluating:  10%|▉         | 22/228 [00:07<00:49,  4.18it/s]step: 22
extend+tolist() time: 0.0012900829315185547
Evaluating:  10%|█         | 23/228 [00:07<00:48,  4.25it/s]step: 23
extend+tolist() time: 0.000797271728515625
Evaluating:  11%|█         | 24/228 [00:07<00:47,  4.30it/s]step: 24
extend+tolist() time: 0.17416095733642578
Evaluating:  11%|█         | 25/228 [00:08<01:00,  3.36it/s]step: 25
extend+tolist() time: 0.0023031234741210938
Evaluating:  11%|█▏        | 26/228 [00:08<01:03,  3.18it/s]step: 26
extend+tolist() time: 0.000812530517578125
Evaluating:  12%|█▏        | 27/228 [00:08<00:57,  3.48it/s]step: 27
extend+tolist() time: 0.0017557144165039062
Evaluating:  12%|█▏        | 28/228 [00:09<00:58,  3.43it/s]step: 28
extend+tolist() time: 0.0007343292236328125
Evaluating:  13%|█▎        | 29/228 [00:09<00:53,  3.71it/s]step: 29
extend+tolist() time: 0.0008480548858642578
Evaluating:  13%|█▎        | 30/228 [00:09<00:50,  3.90it/s]step: 30
extend+tolist() time: 0.001669168472290039
Evaluating:  14%|█▎        | 31/228 [00:09<00:51,  3.79it/s]step: 31
extend+tolist() time: 0.000774383544921875
Evaluating:  14%|█▍        | 32/228 [00:10<00:49,  3.97it/s]step: 32
extend+tolist() time: 0.001573324203491211
Evaluating:  14%|█▍        | 33/228 [00:10<00:50,  3.88it/s]step: 33
extend+tolist() time: 0.0018286705017089844
Evaluating:  15%|█▍        | 34/228 [00:10<00:52,  3.70it/s]step: 34
extend+tolist() time: 0.0009930133819580078
Evaluating:  15%|█▌        | 35/228 [00:10<00:50,  3.80it/s]step: 35
extend+tolist() time: 0.0011548995971679688
Evaluating:  16%|█▌        | 36/228 [00:11<00:48,  3.96it/s]step: 36
extend+tolist() time: 0.0009059906005859375
Evaluating:  16%|█▌        | 37/228 [00:11<00:46,  4.07it/s]step: 37
extend+tolist() time: 0.0018944740295410156
Evaluating:  17%|█▋        | 38/228 [00:11<00:49,  3.80it/s]step: 38
extend+tolist() time: 0.0008912086486816406
Evaluating:  17%|█▋        | 39/228 [00:11<00:47,  3.97it/s]step: 39
extend+tolist() time: 0.0011806488037109375
Evaluating:  18%|█▊        | 40/228 [00:12<00:45,  4.09it/s]step: 40
extend+tolist() time: 0.0007214546203613281
Evaluating:  18%|█▊        | 41/228 [00:12<00:44,  4.20it/s]step: 41
extend+tolist() time: 0.0014111995697021484
Evaluating:  18%|█▊        | 42/228 [00:12<00:44,  4.17it/s]step: 42
extend+tolist() time: 0.001714468002319336
Evaluating:  19%|█▉        | 43/228 [00:12<00:47,  3.87it/s]step: 43
extend+tolist() time: 0.0020890235900878906
Evaluating:  19%|█▉        | 44/228 [00:13<00:51,  3.56it/s]step: 44
extend+tolist() time: 0.0008518695831298828
Evaluating:  20%|█▉        | 45/228 [00:13<00:48,  3.78it/s]step: 45
extend+tolist() time: 0.002119779586791992
Evaluating:  20%|██        | 46/228 [00:13<00:50,  3.63it/s]step: 46
extend+tolist() time: 0.0017747879028320312
Evaluating:  21%|██        | 47/228 [00:14<00:51,  3.52it/s]step: 47
extend+tolist() time: 0.0013027191162109375
Evaluating:  21%|██        | 48/228 [00:14<00:50,  3.54it/s]step: 48
extend+tolist() time: 0.0020456314086914062
Evaluating:  21%|██▏       | 49/228 [00:14<00:51,  3.48it/s]step: 49
extend+tolist() time: 0.0014278888702392578
Evaluating:  22%|██▏       | 50/228 [00:14<00:49,  3.60it/s]step: 50
extend+tolist() time: 0.0017364025115966797
Evaluating:  22%|██▏       | 51/228 [00:15<00:50,  3.51it/s]step: 51
extend+tolist() time: 0.0014111995697021484
Evaluating:  23%|██▎       | 52/228 [00:15<00:50,  3.46it/s]step: 52
extend+tolist() time: 0.0016431808471679688
Evaluating:  23%|██▎       | 53/228 [00:15<00:49,  3.52it/s]step: 53
extend+tolist() time: 0.0018477439880371094
Evaluating:  24%|██▎       | 54/228 [00:16<00:50,  3.45it/s]step: 54
extend+tolist() time: 0.0009517669677734375
Evaluating:  24%|██▍       | 55/228 [00:16<00:58,  2.97it/s]step: 55
extend+tolist() time: 0.1487255096435547
Evaluating:  25%|██▍       | 56/228 [00:16<00:59,  2.87it/s]step: 56
extend+tolist() time: 0.0017561912536621094
Evaluating:  25%|██▌       | 57/228 [00:17<00:57,  2.99it/s]step: 57
extend+tolist() time: 0.0006396770477294922
Evaluating:  25%|██▌       | 58/228 [00:17<00:51,  3.32it/s]step: 58
extend+tolist() time: 0.0015358924865722656
Evaluating:  26%|██▌       | 59/228 [00:17<00:48,  3.51it/s]step: 59
extend+tolist() time: 0.001041412353515625
Evaluating:  26%|██▋       | 60/228 [00:17<00:45,  3.66it/s]step: 60
extend+tolist() time: 0.0008218288421630859
Evaluating:  27%|██▋       | 61/228 [00:18<00:43,  3.85it/s]step: 61
extend+tolist() time: 0.0013275146484375
Evaluating:  27%|██▋       | 62/228 [00:18<00:42,  3.93it/s]step: 62
extend+tolist() time: 0.0009262561798095703
Evaluating:  28%|██▊       | 63/228 [00:18<00:40,  4.05it/s]step: 63
extend+tolist() time: 0.001383066177368164
Evaluating:  28%|██▊       | 64/228 [00:18<00:39,  4.13it/s]step: 64
extend+tolist() time: 0.0009114742279052734
Evaluating:  29%|██▊       | 65/228 [00:19<00:39,  4.18it/s]step: 65
extend+tolist() time: 0.0013725757598876953
Evaluating:  29%|██▉       | 66/228 [00:19<00:38,  4.17it/s]step: 66
extend+tolist() time: 0.0008835792541503906
Evaluating:  29%|██▉       | 67/228 [00:19<00:37,  4.25it/s]step: 67
extend+tolist() time: 0.0015459060668945312
Evaluating:  30%|██▉       | 68/228 [00:19<00:38,  4.19it/s]step: 68
extend+tolist() time: 0.0008175373077392578
Evaluating:  30%|███       | 69/228 [00:19<00:37,  4.25it/s]step: 69
extend+tolist() time: 0.0016291141510009766
Evaluating:  31%|███       | 70/228 [00:20<00:39,  4.01it/s]step: 70
extend+tolist() time: 0.0015778541564941406
Evaluating:  31%|███       | 71/228 [00:20<00:40,  3.88it/s]step: 71
extend+tolist() time: 0.0011818408966064453
Evaluating:  32%|███▏      | 72/228 [00:20<00:40,  3.84it/s]step: 72
extend+tolist() time: 0.0012707710266113281
Evaluating:  32%|███▏      | 73/228 [00:21<00:39,  3.97it/s]step: 73
extend+tolist() time: 0.0006196498870849609
Evaluating:  32%|███▏      | 74/228 [00:21<00:37,  4.11it/s]step: 74
extend+tolist() time: 0.0008418560028076172
Evaluating:  33%|███▎      | 75/228 [00:21<00:36,  4.20it/s]step: 75
extend+tolist() time: 0.001974821090698242
Evaluating:  33%|███▎      | 76/228 [00:21<00:39,  3.88it/s]step: 76
extend+tolist() time: 0.0011341571807861328
Evaluating:  34%|███▍      | 77/228 [00:22<00:37,  4.02it/s]step: 77
extend+tolist() time: 0.0019762516021728516
Evaluating:  34%|███▍      | 78/228 [00:22<00:41,  3.64it/s]step: 78
extend+tolist() time: 0.0010271072387695312
Evaluating:  35%|███▍      | 79/228 [00:22<00:39,  3.74it/s]step: 79
extend+tolist() time: 0.0014355182647705078
Evaluating:  35%|███▌      | 80/228 [00:22<00:38,  3.82it/s]step: 80
extend+tolist() time: 0.0011069774627685547
Evaluating:  36%|███▌      | 81/228 [00:23<00:37,  3.88it/s]step: 81
extend+tolist() time: 0.0013551712036132812
Evaluating:  36%|███▌      | 82/228 [00:23<00:36,  3.96it/s]step: 82
extend+tolist() time: 0.0010151863098144531
Evaluating:  36%|███▋      | 83/228 [00:23<00:36,  4.03it/s]step: 83
extend+tolist() time: 0.0008296966552734375
Evaluating:  37%|███▋      | 84/228 [00:23<00:34,  4.14it/s]step: 84
extend+tolist() time: 0.0018420219421386719
Evaluating:  37%|███▋      | 85/228 [00:24<00:35,  3.99it/s]step: 85
extend+tolist() time: 0.0010616779327392578
Evaluating:  38%|███▊      | 86/228 [00:24<00:35,  4.01it/s]step: 86
extend+tolist() time: 0.00150299072265625
Evaluating:  38%|███▊      | 87/228 [00:24<00:35,  4.02it/s]step: 87
extend+tolist() time: 0.0010406970977783203
Evaluating:  39%|███▊      | 88/228 [00:24<00:34,  4.02it/s]step: 88
extend+tolist() time: 0.00130462646484375
Evaluating:  39%|███▉      | 89/228 [00:25<00:33,  4.13it/s]step: 89
extend+tolist() time: 0.0008473396301269531
Evaluating:  39%|███▉      | 90/228 [00:25<00:32,  4.21it/s]step: 90
extend+tolist() time: 0.0015473365783691406
Evaluating:  40%|███▉      | 91/228 [00:25<00:33,  4.15it/s]step: 91
extend+tolist() time: 0.0009443759918212891
Evaluating:  40%|████      | 92/228 [00:25<00:32,  4.18it/s]step: 92
extend+tolist() time: 0.0014371871948242188
Evaluating:  41%|████      | 93/228 [00:25<00:31,  4.23it/s]step: 93
extend+tolist() time: 0.0010941028594970703
Evaluating:  41%|████      | 94/228 [00:26<00:33,  4.06it/s]step: 94
extend+tolist() time: 0.0012564659118652344
Evaluating:  42%|████▏     | 95/228 [00:26<00:31,  4.16it/s]step: 95
extend+tolist() time: 0.0017240047454833984
Evaluating:  42%|████▏     | 96/228 [00:26<00:33,  3.88it/s]step: 96
extend+tolist() time: 0.001085042953491211
Evaluating:  43%|████▎     | 97/228 [00:27<00:41,  3.13it/s]step: 97
extend+tolist() time: 0.19205927848815918
Evaluating:  43%|████▎     | 98/228 [00:27<00:45,  2.86it/s]step: 98
extend+tolist() time: 0.0010247230529785156
Evaluating:  43%|████▎     | 99/228 [00:27<00:41,  3.13it/s]step: 99
extend+tolist() time: 0.0010216236114501953
Evaluating:  44%|████▍     | 100/228 [00:28<00:38,  3.36it/s]step: 100
extend+tolist() time: 0.0011458396911621094
Evaluating:  44%|████▍     | 101/228 [00:28<00:35,  3.61it/s]step: 101
extend+tolist() time: 0.0009720325469970703
Evaluating:  45%|████▍     | 102/228 [00:28<00:33,  3.77it/s]step: 102
extend+tolist() time: 0.0012841224670410156
Evaluating:  45%|████▌     | 103/228 [00:28<00:31,  3.91it/s]step: 103
extend+tolist() time: 0.0012557506561279297
Evaluating:  46%|████▌     | 104/228 [00:29<00:30,  4.02it/s]step: 104
extend+tolist() time: 0.0011601448059082031
Evaluating:  46%|████▌     | 105/228 [00:29<00:29,  4.13it/s]step: 105
extend+tolist() time: 0.0010693073272705078
Evaluating:  46%|████▋     | 106/228 [00:29<00:29,  4.15it/s]step: 106
extend+tolist() time: 0.002068042755126953
Evaluating:  47%|████▋     | 107/228 [00:29<00:31,  3.79it/s]step: 107
extend+tolist() time: 0.001270294189453125
Evaluating:  47%|████▋     | 108/228 [00:30<00:30,  3.93it/s]step: 108
extend+tolist() time: 0.0009229183197021484
Evaluating:  48%|████▊     | 109/228 [00:30<00:29,  4.04it/s]step: 109
extend+tolist() time: 0.0014488697052001953
Evaluating:  48%|████▊     | 110/228 [00:30<00:29,  4.07it/s]step: 110
extend+tolist() time: 0.0007660388946533203
Evaluating:  49%|████▊     | 111/228 [00:30<00:28,  4.18it/s]step: 111
extend+tolist() time: 0.002038240432739258
Evaluating:  49%|████▉     | 112/228 [00:31<00:30,  3.79it/s]step: 112
extend+tolist() time: 0.0004119873046875
Evaluating:  50%|████▉     | 113/228 [00:31<00:28,  3.99it/s]step: 113
extend+tolist() time: 0.0012192726135253906
Evaluating:  50%|█████     | 114/228 [00:31<00:27,  4.12it/s]step: 114
extend+tolist() time: 0.0013210773468017578
Evaluating:  50%|█████     | 115/228 [00:31<00:28,  3.95it/s]step: 115
extend+tolist() time: 0.0011153221130371094
Evaluating:  51%|█████     | 116/228 [00:32<00:27,  4.08it/s]step: 116
extend+tolist() time: 0.0009074211120605469
Evaluating:  51%|█████▏    | 117/228 [00:32<00:28,  3.83it/s]step: 117
extend+tolist() time: 0.0014395713806152344
Evaluating:  52%|█████▏    | 118/228 [00:32<00:28,  3.91it/s]step: 118
extend+tolist() time: 0.0006914138793945312
Evaluating:  52%|█████▏    | 119/228 [00:32<00:26,  4.06it/s]step: 119
extend+tolist() time: 0.0011970996856689453
Evaluating:  53%|█████▎    | 120/228 [00:33<00:26,  4.15it/s]step: 120
extend+tolist() time: 0.0006756782531738281
Evaluating:  53%|█████▎    | 121/228 [00:33<00:25,  4.24it/s]step: 121
extend+tolist() time: 0.0008044242858886719
Evaluating:  54%|█████▎    | 122/228 [00:33<00:24,  4.30it/s]step: 122
extend+tolist() time: 0.0014579296112060547
Evaluating:  54%|█████▍    | 123/228 [00:33<00:24,  4.33it/s]step: 123
extend+tolist() time: 0.0006949901580810547
Evaluating:  54%|█████▍    | 124/228 [00:33<00:23,  4.37it/s]step: 124
extend+tolist() time: 0.0013828277587890625
Evaluating:  55%|█████▍    | 125/228 [00:34<00:23,  4.35it/s]step: 125
extend+tolist() time: 0.0004482269287109375
Evaluating:  55%|█████▌    | 126/228 [00:34<00:23,  4.42it/s]step: 126
extend+tolist() time: 0.001955747604370117
Evaluating:  56%|█████▌    | 127/228 [00:34<00:25,  3.93it/s]step: 127
extend+tolist() time: 0.0019724369049072266
Evaluating:  56%|█████▌    | 128/228 [00:35<00:27,  3.61it/s]step: 128
extend+tolist() time: 0.0008378028869628906
Evaluating:  57%|█████▋    | 129/228 [00:35<00:25,  3.81it/s]step: 129
extend+tolist() time: 0.0013489723205566406
Evaluating:  57%|█████▋    | 130/228 [00:35<00:24,  3.97it/s]step: 130
extend+tolist() time: 0.001087188720703125
Evaluating:  57%|█████▋    | 131/228 [00:35<00:24,  3.99it/s]step: 131
extend+tolist() time: 0.0008890628814697266
Evaluating:  58%|█████▊    | 132/228 [00:35<00:23,  4.14it/s]step: 132
extend+tolist() time: 0.0012483596801757812
Evaluating:  58%|█████▊    | 133/228 [00:36<00:23,  3.96it/s]step: 133
extend+tolist() time: 0.0005004405975341797
Evaluating:  59%|█████▉    | 134/228 [00:36<00:22,  4.12it/s]step: 134
extend+tolist() time: 0.001631021499633789
Evaluating:  59%|█████▉    | 135/228 [00:36<00:23,  3.99it/s]step: 135
extend+tolist() time: 0.0004851818084716797
Evaluating:  60%|█████▉    | 136/228 [00:36<00:22,  4.15it/s]step: 136
extend+tolist() time: 0.0013549327850341797
Evaluating:  60%|██████    | 137/228 [00:37<00:21,  4.18it/s]step: 137
extend+tolist() time: 0.000415802001953125
Evaluating:  61%|██████    | 138/228 [00:37<00:21,  4.28it/s]step: 138
extend+tolist() time: 0.0008380413055419922
Evaluating:  61%|██████    | 139/228 [00:37<00:20,  4.31it/s]step: 139
extend+tolist() time: 0.0009348392486572266
Evaluating:  61%|██████▏   | 140/228 [00:37<00:20,  4.38it/s]step: 140
extend+tolist() time: 0.0008504390716552734
Evaluating:  62%|██████▏   | 141/228 [00:38<00:19,  4.37it/s]step: 141
extend+tolist() time: 0.0013415813446044922
Evaluating:  62%|██████▏   | 142/228 [00:38<00:19,  4.36it/s]step: 142
extend+tolist() time: 0.0006337165832519531
Evaluating:  63%|██████▎   | 143/228 [00:38<00:19,  4.40it/s]step: 143
extend+tolist() time: 0.0003895759582519531
Evaluating:  63%|██████▎   | 144/228 [00:38<00:18,  4.43it/s]step: 144
extend+tolist() time: 0.001184225082397461
Evaluating:  64%|██████▎   | 145/228 [00:39<00:18,  4.42it/s]step: 145
extend+tolist() time: 0.0005600452423095703
Evaluating:  64%|██████▍   | 146/228 [00:39<00:18,  4.47it/s]step: 146
extend+tolist() time: 0.0004131793975830078
Evaluating:  64%|██████▍   | 147/228 [00:39<00:18,  4.49it/s]step: 147
extend+tolist() time: 0.0012924671173095703
Evaluating:  65%|██████▍   | 148/228 [00:39<00:17,  4.47it/s]step: 148
extend+tolist() time: 0.0007581710815429688
Evaluating:  65%|██████▌   | 149/228 [00:39<00:17,  4.47it/s]step: 149
extend+tolist() time: 0.0003809928894042969
Evaluating:  66%|██████▌   | 150/228 [00:40<00:17,  4.51it/s]step: 150
extend+tolist() time: 0.0013518333435058594
Evaluating:  66%|██████▌   | 151/228 [00:40<00:17,  4.44it/s]step: 151
extend+tolist() time: 0.0006766319274902344
Evaluating:  67%|██████▋   | 152/228 [00:40<00:17,  4.46it/s]step: 152
extend+tolist() time: 0.0009546279907226562
Evaluating:  67%|██████▋   | 153/228 [00:40<00:16,  4.42it/s]step: 153
extend+tolist() time: 0.0014300346374511719
Evaluating:  68%|██████▊   | 154/228 [00:41<00:17,  4.32it/s]step: 154
extend+tolist() time: 0.0020630359649658203
Evaluating:  68%|██████▊   | 155/228 [00:41<00:19,  3.84it/s]step: 155
extend+tolist() time: 0.0007278919219970703
Evaluating:  68%|██████▊   | 156/228 [00:41<00:17,  4.01it/s]step: 156
extend+tolist() time: 0.0009515285491943359
Evaluating:  69%|██████▉   | 157/228 [00:41<00:17,  4.15it/s]step: 157
extend+tolist() time: 0.0007164478302001953
Evaluating:  69%|██████▉   | 158/228 [00:42<00:16,  4.24it/s]step: 158
extend+tolist() time: 0.0005714893341064453
Evaluating:  70%|██████▉   | 159/228 [00:42<00:15,  4.32it/s]step: 159
extend+tolist() time: 0.0011680126190185547
Evaluating:  70%|███████   | 160/228 [00:42<00:21,  3.23it/s]step: 160
extend+tolist() time: 0.00040411949157714844
Evaluating:  71%|███████   | 161/228 [00:42<00:18,  3.54it/s]step: 161
extend+tolist() time: 0.0008213520050048828
Evaluating:  71%|███████   | 162/228 [00:43<00:17,  3.77it/s]step: 162
extend+tolist() time: 0.000919342041015625
Evaluating:  71%|███████▏  | 163/228 [00:43<00:16,  3.97it/s]step: 163
extend+tolist() time: 0.2229316234588623
Evaluating:  72%|███████▏  | 164/228 [00:43<00:19,  3.24it/s]step: 164
extend+tolist() time: 0.0005581378936767578
Evaluating:  72%|███████▏  | 165/228 [00:44<00:17,  3.53it/s]step: 165
extend+tolist() time: 0.0007412433624267578
Evaluating:  73%|███████▎  | 166/228 [00:44<00:16,  3.78it/s]step: 166
extend+tolist() time: 0.00039386749267578125
Evaluating:  73%|███████▎  | 167/228 [00:44<00:15,  3.99it/s]step: 167
extend+tolist() time: 0.0005843639373779297
Evaluating:  74%|███████▎  | 168/228 [00:44<00:14,  4.13it/s]step: 168
extend+tolist() time: 0.0015642642974853516
Evaluating:  74%|███████▍  | 169/228 [00:45<00:14,  3.98it/s]step: 169
extend+tolist() time: 0.0003387928009033203
Evaluating:  75%|███████▍  | 170/228 [00:45<00:14,  4.14it/s]step: 170
extend+tolist() time: 0.001260995864868164
Evaluating:  75%|███████▌  | 171/228 [00:45<00:13,  4.19it/s]step: 171
extend+tolist() time: 0.00029349327087402344
Evaluating:  75%|███████▌  | 172/228 [00:45<00:13,  4.30it/s]step: 172
extend+tolist() time: 0.0007650852203369141
Evaluating:  76%|███████▌  | 173/228 [00:45<00:12,  4.33it/s]step: 173
extend+tolist() time: 0.0019490718841552734
Evaluating:  76%|███████▋  | 174/228 [00:46<00:13,  4.12it/s]step: 174
extend+tolist() time: 0.0018243789672851562
Evaluating:  77%|███████▋  | 175/228 [00:46<00:13,  3.84it/s]step: 175
extend+tolist() time: 0.0008823871612548828
Evaluating:  77%|███████▋  | 176/228 [00:46<00:13,  3.99it/s]step: 176
extend+tolist() time: 0.0006716251373291016
Evaluating:  78%|███████▊  | 177/228 [00:46<00:12,  4.13it/s]step: 177
extend+tolist() time: 0.0010194778442382812
Evaluating:  78%|███████▊  | 178/228 [00:47<00:11,  4.24it/s]step: 178
extend+tolist() time: 0.0011963844299316406
Evaluating:  79%|███████▊  | 179/228 [00:47<00:12,  4.03it/s]step: 179
extend+tolist() time: 0.0008120536804199219
Evaluating:  79%|███████▉  | 180/228 [00:47<00:11,  4.17it/s]step: 180
extend+tolist() time: 0.00038743019104003906
Evaluating:  79%|███████▉  | 181/228 [00:47<00:11,  4.27it/s]step: 181
extend+tolist() time: 0.0006818771362304688
Evaluating:  80%|███████▉  | 182/228 [00:48<00:10,  4.34it/s]step: 182
extend+tolist() time: 0.0007479190826416016
Evaluating:  80%|████████  | 183/228 [00:48<00:10,  4.35it/s]step: 183
extend+tolist() time: 0.0006890296936035156
Evaluating:  81%|████████  | 184/228 [00:48<00:10,  4.39it/s]step: 184
extend+tolist() time: 0.0004444122314453125
Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.39it/s]step: 185
extend+tolist() time: 0.001575469970703125
Evaluating:  82%|████████▏ | 186/228 [00:49<00:10,  4.16it/s]step: 186
extend+tolist() time: 0.0010938644409179688
Evaluating:  82%|████████▏ | 187/228 [00:49<00:09,  4.14it/s]step: 187
extend+tolist() time: 0.0008893013000488281
Evaluating:  82%|████████▏ | 188/228 [00:49<00:09,  4.25it/s]step: 188
extend+tolist() time: 0.0007193088531494141
Evaluating:  83%|████████▎ | 189/228 [00:49<00:09,  4.29it/s]step: 189
extend+tolist() time: 0.000370025634765625
Evaluating:  83%|████████▎ | 190/228 [00:49<00:08,  4.35it/s]step: 190
extend+tolist() time: 0.001615285873413086
Evaluating:  84%|████████▍ | 191/228 [00:50<00:08,  4.13it/s]step: 191
extend+tolist() time: 0.0007491111755371094
Evaluating:  84%|████████▍ | 192/228 [00:50<00:08,  4.20it/s]step: 192
extend+tolist() time: 0.0004496574401855469
Evaluating:  85%|████████▍ | 193/228 [00:50<00:08,  4.29it/s]step: 193
extend+tolist() time: 0.0016009807586669922
Evaluating:  85%|████████▌ | 194/228 [00:50<00:08,  4.20it/s]step: 194
extend+tolist() time: 0.0006225109100341797
Evaluating:  86%|████████▌ | 195/228 [00:51<00:07,  4.28it/s]step: 195
extend+tolist() time: 0.0006046295166015625
Evaluating:  86%|████████▌ | 196/228 [00:51<00:07,  4.34it/s]step: 196
extend+tolist() time: 0.0006604194641113281
Evaluating:  86%|████████▋ | 197/228 [00:51<00:07,  4.35it/s]step: 197
extend+tolist() time: 0.0007135868072509766
Evaluating:  87%|████████▋ | 198/228 [00:51<00:06,  4.35it/s]step: 198
extend+tolist() time: 0.0006330013275146484
Evaluating:  87%|████████▋ | 199/228 [00:52<00:06,  4.37it/s]step: 199
extend+tolist() time: 0.0013682842254638672
Evaluating:  88%|████████▊ | 200/228 [00:52<00:07,  3.98it/s]step: 200
extend+tolist() time: 0.0011739730834960938
Evaluating:  88%|████████▊ | 201/228 [00:52<00:06,  4.11it/s]step: 201
extend+tolist() time: 0.0006167888641357422
Evaluating:  89%|████████▊ | 202/228 [00:52<00:06,  4.20it/s]step: 202
extend+tolist() time: 0.00040793418884277344
Evaluating:  89%|████████▉ | 203/228 [00:53<00:05,  4.29it/s]step: 203
extend+tolist() time: 0.0005311965942382812
Evaluating:  89%|████████▉ | 204/228 [00:53<00:05,  4.35it/s]step: 204
extend+tolist() time: 0.0008790493011474609
Evaluating:  90%|████████▉ | 205/228 [00:53<00:05,  4.40it/s]step: 205
extend+tolist() time: 0.00029015541076660156
Evaluating:  90%|█████████ | 206/228 [00:53<00:04,  4.43it/s]step: 206
extend+tolist() time: 0.0006268024444580078
Evaluating:  91%|█████████ | 207/228 [00:53<00:04,  4.41it/s]step: 207
extend+tolist() time: 0.0006566047668457031
Evaluating:  91%|█████████ | 208/228 [00:54<00:04,  4.38it/s]step: 208
extend+tolist() time: 0.00112152099609375
Evaluating:  92%|█████████▏| 209/228 [00:54<00:04,  4.38it/s]step: 209
extend+tolist() time: 0.0006132125854492188
Evaluating:  92%|█████████▏| 210/228 [00:54<00:04,  4.40it/s]step: 210
extend+tolist() time: 0.0006120204925537109
Evaluating:  93%|█████████▎| 211/228 [00:54<00:03,  4.40it/s]step: 211
extend+tolist() time: 0.0015459060668945312
Evaluating:  93%|█████████▎| 212/228 [00:55<00:03,  4.17it/s]step: 212
extend+tolist() time: 0.0012521743774414062
Evaluating:  93%|█████████▎| 213/228 [00:55<00:03,  4.19it/s]step: 213
extend+tolist() time: 0.0008044242858886719
Evaluating:  94%|█████████▍| 214/228 [00:55<00:03,  4.24it/s]step: 214
extend+tolist() time: 0.0008835792541503906
Evaluating:  94%|█████████▍| 215/228 [00:55<00:03,  4.27it/s]step: 215
extend+tolist() time: 0.0010955333709716797
Evaluating:  95%|█████████▍| 216/228 [00:56<00:02,  4.32it/s]step: 216
extend+tolist() time: 0.0005536079406738281
Evaluating:  95%|█████████▌| 217/228 [00:56<00:02,  4.33it/s]step: 217
extend+tolist() time: 0.000560760498046875
Evaluating:  96%|█████████▌| 218/228 [00:56<00:02,  4.38it/s]step: 218
extend+tolist() time: 0.001573324203491211
Evaluating:  96%|█████████▌| 219/228 [00:56<00:02,  4.28it/s]step: 219
extend+tolist() time: 0.0005028247833251953
Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.33it/s]step: 220
extend+tolist() time: 0.00040721893310546875
Evaluating:  97%|█████████▋| 221/228 [00:57<00:01,  4.38it/s]step: 221
extend+tolist() time: 0.001123666763305664
Evaluating:  97%|█████████▋| 222/228 [00:57<00:01,  4.39it/s]step: 222
extend+tolist() time: 0.0004420280456542969
Evaluating:  98%|█████████▊| 223/228 [00:57<00:01,  4.42it/s]step: 223
extend+tolist() time: 0.0004162788391113281
Evaluating:  98%|█████████▊| 224/228 [00:57<00:00,  4.41it/s]step: 224
extend+tolist() time: 0.0003814697265625
Evaluating:  99%|█████████▊| 225/228 [00:58<00:00,  4.44it/s]step: 225
extend+tolist() time: 0.0008814334869384766
Evaluating:  99%|█████████▉| 226/228 [00:58<00:00,  4.43it/s]step: 226
extend+tolist() time: 0.0005843639373779297
Evaluating: 100%|█████████▉| 227/228 [00:58<00:00,  4.45it/s]step: 227
extend+tolist() time: 0.0005025863647460938
Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.88it/s]09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:03:54 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:03:55 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:03:55 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:03:55 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:03:56 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:03:56 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:03:56 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:03:56 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:01<00:00,  3.72it/s]
09/05/2023 14:03:56 - INFO - __main__ -   Step: 3996, Validation Metrics: {'pred_1_num': 9901, 'pred_-1_num': 588, 'pred_0_num': 312, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7855754096842885, 'f1_micro': 0.7855754096842884, 'f1_macro': 0.4310766524829775, 'f1_weighted': 0.7472159770304276, 'f1_-1': 0.2785515320334262, 'f1_0': 0.1366982124079916, 'f1_1': 0.8779802130075147, 'precision_micro': 0.7855754096842885, 'precision_macro': 0.5128855316156021, 'precision_weighted': 0.7389916731869517, 'precision_-1': 0.5102040816326531, 'precision_0': 0.20833333333333334, 'precision_1': 0.8201191798808202, 'recall_micro': 0.7855754096842885, 'recall_macro': 0.4126392427139287, 'recall_weighted': 0.7855754096842885, 'recall_-1': 0.19157088122605365, 'recall_0': 0.10172143974960876, 'recall_1': 0.9446254071661238, 'roc_auc_micro': 0.9096384679308489, 'roc_auc_macro': 0.7306606243676382, 'roc_auc_weighted': 0.7177606112668589, 'roc_auc_-1': 0.816603881479822, 'roc_auc_0': 0.6722406097896394, 'roc_auc_1': 0.7031373818334531}
  0%|          | 0/66600 [00:00<?, ?it/s][2023-09-05 14:04:12,617] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 3997/66600 [00:16<04:13, 247.20it/s]09/05/2023 14:04:12 - INFO - __main__ -   Step: 3997, LR: 1.9380669038484618e-05, Loss: 0.14888739585876465
[2023-09-05 14:04:25,940] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
09/05/2023 14:04:25 - INFO - __main__ -   Step: 3998, LR: 1.9380359334290836e-05, Loss: 0.1540844440460205
  6%|▌         | 3998/66600 [00:32<04:13, 247.20it/s][2023-09-05 14:04:39,991] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 3999/66600 [00:43<14:25, 72.36it/s] 09/05/2023 14:04:39 - INFO - __main__ -   Step: 3999, LR: 1.9380049630097054e-05, Loss: 0.16469235718250275
[2023-09-05 14:04:53,560] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4000/66600 [00:57<21:37, 48.24it/s]09/05/2023 14:04:53 - INFO - __main__ -   Step: 4000, LR: 1.9379739925903275e-05, Loss: 0.10699887573719025
09/05/2023 14:04:53 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.0016558170318603516

Evaluating:   0%|          | 1/228 [00:00<01:30,  2.51it/s][Astep: 1
extend+tolist() time: 0.0009465217590332031

Evaluating:   1%|          | 2/228 [00:00<01:10,  3.20it/s][Astep: 2
extend+tolist() time: 0.0019919872283935547

Evaluating:   1%|▏         | 3/228 [00:00<01:13,  3.07it/s][Astep: 3
extend+tolist() time: 0.0017707347869873047

Evaluating:   2%|▏         | 4/228 [00:01<01:24,  2.65it/s][Astep: 4
extend+tolist() time: 0.0008578300476074219

Evaluating:   2%|▏         | 5/228 [00:01<01:13,  3.01it/s][Astep: 5
extend+tolist() time: 0.0019459724426269531

Evaluating:   3%|▎         | 6/228 [00:02<01:13,  3.02it/s][Astep: 6
extend+tolist() time: 0.001863241195678711

Evaluating:   3%|▎         | 7/228 [00:02<01:13,  3.00it/s][Astep: 7
extend+tolist() time: 0.14239025115966797

Evaluating:   4%|▎         | 8/228 [00:02<01:17,  2.84it/s][Astep: 8
extend+tolist() time: 0.0012466907501220703

Evaluating:   4%|▍         | 9/228 [00:02<01:08,  3.18it/s][Astep: 9
extend+tolist() time: 0.0007731914520263672

Evaluating:   4%|▍         | 10/228 [00:03<01:03,  3.45it/s][Astep: 10
extend+tolist() time: 0.0011622905731201172

Evaluating:   5%|▍         | 11/228 [00:03<00:59,  3.63it/s][Astep: 11
extend+tolist() time: 0.0005371570587158203

Evaluating:   5%|▌         | 12/228 [00:03<00:55,  3.86it/s][Astep: 12
extend+tolist() time: 0.0010371208190917969

Evaluating:   6%|▌         | 13/228 [00:03<00:53,  4.02it/s][Astep: 13
extend+tolist() time: 0.0005478858947753906

Evaluating:   6%|▌         | 14/228 [00:04<00:52,  4.11it/s][Astep: 14
extend+tolist() time: 0.0005273818969726562

Evaluating:   7%|▋         | 15/228 [00:04<00:50,  4.22it/s][Astep: 15
extend+tolist() time: 0.00101470947265625

Evaluating:   7%|▋         | 16/228 [00:04<00:49,  4.28it/s][Astep: 16
extend+tolist() time: 0.0005991458892822266

Evaluating:   7%|▋         | 17/228 [00:04<00:48,  4.31it/s][Astep: 17
extend+tolist() time: 0.00128936767578125

Evaluating:   8%|▊         | 18/228 [00:05<00:49,  4.27it/s][Astep: 18
extend+tolist() time: 0.0010998249053955078

Evaluating:   8%|▊         | 19/228 [00:05<00:51,  4.03it/s][Astep: 19
extend+tolist() time: 0.0013904571533203125

Evaluating:   9%|▉         | 20/228 [00:05<00:52,  3.93it/s][Astep: 20
extend+tolist() time: 0.0007529258728027344

Evaluating:   9%|▉         | 21/228 [00:05<00:51,  4.05it/s][Astep: 21
extend+tolist() time: 0.0011374950408935547

Evaluating:  10%|▉         | 22/228 [00:06<00:49,  4.15it/s][Astep: 22
extend+tolist() time: 0.0007307529449462891

Evaluating:  10%|█         | 23/228 [00:06<00:48,  4.21it/s][Astep: 23
extend+tolist() time: 0.0015375614166259766

Evaluating:  11%|█         | 24/228 [00:06<00:47,  4.25it/s][Astep: 24
extend+tolist() time: 0.0011181831359863281

Evaluating:  11%|█         | 25/228 [00:06<00:50,  4.02it/s][Astep: 25
extend+tolist() time: 0.001961946487426758

Evaluating:  11%|█▏        | 26/228 [00:07<00:55,  3.62it/s][Astep: 26
extend+tolist() time: 0.00103759765625

Evaluating:  12%|█▏        | 27/228 [00:07<00:52,  3.81it/s][Astep: 27
extend+tolist() time: 0.0015861988067626953

Evaluating:  12%|█▏        | 28/228 [00:07<00:54,  3.64it/s][Astep: 28
extend+tolist() time: 0.0003535747528076172

Evaluating:  13%|█▎        | 29/228 [00:07<00:51,  3.88it/s][Astep: 29
extend+tolist() time: 0.0006947517395019531

Evaluating:  13%|█▎        | 30/228 [00:08<00:49,  4.01it/s][Astep: 30
extend+tolist() time: 0.0015215873718261719

Evaluating:  14%|█▎        | 31/228 [00:08<00:51,  3.86it/s][Astep: 31
extend+tolist() time: 0.0005559921264648438

Evaluating:  14%|█▍        | 32/228 [00:08<00:48,  4.04it/s][Astep: 32
extend+tolist() time: 0.0013704299926757812

Evaluating:  14%|█▍        | 33/228 [00:08<00:49,  3.92it/s][Astep: 33
extend+tolist() time: 0.0016109943389892578

Evaluating:  15%|█▍        | 34/228 [00:09<01:01,  3.17it/s][Astep: 34
extend+tolist() time: 0.0008165836334228516

Evaluating:  15%|█▌        | 35/228 [00:09<00:56,  3.39it/s][Astep: 35
extend+tolist() time: 0.0011105537414550781

Evaluating:  16%|█▌        | 36/228 [00:09<00:52,  3.65it/s][Astep: 36
extend+tolist() time: 0.0007841587066650391

Evaluating:  16%|█▌        | 37/228 [00:10<00:49,  3.83it/s][Astep: 37
extend+tolist() time: 0.001674652099609375

Evaluating:  17%|█▋        | 38/228 [00:10<00:52,  3.65it/s][Astep: 38
extend+tolist() time: 0.001135110855102539

Evaluating:  17%|█▋        | 39/228 [00:10<00:49,  3.83it/s][Astep: 39
extend+tolist() time: 0.0007104873657226562

Evaluating:  18%|█▊        | 40/228 [00:10<00:47,  3.99it/s][Astep: 40
extend+tolist() time: 0.001070261001586914

Evaluating:  18%|█▊        | 41/228 [00:11<00:45,  4.08it/s][Astep: 41
extend+tolist() time: 0.19450640678405762

Evaluating:  18%|█▊        | 42/228 [00:11<00:56,  3.30it/s][Astep: 42
extend+tolist() time: 0.0017004013061523438

Evaluating:  19%|█▉        | 43/228 [00:11<00:56,  3.30it/s][Astep: 43
extend+tolist() time: 0.0019183158874511719

Evaluating:  19%|█▉        | 44/228 [00:12<00:57,  3.19it/s][Astep: 44
extend+tolist() time: 0.001043558120727539

Evaluating:  20%|█▉        | 45/228 [00:12<00:52,  3.47it/s][Astep: 45
extend+tolist() time: 0.0012400150299072266

Evaluating:  20%|██        | 46/228 [00:12<00:53,  3.42it/s][Astep: 46
extend+tolist() time: 0.0016865730285644531

Evaluating:  21%|██        | 47/228 [00:12<00:53,  3.36it/s][Astep: 47
extend+tolist() time: 0.001407623291015625

Evaluating:  21%|██        | 48/228 [00:13<00:52,  3.42it/s][Astep: 48
extend+tolist() time: 0.0015382766723632812

Evaluating:  21%|██▏       | 49/228 [00:13<00:52,  3.38it/s][Astep: 49
extend+tolist() time: 0.0013391971588134766

Evaluating:  22%|██▏       | 50/228 [00:13<00:50,  3.53it/s][Astep: 50
extend+tolist() time: 0.0014481544494628906

Evaluating:  22%|██▏       | 51/228 [00:14<00:51,  3.45it/s][Astep: 51
extend+tolist() time: 0.0016200542449951172

Evaluating:  23%|██▎       | 52/228 [00:14<00:51,  3.41it/s][Astep: 52
extend+tolist() time: 0.0013432502746582031

Evaluating:  23%|██▎       | 53/228 [00:14<00:50,  3.48it/s][Astep: 53
extend+tolist() time: 0.0016326904296875

Evaluating:  24%|██▎       | 54/228 [00:15<00:50,  3.42it/s][Astep: 54
extend+tolist() time: 0.0008182525634765625

Evaluating:  24%|██▍       | 55/228 [00:15<00:47,  3.62it/s][Astep: 55
extend+tolist() time: 0.001157999038696289

Evaluating:  25%|██▍       | 56/228 [00:15<00:45,  3.80it/s][Astep: 56
extend+tolist() time: 0.0011894702911376953

Evaluating:  25%|██▌       | 57/228 [00:15<00:47,  3.64it/s][Astep: 57
extend+tolist() time: 0.0010752677917480469

Evaluating:  25%|██▌       | 58/228 [00:15<00:44,  3.86it/s][Astep: 58
extend+tolist() time: 0.0008633136749267578

Evaluating:  26%|██▌       | 59/228 [00:16<00:43,  3.92it/s][Astep: 59
extend+tolist() time: 0.0013587474822998047

Evaluating:  26%|██▋       | 60/228 [00:16<00:42,  3.94it/s][Astep: 60
extend+tolist() time: 0.0007271766662597656

Evaluating:  27%|██▋       | 61/228 [00:16<00:41,  4.04it/s][Astep: 61
extend+tolist() time: 0.0013158321380615234

Evaluating:  27%|██▋       | 62/228 [00:16<00:40,  4.06it/s][Astep: 62
extend+tolist() time: 0.0008132457733154297

Evaluating:  28%|██▊       | 63/228 [00:17<00:39,  4.14it/s][Astep: 63
extend+tolist() time: 0.0012001991271972656

Evaluating:  28%|██▊       | 64/228 [00:17<00:39,  4.17it/s][Astep: 64
extend+tolist() time: 0.0007867813110351562

Evaluating:  29%|██▊       | 65/228 [00:17<00:38,  4.20it/s][Astep: 65
extend+tolist() time: 0.0012280941009521484

Evaluating:  29%|██▉       | 66/228 [00:17<00:38,  4.21it/s][Astep: 66
extend+tolist() time: 0.0007443428039550781

Evaluating:  29%|██▉       | 67/228 [00:18<00:47,  3.37it/s][Astep: 67
extend+tolist() time: 0.0013132095336914062

Evaluating:  30%|██▉       | 68/228 [00:18<00:44,  3.56it/s][Astep: 68
extend+tolist() time: 0.0007302761077880859

Evaluating:  30%|███       | 69/228 [00:18<00:42,  3.78it/s][Astep: 69
extend+tolist() time: 0.0015244483947753906

Evaluating:  31%|███       | 70/228 [00:19<00:42,  3.72it/s][Astep: 70
extend+tolist() time: 0.0013883113861083984

Evaluating:  31%|███       | 71/228 [00:19<00:42,  3.68it/s][Astep: 71
extend+tolist() time: 0.0009279251098632812

Evaluating:  32%|███▏      | 72/228 [00:19<00:42,  3.67it/s][Astep: 72
extend+tolist() time: 0.0007889270782470703

Evaluating:  32%|███▏      | 73/228 [00:19<00:40,  3.85it/s][Astep: 73
extend+tolist() time: 0.0005259513854980469

Evaluating:  32%|███▏      | 74/228 [00:20<00:38,  4.01it/s][Astep: 74
extend+tolist() time: 0.0011973381042480469

Evaluating:  33%|███▎      | 75/228 [00:20<00:37,  4.09it/s][Astep: 75
extend+tolist() time: 0.18473172187805176

Evaluating:  33%|███▎      | 76/228 [00:20<00:48,  3.15it/s][Astep: 76
extend+tolist() time: 0.0006215572357177734

Evaluating:  34%|███▍      | 77/228 [00:21<00:43,  3.43it/s][Astep: 77
extend+tolist() time: 0.0019142627716064453

Evaluating:  34%|███▍      | 78/228 [00:21<00:45,  3.28it/s][Astep: 78
extend+tolist() time: 0.0011470317840576172

Evaluating:  35%|███▍      | 79/228 [00:21<00:42,  3.48it/s][Astep: 79
extend+tolist() time: 0.0008697509765625

Evaluating:  35%|███▌      | 80/228 [00:21<00:40,  3.62it/s][Astep: 80
extend+tolist() time: 0.0009143352508544922

Evaluating:  36%|███▌      | 81/228 [00:22<00:39,  3.73it/s][Astep: 81
extend+tolist() time: 0.0012662410736083984

Evaluating:  36%|███▌      | 82/228 [00:22<00:38,  3.84it/s][Astep: 82
extend+tolist() time: 0.0008447170257568359

Evaluating:  36%|███▋      | 83/228 [00:22<00:36,  3.92it/s][Astep: 83
extend+tolist() time: 0.0010967254638671875

Evaluating:  37%|███▋      | 84/228 [00:22<00:35,  4.05it/s][Astep: 84
extend+tolist() time: 0.0011739730834960938

Evaluating:  37%|███▋      | 85/228 [00:23<00:36,  3.93it/s][Astep: 85
extend+tolist() time: 0.0015442371368408203

Evaluating:  38%|███▊      | 86/228 [00:23<00:36,  3.94it/s][Astep: 86
extend+tolist() time: 0.0010249614715576172

Evaluating:  38%|███▊      | 87/228 [00:23<00:35,  3.98it/s][Astep: 87
extend+tolist() time: 0.0010120868682861328

Evaluating:  39%|███▊      | 88/228 [00:23<00:35,  3.98it/s][Astep: 88
extend+tolist() time: 0.0013511180877685547

Evaluating:  39%|███▉      | 89/228 [00:24<00:34,  4.08it/s][Astep: 89
extend+tolist() time: 0.0008628368377685547

Evaluating:  39%|███▉      | 90/228 [00:24<00:33,  4.15it/s][Astep: 90
extend+tolist() time: 0.0014300346374511719

Evaluating:  40%|███▉      | 91/228 [00:24<00:33,  4.11it/s][Astep: 91
extend+tolist() time: 0.0008513927459716797

Evaluating:  40%|████      | 92/228 [00:24<00:32,  4.18it/s][Astep: 92
extend+tolist() time: 0.0013701915740966797

Evaluating:  41%|████      | 93/228 [00:25<00:31,  4.22it/s][Astep: 93
extend+tolist() time: 0.0011534690856933594

Evaluating:  41%|████      | 94/228 [00:25<00:33,  4.05it/s][Astep: 94
extend+tolist() time: 0.0012357234954833984

Evaluating:  42%|████▏     | 95/228 [00:25<00:31,  4.16it/s][Astep: 95
extend+tolist() time: 0.001375436782836914

Evaluating:  42%|████▏     | 96/228 [00:25<00:33,  3.89it/s][Astep: 96
extend+tolist() time: 0.001617431640625

Evaluating:  43%|████▎     | 97/228 [00:26<00:33,  3.92it/s][Astep: 97
extend+tolist() time: 0.0013470649719238281

Evaluating:  43%|████▎     | 98/228 [00:26<00:32,  3.99it/s][Astep: 98
extend+tolist() time: 0.00103759765625

Evaluating:  43%|████▎     | 99/228 [00:26<00:32,  3.99it/s][Astep: 99
extend+tolist() time: 0.0014369487762451172

Evaluating:  44%|████▍     | 100/228 [00:26<00:32,  4.00it/s][Astep: 100
extend+tolist() time: 0.0008745193481445312

Evaluating:  44%|████▍     | 101/228 [00:27<00:30,  4.12it/s][Astep: 101
extend+tolist() time: 0.0014052391052246094

Evaluating:  45%|████▍     | 102/228 [00:27<00:30,  4.12it/s][Astep: 102
extend+tolist() time: 0.0008642673492431641

Evaluating:  45%|████▌     | 103/228 [00:27<00:29,  4.19it/s][Astep: 103
extend+tolist() time: 0.0012466907501220703

Evaluating:  46%|████▌     | 104/228 [00:27<00:29,  4.25it/s][Astep: 104
extend+tolist() time: 0.0007951259613037109

Evaluating:  46%|████▌     | 105/228 [00:27<00:28,  4.29it/s][Astep: 105
extend+tolist() time: 0.0014238357543945312

Evaluating:  46%|████▋     | 106/228 [00:28<00:28,  4.24it/s][Astep: 106
extend+tolist() time: 0.0016026496887207031

Evaluating:  47%|████▋     | 107/228 [00:28<00:31,  3.83it/s][Astep: 107
extend+tolist() time: 0.000835418701171875

Evaluating:  47%|████▋     | 108/228 [00:28<00:30,  3.97it/s][Astep: 108
extend+tolist() time: 0.0013852119445800781

Evaluating:  48%|████▊     | 109/228 [00:29<00:29,  4.06it/s][Astep: 109
extend+tolist() time: 0.0010044574737548828

Evaluating:  48%|████▊     | 110/228 [00:29<00:28,  4.07it/s][Astep: 110
extend+tolist() time: 0.0007233619689941406

Evaluating:  49%|████▊     | 111/228 [00:29<00:28,  4.16it/s][Astep: 111
extend+tolist() time: 0.0020678043365478516

Evaluating:  49%|████▉     | 112/228 [00:29<00:30,  3.78it/s][Astep: 112
extend+tolist() time: 0.0008013248443603516

Evaluating:  50%|████▉     | 113/228 [00:30<00:36,  3.15it/s][Astep: 113
extend+tolist() time: 0.0007903575897216797

Evaluating:  50%|█████     | 114/228 [00:30<00:33,  3.44it/s][Astep: 114
extend+tolist() time: 0.0017006397247314453

Evaluating:  50%|█████     | 115/228 [00:30<00:32,  3.48it/s][Astep: 115
extend+tolist() time: 0.0007488727569580078

Evaluating:  51%|█████     | 116/228 [00:30<00:30,  3.70it/s][Astep: 116
extend+tolist() time: 0.0015175342559814453

Evaluating:  51%|█████▏    | 117/228 [00:31<00:28,  3.84it/s][Astep: 117
extend+tolist() time: 0.0009999275207519531

Evaluating:  52%|█████▏    | 118/228 [00:31<00:28,  3.92it/s][Astep: 118
extend+tolist() time: 0.0010673999786376953

Evaluating:  52%|█████▏    | 119/228 [00:31<00:26,  4.06it/s][Astep: 119
extend+tolist() time: 0.0007891654968261719

Evaluating:  53%|█████▎    | 120/228 [00:31<00:25,  4.15it/s][Astep: 120
extend+tolist() time: 0.0007352828979492188

Evaluating:  53%|█████▎    | 121/228 [00:32<00:25,  4.23it/s][Astep: 121
extend+tolist() time: 0.0012629032135009766

Evaluating:  54%|█████▎    | 122/228 [00:32<00:24,  4.27it/s][Astep: 122
extend+tolist() time: 0.0008389949798583984

Evaluating:  54%|█████▍    | 123/228 [00:32<00:24,  4.30it/s][Astep: 123
extend+tolist() time: 0.0010714530944824219

Evaluating:  54%|█████▍    | 124/228 [00:32<00:24,  4.33it/s][Astep: 124
extend+tolist() time: 0.0009663105010986328

Evaluating:  55%|█████▍    | 125/228 [00:33<00:23,  4.31it/s][Astep: 125
extend+tolist() time: 0.0005035400390625

Evaluating:  55%|█████▌    | 126/228 [00:33<00:23,  4.35it/s][Astep: 126
extend+tolist() time: 0.22887349128723145

Evaluating:  56%|█████▌    | 127/228 [00:33<00:32,  3.07it/s][Astep: 127
extend+tolist() time: 0.0020852088928222656

Evaluating:  56%|█████▌    | 128/228 [00:34<00:32,  3.06it/s][Astep: 128
extend+tolist() time: 0.0011649131774902344

Evaluating:  57%|█████▋    | 129/228 [00:34<00:29,  3.36it/s][Astep: 129
extend+tolist() time: 0.0009043216705322266

Evaluating:  57%|█████▋    | 130/228 [00:34<00:27,  3.61it/s][Astep: 130
extend+tolist() time: 0.0014600753784179688

Evaluating:  57%|█████▋    | 131/228 [00:34<00:26,  3.71it/s][Astep: 131
extend+tolist() time: 0.0004832744598388672

Evaluating:  58%|█████▊    | 132/228 [00:35<00:24,  3.93it/s][Astep: 132
extend+tolist() time: 0.0016794204711914062

Evaluating:  58%|█████▊    | 133/228 [00:35<00:24,  3.81it/s][Astep: 133
extend+tolist() time: 0.00046706199645996094

Evaluating:  59%|█████▉    | 134/228 [00:35<00:23,  4.00it/s][Astep: 134
extend+tolist() time: 0.0015499591827392578

Evaluating:  59%|█████▉    | 135/228 [00:35<00:23,  3.90it/s][Astep: 135
extend+tolist() time: 0.0004916191101074219

Evaluating:  60%|█████▉    | 136/228 [00:36<00:22,  4.08it/s][Astep: 136
extend+tolist() time: 0.0014197826385498047

Evaluating:  60%|██████    | 137/228 [00:36<00:22,  4.13it/s][Astep: 137
extend+tolist() time: 0.00043511390686035156

Evaluating:  61%|██████    | 138/228 [00:36<00:21,  4.25it/s][Astep: 138
extend+tolist() time: 0.0008072853088378906

Evaluating:  61%|██████    | 139/228 [00:36<00:20,  4.29it/s][Astep: 139
extend+tolist() time: 0.0005166530609130859

Evaluating:  61%|██████▏   | 140/228 [00:36<00:20,  4.35it/s][Astep: 140
extend+tolist() time: 0.0017046928405761719

Evaluating:  62%|██████▏   | 141/228 [00:37<00:20,  4.33it/s][Astep: 141
extend+tolist() time: 0.0009324550628662109

Evaluating:  62%|██████▏   | 142/228 [00:37<00:19,  4.34it/s][Astep: 142
extend+tolist() time: 0.0010709762573242188

Evaluating:  63%|██████▎   | 143/228 [00:37<00:19,  4.38it/s][Astep: 143
extend+tolist() time: 0.0003795623779296875

Evaluating:  63%|██████▎   | 144/228 [00:37<00:18,  4.42it/s][Astep: 144
extend+tolist() time: 0.0007925033569335938

Evaluating:  64%|██████▎   | 145/228 [00:38<00:18,  4.41it/s][Astep: 145
extend+tolist() time: 0.000990152359008789

Evaluating:  64%|██████▍   | 146/228 [00:38<00:18,  4.43it/s][Astep: 146
extend+tolist() time: 0.0004494190216064453

Evaluating:  64%|██████▍   | 147/228 [00:38<00:18,  4.46it/s][Astep: 147
extend+tolist() time: 0.0008556842803955078

Evaluating:  65%|██████▍   | 148/228 [00:38<00:17,  4.45it/s][Astep: 148
extend+tolist() time: 0.0011508464813232422

Evaluating:  65%|██████▌   | 149/228 [00:39<00:17,  4.46it/s][Astep: 149
extend+tolist() time: 0.00047516822814941406

Evaluating:  66%|██████▌   | 150/228 [00:39<00:17,  4.46it/s][Astep: 150
extend+tolist() time: 0.0009472370147705078

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.40it/s][Astep: 151
extend+tolist() time: 0.0010716915130615234

Evaluating:  67%|██████▋   | 152/228 [00:39<00:17,  4.44it/s][Astep: 152
extend+tolist() time: 0.0009448528289794922

Evaluating:  67%|██████▋   | 153/228 [00:39<00:16,  4.42it/s][Astep: 153
extend+tolist() time: 0.0014843940734863281

Evaluating:  68%|██████▊   | 154/228 [00:40<00:17,  4.32it/s][Astep: 154
extend+tolist() time: 0.002030611038208008

Evaluating:  68%|██████▊   | 155/228 [00:40<00:19,  3.84it/s][Astep: 155
extend+tolist() time: 0.0006856918334960938

Evaluating:  68%|██████▊   | 156/228 [00:40<00:18,  4.00it/s][Astep: 156
extend+tolist() time: 0.0005784034729003906

Evaluating:  69%|██████▉   | 157/228 [00:40<00:17,  4.14it/s][Astep: 157
extend+tolist() time: 0.0011641979217529297

Evaluating:  69%|██████▉   | 158/228 [00:41<00:16,  4.23it/s][Astep: 158
extend+tolist() time: 0.0005125999450683594

Evaluating:  70%|██████▉   | 159/228 [00:41<00:16,  4.30it/s][Astep: 159
extend+tolist() time: 0.0007488727569580078

Evaluating:  70%|███████   | 160/228 [00:41<00:15,  4.33it/s][Astep: 160
extend+tolist() time: 0.0008020401000976562

Evaluating:  71%|███████   | 161/228 [00:41<00:15,  4.41it/s][Astep: 161
extend+tolist() time: 0.0008046627044677734

Evaluating:  71%|███████   | 162/228 [00:42<00:14,  4.43it/s][Astep: 162
extend+tolist() time: 0.0005292892456054688

Evaluating:  71%|███████▏  | 163/228 [00:42<00:14,  4.44it/s][Astep: 163
extend+tolist() time: 0.00041747093200683594

Evaluating:  72%|███████▏  | 164/228 [00:42<00:14,  4.47it/s][Astep: 164
extend+tolist() time: 0.0015790462493896484

Evaluating:  72%|███████▏  | 165/228 [00:42<00:14,  4.47it/s][Astep: 165
extend+tolist() time: 0.0004622936248779297

Evaluating:  73%|███████▎  | 166/228 [00:42<00:13,  4.50it/s][Astep: 166
extend+tolist() time: 0.00042700767517089844

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.52it/s][Astep: 167
extend+tolist() time: 0.0005846023559570312

Evaluating:  74%|███████▎  | 168/228 [00:43<00:13,  4.50it/s][Astep: 168
extend+tolist() time: 0.0016939640045166016

Evaluating:  74%|███████▍  | 169/228 [00:43<00:14,  4.21it/s][Astep: 169
extend+tolist() time: 0.00037360191345214844

Evaluating:  75%|███████▍  | 170/228 [00:43<00:13,  4.32it/s][Astep: 170
extend+tolist() time: 0.001312255859375

Evaluating:  75%|███████▌  | 171/228 [00:44<00:13,  4.33it/s][Astep: 171
extend+tolist() time: 0.0003273487091064453

Evaluating:  75%|███████▌  | 172/228 [00:44<00:12,  4.37it/s][Astep: 172
extend+tolist() time: 0.0008301734924316406

Evaluating:  76%|███████▌  | 173/228 [00:44<00:12,  4.38it/s][Astep: 173
extend+tolist() time: 0.0016109943389892578

Evaluating:  76%|███████▋  | 174/228 [00:44<00:13,  4.15it/s][Astep: 174
extend+tolist() time: 0.0018510818481445312

Evaluating:  77%|███████▋  | 175/228 [00:45<00:13,  3.86it/s][Astep: 175
extend+tolist() time: 0.0008587837219238281

Evaluating:  77%|███████▋  | 176/228 [00:45<00:13,  3.99it/s][Astep: 176
extend+tolist() time: 0.0010900497436523438

Evaluating:  78%|███████▊  | 177/228 [00:45<00:12,  4.12it/s][Astep: 177
extend+tolist() time: 0.0005474090576171875

Evaluating:  78%|███████▊  | 178/228 [00:45<00:11,  4.23it/s][Astep: 178
extend+tolist() time: 0.0016498565673828125

Evaluating:  79%|███████▊  | 179/228 [00:46<00:12,  4.02it/s][Astep: 179
extend+tolist() time: 0.00041294097900390625

Evaluating:  79%|███████▉  | 180/228 [00:46<00:11,  4.15it/s][Astep: 180
extend+tolist() time: 0.0004191398620605469

Evaluating:  79%|███████▉  | 181/228 [00:46<00:11,  4.27it/s][Astep: 181
extend+tolist() time: 0.0006535053253173828

Evaluating:  80%|███████▉  | 182/228 [00:46<00:10,  4.33it/s][Astep: 182
extend+tolist() time: 0.001196146011352539

Evaluating:  80%|████████  | 183/228 [00:46<00:10,  4.37it/s][Astep: 183
extend+tolist() time: 0.0006563663482666016

Evaluating:  81%|████████  | 184/228 [00:47<00:10,  4.39it/s][Astep: 184
extend+tolist() time: 0.00046896934509277344

Evaluating:  81%|████████  | 185/228 [00:47<00:09,  4.42it/s][Astep: 185
extend+tolist() time: 0.0016095638275146484

Evaluating:  82%|████████▏ | 186/228 [00:47<00:10,  4.18it/s][Astep: 186
extend+tolist() time: 0.0014185905456542969

Evaluating:  82%|████████▏ | 187/228 [00:47<00:09,  4.16it/s][Astep: 187
extend+tolist() time: 0.0004830360412597656

Evaluating:  82%|████████▏ | 188/228 [00:48<00:12,  3.28it/s][Astep: 188
extend+tolist() time: 0.0007250308990478516

Evaluating:  83%|████████▎ | 189/228 [00:48<00:10,  3.56it/s][Astep: 189
extend+tolist() time: 0.000339508056640625

Evaluating:  83%|████████▎ | 190/228 [00:48<00:09,  3.80it/s][Astep: 190
extend+tolist() time: 0.0016565322875976562

Evaluating:  84%|████████▍ | 191/228 [00:49<00:09,  3.76it/s][Astep: 191
extend+tolist() time: 0.0007498264312744141

Evaluating:  84%|████████▍ | 192/228 [00:49<00:09,  3.91it/s][Astep: 192
extend+tolist() time: 0.0008742809295654297

Evaluating:  85%|████████▍ | 193/228 [00:49<00:08,  4.06it/s][Astep: 193
extend+tolist() time: 0.0010628700256347656

Evaluating:  85%|████████▌ | 194/228 [00:49<00:08,  4.05it/s][Astep: 194
extend+tolist() time: 0.0006039142608642578

Evaluating:  86%|████████▌ | 195/228 [00:50<00:07,  4.16it/s][Astep: 195
extend+tolist() time: 0.0005819797515869141

Evaluating:  86%|████████▌ | 196/228 [00:50<00:07,  4.26it/s][Astep: 196
extend+tolist() time: 0.0006623268127441406

Evaluating:  86%|████████▋ | 197/228 [00:50<00:07,  4.30it/s][Astep: 197
extend+tolist() time: 0.0006921291351318359

Evaluating:  87%|████████▋ | 198/228 [00:50<00:06,  4.35it/s][Astep: 198
extend+tolist() time: 0.0011353492736816406

Evaluating:  87%|████████▋ | 199/228 [00:50<00:06,  4.39it/s][Astep: 199
extend+tolist() time: 0.0018932819366455078

Evaluating:  88%|████████▊ | 200/228 [00:51<00:07,  3.99it/s][Astep: 200
extend+tolist() time: 0.0007126331329345703

Evaluating:  88%|████████▊ | 201/228 [00:51<00:06,  4.11it/s][Astep: 201
extend+tolist() time: 0.0005967617034912109

Evaluating:  89%|████████▊ | 202/228 [00:51<00:06,  4.24it/s][Astep: 202
extend+tolist() time: 0.0004582405090332031

Evaluating:  89%|████████▉ | 203/228 [00:51<00:05,  4.32it/s][Astep: 203
extend+tolist() time: 0.0010204315185546875

Evaluating:  89%|████████▉ | 204/228 [00:52<00:05,  4.39it/s][Astep: 204
extend+tolist() time: 0.0004303455352783203

Evaluating:  90%|████████▉ | 205/228 [00:52<00:05,  4.42it/s][Astep: 205
extend+tolist() time: 0.0003027915954589844

Evaluating:  90%|█████████ | 206/228 [00:52<00:04,  4.46it/s][Astep: 206
extend+tolist() time: 0.0006356239318847656

Evaluating:  91%|█████████ | 207/228 [00:52<00:04,  4.47it/s][Astep: 207
extend+tolist() time: 0.0011150836944580078

Evaluating:  91%|█████████ | 208/228 [00:53<00:04,  4.49it/s][Astep: 208
extend+tolist() time: 0.0009937286376953125

Evaluating:  92%|█████████▏| 209/228 [00:53<00:04,  4.41it/s][Astep: 209
extend+tolist() time: 0.0005886554718017578

Evaluating:  92%|█████████▏| 210/228 [00:53<00:04,  4.38it/s][Astep: 210
extend+tolist() time: 0.0010197162628173828

Evaluating:  93%|█████████▎| 211/228 [00:53<00:03,  4.41it/s][Astep: 211
extend+tolist() time: 0.0011606216430664062

Evaluating:  93%|█████████▎| 212/228 [00:53<00:03,  4.17it/s][Astep: 212
extend+tolist() time: 0.28520703315734863

Evaluating:  93%|█████████▎| 213/228 [00:54<00:04,  3.10it/s][Astep: 213
extend+tolist() time: 0.0007877349853515625

Evaluating:  94%|█████████▍| 214/228 [00:54<00:04,  3.38it/s][Astep: 214
extend+tolist() time: 0.0008356571197509766

Evaluating:  94%|█████████▍| 215/228 [00:54<00:03,  3.61it/s][Astep: 215
extend+tolist() time: 0.0006754398345947266

Evaluating:  95%|█████████▍| 216/228 [00:55<00:03,  3.81it/s][Astep: 216
extend+tolist() time: 0.000972747802734375

Evaluating:  95%|█████████▌| 217/228 [00:55<00:02,  3.97it/s][Astep: 217
extend+tolist() time: 0.0005204677581787109

Evaluating:  96%|█████████▌| 218/228 [00:55<00:02,  4.08it/s][Astep: 218
extend+tolist() time: 0.0014369487762451172

Evaluating:  96%|█████████▌| 219/228 [00:55<00:02,  4.08it/s][Astep: 219
extend+tolist() time: 0.0005326271057128906

Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.19it/s][Astep: 220
extend+tolist() time: 0.0004544258117675781

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.24it/s][Astep: 221
extend+tolist() time: 0.0011224746704101562

Evaluating:  97%|█████████▋| 222/228 [00:56<00:01,  4.29it/s][Astep: 222
extend+tolist() time: 0.00044226646423339844

Evaluating:  98%|█████████▊| 223/228 [00:56<00:01,  4.37it/s][Astep: 223
extend+tolist() time: 0.00037932395935058594

Evaluating:  98%|█████████▊| 224/228 [00:56<00:00,  4.43it/s][Astep: 224
extend+tolist() time: 0.00039267539978027344

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.46it/s][Astep: 225
extend+tolist() time: 0.0004634857177734375

Evaluating:  99%|█████████▉| 226/228 [00:57<00:00,  4.48it/s][Astep: 226
extend+tolist() time: 0.0010492801666259766

Evaluating: 100%|█████████▉| 227/228 [00:57<00:00,  4.49it/s][Astep: 227
extend+tolist() time: 0.0004787445068359375

Evaluating: 100%|██████████| 228/228 [00:57<00:00,  3.90it/s][A09/05/2023 14:05:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 14:05:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:05:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:05:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:05:53 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-74c6e6ad-f8f8-485e-85bc-38de38ae94ab-1-0.arrow
09/05/2023 14:05:53 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:05:53 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:05:53 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:00<00:00,  3.78it/s]
09/05/2023 14:05:53 - INFO - __main__ -   Step: 4000, Validation Metrics: {'pred_1_num': 9464, 'pred_-1_num': 908, 'pred_0_num': 429, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7767799277844644, 'f1_micro': 0.7767799277844644, 'f1_macro': 0.4663091663061456, 'f1_weighted': 0.7542520970882287, 'f1_-1': 0.35246564268391267, 'f1_0': 0.17602996254681647, 'f1_1': 0.8704318936877077, 'precision_micro': 0.7767799277844644, 'precision_macro': 0.5099353562586365, 'precision_weighted': 0.7435498897496577, 'precision_-1': 0.4801762114537445, 'precision_0': 0.2191142191142191, 'precision_1': 0.8305156382079459, 'recall_micro': 0.7767799277844644, 'recall_macro': 0.44663332651345344, 'recall_weighted': 0.7767799277844644, 'recall_-1': 0.2784163473818646, 'recall_0': 0.14710485133020346, 'recall_1': 0.9143787808282923, 'roc_auc_micro': 0.9091285657908981, 'roc_auc_macro': 0.7375656504004403, 'roc_auc_weighted': 0.7268934171990218, 'roc_auc_-1': 0.8181602695614233, 'roc_auc_0': 0.6808471155389112, 'roc_auc_1': 0.7136895661009867}
[2023-09-05 14:06:08,332] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4001/66600 [02:11<1:18:24, 13.31it/s]09/05/2023 14:06:08 - INFO - __main__ -   Step: 4001, LR: 1.9379430221709493e-05, Loss: 0.1820152848958969
[2023-09-05 14:06:22,961] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4002/66600 [02:26<1:34:10, 11.08it/s]09/05/2023 14:06:22 - INFO - __main__ -   Step: 4002, LR: 1.937912051751571e-05, Loss: 0.11588536202907562
[2023-09-05 14:06:37,096] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4003/66600 [02:40<1:55:53,  9.00it/s]09/05/2023 14:06:37 - INFO - __main__ -   Step: 4003, LR: 1.9378810813321926e-05, Loss: 0.12177518010139465
[2023-09-05 14:06:49,666] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4004/66600 [02:53<2:23:20,  7.28it/s]09/05/2023 14:06:49 - INFO - __main__ -   Step: 4004, LR: 1.9378501109128144e-05, Loss: 0.12032757699489594
[2023-09-05 14:07:05,017] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4005/66600 [03:08<3:11:06,  5.46it/s]09/05/2023 14:07:05 - INFO - __main__ -   Step: 4005, LR: 1.9378191404934362e-05, Loss: 0.14536304771900177
[2023-09-05 14:07:18,555] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4006/66600 [03:22<4:10:43,  4.16it/s]09/05/2023 14:07:18 - INFO - __main__ -   Step: 4006, LR: 1.937788170074058e-05, Loss: 0.16644898056983948
[2023-09-05 14:07:32,460] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4007/66600 [03:36<5:37:21,  3.09it/s]09/05/2023 14:07:32 - INFO - __main__ -   Step: 4007, LR: 1.93775719965468e-05, Loss: 0.11678099632263184
[2023-09-05 14:07:46,219] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4008/66600 [03:49<7:37:59,  2.28it/s]09/05/2023 14:07:46 - INFO - __main__ -   Step: 4008, LR: 1.937726229235302e-05, Loss: 0.12773950397968292
[2023-09-05 14:07:59,917] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4009/66600 [04:03<10:25:58,  1.67it/s]09/05/2023 14:07:59 - INFO - __main__ -   Step: 4009, LR: 1.9376952588159238e-05, Loss: 0.11474424600601196
[2023-09-05 14:08:13,591] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4010/66600 [04:17<14:18:35,  1.21it/s]09/05/2023 14:08:13 - INFO - __main__ -   Step: 4010, LR: 1.9376642883965452e-05, Loss: 0.12756985425949097
[2023-09-05 14:08:27,778] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4011/66600 [04:31<19:50:09,  1.14s/it]09/05/2023 14:08:27 - INFO - __main__ -   Step: 4011, LR: 1.937633317977167e-05, Loss: 0.12889964878559113
[2023-09-05 14:08:42,049] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4012/66600 [04:45<27:20:15,  1.57s/it]09/05/2023 14:08:42 - INFO - __main__ -   Step: 4012, LR: 1.937602347557789e-05, Loss: 0.09954595565795898
[2023-09-05 14:08:55,540] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4013/66600 [04:59<36:37:44,  2.11s/it]09/05/2023 14:08:55 - INFO - __main__ -   Step: 4013, LR: 1.9375713771384106e-05, Loss: 0.11053310334682465
[2023-09-05 14:09:09,955] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4014/66600 [05:13<49:30:35,  2.85s/it]09/05/2023 14:09:09 - INFO - __main__ -   Step: 4014, LR: 1.9375404067190328e-05, Loss: 0.0864570364356041
[2023-09-05 14:09:25,320] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4015/66600 [05:28<66:44:34,  3.84s/it]09/05/2023 14:09:25 - INFO - __main__ -   Step: 4015, LR: 1.9375094362996546e-05, Loss: 0.16876676678657532
[2023-09-05 14:09:40,337] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4016/66600 [05:43<86:29:32,  4.98s/it]09/05/2023 14:09:40 - INFO - __main__ -   Step: 4016, LR: 1.9374784658802764e-05, Loss: 0.14587098360061646
[2023-09-05 14:09:55,780] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4017/66600 [05:59<109:33:46,  6.30s/it]09/05/2023 14:09:55 - INFO - __main__ -   Step: 4017, LR: 1.9374474954608982e-05, Loss: 0.13680823147296906
[2023-09-05 14:10:09,621] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4018/66600 [06:13<129:39:18,  7.46s/it]09/05/2023 14:10:09 - INFO - __main__ -   Step: 4018, LR: 1.9374165250415197e-05, Loss: 0.10244456678628922
[2023-09-05 14:10:25,409] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4019/66600 [06:28<155:40:32,  8.96s/it]09/05/2023 14:10:25 - INFO - __main__ -   Step: 4019, LR: 1.9373855546221415e-05, Loss: 0.10727329552173615
[2023-09-05 14:10:40,377] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4020/66600 [06:43<177:01:17, 10.18s/it]09/05/2023 14:10:40 - INFO - __main__ -   Step: 4020, LR: 1.9373545842027633e-05, Loss: 0.11810692399740219
[2023-09-05 14:10:54,274] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4021/66600 [06:57<191:36:15, 11.02s/it]09/05/2023 14:10:54 - INFO - __main__ -   Step: 4021, LR: 1.9373236137833854e-05, Loss: 0.13227415084838867
[2023-09-05 14:11:07,788] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4022/66600 [07:11<202:10:01, 11.63s/it]09/05/2023 14:11:07 - INFO - __main__ -   Step: 4022, LR: 1.9372926433640072e-05, Loss: 0.14334356784820557
[2023-09-05 14:11:22,653] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4023/66600 [07:26<216:41:39, 12.47s/it]09/05/2023 14:11:22 - INFO - __main__ -   Step: 4023, LR: 1.937261672944629e-05, Loss: 0.11109663546085358
[2023-09-05 14:11:36,296] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4024/66600 [07:39<222:12:34, 12.78s/it]09/05/2023 14:11:36 - INFO - __main__ -   Step: 4024, LR: 1.937230702525251e-05, Loss: 0.09944961965084076
[2023-09-05 14:11:50,097] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4025/66600 [07:53<227:07:12, 13.07s/it]09/05/2023 14:11:50 - INFO - __main__ -   Step: 4025, LR: 1.9371997321058727e-05, Loss: 0.13562160730361938
[2023-09-05 14:12:04,739] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4026/66600 [08:08<234:54:22, 13.51s/it]09/05/2023 14:12:04 - INFO - __main__ -   Step: 4026, LR: 1.937168761686494e-05, Loss: 0.12823279201984406
[2023-09-05 14:12:18,244] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4027/66600 [08:21<234:51:21, 13.51s/it]09/05/2023 14:12:18 - INFO - __main__ -   Step: 4027, LR: 1.937137791267116e-05, Loss: 0.10757288336753845
[2023-09-05 14:12:32,454] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4028/66600 [08:36<238:23:36, 13.72s/it]09/05/2023 14:12:32 - INFO - __main__ -   Step: 4028, LR: 1.937106820847738e-05, Loss: 0.1998845487833023
[2023-09-05 14:12:46,878] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4029/66600 [08:50<242:00:43, 13.92s/it]09/05/2023 14:12:46 - INFO - __main__ -   Step: 4029, LR: 1.93707585042836e-05, Loss: 0.12505513429641724
[2023-09-05 14:13:01,049] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4030/66600 [09:04<243:16:46, 14.00s/it]09/05/2023 14:13:01 - INFO - __main__ -   Step: 4030, LR: 1.9370448800089817e-05, Loss: 0.1422909051179886
[2023-09-05 14:13:15,381] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4031/66600 [09:18<245:00:29, 14.10s/it]09/05/2023 14:13:15 - INFO - __main__ -   Step: 4031, LR: 1.9370139095896035e-05, Loss: 0.12419717758893967
[2023-09-05 14:13:29,989] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4032/66600 [09:33<247:38:52, 14.25s/it]09/05/2023 14:13:29 - INFO - __main__ -   Step: 4032, LR: 1.9369829391702253e-05, Loss: 0.12079105526208878
[2023-09-05 14:13:43,385] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4033/66600 [09:46<243:13:19, 13.99s/it]09/05/2023 14:13:43 - INFO - __main__ -   Step: 4033, LR: 1.9369519687508468e-05, Loss: 0.17882105708122253
[2023-09-05 14:13:58,331] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4034/66600 [10:01<248:09:34, 14.28s/it]09/05/2023 14:13:58 - INFO - __main__ -   Step: 4034, LR: 1.9369209983314686e-05, Loss: 0.12804356217384338
[2023-09-05 14:14:12,343] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4035/66600 [10:15<246:46:03, 14.20s/it]09/05/2023 14:14:12 - INFO - __main__ -   Step: 4035, LR: 1.9368900279120907e-05, Loss: 0.11175531148910522
[2023-09-05 14:14:27,709] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4036/66600 [10:31<252:50:22, 14.55s/it]09/05/2023 14:14:27 - INFO - __main__ -   Step: 4036, LR: 1.9368590574927125e-05, Loss: 0.14532199501991272
[2023-09-05 14:14:43,028] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4037/66600 [10:46<256:50:47, 14.78s/it]09/05/2023 14:14:43 - INFO - __main__ -   Step: 4037, LR: 1.9368280870733343e-05, Loss: 0.1402091234922409
[2023-09-05 14:14:56,892] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4038/66600 [11:00<252:04:19, 14.50s/it]09/05/2023 14:14:56 - INFO - __main__ -   Step: 4038, LR: 1.936797116653956e-05, Loss: 0.10111825168132782
[2023-09-05 14:15:12,410] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4039/66600 [11:15<257:20:54, 14.81s/it]09/05/2023 14:15:12 - INFO - __main__ -   Step: 4039, LR: 1.936766146234578e-05, Loss: 0.16066664457321167
[2023-09-05 14:15:27,598] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4040/66600 [11:31<259:19:10, 14.92s/it]09/05/2023 14:15:27 - INFO - __main__ -   Step: 4040, LR: 1.9367351758151997e-05, Loss: 0.10926024615764618
[2023-09-05 14:15:42,279] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4041/66600 [11:45<258:03:31, 14.85s/it]09/05/2023 14:15:42 - INFO - __main__ -   Step: 4041, LR: 1.9367042053958212e-05, Loss: 0.13232304155826569
[2023-09-05 14:15:58,128] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4042/66600 [12:01<263:15:32, 15.15s/it]09/05/2023 14:15:58 - INFO - __main__ -   Step: 4042, LR: 1.9366732349764434e-05, Loss: 0.13777905702590942
[2023-09-05 14:16:12,336] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4043/66600 [12:15<258:20:47, 14.87s/it]09/05/2023 14:16:12 - INFO - __main__ -   Step: 4043, LR: 1.936642264557065e-05, Loss: 0.10212532430887222
[2023-09-05 14:16:25,801] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4044/66600 [12:29<251:02:01, 14.45s/it]09/05/2023 14:16:25 - INFO - __main__ -   Step: 4044, LR: 1.936611294137687e-05, Loss: 0.11525858938694
[2023-09-05 14:16:40,777] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4045/66600 [12:44<253:47:21, 14.61s/it]09/05/2023 14:16:40 - INFO - __main__ -   Step: 4045, LR: 1.9365803237183088e-05, Loss: 0.12995809316635132
[2023-09-05 14:16:54,931] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4046/66600 [12:58<251:25:49, 14.47s/it]09/05/2023 14:16:54 - INFO - __main__ -   Step: 4046, LR: 1.9365493532989306e-05, Loss: 0.10980907082557678
[2023-09-05 14:17:09,163] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4047/66600 [13:12<250:11:21, 14.40s/it]09/05/2023 14:17:09 - INFO - __main__ -   Step: 4047, LR: 1.9365183828795524e-05, Loss: 0.12125896662473679
[2023-09-05 14:17:24,224] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4048/66600 [13:27<253:38:13, 14.60s/it]09/05/2023 14:17:24 - INFO - __main__ -   Step: 4048, LR: 1.9364874124601742e-05, Loss: 0.17076927423477173
[2023-09-05 14:17:37,831] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4049/66600 [13:41<248:28:06, 14.30s/it]09/05/2023 14:17:37 - INFO - __main__ -   Step: 4049, LR: 1.936456442040796e-05, Loss: 0.1770315021276474
[2023-09-05 14:17:53,312] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4050/66600 [13:56<254:37:04, 14.65s/it]09/05/2023 14:17:53 - INFO - __main__ -   Step: 4050, LR: 1.9364254716214178e-05, Loss: 0.17592379450798035
[2023-09-05 14:18:06,640] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4051/66600 [14:10<247:42:14, 14.26s/it]09/05/2023 14:18:06 - INFO - __main__ -   Step: 4051, LR: 1.9363945012020396e-05, Loss: 0.20629426836967468
[2023-09-05 14:18:21,377] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4052/66600 [14:24<250:12:06, 14.40s/it]09/05/2023 14:18:21 - INFO - __main__ -   Step: 4052, LR: 1.9363635307826614e-05, Loss: 0.22511659562587738
[2023-09-05 14:18:35,587] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4053/66600 [14:39<249:12:15, 14.34s/it]09/05/2023 14:18:35 - INFO - __main__ -   Step: 4053, LR: 1.9363325603632832e-05, Loss: 0.15257394313812256
[2023-09-05 14:18:49,431] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4054/66600 [14:52<246:35:58, 14.19s/it]09/05/2023 14:18:49 - INFO - __main__ -   Step: 4054, LR: 1.936301589943905e-05, Loss: 0.1672370433807373
[2023-09-05 14:19:03,901] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4055/66600 [15:07<248:02:00, 14.28s/it]09/05/2023 14:19:03 - INFO - __main__ -   Step: 4055, LR: 1.936270619524527e-05, Loss: 0.13000985980033875
[2023-09-05 14:19:18,178] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4056/66600 [15:21<248:02:14, 14.28s/it]09/05/2023 14:19:18 - INFO - __main__ -   Step: 4056, LR: 1.9362396491051486e-05, Loss: 0.10726795345544815
[2023-09-05 14:19:32,365] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4057/66600 [15:35<247:33:52, 14.25s/it]09/05/2023 14:19:32 - INFO - __main__ -   Step: 4057, LR: 1.9362086786857705e-05, Loss: 0.17711663246154785
[2023-09-05 14:19:48,104] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4058/66600 [15:51<255:19:08, 14.70s/it]09/05/2023 14:19:48 - INFO - __main__ -   Step: 4058, LR: 1.9361777082663923e-05, Loss: 0.11813880503177643
[2023-09-05 14:20:01,690] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4059/66600 [16:05<249:31:34, 14.36s/it]09/05/2023 14:20:01 - INFO - __main__ -   Step: 4059, LR: 1.936146737847014e-05, Loss: 0.18571683764457703
[2023-09-05 14:20:16,765] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4060/66600 [16:20<253:14:01, 14.58s/it]09/05/2023 14:20:16 - INFO - __main__ -   Step: 4060, LR: 1.936115767427636e-05, Loss: 0.15026573836803436
[2023-09-05 14:20:31,377] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4061/66600 [16:34<253:24:49, 14.59s/it]09/05/2023 14:20:31 - INFO - __main__ -   Step: 4061, LR: 1.9360847970082577e-05, Loss: 0.1902247965335846
[2023-09-05 14:20:44,895] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4062/66600 [16:48<247:50:05, 14.27s/it]09/05/2023 14:20:44 - INFO - __main__ -   Step: 4062, LR: 1.9360538265888795e-05, Loss: 0.10941804200410843
[2023-09-05 14:20:58,468] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4063/66600 [17:02<244:12:53, 14.06s/it]09/05/2023 14:20:58 - INFO - __main__ -   Step: 4063, LR: 1.9360228561695013e-05, Loss: 0.14360162615776062
[2023-09-05 14:21:13,380] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4064/66600 [17:16<248:39:35, 14.31s/it]09/05/2023 14:21:13 - INFO - __main__ -   Step: 4064, LR: 1.935991885750123e-05, Loss: 0.10872916877269745
[2023-09-05 14:21:28,591] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4065/66600 [17:32<253:19:42, 14.58s/it]09/05/2023 14:21:28 - INFO - __main__ -   Step: 4065, LR: 1.935960915330745e-05, Loss: 0.16385555267333984
[2023-09-05 14:21:42,998] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4066/66600 [17:46<252:24:13, 14.53s/it]09/05/2023 14:21:43 - INFO - __main__ -   Step: 4066, LR: 1.9359299449113667e-05, Loss: 0.12031706422567368
[2023-09-05 14:21:57,367] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4067/66600 [18:00<251:33:12, 14.48s/it]09/05/2023 14:21:57 - INFO - __main__ -   Step: 4067, LR: 1.9358989744919885e-05, Loss: 0.14360572397708893
[2023-09-05 14:22:11,162] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4068/66600 [18:14<247:58:25, 14.28s/it]09/05/2023 14:22:11 - INFO - __main__ -   Step: 4068, LR: 1.9358680040726103e-05, Loss: 0.1332223117351532
[2023-09-05 14:22:25,806] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4069/66600 [18:29<249:53:21, 14.39s/it]09/05/2023 14:22:25 - INFO - __main__ -   Step: 4069, LR: 1.935837033653232e-05, Loss: 0.1303793489933014
[2023-09-05 14:22:40,764] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4070/66600 [18:44<252:51:39, 14.56s/it]09/05/2023 14:22:40 - INFO - __main__ -   Step: 4070, LR: 1.935806063233854e-05, Loss: 0.15395742654800415
[2023-09-05 14:22:56,725] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4071/66600 [19:00<260:10:10, 14.98s/it]09/05/2023 14:22:56 - INFO - __main__ -   Step: 4071, LR: 1.9357750928144757e-05, Loss: 0.15345416963100433
[2023-09-05 14:23:10,665] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4072/66600 [19:14<254:45:05, 14.67s/it]09/05/2023 14:23:10 - INFO - __main__ -   Step: 4072, LR: 1.9357441223950975e-05, Loss: 0.1430564820766449
[2023-09-05 14:23:24,267] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4073/66600 [19:27<249:11:57, 14.35s/it]09/05/2023 14:23:24 - INFO - __main__ -   Step: 4073, LR: 1.9357131519757193e-05, Loss: 0.2055864781141281
[2023-09-05 14:23:38,502] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4074/66600 [19:42<248:36:26, 14.31s/it]09/05/2023 14:23:38 - INFO - __main__ -   Step: 4074, LR: 1.935682181556341e-05, Loss: 0.12670934200286865
[2023-09-05 14:23:52,659] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4075/66600 [19:56<247:47:12, 14.27s/it]09/05/2023 14:23:52 - INFO - __main__ -   Step: 4075, LR: 1.935651211136963e-05, Loss: 0.1078312024474144
[2023-09-05 14:24:07,217] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4076/66600 [20:10<249:18:01, 14.35s/it]09/05/2023 14:24:07 - INFO - __main__ -   Step: 4076, LR: 1.9356202407175848e-05, Loss: 0.10968148708343506
[2023-09-05 14:24:21,719] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4077/66600 [20:25<250:03:45, 14.40s/it]09/05/2023 14:24:21 - INFO - __main__ -   Step: 4077, LR: 1.9355892702982066e-05, Loss: 0.12152118235826492
[2023-09-05 14:24:35,930] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4078/66600 [20:39<249:05:04, 14.34s/it]09/05/2023 14:24:35 - INFO - __main__ -   Step: 4078, LR: 1.9355582998788284e-05, Loss: 0.14055825769901276
[2023-09-05 14:24:50,373] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4079/66600 [20:53<249:36:20, 14.37s/it]09/05/2023 14:24:50 - INFO - __main__ -   Step: 4079, LR: 1.9355273294594502e-05, Loss: 0.144733726978302
[2023-09-05 14:25:05,028] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4080/66600 [21:08<251:04:23, 14.46s/it]09/05/2023 14:25:05 - INFO - __main__ -   Step: 4080, LR: 1.935496359040072e-05, Loss: 0.16205543279647827
[2023-09-05 14:25:18,589] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4081/66600 [21:22<246:23:57, 14.19s/it]09/05/2023 14:25:18 - INFO - __main__ -   Step: 4081, LR: 1.9354653886206938e-05, Loss: 0.11630681157112122
[2023-09-05 14:25:33,057] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4082/66600 [21:36<247:51:13, 14.27s/it]09/05/2023 14:25:33 - INFO - __main__ -   Step: 4082, LR: 1.9354344182013156e-05, Loss: 0.12776167690753937
[2023-09-05 14:25:48,132] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4083/66600 [21:51<252:02:04, 14.51s/it]09/05/2023 14:25:48 - INFO - __main__ -   Step: 4083, LR: 1.9354034477819374e-05, Loss: 0.15368741750717163
[2023-09-05 14:26:04,174] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4084/66600 [22:07<259:59:35, 14.97s/it]09/05/2023 14:26:04 - INFO - __main__ -   Step: 4084, LR: 1.9353724773625592e-05, Loss: 0.12616263329982758
[2023-09-05 14:26:19,324] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4085/66600 [22:22<260:55:01, 15.03s/it]09/05/2023 14:26:19 - INFO - __main__ -   Step: 4085, LR: 1.935341506943181e-05, Loss: 0.14006094634532928
[2023-09-05 14:26:33,208] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4086/66600 [22:36<254:57:53, 14.68s/it]09/05/2023 14:26:33 - INFO - __main__ -   Step: 4086, LR: 1.9353105365238028e-05, Loss: 0.16039031744003296
[2023-09-05 14:26:47,408] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4087/66600 [22:50<252:26:52, 14.54s/it]09/05/2023 14:26:47 - INFO - __main__ -   Step: 4087, LR: 1.9352795661044246e-05, Loss: 0.12383687496185303
[2023-09-05 14:27:02,649] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4088/66600 [23:06<256:06:21, 14.75s/it]09/05/2023 14:27:02 - INFO - __main__ -   Step: 4088, LR: 1.9352485956850464e-05, Loss: 0.10695895552635193
[2023-09-05 14:27:17,234] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4089/66600 [23:20<255:14:50, 14.70s/it]09/05/2023 14:27:17 - INFO - __main__ -   Step: 4089, LR: 1.9352176252656682e-05, Loss: 0.15404903888702393
[2023-09-05 14:27:31,781] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4090/66600 [23:35<254:27:03, 14.65s/it]09/05/2023 14:27:31 - INFO - __main__ -   Step: 4090, LR: 1.93518665484629e-05, Loss: 0.1218695119023323
[2023-09-05 14:27:45,836] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4091/66600 [23:49<251:19:40, 14.47s/it]09/05/2023 14:27:45 - INFO - __main__ -   Step: 4091, LR: 1.935155684426912e-05, Loss: 0.12916898727416992
[2023-09-05 14:27:59,690] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4092/66600 [24:03<248:05:17, 14.29s/it]09/05/2023 14:27:59 - INFO - __main__ -   Step: 4092, LR: 1.9351247140075337e-05, Loss: 0.1441037952899933
[2023-09-05 14:28:14,105] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4093/66600 [24:17<248:44:46, 14.33s/it]09/05/2023 14:28:14 - INFO - __main__ -   Step: 4093, LR: 1.9350937435881555e-05, Loss: 0.11916261911392212
[2023-09-05 14:28:27,421] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4094/66600 [24:30<243:28:57, 14.02s/it]09/05/2023 14:28:27 - INFO - __main__ -   Step: 4094, LR: 1.9350627731687773e-05, Loss: 0.10259421169757843
[2023-09-05 14:28:42,665] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4095/66600 [24:46<249:50:08, 14.39s/it]09/05/2023 14:28:42 - INFO - __main__ -   Step: 4095, LR: 1.935031802749399e-05, Loss: 0.14295095205307007
[2023-09-05 14:28:56,969] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4096/66600 [25:00<249:23:21, 14.36s/it]09/05/2023 14:28:56 - INFO - __main__ -   Step: 4096, LR: 1.935000832330021e-05, Loss: 0.09749477356672287
[2023-09-05 14:29:10,532] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4097/66600 [25:14<245:12:43, 14.12s/it]09/05/2023 14:29:10 - INFO - __main__ -   Step: 4097, LR: 1.9349698619106427e-05, Loss: 0.13834235072135925
[2023-09-05 14:29:24,437] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4098/66600 [25:27<244:04:00, 14.06s/it]09/05/2023 14:29:24 - INFO - __main__ -   Step: 4098, LR: 1.9349388914912645e-05, Loss: 0.11247806251049042
[2023-09-05 14:29:39,321] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4099/66600 [25:42<248:22:02, 14.31s/it]09/05/2023 14:29:39 - INFO - __main__ -   Step: 4099, LR: 1.9349079210718863e-05, Loss: 0.1413184404373169
[2023-09-05 14:29:52,830] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4100/66600 [25:56<244:12:50, 14.07s/it]09/05/2023 14:29:52 - INFO - __main__ -   Step: 4100, LR: 1.934876950652508e-05, Loss: 0.14687906205654144
09/05/2023 14:29:52 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.0021758079528808594

Evaluating:   0%|          | 1/228 [00:00<01:29,  2.55it/s][Astep: 1
extend+tolist() time: 0.0009868144989013672

Evaluating:   1%|          | 2/228 [00:00<01:09,  3.25it/s][Astep: 2
extend+tolist() time: 0.0021224021911621094

Evaluating:   1%|▏         | 3/228 [00:00<01:12,  3.09it/s][Astep: 3
extend+tolist() time: 0.001795053482055664

Evaluating:   2%|▏         | 4/228 [00:01<01:10,  3.16it/s][Astep: 4
extend+tolist() time: 0.0013320446014404297

Evaluating:   2%|▏         | 5/228 [00:01<01:05,  3.41it/s][Astep: 5
extend+tolist() time: 0.0014300346374511719

Evaluating:   3%|▎         | 6/228 [00:01<01:08,  3.26it/s][Astep: 6
extend+tolist() time: 0.0019309520721435547

Evaluating:   3%|▎         | 7/228 [00:02<01:10,  3.15it/s][Astep: 7
extend+tolist() time: 0.0012772083282470703

Evaluating:   4%|▎         | 8/228 [00:02<01:05,  3.38it/s][Astep: 8
extend+tolist() time: 0.001117706298828125

Evaluating:   4%|▍         | 9/228 [00:02<01:00,  3.60it/s][Astep: 9
extend+tolist() time: 0.0007960796356201172

Evaluating:   4%|▍         | 10/228 [00:02<00:57,  3.78it/s][Astep: 10
extend+tolist() time: 0.0012781620025634766

Evaluating:   5%|▍         | 11/228 [00:03<00:56,  3.87it/s][Astep: 11
extend+tolist() time: 0.0005996227264404297

Evaluating:   5%|▌         | 12/228 [00:03<00:53,  4.03it/s][Astep: 12
extend+tolist() time: 0.0006635189056396484

Evaluating:   6%|▌         | 13/228 [00:03<00:52,  4.13it/s][Astep: 13
extend+tolist() time: 0.0010876655578613281

Evaluating:   6%|▌         | 14/228 [00:03<00:50,  4.20it/s][Astep: 14
extend+tolist() time: 0.0005424022674560547

Evaluating:   7%|▋         | 15/228 [00:04<00:49,  4.27it/s][Astep: 15
extend+tolist() time: 0.0006079673767089844

Evaluating:   7%|▋         | 16/228 [00:04<00:49,  4.30it/s][Astep: 16
extend+tolist() time: 0.0010619163513183594

Evaluating:   7%|▋         | 17/228 [00:04<00:49,  4.29it/s][Astep: 17
extend+tolist() time: 0.00087738037109375

Evaluating:   8%|▊         | 18/228 [00:04<01:00,  3.50it/s][Astep: 18
extend+tolist() time: 0.0015301704406738281

Evaluating:   8%|▊         | 19/228 [00:05<00:59,  3.52it/s][Astep: 19
extend+tolist() time: 0.1549062728881836

Evaluating:   9%|▉         | 20/228 [00:05<01:16,  2.72it/s][Astep: 20
extend+tolist() time: 0.0008463859558105469

Evaluating:   9%|▉         | 21/228 [00:06<01:07,  3.06it/s][Astep: 21
extend+tolist() time: 0.0011034011840820312

Evaluating:  10%|▉         | 22/228 [00:06<01:01,  3.35it/s][Astep: 22
extend+tolist() time: 0.0007243156433105469

Evaluating:  10%|█         | 23/228 [00:06<00:56,  3.61it/s][Astep: 23
extend+tolist() time: 0.0011091232299804688

Evaluating:  11%|█         | 24/228 [00:06<00:54,  3.77it/s][Astep: 24
extend+tolist() time: 0.0010824203491210938

Evaluating:  11%|█         | 25/228 [00:07<00:54,  3.71it/s][Astep: 25
extend+tolist() time: 0.00147247314453125

Evaluating:  11%|█▏        | 26/228 [00:07<00:58,  3.44it/s][Astep: 26
extend+tolist() time: 0.0006892681121826172

Evaluating:  12%|█▏        | 27/228 [00:07<00:54,  3.66it/s][Astep: 27
extend+tolist() time: 0.0017616748809814453

Evaluating:  12%|█▏        | 28/228 [00:07<00:56,  3.53it/s][Astep: 28
extend+tolist() time: 0.0003368854522705078

Evaluating:  13%|█▎        | 29/228 [00:08<00:52,  3.79it/s][Astep: 29
extend+tolist() time: 0.0007162094116210938

Evaluating:  13%|█▎        | 30/228 [00:08<00:50,  3.95it/s][Astep: 30
extend+tolist() time: 0.0018002986907958984

Evaluating:  14%|█▎        | 31/228 [00:08<00:51,  3.82it/s][Astep: 31
extend+tolist() time: 0.0005967617034912109

Evaluating:  14%|█▍        | 32/228 [00:08<00:49,  3.99it/s][Astep: 32
extend+tolist() time: 0.000986337661743164

Evaluating:  14%|█▍        | 33/228 [00:09<00:50,  3.88it/s][Astep: 33
extend+tolist() time: 0.0017638206481933594

Evaluating:  15%|█▍        | 34/228 [00:09<00:52,  3.68it/s][Astep: 34
extend+tolist() time: 0.0012478828430175781

Evaluating:  15%|█▌        | 35/228 [00:09<00:51,  3.77it/s][Astep: 35
extend+tolist() time: 0.0006768703460693359

Evaluating:  16%|█▌        | 36/228 [00:09<00:48,  3.93it/s][Astep: 36
extend+tolist() time: 0.0007700920104980469

Evaluating:  16%|█▌        | 37/228 [00:10<00:47,  4.02it/s][Astep: 37
extend+tolist() time: 0.0016522407531738281

Evaluating:  17%|█▋        | 38/228 [00:10<00:50,  3.77it/s][Astep: 38
extend+tolist() time: 0.001154184341430664

Evaluating:  17%|█▋        | 39/228 [00:10<00:48,  3.90it/s][Astep: 39
extend+tolist() time: 0.0007154941558837891

Evaluating:  18%|█▊        | 40/228 [00:10<00:46,  4.02it/s][Astep: 40
extend+tolist() time: 0.0009868144989013672

Evaluating:  18%|█▊        | 41/228 [00:11<00:45,  4.11it/s][Astep: 41
extend+tolist() time: 0.0008299350738525391

Evaluating:  18%|█▊        | 42/228 [00:11<00:45,  4.10it/s][Astep: 42
extend+tolist() time: 0.0016505718231201172

Evaluating:  19%|█▉        | 43/228 [00:11<00:48,  3.81it/s][Astep: 43
extend+tolist() time: 0.0018787384033203125

Evaluating:  19%|█▉        | 44/228 [00:12<00:52,  3.50it/s][Astep: 44
extend+tolist() time: 0.0007271766662597656

Evaluating:  20%|█▉        | 45/228 [00:12<00:49,  3.72it/s][Astep: 45
extend+tolist() time: 0.001728057861328125

Evaluating:  20%|██        | 46/228 [00:12<00:50,  3.58it/s][Astep: 46
extend+tolist() time: 0.001604318618774414

Evaluating:  21%|██        | 47/228 [00:12<00:52,  3.47it/s][Astep: 47
extend+tolist() time: 0.0014805793762207031

Evaluating:  21%|██        | 48/228 [00:13<00:51,  3.49it/s][Astep: 48
extend+tolist() time: 0.0016469955444335938

Evaluating:  21%|██▏       | 49/228 [00:13<01:01,  2.93it/s][Astep: 49
extend+tolist() time: 0.0009677410125732422

Evaluating:  22%|██▏       | 50/228 [00:13<00:56,  3.17it/s][Astep: 50
extend+tolist() time: 0.15421295166015625

Evaluating:  22%|██▏       | 51/228 [00:14<01:12,  2.44it/s][Astep: 51
extend+tolist() time: 0.001607656478881836

Evaluating:  23%|██▎       | 52/228 [00:14<01:06,  2.65it/s][Astep: 52
extend+tolist() time: 0.0012886524200439453

Evaluating:  23%|██▎       | 53/228 [00:15<01:00,  2.88it/s][Astep: 53
extend+tolist() time: 0.0012331008911132812

Evaluating:  24%|██▎       | 54/228 [00:15<00:58,  2.99it/s][Astep: 54
extend+tolist() time: 0.0008046627044677734

Evaluating:  24%|██▍       | 55/228 [00:15<00:52,  3.27it/s][Astep: 55
extend+tolist() time: 0.0012140274047851562

Evaluating:  25%|██▍       | 56/228 [00:15<00:49,  3.51it/s][Astep: 56
extend+tolist() time: 0.0011749267578125

Evaluating:  25%|██▌       | 57/228 [00:16<00:49,  3.44it/s][Astep: 57
extend+tolist() time: 0.0010426044464111328

Evaluating:  25%|██▌       | 58/228 [00:16<00:46,  3.69it/s][Astep: 58
extend+tolist() time: 0.001177072525024414

Evaluating:  26%|██▌       | 59/228 [00:16<00:44,  3.78it/s][Astep: 59
extend+tolist() time: 0.0014379024505615234

Evaluating:  26%|██▋       | 60/228 [00:16<00:43,  3.83it/s][Astep: 60
extend+tolist() time: 0.0007042884826660156

Evaluating:  27%|██▋       | 61/228 [00:17<00:42,  3.97it/s][Astep: 61
extend+tolist() time: 0.0013012886047363281

Evaluating:  27%|██▋       | 62/228 [00:17<00:41,  4.00it/s][Astep: 62
extend+tolist() time: 0.0007779598236083984

Evaluating:  28%|██▊       | 63/228 [00:17<00:40,  4.09it/s][Astep: 63
extend+tolist() time: 0.0012369155883789062

Evaluating:  28%|██▊       | 64/228 [00:17<00:39,  4.14it/s][Astep: 64
extend+tolist() time: 0.000797271728515625

Evaluating:  29%|██▊       | 65/228 [00:18<00:39,  4.16it/s][Astep: 65
extend+tolist() time: 0.0012087821960449219

Evaluating:  29%|██▉       | 66/228 [00:18<00:38,  4.16it/s][Astep: 66
extend+tolist() time: 0.0007157325744628906

Evaluating:  29%|██▉       | 67/228 [00:18<00:38,  4.20it/s][Astep: 67
extend+tolist() time: 0.001344442367553711

Evaluating:  30%|██▉       | 68/228 [00:18<00:38,  4.16it/s][Astep: 68
extend+tolist() time: 0.0007302761077880859

Evaluating:  30%|███       | 69/228 [00:19<00:37,  4.22it/s][Astep: 69
extend+tolist() time: 0.0015048980712890625

Evaluating:  31%|███       | 70/228 [00:19<00:39,  3.97it/s][Astep: 70
extend+tolist() time: 0.0014004707336425781

Evaluating:  31%|███       | 71/228 [00:19<00:40,  3.85it/s][Astep: 71
extend+tolist() time: 0.0013434886932373047

Evaluating:  32%|███▏      | 72/228 [00:19<00:41,  3.79it/s][Astep: 72
extend+tolist() time: 0.0008001327514648438

Evaluating:  32%|███▏      | 73/228 [00:20<00:39,  3.93it/s][Astep: 73
extend+tolist() time: 0.0006070137023925781

Evaluating:  32%|███▏      | 74/228 [00:20<00:37,  4.06it/s][Astep: 74
extend+tolist() time: 0.001192331314086914

Evaluating:  33%|███▎      | 75/228 [00:20<00:36,  4.15it/s][Astep: 75
extend+tolist() time: 0.0016477108001708984

Evaluating:  33%|███▎      | 76/228 [00:20<00:39,  3.83it/s][Astep: 76
extend+tolist() time: 0.0006439685821533203

Evaluating:  34%|███▍      | 77/228 [00:21<00:38,  3.97it/s][Astep: 77
extend+tolist() time: 0.001913309097290039

Evaluating:  34%|███▍      | 78/228 [00:21<00:41,  3.60it/s][Astep: 78
extend+tolist() time: 0.0011987686157226562

Evaluating:  35%|███▍      | 79/228 [00:21<00:40,  3.70it/s][Astep: 79
extend+tolist() time: 0.0008871555328369141

Evaluating:  35%|███▌      | 80/228 [00:21<00:39,  3.77it/s][Astep: 80
extend+tolist() time: 0.0013365745544433594

Evaluating:  36%|███▌      | 81/228 [00:22<00:38,  3.84it/s][Astep: 81
extend+tolist() time: 0.0008251667022705078

Evaluating:  36%|███▌      | 82/228 [00:22<00:37,  3.91it/s][Astep: 82
extend+tolist() time: 0.0014955997467041016

Evaluating:  36%|███▋      | 83/228 [00:22<00:36,  3.96it/s][Astep: 83
extend+tolist() time: 0.0006947517395019531

Evaluating:  37%|███▋      | 84/228 [00:22<00:35,  4.07it/s][Astep: 84
extend+tolist() time: 0.0018401145935058594

Evaluating:  37%|███▋      | 85/228 [00:23<00:36,  3.94it/s][Astep: 85
extend+tolist() time: 0.0012819766998291016

Evaluating:  38%|███▊      | 86/228 [00:23<00:36,  3.94it/s][Astep: 86
extend+tolist() time: 0.0012011528015136719

Evaluating:  38%|███▊      | 87/228 [00:23<00:35,  3.97it/s][Astep: 87
extend+tolist() time: 0.0013430118560791016

Evaluating:  39%|███▊      | 88/228 [00:24<00:43,  3.24it/s][Astep: 88
extend+tolist() time: 0.0009212493896484375

Evaluating:  39%|███▉      | 89/228 [00:24<00:39,  3.49it/s][Astep: 89
extend+tolist() time: 0.0011937618255615234

Evaluating:  39%|███▉      | 90/228 [00:24<00:37,  3.69it/s][Astep: 90
extend+tolist() time: 0.0009355545043945312

Evaluating:  40%|███▉      | 91/228 [00:25<00:43,  3.15it/s][Astep: 91
extend+tolist() time: 0.1981208324432373

Evaluating:  40%|████      | 92/228 [00:25<00:47,  2.84it/s][Astep: 92
extend+tolist() time: 0.001088857650756836

Evaluating:  41%|████      | 93/228 [00:25<00:42,  3.16it/s][Astep: 93
extend+tolist() time: 0.0013725757598876953

Evaluating:  41%|████      | 94/228 [00:25<00:40,  3.30it/s][Astep: 94
extend+tolist() time: 0.0010161399841308594

Evaluating:  42%|████▏     | 95/228 [00:26<00:37,  3.56it/s][Astep: 95
extend+tolist() time: 0.001177072525024414

Evaluating:  42%|████▏     | 96/228 [00:26<00:37,  3.49it/s][Astep: 96
extend+tolist() time: 0.0017428398132324219

Evaluating:  43%|████▎     | 97/228 [00:26<00:36,  3.61it/s][Astep: 97
extend+tolist() time: 0.001313924789428711

Evaluating:  43%|████▎     | 98/228 [00:26<00:34,  3.75it/s][Astep: 98
extend+tolist() time: 0.0009713172912597656

Evaluating:  43%|████▎     | 99/228 [00:27<00:33,  3.82it/s][Astep: 99
extend+tolist() time: 0.0015096664428710938

Evaluating:  44%|████▍     | 100/228 [00:27<00:32,  3.88it/s][Astep: 100
extend+tolist() time: 0.0008761882781982422

Evaluating:  44%|████▍     | 101/228 [00:27<00:31,  4.01it/s][Astep: 101
extend+tolist() time: 0.0014147758483886719

Evaluating:  45%|████▍     | 102/228 [00:27<00:31,  4.04it/s][Astep: 102
extend+tolist() time: 0.0008978843688964844

Evaluating:  45%|████▌     | 103/228 [00:28<00:30,  4.12it/s][Astep: 103
extend+tolist() time: 0.0012683868408203125

Evaluating:  46%|████▌     | 104/228 [00:28<00:29,  4.17it/s][Astep: 104
extend+tolist() time: 0.0008161067962646484

Evaluating:  46%|████▌     | 105/228 [00:28<00:29,  4.20it/s][Astep: 105
extend+tolist() time: 0.0017399787902832031

Evaluating:  46%|████▋     | 106/228 [00:28<00:29,  4.15it/s][Astep: 106
extend+tolist() time: 0.0019576549530029297

Evaluating:  47%|████▋     | 107/228 [00:29<00:32,  3.77it/s][Astep: 107
extend+tolist() time: 0.0009021759033203125

Evaluating:  47%|████▋     | 108/228 [00:29<00:30,  3.92it/s][Astep: 108
extend+tolist() time: 0.0014007091522216797

Evaluating:  48%|████▊     | 109/228 [00:29<00:29,  4.01it/s][Astep: 109
extend+tolist() time: 0.0010378360748291016

Evaluating:  48%|████▊     | 110/228 [00:29<00:29,  4.03it/s][Astep: 110
extend+tolist() time: 0.0007193088531494141

Evaluating:  49%|████▊     | 111/228 [00:30<00:28,  4.10it/s][Astep: 111
extend+tolist() time: 0.00214385986328125

Evaluating:  49%|████▉     | 112/228 [00:30<00:31,  3.74it/s][Astep: 112
extend+tolist() time: 0.0007851123809814453

Evaluating:  50%|████▉     | 113/228 [00:30<00:29,  3.92it/s][Astep: 113
extend+tolist() time: 0.0008018016815185547

Evaluating:  50%|█████     | 114/228 [00:30<00:28,  4.02it/s][Astep: 114
extend+tolist() time: 0.001779794692993164

Evaluating:  50%|█████     | 115/228 [00:31<00:29,  3.87it/s][Astep: 115
extend+tolist() time: 0.0007462501525878906

Evaluating:  51%|█████     | 116/228 [00:31<00:28,  3.99it/s][Astep: 116
extend+tolist() time: 0.0013813972473144531

Evaluating:  51%|█████▏    | 117/228 [00:31<00:27,  4.04it/s][Astep: 117
extend+tolist() time: 0.0009789466857910156

Evaluating:  52%|█████▏    | 118/228 [00:31<00:27,  4.04it/s][Astep: 118
extend+tolist() time: 0.0010814666748046875

Evaluating:  52%|█████▏    | 119/228 [00:32<00:26,  4.13it/s][Astep: 119
extend+tolist() time: 0.0008707046508789062

Evaluating:  53%|█████▎    | 120/228 [00:32<00:25,  4.19it/s][Astep: 120
extend+tolist() time: 0.0007350444793701172

Evaluating:  53%|█████▎    | 121/228 [00:32<00:25,  4.23it/s][Astep: 121
extend+tolist() time: 0.0011997222900390625

Evaluating:  54%|█████▎    | 122/228 [00:32<00:24,  4.25it/s][Astep: 122
extend+tolist() time: 0.0008203983306884766

Evaluating:  54%|█████▍    | 123/228 [00:33<00:24,  4.28it/s][Astep: 123
extend+tolist() time: 0.001161813735961914

Evaluating:  54%|█████▍    | 124/228 [00:33<00:24,  4.30it/s][Astep: 124
extend+tolist() time: 0.0009975433349609375

Evaluating:  55%|█████▍    | 125/228 [00:33<00:24,  4.28it/s][Astep: 125
extend+tolist() time: 0.0004956722259521484

Evaluating:  55%|█████▌    | 126/228 [00:33<00:23,  4.30it/s][Astep: 126
extend+tolist() time: 0.0019483566284179688

Evaluating:  56%|█████▌    | 127/228 [00:34<00:26,  3.85it/s][Astep: 127
extend+tolist() time: 0.001947164535522461

Evaluating:  56%|█████▌    | 128/228 [00:34<00:28,  3.56it/s][Astep: 128
extend+tolist() time: 0.0011892318725585938

Evaluating:  57%|█████▋    | 129/228 [00:34<00:26,  3.76it/s][Astep: 129
extend+tolist() time: 0.0009098052978515625

Evaluating:  57%|█████▋    | 130/228 [00:34<00:25,  3.89it/s][Astep: 130
extend+tolist() time: 0.0017287731170654297

Evaluating:  57%|█████▋    | 131/228 [00:35<00:24,  3.91it/s][Astep: 131
extend+tolist() time: 0.0005321502685546875

Evaluating:  58%|█████▊    | 132/228 [00:35<00:23,  4.06it/s][Astep: 132
extend+tolist() time: 0.001665353775024414

Evaluating:  58%|█████▊    | 133/228 [00:35<00:24,  3.89it/s][Astep: 133
extend+tolist() time: 0.0004801750183105469

Evaluating:  59%|█████▉    | 134/228 [00:35<00:23,  4.04it/s][Astep: 134
extend+tolist() time: 0.0011556148529052734

Evaluating:  59%|█████▉    | 135/228 [00:36<00:23,  3.93it/s][Astep: 135
extend+tolist() time: 0.0009775161743164062

Evaluating:  60%|█████▉    | 136/228 [00:36<00:22,  4.07it/s][Astep: 136
extend+tolist() time: 0.0009429454803466797

Evaluating:  60%|██████    | 137/228 [00:36<00:22,  4.11it/s][Astep: 137
extend+tolist() time: 0.0004315376281738281

Evaluating:  61%|██████    | 138/228 [00:36<00:21,  4.18it/s][Astep: 138
extend+tolist() time: 0.0012743473052978516

Evaluating:  61%|██████    | 139/228 [00:37<00:21,  4.21it/s][Astep: 139
extend+tolist() time: 0.0005202293395996094

Evaluating:  61%|██████▏   | 140/228 [00:37<00:20,  4.27it/s][Astep: 140
extend+tolist() time: 0.0012862682342529297

Evaluating:  62%|██████▏   | 141/228 [00:37<00:20,  4.26it/s][Astep: 141
extend+tolist() time: 0.0009112358093261719

Evaluating:  62%|██████▏   | 142/228 [00:37<00:20,  4.26it/s][Astep: 142
extend+tolist() time: 0.0006794929504394531

Evaluating:  63%|██████▎   | 143/228 [00:38<00:19,  4.32it/s][Astep: 143
extend+tolist() time: 0.0008256435394287109

Evaluating:  63%|██████▎   | 144/228 [00:38<00:19,  4.35it/s][Astep: 144
extend+tolist() time: 0.0008232593536376953

Evaluating:  64%|██████▎   | 145/228 [00:38<00:19,  4.32it/s][Astep: 145
extend+tolist() time: 0.0005795955657958984

Evaluating:  64%|██████▍   | 146/228 [00:38<00:24,  3.30it/s][Astep: 146
extend+tolist() time: 0.0004317760467529297

Evaluating:  64%|██████▍   | 147/228 [00:39<00:22,  3.57it/s][Astep: 147
extend+tolist() time: 0.0013644695281982422

Evaluating:  65%|██████▍   | 148/228 [00:39<00:21,  3.78it/s][Astep: 148
extend+tolist() time: 0.0008680820465087891

Evaluating:  65%|██████▌   | 149/228 [00:39<00:20,  3.95it/s][Astep: 149
extend+tolist() time: 0.0004050731658935547

Evaluating:  66%|██████▌   | 150/228 [00:39<00:19,  4.08it/s][Astep: 150
extend+tolist() time: 0.0013387203216552734

Evaluating:  66%|██████▌   | 151/228 [00:40<00:18,  4.12it/s][Astep: 151
extend+tolist() time: 0.0006597042083740234

Evaluating:  67%|██████▋   | 152/228 [00:40<00:22,  3.37it/s][Astep: 152
extend+tolist() time: 0.0014033317565917969

Evaluating:  67%|██████▋   | 153/228 [00:40<00:20,  3.60it/s][Astep: 153
extend+tolist() time: 0.0010335445404052734

Evaluating:  68%|██████▊   | 154/228 [00:40<00:19,  3.74it/s][Astep: 154
extend+tolist() time: 0.25725269317626953

Evaluating:  68%|██████▊   | 155/228 [00:41<00:26,  2.76it/s][Astep: 155
extend+tolist() time: 0.0011713504791259766

Evaluating:  68%|██████▊   | 156/228 [00:41<00:23,  3.09it/s][Astep: 156
extend+tolist() time: 0.0005536079406738281

Evaluating:  69%|██████▉   | 157/228 [00:42<00:20,  3.40it/s][Astep: 157
extend+tolist() time: 0.0010385513305664062

Evaluating:  69%|██████▉   | 158/228 [00:42<00:19,  3.65it/s][Astep: 158
extend+tolist() time: 0.0005502700805664062

Evaluating:  70%|██████▉   | 159/228 [00:42<00:17,  3.85it/s][Astep: 159
extend+tolist() time: 0.0007450580596923828

Evaluating:  70%|███████   | 160/228 [00:42<00:17,  3.97it/s][Astep: 160
extend+tolist() time: 0.0007567405700683594

Evaluating:  71%|███████   | 161/228 [00:42<00:16,  4.09it/s][Astep: 161
extend+tolist() time: 0.0008101463317871094

Evaluating:  71%|███████   | 162/228 [00:43<00:15,  4.17it/s][Astep: 162
extend+tolist() time: 0.0005459785461425781

Evaluating:  71%|███████▏  | 163/228 [00:43<00:15,  4.23it/s][Astep: 163
extend+tolist() time: 0.0009081363677978516

Evaluating:  72%|███████▏  | 164/228 [00:43<00:14,  4.28it/s][Astep: 164
extend+tolist() time: 0.0005872249603271484

Evaluating:  72%|███████▏  | 165/228 [00:43<00:14,  4.31it/s][Astep: 165
extend+tolist() time: 0.0004734992980957031

Evaluating:  73%|███████▎  | 166/228 [00:44<00:14,  4.38it/s][Astep: 166
extend+tolist() time: 0.0003962516784667969

Evaluating:  73%|███████▎  | 167/228 [00:44<00:13,  4.40it/s][Astep: 167
extend+tolist() time: 0.0010631084442138672

Evaluating:  74%|███████▎  | 168/228 [00:44<00:13,  4.39it/s][Astep: 168
extend+tolist() time: 0.0012335777282714844

Evaluating:  74%|███████▍  | 169/228 [00:44<00:14,  4.14it/s][Astep: 169
extend+tolist() time: 0.000362396240234375

Evaluating:  75%|███████▍  | 170/228 [00:45<00:13,  4.22it/s][Astep: 170
extend+tolist() time: 0.0014221668243408203

Evaluating:  75%|███████▌  | 171/228 [00:45<00:13,  4.25it/s][Astep: 171
extend+tolist() time: 0.000396728515625

Evaluating:  75%|███████▌  | 172/228 [00:45<00:13,  4.27it/s][Astep: 172
extend+tolist() time: 0.0008180141448974609

Evaluating:  76%|███████▌  | 173/228 [00:45<00:12,  4.24it/s][Astep: 173
extend+tolist() time: 0.001617431640625

Evaluating:  76%|███████▋  | 174/228 [00:46<00:13,  4.05it/s][Astep: 174
extend+tolist() time: 0.0018532276153564453

Evaluating:  77%|███████▋  | 175/228 [00:46<00:13,  3.79it/s][Astep: 175
extend+tolist() time: 0.0008802413940429688

Evaluating:  77%|███████▋  | 176/228 [00:46<00:13,  3.94it/s][Astep: 176
extend+tolist() time: 0.0011434555053710938

Evaluating:  78%|███████▊  | 177/228 [00:46<00:12,  4.07it/s][Astep: 177
extend+tolist() time: 0.00058746337890625

Evaluating:  78%|███████▊  | 178/228 [00:47<00:11,  4.18it/s][Astep: 178
extend+tolist() time: 0.0016126632690429688

Evaluating:  79%|███████▊  | 179/228 [00:47<00:12,  3.99it/s][Astep: 179
extend+tolist() time: 0.0003941059112548828

Evaluating:  79%|███████▉  | 180/228 [00:47<00:11,  4.12it/s][Astep: 180
extend+tolist() time: 0.00037384033203125

Evaluating:  79%|███████▉  | 181/228 [00:47<00:11,  4.21it/s][Astep: 181
extend+tolist() time: 0.0006721019744873047

Evaluating:  80%|███████▉  | 182/228 [00:47<00:10,  4.27it/s][Astep: 182
extend+tolist() time: 0.0011746883392333984

Evaluating:  80%|████████  | 183/228 [00:48<00:10,  4.29it/s][Astep: 183
extend+tolist() time: 0.0006604194641113281

Evaluating:  81%|████████  | 184/228 [00:48<00:10,  4.31it/s][Astep: 184
extend+tolist() time: 0.00044035911560058594

Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.32it/s][Astep: 185
extend+tolist() time: 0.0016064643859863281

Evaluating:  82%|████████▏ | 186/228 [00:48<00:10,  4.12it/s][Astep: 186
extend+tolist() time: 0.001434326171875

Evaluating:  82%|████████▏ | 187/228 [00:49<00:09,  4.10it/s][Astep: 187
extend+tolist() time: 0.0004525184631347656

Evaluating:  82%|████████▏ | 188/228 [00:49<00:09,  4.20it/s][Astep: 188
extend+tolist() time: 0.0006890296936035156

Evaluating:  83%|████████▎ | 189/228 [00:49<00:09,  4.26it/s][Astep: 189
extend+tolist() time: 0.0003592967987060547

Evaluating:  83%|████████▎ | 190/228 [00:49<00:08,  4.29it/s][Astep: 190
extend+tolist() time: 0.0017025470733642578

Evaluating:  84%|████████▍ | 191/228 [00:50<00:09,  4.08it/s][Astep: 191
extend+tolist() time: 0.0007798671722412109

Evaluating:  84%|████████▍ | 192/228 [00:50<00:08,  4.17it/s][Astep: 192
extend+tolist() time: 0.000804901123046875

Evaluating:  85%|████████▍ | 193/228 [00:50<00:08,  4.24it/s][Astep: 193
extend+tolist() time: 0.0011208057403564453

Evaluating:  85%|████████▌ | 194/228 [00:50<00:08,  4.18it/s][Astep: 194
extend+tolist() time: 0.0009944438934326172

Evaluating:  86%|████████▌ | 195/228 [00:51<00:07,  4.23it/s][Astep: 195
extend+tolist() time: 0.0005602836608886719

Evaluating:  86%|████████▌ | 196/228 [00:51<00:07,  4.30it/s][Astep: 196
extend+tolist() time: 0.0006656646728515625

Evaluating:  86%|████████▋ | 197/228 [00:51<00:07,  4.33it/s][Astep: 197
extend+tolist() time: 0.0011341571807861328

Evaluating:  87%|████████▋ | 198/228 [00:51<00:06,  4.37it/s][Astep: 198
extend+tolist() time: 0.0006577968597412109

Evaluating:  87%|████████▋ | 199/228 [00:51<00:06,  4.38it/s][Astep: 199
extend+tolist() time: 0.0018777847290039062

Evaluating:  88%|████████▊ | 200/228 [00:52<00:07,  3.96it/s][Astep: 200
extend+tolist() time: 0.0007104873657226562

Evaluating:  88%|████████▊ | 201/228 [00:52<00:06,  4.09it/s][Astep: 201
extend+tolist() time: 0.0006058216094970703

Evaluating:  89%|████████▊ | 202/228 [00:52<00:06,  4.19it/s][Astep: 202
extend+tolist() time: 0.0008847713470458984

Evaluating:  89%|████████▉ | 203/228 [00:52<00:05,  4.26it/s][Astep: 203
extend+tolist() time: 0.0005366802215576172

Evaluating:  89%|████████▉ | 204/228 [00:53<00:05,  4.32it/s][Astep: 204
extend+tolist() time: 0.0003962516784667969

Evaluating:  90%|████████▉ | 205/228 [00:53<00:05,  4.36it/s][Astep: 205
extend+tolist() time: 0.00029921531677246094

Evaluating:  90%|█████████ | 206/228 [00:53<00:05,  4.38it/s][Astep: 206
extend+tolist() time: 0.0006840229034423828

Evaluating:  91%|█████████ | 207/228 [00:53<00:04,  4.38it/s][Astep: 207
extend+tolist() time: 0.0011107921600341797

Evaluating:  91%|█████████ | 208/228 [00:54<00:04,  4.41it/s][Astep: 208
extend+tolist() time: 0.0007145404815673828

Evaluating:  92%|█████████▏| 209/228 [00:54<00:04,  4.39it/s][Astep: 209
extend+tolist() time: 0.00060272216796875

Evaluating:  92%|█████████▏| 210/228 [00:54<00:04,  4.39it/s][Astep: 210
extend+tolist() time: 0.0010280609130859375

Evaluating:  93%|█████████▎| 211/228 [00:54<00:03,  4.39it/s][Astep: 211
extend+tolist() time: 0.001149892807006836

Evaluating:  93%|█████████▎| 212/228 [00:55<00:03,  4.16it/s][Astep: 212
extend+tolist() time: 0.0013039112091064453

Evaluating:  93%|█████████▎| 213/228 [00:55<00:03,  4.20it/s][Astep: 213
extend+tolist() time: 0.0008046627044677734

Evaluating:  94%|█████████▍| 214/228 [00:55<00:03,  4.26it/s][Astep: 214
extend+tolist() time: 0.001262664794921875

Evaluating:  94%|█████████▍| 215/228 [00:55<00:03,  4.28it/s][Astep: 215
extend+tolist() time: 0.0006744861602783203

Evaluating:  95%|█████████▍| 216/228 [00:55<00:02,  4.29it/s][Astep: 216
extend+tolist() time: 0.0005838871002197266

Evaluating:  95%|█████████▌| 217/228 [00:56<00:02,  4.34it/s][Astep: 217
extend+tolist() time: 0.0009875297546386719

Evaluating:  96%|█████████▌| 218/228 [00:56<00:02,  4.37it/s][Astep: 218
extend+tolist() time: 0.0010673999786376953

Evaluating:  96%|█████████▌| 219/228 [00:56<00:02,  4.26it/s][Astep: 219
extend+tolist() time: 0.0009062290191650391

Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.31it/s][Astep: 220
extend+tolist() time: 0.0004119873046875

Evaluating:  97%|█████████▋| 221/228 [00:57<00:01,  4.35it/s][Astep: 221
extend+tolist() time: 0.000736236572265625

Evaluating:  97%|█████████▋| 222/228 [00:57<00:01,  4.38it/s][Astep: 222
extend+tolist() time: 0.0004379749298095703

Evaluating:  98%|█████████▊| 223/228 [00:57<00:01,  4.42it/s][Astep: 223
extend+tolist() time: 0.0008366107940673828

Evaluating:  98%|█████████▊| 224/228 [00:57<00:00,  4.43it/s][Astep: 224
extend+tolist() time: 0.00035953521728515625

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.42it/s][Astep: 225
extend+tolist() time: 0.00043201446533203125

Evaluating:  99%|█████████▉| 226/228 [00:58<00:00,  4.46it/s][Astep: 226
extend+tolist() time: 0.0005946159362792969

Evaluating: 100%|█████████▉| 227/228 [00:58<00:00,  4.45it/s][Astep: 227
extend+tolist() time: 0.0009250640869140625

Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.88it/s][A09/05/2023 14:30:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:30:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:30:53 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:30:53 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:00<00:00,  3.77it/s]
09/05/2023 14:30:53 - INFO - __main__ -   Step: 4100, Validation Metrics: {'pred_1_num': 9727, 'pred_-1_num': 787, 'pred_0_num': 287, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7877048421442459, 'f1_micro': 0.7877048421442459, 'f1_macro': 0.4518339245071891, 'f1_weighted': 0.7562115230375807, 'f1_-1': 0.3391415214619634, 'f1_0': 0.13822894168466524, 'f1_1': 0.8781313103749386, 'precision_micro': 0.7877048421442459, 'precision_macro': 0.5190214479172545, 'precision_weighted': 0.7449321560531177, 'precision_-1': 0.5069885641677255, 'precision_0': 0.2229965156794425, 'precision_1': 0.8270792639045954, 'recall_micro': 0.7877048421442459, 'recall_macro': 0.43028206178426154, 'recall_weighted': 0.7877048421442459, 'recall_-1': 0.2547892720306513, 'recall_0': 0.10015649452269171, 'recall_1': 0.9359004187994416, 'roc_auc_micro': 0.9058529057045943, 'roc_auc_macro': 0.7175299130433479, 'roc_auc_weighted': 0.7087763561564282, 'roc_auc_-1': 0.804125740474526, 'roc_auc_0': 0.6529048660525774, 'roc_auc_1': 0.6955591326029403}
[2023-09-05 14:31:08,073] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4101/66600 [27:11<562:49:55, 32.42s/it]09/05/2023 14:31:08 - INFO - __main__ -   Step: 4101, LR: 1.93484598023313e-05, Loss: 0.15432938933372498
[2023-09-05 14:31:22,627] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4102/66600 [27:26<469:46:23, 27.06s/it]09/05/2023 14:31:22 - INFO - __main__ -   Step: 4102, LR: 1.9348150098137517e-05, Loss: 0.14899887144565582
[2023-09-05 14:31:37,532] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4103/66600 [27:41<406:27:48, 23.41s/it]09/05/2023 14:31:37 - INFO - __main__ -   Step: 4103, LR: 1.9347840393943735e-05, Loss: 0.1485428512096405
[2023-09-05 14:31:52,313] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4104/66600 [27:55<361:30:05, 20.82s/it]09/05/2023 14:31:52 - INFO - __main__ -   Step: 4104, LR: 1.9347530689749953e-05, Loss: 0.08991734683513641
[2023-09-05 14:32:05,885] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4105/66600 [28:09<323:43:35, 18.65s/it]09/05/2023 14:32:05 - INFO - __main__ -   Step: 4105, LR: 1.934722098555617e-05, Loss: 0.11856666207313538
[2023-09-05 14:32:19,210] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4106/66600 [28:22<296:00:11, 17.05s/it]09/05/2023 14:32:19 - INFO - __main__ -   Step: 4106, LR: 1.934691128136239e-05, Loss: 0.12335143983364105
[2023-09-05 14:32:33,817] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4107/66600 [28:37<283:16:01, 16.32s/it]09/05/2023 14:32:33 - INFO - __main__ -   Step: 4107, LR: 1.9346601577168608e-05, Loss: 0.18840576708316803
[2023-09-05 14:32:48,108] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4108/66600 [28:51<272:42:16, 15.71s/it]09/05/2023 14:32:48 - INFO - __main__ -   Step: 4108, LR: 1.9346291872974826e-05, Loss: 0.13500860333442688
[2023-09-05 14:33:01,111] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4109/66600 [29:04<258:36:11, 14.90s/it]09/05/2023 14:33:01 - INFO - __main__ -   Step: 4109, LR: 1.9345982168781047e-05, Loss: 0.15277744829654694
[2023-09-05 14:33:15,133] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4110/66600 [29:18<254:02:20, 14.63s/it]09/05/2023 14:33:15 - INFO - __main__ -   Step: 4110, LR: 1.934567246458726e-05, Loss: 0.134995698928833
[2023-09-05 14:33:29,907] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4111/66600 [29:33<254:45:38, 14.68s/it]09/05/2023 14:33:29 - INFO - __main__ -   Step: 4111, LR: 1.934536276039348e-05, Loss: 0.14096572995185852
[2023-09-05 14:33:44,807] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4112/66600 [29:48<255:55:14, 14.74s/it]09/05/2023 14:33:44 - INFO - __main__ -   Step: 4112, LR: 1.9345053056199698e-05, Loss: 0.14185792207717896
[2023-09-05 14:34:00,126] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4113/66600 [30:03<258:54:47, 14.92s/it]09/05/2023 14:34:00 - INFO - __main__ -   Step: 4113, LR: 1.9344743352005916e-05, Loss: 0.16336631774902344
[2023-09-05 14:34:14,741] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4114/66600 [30:18<257:20:11, 14.83s/it]09/05/2023 14:34:14 - INFO - __main__ -   Step: 4114, LR: 1.9344433647812134e-05, Loss: 0.15693867206573486
[2023-09-05 14:34:27,633] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4115/66600 [30:31<247:15:38, 14.25s/it]09/05/2023 14:34:27 - INFO - __main__ -   Step: 4115, LR: 1.9344123943618352e-05, Loss: 0.1553354561328888
[2023-09-05 14:34:41,669] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4116/66600 [30:45<246:09:59, 14.18s/it]09/05/2023 14:34:41 - INFO - __main__ -   Step: 4116, LR: 1.9343814239424573e-05, Loss: 0.19721820950508118
[2023-09-05 14:34:56,450] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4117/66600 [31:00<249:16:36, 14.36s/it]09/05/2023 14:34:56 - INFO - __main__ -   Step: 4117, LR: 1.9343504535230788e-05, Loss: 0.12801074981689453
[2023-09-05 14:35:11,192] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4118/66600 [31:14<251:15:07, 14.48s/it]09/05/2023 14:35:11 - INFO - __main__ -   Step: 4118, LR: 1.9343194831037006e-05, Loss: 0.11533975601196289
[2023-09-05 14:35:25,955] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4119/66600 [31:29<252:44:22, 14.56s/it]09/05/2023 14:35:25 - INFO - __main__ -   Step: 4119, LR: 1.9342885126843224e-05, Loss: 0.16035445034503937
[2023-09-05 14:35:40,350] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4120/66600 [31:43<251:51:53, 14.51s/it]09/05/2023 14:35:40 - INFO - __main__ -   Step: 4120, LR: 1.9342575422649442e-05, Loss: 0.17035509645938873
[2023-09-05 14:35:53,053] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4121/66600 [31:56<242:26:32, 13.97s/it]09/05/2023 14:35:53 - INFO - __main__ -   Step: 4121, LR: 1.934226571845566e-05, Loss: 0.14655783772468567
[2023-09-05 14:36:07,040] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4122/66600 [32:10<242:31:46, 13.97s/it]09/05/2023 14:36:07 - INFO - __main__ -   Step: 4122, LR: 1.934195601426188e-05, Loss: 0.1479015052318573
[2023-09-05 14:36:21,377] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4123/66600 [32:24<244:24:46, 14.08s/it]09/05/2023 14:36:21 - INFO - __main__ -   Step: 4123, LR: 1.93416463100681e-05, Loss: 0.16621825098991394
[2023-09-05 14:36:35,867] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4124/66600 [32:39<246:31:35, 14.21s/it]09/05/2023 14:36:35 - INFO - __main__ -   Step: 4124, LR: 1.9341336605874318e-05, Loss: 0.11580109596252441
[2023-09-05 14:36:49,220] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4125/66600 [32:52<242:04:52, 13.95s/it]09/05/2023 14:36:49 - INFO - __main__ -   Step: 4125, LR: 1.9341026901680533e-05, Loss: 0.1349913775920868
[2023-09-05 14:37:03,819] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4126/66600 [33:07<245:27:35, 14.14s/it]09/05/2023 14:37:03 - INFO - __main__ -   Step: 4126, LR: 1.934071719748675e-05, Loss: 0.16069543361663818
[2023-09-05 14:37:19,505] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4127/66600 [33:23<253:29:04, 14.61s/it]09/05/2023 14:37:19 - INFO - __main__ -   Step: 4127, LR: 1.934040749329297e-05, Loss: 0.09878215193748474
[2023-09-05 14:37:34,054] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4128/66600 [33:37<253:10:39, 14.59s/it]09/05/2023 14:37:34 - INFO - __main__ -   Step: 4128, LR: 1.9340097789099187e-05, Loss: 0.12607234716415405
[2023-09-05 14:37:49,113] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4129/66600 [33:52<255:37:06, 14.73s/it]09/05/2023 14:37:49 - INFO - __main__ -   Step: 4129, LR: 1.9339788084905405e-05, Loss: 0.15047791600227356
[2023-09-05 14:38:03,552] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4130/66600 [34:07<254:05:45, 14.64s/it]09/05/2023 14:38:03 - INFO - __main__ -   Step: 4130, LR: 1.9339478380711626e-05, Loss: 0.17306683957576752
[2023-09-05 14:38:17,937] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4131/66600 [34:21<252:45:00, 14.57s/it]09/05/2023 14:38:17 - INFO - __main__ -   Step: 4131, LR: 1.9339168676517844e-05, Loss: 0.12688013911247253
[2023-09-05 14:38:32,910] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4132/66600 [34:36<254:51:43, 14.69s/it]09/05/2023 14:38:32 - INFO - __main__ -   Step: 4132, LR: 1.9338858972324062e-05, Loss: 0.14091986417770386
[2023-09-05 14:38:47,554] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4133/66600 [34:51<254:38:01, 14.67s/it]09/05/2023 14:38:47 - INFO - __main__ -   Step: 4133, LR: 1.9338549268130277e-05, Loss: 0.16465625166893005
[2023-09-05 14:39:01,257] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4134/66600 [35:04<249:34:34, 14.38s/it]09/05/2023 14:39:01 - INFO - __main__ -   Step: 4134, LR: 1.9338239563936495e-05, Loss: 0.1853565275669098
[2023-09-05 14:39:15,142] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4135/66600 [35:18<246:58:21, 14.23s/it]09/05/2023 14:39:15 - INFO - __main__ -   Step: 4135, LR: 1.9337929859742713e-05, Loss: 0.11339523643255234
[2023-09-05 14:39:27,835] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4136/66600 [35:31<238:56:55, 13.77s/it]09/05/2023 14:39:27 - INFO - __main__ -   Step: 4136, LR: 1.933762015554893e-05, Loss: 0.1034291610121727
[2023-09-05 14:39:42,444] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4137/66600 [35:45<243:18:24, 14.02s/it]09/05/2023 14:39:42 - INFO - __main__ -   Step: 4137, LR: 1.9337310451355153e-05, Loss: 0.14440816640853882
[2023-09-05 14:39:57,808] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4138/66600 [36:01<250:17:03, 14.43s/it]09/05/2023 14:39:57 - INFO - __main__ -   Step: 4138, LR: 1.933700074716137e-05, Loss: 0.1226501390337944
[2023-09-05 14:40:12,049] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4139/66600 [36:15<249:19:21, 14.37s/it]09/05/2023 14:40:12 - INFO - __main__ -   Step: 4139, LR: 1.933669104296759e-05, Loss: 0.10338664054870605
[2023-09-05 14:40:26,068] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4140/66600 [36:29<247:29:29, 14.26s/it]09/05/2023 14:40:26 - INFO - __main__ -   Step: 4140, LR: 1.9336381338773803e-05, Loss: 0.20711193978786469
[2023-09-05 14:40:40,628] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4141/66600 [36:44<249:01:38, 14.35s/it]09/05/2023 14:40:40 - INFO - __main__ -   Step: 4141, LR: 1.933607163458002e-05, Loss: 0.18474403023719788
[2023-09-05 14:40:55,255] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4142/66600 [36:58<250:26:48, 14.44s/it]09/05/2023 14:40:55 - INFO - __main__ -   Step: 4142, LR: 1.933576193038624e-05, Loss: 0.13668565452098846
[2023-09-05 14:41:09,939] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4143/66600 [37:13<251:44:02, 14.51s/it]09/05/2023 14:41:09 - INFO - __main__ -   Step: 4143, LR: 1.9335452226192458e-05, Loss: 0.16967906057834625
[2023-09-05 14:41:25,346] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4144/66600 [37:28<256:24:07, 14.78s/it]09/05/2023 14:41:25 - INFO - __main__ -   Step: 4144, LR: 1.933514252199868e-05, Loss: 0.14592483639717102
[2023-09-05 14:41:40,043] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4145/66600 [37:43<255:58:06, 14.75s/it]09/05/2023 14:41:40 - INFO - __main__ -   Step: 4145, LR: 1.9334832817804897e-05, Loss: 0.1286855936050415
[2023-09-05 14:41:54,417] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4146/66600 [37:57<253:58:53, 14.64s/it]09/05/2023 14:41:54 - INFO - __main__ -   Step: 4146, LR: 1.9334523113611115e-05, Loss: 0.15158483386039734
[2023-09-05 14:42:09,267] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4147/66600 [38:12<255:04:15, 14.70s/it]09/05/2023 14:42:09 - INFO - __main__ -   Step: 4147, LR: 1.9334213409417333e-05, Loss: 0.11165395379066467
[2023-09-05 14:42:22,300] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4148/66600 [38:25<246:22:33, 14.20s/it]09/05/2023 14:42:22 - INFO - __main__ -   Step: 4148, LR: 1.9333903705223548e-05, Loss: 0.09657624363899231
[2023-09-05 14:42:35,550] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4149/66600 [38:39<241:25:04, 13.92s/it]09/05/2023 14:42:35 - INFO - __main__ -   Step: 4149, LR: 1.9333594001029766e-05, Loss: 0.15807026624679565
[2023-09-05 14:42:50,098] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4150/66600 [38:53<244:41:43, 14.11s/it]09/05/2023 14:42:50 - INFO - __main__ -   Step: 4150, LR: 1.9333284296835984e-05, Loss: 0.13705375790596008
[2023-09-05 14:43:03,636] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4151/66600 [39:07<241:44:14, 13.94s/it]09/05/2023 14:43:03 - INFO - __main__ -   Step: 4151, LR: 1.9332974592642206e-05, Loss: 0.16875742375850677
[2023-09-05 14:43:18,437] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4152/66600 [39:21<246:14:31, 14.20s/it]09/05/2023 14:43:18 - INFO - __main__ -   Step: 4152, LR: 1.9332664888448424e-05, Loss: 0.17269344627857208
[2023-09-05 14:43:32,675] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4153/66600 [39:36<246:27:32, 14.21s/it]09/05/2023 14:43:32 - INFO - __main__ -   Step: 4153, LR: 1.933235518425464e-05, Loss: 0.13358142971992493
[2023-09-05 14:43:46,233] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4154/66600 [39:49<243:04:18, 14.01s/it]09/05/2023 14:43:46 - INFO - __main__ -   Step: 4154, LR: 1.933204548006086e-05, Loss: 0.1220133900642395
[2023-09-05 14:44:00,657] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4155/66600 [40:04<245:12:12, 14.14s/it]09/05/2023 14:44:00 - INFO - __main__ -   Step: 4155, LR: 1.9331735775867078e-05, Loss: 0.13673248887062073
[2023-09-05 14:44:15,091] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4156/66600 [40:18<246:45:11, 14.23s/it]09/05/2023 14:44:15 - INFO - __main__ -   Step: 4156, LR: 1.9331426071673292e-05, Loss: 0.18164673447608948
[2023-09-05 14:44:30,039] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4157/66600 [40:33<250:30:26, 14.44s/it]09/05/2023 14:44:30 - INFO - __main__ -   Step: 4157, LR: 1.933111636747951e-05, Loss: 0.17897459864616394
[2023-09-05 14:44:44,603] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4158/66600 [40:48<251:08:11, 14.48s/it]09/05/2023 14:44:44 - INFO - __main__ -   Step: 4158, LR: 1.9330806663285732e-05, Loss: 0.1449737697839737
[2023-09-05 14:44:57,692] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4159/66600 [41:01<243:53:47, 14.06s/it]09/05/2023 14:44:57 - INFO - __main__ -   Step: 4159, LR: 1.933049695909195e-05, Loss: 0.15784451365470886
[2023-09-05 14:45:12,541] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4160/66600 [41:16<247:59:23, 14.30s/it]09/05/2023 14:45:12 - INFO - __main__ -   Step: 4160, LR: 1.9330187254898168e-05, Loss: 0.19865594804286957
[2023-09-05 14:45:27,906] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4161/66600 [41:31<253:32:14, 14.62s/it]09/05/2023 14:45:27 - INFO - __main__ -   Step: 4161, LR: 1.9329877550704386e-05, Loss: 0.16286444664001465
[2023-09-05 14:45:42,681] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▌         | 4162/66600 [41:46<254:21:04, 14.67s/it]09/05/2023 14:45:42 - INFO - __main__ -   Step: 4162, LR: 1.9329567846510604e-05, Loss: 0.12098103761672974
[2023-09-05 14:45:57,474] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4163/66600 [42:01<255:00:45, 14.70s/it]09/05/2023 14:45:57 - INFO - __main__ -   Step: 4163, LR: 1.932925814231682e-05, Loss: 0.1112116277217865
[2023-09-05 14:46:11,758] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4164/66600 [42:15<252:49:43, 14.58s/it]09/05/2023 14:46:11 - INFO - __main__ -   Step: 4164, LR: 1.9328948438123037e-05, Loss: 0.16548055410385132
[2023-09-05 14:46:24,849] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4165/66600 [42:28<245:05:20, 14.13s/it]09/05/2023 14:46:24 - INFO - __main__ -   Step: 4165, LR: 1.932863873392926e-05, Loss: 0.1316417008638382
[2023-09-05 14:46:39,623] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4166/66600 [42:43<248:25:28, 14.32s/it]09/05/2023 14:46:39 - INFO - __main__ -   Step: 4166, LR: 1.9328329029735476e-05, Loss: 0.15054677426815033
[2023-09-05 14:46:53,765] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4167/66600 [42:57<247:28:03, 14.27s/it]09/05/2023 14:46:53 - INFO - __main__ -   Step: 4167, LR: 1.9328019325541694e-05, Loss: 0.10942119359970093
[2023-09-05 14:47:08,675] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4168/66600 [43:12<250:47:54, 14.46s/it]09/05/2023 14:47:08 - INFO - __main__ -   Step: 4168, LR: 1.9327709621347913e-05, Loss: 0.12363988161087036
[2023-09-05 14:47:24,013] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4169/66600 [43:27<255:21:19, 14.72s/it]09/05/2023 14:47:24 - INFO - __main__ -   Step: 4169, LR: 1.932739991715413e-05, Loss: 0.14972326159477234
[2023-09-05 14:47:38,334] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4170/66600 [43:41<253:14:56, 14.60s/it]09/05/2023 14:47:38 - INFO - __main__ -   Step: 4170, LR: 1.932709021296035e-05, Loss: 0.09816045314073563
[2023-09-05 14:47:52,157] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4171/66600 [43:55<249:10:59, 14.37s/it]09/05/2023 14:47:52 - INFO - __main__ -   Step: 4171, LR: 1.9326780508766563e-05, Loss: 0.11607954651117325
[2023-09-05 14:48:06,823] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4172/66600 [44:10<250:43:36, 14.46s/it]09/05/2023 14:48:06 - INFO - __main__ -   Step: 4172, LR: 1.9326470804572785e-05, Loss: 0.1144343763589859
[2023-09-05 14:48:21,499] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4173/66600 [44:25<251:51:10, 14.52s/it]09/05/2023 14:48:21 - INFO - __main__ -   Step: 4173, LR: 1.9326161100379003e-05, Loss: 0.14780184626579285
[2023-09-05 14:48:34,536] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4174/66600 [44:38<244:06:49, 14.08s/it]09/05/2023 14:48:34 - INFO - __main__ -   Step: 4174, LR: 1.932585139618522e-05, Loss: 0.11358549445867538
[2023-09-05 14:48:48,686] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4175/66600 [44:52<244:29:05, 14.10s/it]09/05/2023 14:48:48 - INFO - __main__ -   Step: 4175, LR: 1.932554169199144e-05, Loss: 0.08767564594745636
[2023-09-05 14:49:03,505] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4176/66600 [45:07<248:13:36, 14.32s/it]09/05/2023 14:49:03 - INFO - __main__ -   Step: 4176, LR: 1.9325231987797657e-05, Loss: 0.12540428340435028
[2023-09-05 14:49:18,529] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4177/66600 [45:22<251:54:39, 14.53s/it]09/05/2023 14:49:18 - INFO - __main__ -   Step: 4177, LR: 1.9324922283603875e-05, Loss: 0.11596330255270004
[2023-09-05 14:49:33,302] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4178/66600 [45:36<253:10:55, 14.60s/it]09/05/2023 14:49:33 - INFO - __main__ -   Step: 4178, LR: 1.9324612579410093e-05, Loss: 0.12371509522199631
[2023-09-05 14:49:47,702] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4179/66600 [45:51<252:07:35, 14.54s/it]09/05/2023 14:49:47 - INFO - __main__ -   Step: 4179, LR: 1.932430287521631e-05, Loss: 0.10156343877315521
[2023-09-05 14:50:01,472] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4180/66600 [46:05<248:06:57, 14.31s/it]09/05/2023 14:50:01 - INFO - __main__ -   Step: 4180, LR: 1.932399317102253e-05, Loss: 0.14966952800750732
[2023-09-05 14:50:15,786] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4181/66600 [46:19<248:07:48, 14.31s/it]09/05/2023 14:50:15 - INFO - __main__ -   Step: 4181, LR: 1.9323683466828747e-05, Loss: 0.13930460810661316
[2023-09-05 14:50:30,257] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4182/66600 [46:33<248:57:39, 14.36s/it]09/05/2023 14:50:30 - INFO - __main__ -   Step: 4182, LR: 1.9323373762634965e-05, Loss: 0.1744024157524109
[2023-09-05 14:50:44,373] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4183/66600 [46:47<247:41:30, 14.29s/it]09/05/2023 14:50:44 - INFO - __main__ -   Step: 4183, LR: 1.9323064058441183e-05, Loss: 0.11829099804162979
[2023-09-05 14:50:57,700] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4184/66600 [47:01<242:42:07, 14.00s/it]09/05/2023 14:50:57 - INFO - __main__ -   Step: 4184, LR: 1.93227543542474e-05, Loss: 0.14345400035381317
[2023-09-05 14:51:10,678] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4185/66600 [47:14<237:23:16, 13.69s/it]09/05/2023 14:51:10 - INFO - __main__ -   Step: 4185, LR: 1.932244465005362e-05, Loss: 0.1755877137184143
[2023-09-05 14:51:24,647] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4186/66600 [47:28<238:49:22, 13.78s/it]09/05/2023 14:51:24 - INFO - __main__ -   Step: 4186, LR: 1.9322134945859838e-05, Loss: 0.12692290544509888
[2023-09-05 14:51:38,610] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4187/66600 [47:42<239:47:51, 13.83s/it]09/05/2023 14:51:38 - INFO - __main__ -   Step: 4187, LR: 1.9321825241666056e-05, Loss: 0.1941763162612915
[2023-09-05 14:51:52,264] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4188/66600 [47:55<238:52:14, 13.78s/it]09/05/2023 14:51:52 - INFO - __main__ -   Step: 4188, LR: 1.9321515537472274e-05, Loss: 0.14527183771133423
[2023-09-05 14:52:05,723] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4189/66600 [48:09<237:12:24, 13.68s/it]09/05/2023 14:52:05 - INFO - __main__ -   Step: 4189, LR: 1.9321205833278492e-05, Loss: 0.12176563590765
[2023-09-05 14:52:20,386] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4190/66600 [48:23<242:18:10, 13.98s/it]09/05/2023 14:52:20 - INFO - __main__ -   Step: 4190, LR: 1.932089612908471e-05, Loss: 0.184147909283638
[2023-09-05 14:52:34,372] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4191/66600 [48:37<242:20:52, 13.98s/it]09/05/2023 14:52:34 - INFO - __main__ -   Step: 4191, LR: 1.9320586424890928e-05, Loss: 0.14751139283180237
[2023-09-05 14:52:48,989] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4192/66600 [48:52<245:39:29, 14.17s/it]09/05/2023 14:52:48 - INFO - __main__ -   Step: 4192, LR: 1.9320276720697146e-05, Loss: 0.15178696811199188
[2023-09-05 14:53:02,309] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4193/66600 [49:05<241:13:39, 13.92s/it]09/05/2023 14:53:02 - INFO - __main__ -   Step: 4193, LR: 1.9319967016503364e-05, Loss: 0.16674372553825378
[2023-09-05 14:53:17,329] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4194/66600 [49:20<246:58:16, 14.25s/it]09/05/2023 14:53:17 - INFO - __main__ -   Step: 4194, LR: 1.9319657312309582e-05, Loss: 0.14290279150009155
[2023-09-05 14:53:32,042] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4195/66600 [49:35<249:23:10, 14.39s/it]09/05/2023 14:53:32 - INFO - __main__ -   Step: 4195, LR: 1.93193476081158e-05, Loss: 0.14818444848060608
[2023-09-05 14:53:45,573] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4196/66600 [49:49<244:56:02, 14.13s/it]09/05/2023 14:53:45 - INFO - __main__ -   Step: 4196, LR: 1.9319037903922018e-05, Loss: 0.16300509870052338
[2023-09-05 14:53:59,259] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4197/66600 [50:02<242:37:14, 14.00s/it]09/05/2023 14:53:59 - INFO - __main__ -   Step: 4197, LR: 1.9318728199728236e-05, Loss: 0.1204303577542305
[2023-09-05 14:54:14,405] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4198/66600 [50:17<248:35:47, 14.34s/it]09/05/2023 14:54:14 - INFO - __main__ -   Step: 4198, LR: 1.9318418495534454e-05, Loss: 0.13045990467071533
[2023-09-05 14:54:29,233] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4199/66600 [50:32<251:07:12, 14.49s/it]09/05/2023 14:54:29 - INFO - __main__ -   Step: 4199, LR: 1.9318108791340672e-05, Loss: 0.11955837905406952
[2023-09-05 14:54:43,721] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4200/66600 [50:47<251:07:12, 14.49s/it]09/05/2023 14:54:43 - INFO - __main__ -   Step: 4200, LR: 1.931779908714689e-05, Loss: 0.15712502598762512
09/05/2023 14:54:43 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.002662181854248047

Evaluating:   0%|          | 1/228 [00:00<01:32,  2.47it/s][Astep: 1
extend+tolist() time: 0.001425027847290039

Evaluating:   1%|          | 2/228 [00:00<01:10,  3.19it/s][Astep: 2
extend+tolist() time: 0.001992464065551758

Evaluating:   1%|▏         | 3/228 [00:00<01:13,  3.06it/s][Astep: 3
extend+tolist() time: 0.1298215389251709

Evaluating:   2%|▏         | 4/228 [00:01<01:22,  2.71it/s][Astep: 4
extend+tolist() time: 0.0009045600891113281

Evaluating:   2%|▏         | 5/228 [00:01<01:12,  3.06it/s][Astep: 5
extend+tolist() time: 0.001928567886352539

Evaluating:   3%|▎         | 6/228 [00:02<01:12,  3.05it/s][Astep: 6
extend+tolist() time: 0.001888275146484375

Evaluating:   3%|▎         | 7/228 [00:02<01:13,  3.02it/s][Astep: 7
extend+tolist() time: 0.0012927055358886719

Evaluating:   4%|▎         | 8/228 [00:02<01:07,  3.26it/s][Astep: 8
extend+tolist() time: 0.0007498264312744141

Evaluating:   4%|▍         | 9/228 [00:02<01:02,  3.53it/s][Astep: 9
extend+tolist() time: 0.001203775405883789

Evaluating:   4%|▍         | 10/228 [00:03<00:58,  3.73it/s][Astep: 10
extend+tolist() time: 0.0008690357208251953

Evaluating:   5%|▍         | 11/228 [00:03<00:56,  3.84it/s][Astep: 11
extend+tolist() time: 0.0010454654693603516

Evaluating:   5%|▌         | 12/228 [00:03<00:54,  3.99it/s][Astep: 12
extend+tolist() time: 0.0006668567657470703

Evaluating:   6%|▌         | 13/228 [00:03<00:52,  4.11it/s][Astep: 13
extend+tolist() time: 0.0010509490966796875

Evaluating:   6%|▌         | 14/228 [00:03<00:51,  4.19it/s][Astep: 14
extend+tolist() time: 0.0005509853363037109

Evaluating:   7%|▋         | 15/228 [00:04<00:50,  4.26it/s][Astep: 15
extend+tolist() time: 0.0006000995635986328

Evaluating:   7%|▋         | 16/228 [00:04<00:49,  4.32it/s][Astep: 16
extend+tolist() time: 0.0010745525360107422

Evaluating:   7%|▋         | 17/228 [00:04<00:59,  3.55it/s][Astep: 17
extend+tolist() time: 0.0008475780487060547

Evaluating:   8%|▊         | 18/228 [00:05<00:56,  3.71it/s][Astep: 18
extend+tolist() time: 0.0021011829376220703

Evaluating:   8%|▊         | 19/228 [00:05<00:56,  3.67it/s][Astep: 19
extend+tolist() time: 0.0009551048278808594

Evaluating:   9%|▉         | 20/228 [00:05<00:56,  3.68it/s][Astep: 20
extend+tolist() time: 0.0007653236389160156

Evaluating:   9%|▉         | 21/228 [00:06<01:02,  3.33it/s][Astep: 21
extend+tolist() time: 0.0006756782531738281

Evaluating:  10%|▉         | 22/228 [00:06<00:57,  3.58it/s][Astep: 22
extend+tolist() time: 0.0012328624725341797

Evaluating:  10%|█         | 23/228 [00:06<00:54,  3.78it/s][Astep: 23
extend+tolist() time: 0.0006656646728515625

Evaluating:  11%|█         | 24/228 [00:06<00:52,  3.91it/s][Astep: 24
extend+tolist() time: 0.0015230178833007812

Evaluating:  11%|█         | 25/228 [00:06<00:53,  3.81it/s][Astep: 25
extend+tolist() time: 0.0018935203552246094

Evaluating:  11%|█▏        | 26/228 [00:07<00:57,  3.50it/s][Astep: 26
extend+tolist() time: 0.0007014274597167969

Evaluating:  12%|█▏        | 27/228 [00:07<00:54,  3.72it/s][Astep: 27
extend+tolist() time: 0.001664876937866211

Evaluating:  12%|█▏        | 28/228 [00:07<00:55,  3.58it/s][Astep: 28
extend+tolist() time: 0.00032448768615722656

Evaluating:  13%|█▎        | 29/228 [00:08<00:52,  3.81it/s][Astep: 29
extend+tolist() time: 0.0011205673217773438

Evaluating:  13%|█▎        | 30/228 [00:08<00:50,  3.96it/s][Astep: 30
extend+tolist() time: 0.0014836788177490234

Evaluating:  14%|█▎        | 31/228 [00:08<00:51,  3.82it/s][Astep: 31
extend+tolist() time: 0.00057220458984375

Evaluating:  14%|█▍        | 32/228 [00:08<00:49,  3.99it/s][Astep: 32
extend+tolist() time: 0.0014204978942871094

Evaluating:  14%|█▍        | 33/228 [00:09<00:50,  3.89it/s][Astep: 33
extend+tolist() time: 0.0012717247009277344

Evaluating:  15%|█▍        | 34/228 [00:09<01:02,  3.12it/s][Astep: 34
extend+tolist() time: 0.0008261203765869141

Evaluating:  15%|█▌        | 35/228 [00:09<00:57,  3.35it/s][Astep: 35
extend+tolist() time: 0.0010406970977783203

Evaluating:  16%|█▌        | 36/228 [00:10<00:53,  3.61it/s][Astep: 36
extend+tolist() time: 0.0007464885711669922

Evaluating:  16%|█▌        | 37/228 [00:10<00:50,  3.79it/s][Astep: 37
extend+tolist() time: 0.001659393310546875

Evaluating:  17%|█▋        | 38/228 [00:10<00:52,  3.62it/s][Astep: 38
extend+tolist() time: 0.0010907649993896484

Evaluating:  17%|█▋        | 39/228 [00:10<00:49,  3.81it/s][Astep: 39
extend+tolist() time: 0.0007379055023193359

Evaluating:  18%|█▊        | 40/228 [00:11<00:47,  3.96it/s][Astep: 40
extend+tolist() time: 0.0006124973297119141

Evaluating:  18%|█▊        | 41/228 [00:11<00:46,  4.05it/s][Astep: 41
extend+tolist() time: 0.0012803077697753906

Evaluating:  18%|█▊        | 42/228 [00:11<00:45,  4.07it/s][Astep: 42
extend+tolist() time: 0.0015439987182617188

Evaluating:  19%|█▉        | 43/228 [00:11<00:48,  3.81it/s][Astep: 43
extend+tolist() time: 0.00395965576171875

Evaluating:  19%|█▉        | 44/228 [00:12<00:52,  3.50it/s][Astep: 44
extend+tolist() time: 0.0007719993591308594

Evaluating:  20%|█▉        | 45/228 [00:12<00:49,  3.71it/s][Astep: 45
extend+tolist() time: 0.0016622543334960938

Evaluating:  20%|██        | 46/228 [00:12<00:50,  3.57it/s][Astep: 46
extend+tolist() time: 0.0015850067138671875

Evaluating:  21%|██        | 47/228 [00:12<00:52,  3.47it/s][Astep: 47
extend+tolist() time: 0.0014774799346923828

Evaluating:  21%|██        | 48/228 [00:13<00:51,  3.49it/s][Astep: 48
extend+tolist() time: 0.0012536048889160156

Evaluating:  21%|██▏       | 49/228 [00:13<01:01,  2.91it/s][Astep: 49
extend+tolist() time: 0.0014622211456298828

Evaluating:  22%|██▏       | 50/228 [00:14<00:56,  3.15it/s][Astep: 50
extend+tolist() time: 0.001653432846069336

Evaluating:  22%|██▏       | 51/228 [00:14<01:03,  2.79it/s][Astep: 51
extend+tolist() time: 0.0015344619750976562

Evaluating:  23%|██▎       | 52/228 [00:14<01:00,  2.92it/s][Astep: 52
extend+tolist() time: 0.0009641647338867188

Evaluating:  23%|██▎       | 53/228 [00:15<00:56,  3.10it/s][Astep: 53
extend+tolist() time: 0.0017790794372558594

Evaluating:  24%|██▎       | 54/228 [00:15<00:55,  3.15it/s][Astep: 54
extend+tolist() time: 0.00113677978515625

Evaluating:  24%|██▍       | 55/228 [00:15<00:50,  3.41it/s][Astep: 55
extend+tolist() time: 0.0007791519165039062

Evaluating:  25%|██▍       | 56/228 [00:15<00:47,  3.63it/s][Astep: 56
extend+tolist() time: 0.0016450881958007812

Evaluating:  25%|██▌       | 57/228 [00:16<00:48,  3.53it/s][Astep: 57
extend+tolist() time: 0.0005996227264404297

Evaluating:  25%|██▌       | 58/228 [00:16<00:45,  3.75it/s][Astep: 58
extend+tolist() time: 0.0012862682342529297

Evaluating:  26%|██▌       | 59/228 [00:16<00:44,  3.82it/s][Astep: 59
extend+tolist() time: 0.0008955001831054688

Evaluating:  26%|██▋       | 60/228 [00:16<00:43,  3.88it/s][Astep: 60
extend+tolist() time: 0.001165151596069336

Evaluating:  27%|██▋       | 61/228 [00:17<00:41,  4.01it/s][Astep: 61
extend+tolist() time: 0.00084686279296875

Evaluating:  27%|██▋       | 62/228 [00:17<00:41,  4.03it/s][Astep: 62
extend+tolist() time: 0.0011932849884033203

Evaluating:  28%|██▊       | 63/228 [00:17<00:40,  4.10it/s][Astep: 63
extend+tolist() time: 0.0007984638214111328

Evaluating:  28%|██▊       | 64/228 [00:17<00:39,  4.13it/s][Astep: 64
extend+tolist() time: 0.0012543201446533203

Evaluating:  29%|██▊       | 65/228 [00:18<00:39,  4.16it/s][Astep: 65
extend+tolist() time: 0.0007991790771484375

Evaluating:  29%|██▉       | 66/228 [00:18<00:38,  4.16it/s][Astep: 66
extend+tolist() time: 0.16050362586975098

Evaluating:  29%|██▉       | 67/228 [00:18<00:45,  3.51it/s][Astep: 67
extend+tolist() time: 0.0013523101806640625

Evaluating:  30%|██▉       | 68/228 [00:18<00:43,  3.64it/s][Astep: 68
extend+tolist() time: 0.0007088184356689453

Evaluating:  30%|███       | 69/228 [00:19<00:41,  3.83it/s][Astep: 69
extend+tolist() time: 0.0014450550079345703

Evaluating:  31%|███       | 70/228 [00:19<00:42,  3.75it/s][Astep: 70
extend+tolist() time: 0.0013568401336669922

Evaluating:  31%|███       | 71/228 [00:19<00:42,  3.70it/s][Astep: 71
extend+tolist() time: 0.0009615421295166016

Evaluating:  32%|███▏      | 72/228 [00:19<00:42,  3.69it/s][Astep: 72
extend+tolist() time: 0.0011954307556152344

Evaluating:  32%|███▏      | 73/228 [00:20<00:40,  3.84it/s][Astep: 73
extend+tolist() time: 0.0005648136138916016

Evaluating:  32%|███▏      | 74/228 [00:20<00:38,  4.00it/s][Astep: 74
extend+tolist() time: 0.0010914802551269531

Evaluating:  33%|███▎      | 75/228 [00:20<00:37,  4.11it/s][Astep: 75
extend+tolist() time: 0.0016248226165771484

Evaluating:  33%|███▎      | 76/228 [00:20<00:39,  3.82it/s][Astep: 76
extend+tolist() time: 0.0006270408630371094

Evaluating:  34%|███▍      | 77/228 [00:21<00:38,  3.95it/s][Astep: 77
extend+tolist() time: 0.0019190311431884766

Evaluating:  34%|███▍      | 78/228 [00:21<00:41,  3.59it/s][Astep: 78
extend+tolist() time: 0.0008401870727539062

Evaluating:  35%|███▍      | 79/228 [00:21<00:40,  3.70it/s][Astep: 79
extend+tolist() time: 0.0013227462768554688

Evaluating:  35%|███▌      | 80/228 [00:22<00:39,  3.78it/s][Astep: 80
extend+tolist() time: 0.0013203620910644531

Evaluating:  36%|███▌      | 81/228 [00:22<00:38,  3.85it/s][Astep: 81
extend+tolist() time: 0.0008130073547363281

Evaluating:  36%|███▌      | 82/228 [00:22<00:37,  3.92it/s][Astep: 82
extend+tolist() time: 0.0012478828430175781

Evaluating:  36%|███▋      | 83/228 [00:22<00:36,  3.99it/s][Astep: 83
extend+tolist() time: 0.0006976127624511719

Evaluating:  37%|███▋      | 84/228 [00:22<00:35,  4.09it/s][Astep: 84
extend+tolist() time: 0.0014407634735107422

Evaluating:  37%|███▋      | 85/228 [00:23<00:36,  3.95it/s][Astep: 85
extend+tolist() time: 0.0009036064147949219

Evaluating:  38%|███▊      | 86/228 [00:23<00:35,  3.97it/s][Astep: 86
extend+tolist() time: 0.0013480186462402344

Evaluating:  38%|███▊      | 87/228 [00:23<00:42,  3.29it/s][Astep: 87
extend+tolist() time: 0.0015516281127929688

Evaluating:  39%|███▊      | 88/228 [00:24<00:40,  3.47it/s][Astep: 88
extend+tolist() time: 0.0008096694946289062

Evaluating:  39%|███▉      | 89/228 [00:24<00:37,  3.70it/s][Astep: 89
extend+tolist() time: 0.0007028579711914062

Evaluating:  39%|███▉      | 90/228 [00:24<00:35,  3.87it/s][Astep: 90
extend+tolist() time: 0.0014224052429199219

Evaluating:  40%|███▉      | 91/228 [00:24<00:35,  3.91it/s][Astep: 91
extend+tolist() time: 0.0011179447174072266

Evaluating:  40%|████      | 92/228 [00:25<00:40,  3.32it/s][Astep: 92
extend+tolist() time: 0.0007941722869873047

Evaluating:  41%|████      | 93/228 [00:25<00:37,  3.57it/s][Astep: 93
extend+tolist() time: 0.0014276504516601562

Evaluating:  41%|████      | 94/228 [00:25<00:37,  3.60it/s][Astep: 94
extend+tolist() time: 0.0006835460662841797

Evaluating:  42%|████▏     | 95/228 [00:26<00:34,  3.81it/s][Astep: 95
extend+tolist() time: 0.0016617774963378906

Evaluating:  42%|████▏     | 96/228 [00:26<00:36,  3.66it/s][Astep: 96
extend+tolist() time: 0.0009751319885253906

Evaluating:  43%|████▎     | 97/228 [00:26<00:34,  3.75it/s][Astep: 97
extend+tolist() time: 0.0013458728790283203

Evaluating:  43%|████▎     | 98/228 [00:26<00:33,  3.90it/s][Astep: 98
extend+tolist() time: 0.0012664794921875

Evaluating:  43%|████▎     | 99/228 [00:27<00:32,  3.93it/s][Astep: 99
extend+tolist() time: 0.0008797645568847656

Evaluating:  44%|████▍     | 100/228 [00:27<00:32,  3.95it/s][Astep: 100
extend+tolist() time: 0.0011107921600341797

Evaluating:  44%|████▍     | 101/228 [00:27<00:31,  4.05it/s][Astep: 101
extend+tolist() time: 0.0008060932159423828

Evaluating:  45%|████▍     | 102/228 [00:27<00:30,  4.07it/s][Astep: 102
extend+tolist() time: 0.0012052059173583984

Evaluating:  45%|████▌     | 103/228 [00:28<00:30,  4.15it/s][Astep: 103
extend+tolist() time: 0.0007660388946533203

Evaluating:  46%|████▌     | 104/228 [00:28<00:29,  4.17it/s][Astep: 104
extend+tolist() time: 0.0011303424835205078

Evaluating:  46%|████▌     | 105/228 [00:28<00:29,  4.22it/s][Astep: 105
extend+tolist() time: 0.0008499622344970703

Evaluating:  46%|████▋     | 106/228 [00:28<00:29,  4.21it/s][Astep: 106
extend+tolist() time: 0.0019083023071289062

Evaluating:  47%|████▋     | 107/228 [00:29<00:31,  3.81it/s][Astep: 107
extend+tolist() time: 0.0009279251098632812

Evaluating:  47%|████▋     | 108/228 [00:29<00:30,  3.94it/s][Astep: 108
extend+tolist() time: 0.0013871192932128906

Evaluating:  48%|████▊     | 109/228 [00:29<00:29,  4.03it/s][Astep: 109
extend+tolist() time: 0.0013744831085205078

Evaluating:  48%|████▊     | 110/228 [00:29<00:29,  4.06it/s][Astep: 110
extend+tolist() time: 0.0007112026214599609

Evaluating:  49%|████▊     | 111/228 [00:29<00:28,  4.14it/s][Astep: 111
extend+tolist() time: 0.18249225616455078

Evaluating:  49%|████▉     | 112/228 [00:30<00:37,  3.13it/s][Astep: 112
extend+tolist() time: 0.00043845176696777344

Evaluating:  50%|████▉     | 113/228 [00:30<00:33,  3.45it/s][Astep: 113
extend+tolist() time: 0.0011768341064453125

Evaluating:  50%|█████     | 114/228 [00:30<00:31,  3.67it/s][Astep: 114
extend+tolist() time: 0.0015964508056640625

Evaluating:  50%|█████     | 115/228 [00:31<00:31,  3.64it/s][Astep: 115
extend+tolist() time: 0.0007381439208984375

Evaluating:  51%|█████     | 116/228 [00:31<00:29,  3.83it/s][Astep: 116
extend+tolist() time: 0.0009171962738037109

Evaluating:  51%|█████▏    | 117/228 [00:31<00:28,  3.94it/s][Astep: 117
extend+tolist() time: 0.0009756088256835938

Evaluating:  52%|█████▏    | 118/228 [00:31<00:27,  3.98it/s][Astep: 118
extend+tolist() time: 0.0006945133209228516

Evaluating:  52%|█████▏    | 119/228 [00:32<00:26,  4.10it/s][Astep: 119
extend+tolist() time: 0.00124359130859375

Evaluating:  53%|█████▎    | 120/228 [00:32<00:25,  4.16it/s][Astep: 120
extend+tolist() time: 0.0007150173187255859

Evaluating:  53%|█████▎    | 121/228 [00:32<00:25,  4.22it/s][Astep: 121
extend+tolist() time: 0.001123189926147461

Evaluating:  54%|█████▎    | 122/228 [00:32<00:24,  4.27it/s][Astep: 122
extend+tolist() time: 0.0007984638214111328

Evaluating:  54%|█████▍    | 123/228 [00:33<00:24,  4.31it/s][Astep: 123
extend+tolist() time: 0.0006921291351318359

Evaluating:  54%|█████▍    | 124/228 [00:33<00:24,  4.31it/s][Astep: 124
extend+tolist() time: 0.001453399658203125

Evaluating:  55%|█████▍    | 125/228 [00:33<00:23,  4.30it/s][Astep: 125
extend+tolist() time: 0.0004565715789794922

Evaluating:  55%|█████▌    | 126/228 [00:33<00:23,  4.35it/s][Astep: 126
extend+tolist() time: 0.0019404888153076172

Evaluating:  56%|█████▌    | 127/228 [00:34<00:26,  3.88it/s][Astep: 127
extend+tolist() time: 0.001863241195678711

Evaluating:  56%|█████▌    | 128/228 [00:34<00:27,  3.61it/s][Astep: 128
extend+tolist() time: 0.0008428096771240234

Evaluating:  57%|█████▋    | 129/228 [00:34<00:25,  3.81it/s][Astep: 129
extend+tolist() time: 0.0012671947479248047

Evaluating:  57%|█████▋    | 130/228 [00:34<00:24,  3.96it/s][Astep: 130
extend+tolist() time: 0.0010459423065185547

Evaluating:  57%|█████▋    | 131/228 [00:35<00:24,  3.97it/s][Astep: 131
extend+tolist() time: 0.0005228519439697266

Evaluating:  58%|█████▊    | 132/228 [00:35<00:23,  4.10it/s][Astep: 132
extend+tolist() time: 0.0012679100036621094

Evaluating:  58%|█████▊    | 133/228 [00:35<00:24,  3.93it/s][Astep: 133
extend+tolist() time: 0.0009531974792480469

Evaluating:  59%|█████▉    | 134/228 [00:35<00:23,  4.08it/s][Astep: 134
extend+tolist() time: 0.0011167526245117188

Evaluating:  59%|█████▉    | 135/228 [00:36<00:23,  3.96it/s][Astep: 135
extend+tolist() time: 0.0009553432464599609

Evaluating:  60%|█████▉    | 136/228 [00:36<00:22,  4.09it/s][Astep: 136
extend+tolist() time: 0.0009448528289794922

Evaluating:  60%|██████    | 137/228 [00:36<00:21,  4.15it/s][Astep: 137
extend+tolist() time: 0.00040912628173828125

Evaluating:  61%|██████    | 138/228 [00:36<00:21,  4.24it/s][Astep: 138
extend+tolist() time: 0.001234292984008789

Evaluating:  61%|██████    | 139/228 [00:37<00:20,  4.28it/s][Astep: 139
extend+tolist() time: 0.0008082389831542969

Evaluating:  61%|██████▏   | 140/228 [00:37<00:20,  4.34it/s][Astep: 140
extend+tolist() time: 0.0008523464202880859

Evaluating:  62%|██████▏   | 141/228 [00:37<00:20,  4.33it/s][Astep: 141
extend+tolist() time: 0.0013659000396728516

Evaluating:  62%|██████▏   | 142/228 [00:37<00:19,  4.33it/s][Astep: 142
extend+tolist() time: 0.0006594657897949219

Evaluating:  63%|██████▎   | 143/228 [00:37<00:19,  4.38it/s][Astep: 143
extend+tolist() time: 0.00039267539978027344

Evaluating:  63%|██████▎   | 144/228 [00:38<00:24,  3.43it/s][Astep: 144
extend+tolist() time: 0.0012350082397460938

Evaluating:  64%|██████▎   | 145/228 [00:38<00:22,  3.67it/s][Astep: 145
extend+tolist() time: 0.0005781650543212891

Evaluating:  64%|██████▍   | 146/228 [00:38<00:21,  3.89it/s][Astep: 146
extend+tolist() time: 0.0003948211669921875

Evaluating:  64%|██████▍   | 147/228 [00:39<00:19,  4.05it/s][Astep: 147
extend+tolist() time: 0.0012919902801513672

Evaluating:  65%|██████▍   | 148/228 [00:39<00:19,  4.14it/s][Astep: 148
extend+tolist() time: 0.0007500648498535156

Evaluating:  65%|██████▌   | 149/228 [00:39<00:18,  4.19it/s][Astep: 149
extend+tolist() time: 0.0003707408905029297

Evaluating:  66%|██████▌   | 150/228 [00:39<00:18,  4.29it/s][Astep: 150
extend+tolist() time: 0.0013294219970703125

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.29it/s][Astep: 151
extend+tolist() time: 0.0006613731384277344

Evaluating:  67%|██████▋   | 152/228 [00:40<00:17,  4.34it/s][Astep: 152
extend+tolist() time: 0.0013873577117919922

Evaluating:  67%|██████▋   | 153/228 [00:40<00:17,  4.32it/s][Astep: 153
extend+tolist() time: 0.0010521411895751953

Evaluating:  68%|██████▊   | 154/228 [00:40<00:17,  4.25it/s][Astep: 154
extend+tolist() time: 0.002129077911376953

Evaluating:  68%|██████▊   | 155/228 [00:41<00:24,  2.98it/s][Astep: 155
extend+tolist() time: 0.0010833740234375

Evaluating:  68%|██████▊   | 156/228 [00:41<00:21,  3.30it/s][Astep: 156
extend+tolist() time: 0.0005588531494140625

Evaluating:  69%|██████▉   | 157/228 [00:41<00:19,  3.57it/s][Astep: 157
extend+tolist() time: 0.0007171630859375

Evaluating:  69%|██████▉   | 158/228 [00:41<00:18,  3.80it/s][Astep: 158
extend+tolist() time: 0.0009496212005615234

Evaluating:  70%|██████▉   | 159/228 [00:42<00:17,  3.98it/s][Astep: 159
extend+tolist() time: 0.0007359981536865234

Evaluating:  70%|███████   | 160/228 [00:42<00:16,  4.07it/s][Astep: 160
extend+tolist() time: 0.0004031658172607422

Evaluating:  71%|███████   | 161/228 [00:42<00:15,  4.19it/s][Astep: 161
extend+tolist() time: 0.0012118816375732422

Evaluating:  71%|███████   | 162/228 [00:42<00:15,  4.23it/s][Astep: 162
extend+tolist() time: 0.0005166530609130859

Evaluating:  71%|███████▏  | 163/228 [00:43<00:15,  4.31it/s][Astep: 163
extend+tolist() time: 0.0008866786956787109

Evaluating:  72%|███████▏  | 164/228 [00:43<00:14,  4.34it/s][Astep: 164
extend+tolist() time: 0.0006067752838134766

Evaluating:  72%|███████▏  | 165/228 [00:43<00:14,  4.40it/s][Astep: 165
extend+tolist() time: 0.0009152889251708984

Evaluating:  73%|███████▎  | 166/228 [00:43<00:14,  4.42it/s][Astep: 166
extend+tolist() time: 0.00036334991455078125

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.47it/s][Astep: 167
extend+tolist() time: 0.0005841255187988281

Evaluating:  74%|███████▎  | 168/228 [00:44<00:13,  4.46it/s][Astep: 168
extend+tolist() time: 0.001730203628540039

Evaluating:  74%|███████▍  | 169/228 [00:44<00:14,  4.17it/s][Astep: 169
extend+tolist() time: 0.00038051605224609375

Evaluating:  75%|███████▍  | 170/228 [00:44<00:13,  4.27it/s][Astep: 170
extend+tolist() time: 0.0008695125579833984

Evaluating:  75%|███████▌  | 171/228 [00:44<00:13,  4.27it/s][Astep: 171
extend+tolist() time: 0.0002913475036621094

Evaluating:  75%|███████▌  | 172/228 [00:45<00:12,  4.36it/s][Astep: 172
extend+tolist() time: 0.001332998275756836

Evaluating:  76%|███████▌  | 173/228 [00:45<00:12,  4.35it/s][Astep: 173
extend+tolist() time: 0.0015416145324707031

Evaluating:  76%|███████▋  | 174/228 [00:45<00:13,  4.12it/s][Astep: 174
extend+tolist() time: 0.0014224052429199219

Evaluating:  77%|███████▋  | 175/228 [00:45<00:13,  3.83it/s][Astep: 175
extend+tolist() time: 0.0012831687927246094

Evaluating:  77%|███████▋  | 176/228 [00:46<00:13,  3.98it/s][Astep: 176
extend+tolist() time: 0.000705718994140625

Evaluating:  78%|███████▊  | 177/228 [00:46<00:12,  4.09it/s][Astep: 177
extend+tolist() time: 0.0009598731994628906

Evaluating:  78%|███████▊  | 178/228 [00:46<00:11,  4.18it/s][Astep: 178
extend+tolist() time: 0.0012519359588623047

Evaluating:  79%|███████▊  | 179/228 [00:46<00:12,  3.99it/s][Astep: 179
extend+tolist() time: 0.0004088878631591797

Evaluating:  79%|███████▉  | 180/228 [00:47<00:11,  4.13it/s][Astep: 180
extend+tolist() time: 0.0008473396301269531

Evaluating:  79%|███████▉  | 181/228 [00:47<00:11,  4.22it/s][Astep: 181
extend+tolist() time: 0.0006895065307617188

Evaluating:  80%|███████▉  | 182/228 [00:47<00:10,  4.27it/s][Astep: 182
extend+tolist() time: 0.0007784366607666016

Evaluating:  80%|████████  | 183/228 [00:47<00:10,  4.29it/s][Astep: 183
extend+tolist() time: 0.0010750293731689453

Evaluating:  81%|████████  | 184/228 [00:48<00:10,  4.33it/s][Astep: 184
extend+tolist() time: 0.0004620552062988281

Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.37it/s][Astep: 185
extend+tolist() time: 0.0016016960144042969

Evaluating:  82%|████████▏ | 186/228 [00:48<00:10,  4.14it/s][Astep: 186
extend+tolist() time: 0.0010976791381835938

Evaluating:  82%|████████▏ | 187/228 [00:48<00:09,  4.12it/s][Astep: 187
extend+tolist() time: 0.00041484832763671875

Evaluating:  82%|████████▏ | 188/228 [00:48<00:09,  4.23it/s][Astep: 188
extend+tolist() time: 0.0011782646179199219

Evaluating:  83%|████████▎ | 189/228 [00:49<00:09,  4.28it/s][Astep: 189
extend+tolist() time: 0.00037932395935058594

Evaluating:  83%|████████▎ | 190/228 [00:49<00:08,  4.34it/s][Astep: 190
extend+tolist() time: 0.0014979839324951172

Evaluating:  84%|████████▍ | 191/228 [00:49<00:09,  4.11it/s][Astep: 191
extend+tolist() time: 0.21880125999450684

Evaluating:  84%|████████▍ | 192/228 [00:50<00:10,  3.30it/s][Astep: 192
extend+tolist() time: 0.00043654441833496094

Evaluating:  85%|████████▍ | 193/228 [00:50<00:09,  3.58it/s][Astep: 193
extend+tolist() time: 0.0014979839324951172

Evaluating:  85%|████████▌ | 194/228 [00:50<00:09,  3.71it/s][Astep: 194
extend+tolist() time: 0.0006020069122314453

Evaluating:  86%|████████▌ | 195/228 [00:50<00:08,  3.90it/s][Astep: 195
extend+tolist() time: 0.0009357929229736328

Evaluating:  86%|████████▌ | 196/228 [00:51<00:07,  4.06it/s][Astep: 196
extend+tolist() time: 0.0006444454193115234

Evaluating:  86%|████████▋ | 197/228 [00:51<00:07,  4.17it/s][Astep: 197
extend+tolist() time: 0.0006954669952392578

Evaluating:  87%|████████▋ | 198/228 [00:51<00:07,  4.23it/s][Astep: 198
extend+tolist() time: 0.0010046958923339844

Evaluating:  87%|████████▋ | 199/228 [00:51<00:06,  4.30it/s][Astep: 199
extend+tolist() time: 0.0018467903137207031

Evaluating:  88%|████████▊ | 200/228 [00:52<00:07,  3.93it/s][Astep: 200
extend+tolist() time: 0.0006952285766601562

Evaluating:  88%|████████▊ | 201/228 [00:52<00:06,  4.07it/s][Astep: 201
extend+tolist() time: 0.0005929470062255859

Evaluating:  89%|████████▊ | 202/228 [00:52<00:06,  4.16it/s][Astep: 202
extend+tolist() time: 0.0004177093505859375

Evaluating:  89%|████████▉ | 203/228 [00:52<00:05,  4.26it/s][Astep: 203
extend+tolist() time: 0.0009849071502685547

Evaluating:  89%|████████▉ | 204/228 [00:52<00:05,  4.33it/s][Astep: 204
extend+tolist() time: 0.0003917217254638672

Evaluating:  90%|████████▉ | 205/228 [00:53<00:05,  4.40it/s][Astep: 205
extend+tolist() time: 0.0003917217254638672

Evaluating:  90%|█████████ | 206/228 [00:53<00:05,  4.30it/s][Astep: 206
extend+tolist() time: 0.0006601810455322266

Evaluating:  91%|█████████ | 207/228 [00:53<00:04,  4.33it/s][Astep: 207
extend+tolist() time: 0.0011246204376220703

Evaluating:  91%|█████████ | 208/228 [00:53<00:04,  4.37it/s][Astep: 208
extend+tolist() time: 0.0007264614105224609

Evaluating:  92%|█████████▏| 209/228 [00:54<00:04,  4.37it/s][Astep: 209
extend+tolist() time: 0.0006287097930908203

Evaluating:  92%|█████████▏| 210/228 [00:54<00:04,  4.39it/s][Astep: 210
extend+tolist() time: 0.0011034011840820312

Evaluating:  93%|█████████▎| 211/228 [00:54<00:03,  4.40it/s][Astep: 211
extend+tolist() time: 0.0011768341064453125

Evaluating:  93%|█████████▎| 212/228 [00:54<00:03,  4.15it/s][Astep: 212
extend+tolist() time: 0.001398324966430664

Evaluating:  93%|█████████▎| 213/228 [00:55<00:03,  4.18it/s][Astep: 213
extend+tolist() time: 0.0007851123809814453

Evaluating:  94%|█████████▍| 214/228 [00:55<00:03,  4.21it/s][Astep: 214
extend+tolist() time: 0.0012617111206054688

Evaluating:  94%|█████████▍| 215/228 [00:55<00:03,  4.23it/s][Astep: 215
extend+tolist() time: 0.0006494522094726562

Evaluating:  95%|█████████▍| 216/228 [00:55<00:02,  4.17it/s][Astep: 216
extend+tolist() time: 0.0005786418914794922

Evaluating:  95%|█████████▌| 217/228 [00:55<00:02,  4.26it/s][Astep: 217
extend+tolist() time: 0.0009849071502685547

Evaluating:  96%|█████████▌| 218/228 [00:56<00:02,  4.31it/s][Astep: 218
extend+tolist() time: 0.0010578632354736328

Evaluating:  96%|█████████▌| 219/228 [00:56<00:02,  4.23it/s][Astep: 219
extend+tolist() time: 0.0008866786956787109

Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.30it/s][Astep: 220
extend+tolist() time: 0.00041174888610839844

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.35it/s][Astep: 221
extend+tolist() time: 0.0007228851318359375

Evaluating:  97%|█████████▋| 222/228 [00:57<00:01,  4.36it/s][Astep: 222
extend+tolist() time: 0.0004391670227050781

Evaluating:  98%|█████████▊| 223/228 [00:57<00:01,  4.38it/s][Astep: 223
extend+tolist() time: 0.00045490264892578125

Evaluating:  98%|█████████▊| 224/228 [00:57<00:00,  4.40it/s][Astep: 224
extend+tolist() time: 0.0003764629364013672

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.30it/s][Astep: 225
extend+tolist() time: 0.00044918060302734375

Evaluating:  99%|█████████▉| 226/228 [00:58<00:00,  4.38it/s][Astep: 226
extend+tolist() time: 0.0005471706390380859

Evaluating: 100%|█████████▉| 227/228 [00:58<00:00,  4.38it/s][Astep: 227
extend+tolist() time: 0.0004963874816894531

Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.84it/s][A09/05/2023 14:55:42 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 14:55:42 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:55:43 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 14:55:44 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-c3c9972b-68da-4a65-af0f-6adedd974ca3-1-0.arrow
09/05/2023 14:55:44 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:55:44 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 14:55:45 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:01<00:00,  3.71it/s]
09/05/2023 14:55:45 - INFO - __main__ -   Step: 4200, Validation Metrics: {'pred_1_num': 9613, 'pred_-1_num': 848, 'pred_0_num': 340, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.787149338024257, 'f1_micro': 0.787149338024257, 'f1_macro': 0.46533590717841805, 'f1_weighted': 0.7600221649160918, 'f1_-1': 0.3612261806130903, 'f1_0': 0.15730337078651685, 'f1_1': 0.8774781701356472, 'precision_micro': 0.787149338024257, 'precision_macro': 0.5238945450110547, 'precision_weighted': 0.7493458868698613, 'precision_-1': 0.5141509433962265, 'precision_0': 0.22647058823529412, 'precision_1': 0.8310621034016437, 'recall_micro': 0.787149338024257, 'recall_macro': 0.4427676302244879, 'recall_weighted': 0.787149338024257, 'recall_-1': 0.2784163473818646, 'recall_0': 0.12050078247261346, 'recall_1': 0.9293857608189856, 'roc_auc_micro': 0.89662455000939, 'roc_auc_macro': 0.6993840136837998, 'roc_auc_weighted': 0.6700113123424549, 'roc_auc_-1': 0.7498752593864891, 'roc_auc_0': 0.6946461840869618, 'roc_auc_1': 0.6536305975779485}
[2023-09-05 14:55:59,484] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4201/66600 [52:03<569:44:24, 32.87s/it]09/05/2023 14:55:59 - INFO - __main__ -   Step: 4201, LR: 1.931748938295311e-05, Loss: 0.1387307345867157
[2023-09-05 14:56:13,126] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4202/66600 [52:16<469:45:00, 27.10s/it]09/05/2023 14:56:13 - INFO - __main__ -   Step: 4202, LR: 1.9317179678759327e-05, Loss: 0.20278584957122803
[2023-09-05 14:56:28,077] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4203/66600 [52:31<406:33:48, 23.46s/it]09/05/2023 14:56:28 - INFO - __main__ -   Step: 4203, LR: 1.9316869974565545e-05, Loss: 0.1651972234249115
[2023-09-05 14:56:42,209] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4204/66600 [52:45<358:04:02, 20.66s/it]09/05/2023 14:56:42 - INFO - __main__ -   Step: 4204, LR: 1.9316560270371763e-05, Loss: 0.12737630307674408
[2023-09-05 14:56:56,874] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4205/66600 [53:00<326:53:44, 18.86s/it]09/05/2023 14:56:56 - INFO - __main__ -   Step: 4205, LR: 1.931625056617798e-05, Loss: 0.16016417741775513
[2023-09-05 14:57:09,510] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4206/66600 [53:13<294:31:33, 16.99s/it]09/05/2023 14:57:09 - INFO - __main__ -   Step: 4206, LR: 1.93159408619842e-05, Loss: 0.20897778868675232
[2023-09-05 14:57:23,761] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4207/66600 [53:27<280:15:44, 16.17s/it]09/05/2023 14:57:23 - INFO - __main__ -   Step: 4207, LR: 1.9315631157790417e-05, Loss: 0.23754377663135529
[2023-09-05 14:57:37,679] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4208/66600 [53:41<268:32:29, 15.49s/it]09/05/2023 14:57:37 - INFO - __main__ -   Step: 4208, LR: 1.9315321453596635e-05, Loss: 0.1302826702594757
[2023-09-05 14:57:51,580] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4209/66600 [53:55<260:15:13, 15.02s/it]09/05/2023 14:57:51 - INFO - __main__ -   Step: 4209, LR: 1.9315011749402853e-05, Loss: 0.08677580952644348
[2023-09-05 14:58:06,862] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4210/66600 [54:10<261:37:39, 15.10s/it]09/05/2023 14:58:06 - INFO - __main__ -   Step: 4210, LR: 1.931470204520907e-05, Loss: 0.11381229013204575
[2023-09-05 14:58:20,321] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4211/66600 [54:23<253:06:45, 14.61s/it]09/05/2023 14:58:20 - INFO - __main__ -   Step: 4211, LR: 1.931439234101529e-05, Loss: 0.15164518356323242
[2023-09-05 14:58:33,754] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4212/66600 [54:37<247:00:37, 14.25s/it]09/05/2023 14:58:33 - INFO - __main__ -   Step: 4212, LR: 1.9314082636821507e-05, Loss: 0.12529847025871277
[2023-09-05 14:58:47,506] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4213/66600 [54:51<244:23:58, 14.10s/it]09/05/2023 14:58:47 - INFO - __main__ -   Step: 4213, LR: 1.9313772932627725e-05, Loss: 0.15373027324676514
[2023-09-05 14:59:02,323] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4214/66600 [55:05<248:06:27, 14.32s/it]09/05/2023 14:59:02 - INFO - __main__ -   Step: 4214, LR: 1.9313463228433943e-05, Loss: 0.12382636964321136
[2023-09-05 14:59:16,517] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4215/66600 [55:20<247:27:53, 14.28s/it]09/05/2023 14:59:16 - INFO - __main__ -   Step: 4215, LR: 1.931315352424016e-05, Loss: 0.18331320583820343
[2023-09-05 14:59:31,355] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4216/66600 [55:34<250:21:33, 14.45s/it]09/05/2023 14:59:31 - INFO - __main__ -   Step: 4216, LR: 1.931284382004638e-05, Loss: 0.1164875328540802
[2023-09-05 14:59:45,065] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4217/66600 [55:48<246:31:18, 14.23s/it]09/05/2023 14:59:45 - INFO - __main__ -   Step: 4217, LR: 1.9312534115852597e-05, Loss: 0.19311541318893433
[2023-09-05 14:59:58,749] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4218/66600 [56:02<243:42:02, 14.06s/it]09/05/2023 14:59:58 - INFO - __main__ -   Step: 4218, LR: 1.9312224411658816e-05, Loss: 0.1553640365600586
[2023-09-05 15:00:13,176] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4219/66600 [56:16<245:35:13, 14.17s/it]09/05/2023 15:00:13 - INFO - __main__ -   Step: 4219, LR: 1.9311914707465034e-05, Loss: 0.1517135053873062
[2023-09-05 15:00:27,211] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4220/66600 [56:30<244:51:45, 14.13s/it]09/05/2023 15:00:27 - INFO - __main__ -   Step: 4220, LR: 1.931160500327125e-05, Loss: 0.16601121425628662
[2023-09-05 15:00:40,405] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4221/66600 [56:43<239:59:11, 13.85s/it]09/05/2023 15:00:40 - INFO - __main__ -   Step: 4221, LR: 1.931129529907747e-05, Loss: 0.1923680603504181
[2023-09-05 15:00:54,860] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4222/66600 [56:58<243:07:46, 14.03s/it]09/05/2023 15:00:54 - INFO - __main__ -   Step: 4222, LR: 1.9310985594883688e-05, Loss: 0.12956011295318604
[2023-09-05 15:01:08,746] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4223/66600 [57:12<242:21:58, 13.99s/it]09/05/2023 15:01:08 - INFO - __main__ -   Step: 4223, LR: 1.9310675890689906e-05, Loss: 0.13879720866680145
[2023-09-05 15:01:22,229] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4224/66600 [57:25<239:44:37, 13.84s/it]09/05/2023 15:01:22 - INFO - __main__ -   Step: 4224, LR: 1.9310366186496124e-05, Loss: 0.1534913331270218
[2023-09-05 15:01:36,538] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4225/66600 [57:40<242:11:23, 13.98s/it]09/05/2023 15:01:36 - INFO - __main__ -   Step: 4225, LR: 1.9310056482302342e-05, Loss: 0.1428471803665161
[2023-09-05 15:01:50,879] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4226/66600 [57:54<244:04:34, 14.09s/it]09/05/2023 15:01:50 - INFO - __main__ -   Step: 4226, LR: 1.930974677810856e-05, Loss: 0.13469679653644562
[2023-09-05 15:02:05,345] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4227/66600 [58:08<246:02:22, 14.20s/it]09/05/2023 15:02:05 - INFO - __main__ -   Step: 4227, LR: 1.9309437073914778e-05, Loss: 0.19104990363121033
[2023-09-05 15:02:19,162] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4228/66600 [58:22<244:02:34, 14.09s/it]09/05/2023 15:02:19 - INFO - __main__ -   Step: 4228, LR: 1.9309127369720996e-05, Loss: 0.16136673092842102
[2023-09-05 15:02:33,867] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4229/66600 [58:37<247:15:34, 14.27s/it]09/05/2023 15:02:33 - INFO - __main__ -   Step: 4229, LR: 1.9308817665527214e-05, Loss: 0.17540930211544037
[2023-09-05 15:02:48,683] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4230/66600 [58:52<250:05:03, 14.43s/it]09/05/2023 15:02:48 - INFO - __main__ -   Step: 4230, LR: 1.9308507961333432e-05, Loss: 0.1186346784234047
[2023-09-05 15:03:02,984] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4231/66600 [59:06<249:23:01, 14.39s/it]09/05/2023 15:03:02 - INFO - __main__ -   Step: 4231, LR: 1.9308198257139654e-05, Loss: 0.13192953169345856
[2023-09-05 15:03:17,350] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4232/66600 [59:20<249:13:49, 14.39s/it]09/05/2023 15:03:17 - INFO - __main__ -   Step: 4232, LR: 1.930788855294587e-05, Loss: 0.13681268692016602
[2023-09-05 15:03:31,450] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4233/66600 [59:35<247:44:11, 14.30s/it]09/05/2023 15:03:31 - INFO - __main__ -   Step: 4233, LR: 1.9307578848752086e-05, Loss: 0.1525837779045105
[2023-09-05 15:03:45,332] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4234/66600 [59:48<245:33:34, 14.17s/it]09/05/2023 15:03:45 - INFO - __main__ -   Step: 4234, LR: 1.9307269144558304e-05, Loss: 0.1351740062236786
[2023-09-05 15:04:00,263] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4235/66600 [1:00:03<249:29:17, 14.40s/it]09/05/2023 15:04:00 - INFO - __main__ -   Step: 4235, LR: 1.9306959440364523e-05, Loss: 0.13711223006248474
[2023-09-05 15:04:14,013] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4236/66600 [1:00:17<246:06:03, 14.21s/it]09/05/2023 15:04:14 - INFO - __main__ -   Step: 4236, LR: 1.930664973617074e-05, Loss: 0.15790781378746033
[2023-09-05 15:04:28,671] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4237/66600 [1:00:32<248:26:31, 14.34s/it]09/05/2023 15:04:28 - INFO - __main__ -   Step: 4237, LR: 1.930634003197696e-05, Loss: 0.13446563482284546
[2023-09-05 15:04:43,147] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4238/66600 [1:00:46<249:08:12, 14.38s/it]09/05/2023 15:04:43 - INFO - __main__ -   Step: 4238, LR: 1.930603032778318e-05, Loss: 0.16057077050209045
[2023-09-05 15:04:56,973] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4239/66600 [1:01:00<246:14:27, 14.22s/it]09/05/2023 15:04:56 - INFO - __main__ -   Step: 4239, LR: 1.9305720623589398e-05, Loss: 0.17161253094673157
[2023-09-05 15:05:10,835] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4240/66600 [1:01:14<244:24:16, 14.11s/it]09/05/2023 15:05:10 - INFO - __main__ -   Step: 4240, LR: 1.9305410919395613e-05, Loss: 0.1167498379945755
[2023-09-05 15:05:24,869] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4241/66600 [1:01:28<244:00:34, 14.09s/it]09/05/2023 15:05:24 - INFO - __main__ -   Step: 4241, LR: 1.930510121520183e-05, Loss: 0.16525182127952576
[2023-09-05 15:05:39,072] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4242/66600 [1:01:42<244:36:34, 14.12s/it]09/05/2023 15:05:39 - INFO - __main__ -   Step: 4242, LR: 1.930479151100805e-05, Loss: 0.1056864857673645
[2023-09-05 15:05:54,165] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4243/66600 [1:01:57<249:39:15, 14.41s/it]09/05/2023 15:05:54 - INFO - __main__ -   Step: 4243, LR: 1.9304481806814267e-05, Loss: 0.13614647090435028
[2023-09-05 15:06:07,245] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4244/66600 [1:02:10<242:43:09, 14.01s/it]09/05/2023 15:06:07 - INFO - __main__ -   Step: 4244, LR: 1.9304172102620485e-05, Loss: 0.15542809665203094
[2023-09-05 15:06:20,273] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4245/66600 [1:02:23<237:35:57, 13.72s/it]09/05/2023 15:06:20 - INFO - __main__ -   Step: 4245, LR: 1.9303862398426707e-05, Loss: 0.13545387983322144
[2023-09-05 15:06:34,174] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4246/66600 [1:02:37<238:33:01, 13.77s/it]09/05/2023 15:06:34 - INFO - __main__ -   Step: 4246, LR: 1.9303552694232925e-05, Loss: 0.1686604768037796
[2023-09-05 15:06:48,687] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4247/66600 [1:02:52<242:23:30, 13.99s/it]09/05/2023 15:06:48 - INFO - __main__ -   Step: 4247, LR: 1.930324299003914e-05, Loss: 0.11189275234937668
[2023-09-05 15:07:03,184] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4248/66600 [1:03:06<244:59:51, 14.15s/it]09/05/2023 15:07:03 - INFO - __main__ -   Step: 4248, LR: 1.9302933285845357e-05, Loss: 0.1825242042541504
[2023-09-05 15:07:17,493] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4249/66600 [1:03:21<245:50:37, 14.19s/it]09/05/2023 15:07:17 - INFO - __main__ -   Step: 4249, LR: 1.9302623581651575e-05, Loss: 0.17401927709579468
[2023-09-05 15:07:31,626] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4250/66600 [1:03:35<245:31:19, 14.18s/it]09/05/2023 15:07:31 - INFO - __main__ -   Step: 4250, LR: 1.9302313877457793e-05, Loss: 0.08937306702136993
[2023-09-05 15:07:47,556] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4251/66600 [1:03:51<254:37:38, 14.70s/it]09/05/2023 15:07:47 - INFO - __main__ -   Step: 4251, LR: 1.930200417326401e-05, Loss: 0.1280188262462616
[2023-09-05 15:08:02,929] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4252/66600 [1:04:06<258:06:38, 14.90s/it]09/05/2023 15:08:02 - INFO - __main__ -   Step: 4252, LR: 1.930169446907023e-05, Loss: 0.13512396812438965
[2023-09-05 15:08:17,657] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4253/66600 [1:04:21<257:11:42, 14.85s/it]09/05/2023 15:08:17 - INFO - __main__ -   Step: 4253, LR: 1.930138476487645e-05, Loss: 0.14615584909915924
[2023-09-05 15:08:34,017] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4254/66600 [1:04:37<265:02:00, 15.30s/it]09/05/2023 15:08:34 - INFO - __main__ -   Step: 4254, LR: 1.930107506068267e-05, Loss: 0.1697172075510025
[2023-09-05 15:08:47,411] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4255/66600 [1:04:50<255:06:30, 14.73s/it]09/05/2023 15:08:47 - INFO - __main__ -   Step: 4255, LR: 1.9300765356488884e-05, Loss: 0.20020222663879395
[2023-09-05 15:09:01,829] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4256/66600 [1:05:05<253:28:40, 14.64s/it]09/05/2023 15:09:01 - INFO - __main__ -   Step: 4256, LR: 1.9300455652295102e-05, Loss: 0.10607866197824478
[2023-09-05 15:09:15,851] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4257/66600 [1:05:19<250:16:42, 14.45s/it]09/05/2023 15:09:15 - INFO - __main__ -   Step: 4257, LR: 1.930014594810132e-05, Loss: 0.14421968162059784
[2023-09-05 15:09:30,194] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4258/66600 [1:05:33<249:42:42, 14.42s/it]09/05/2023 15:09:30 - INFO - __main__ -   Step: 4258, LR: 1.9299836243907538e-05, Loss: 0.13843801617622375
[2023-09-05 15:09:45,740] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4259/66600 [1:05:49<255:33:21, 14.76s/it]09/05/2023 15:09:45 - INFO - __main__ -   Step: 4259, LR: 1.9299526539713756e-05, Loss: 0.1804879903793335
[2023-09-05 15:09:59,243] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4260/66600 [1:06:02<249:01:47, 14.38s/it]09/05/2023 15:09:59 - INFO - __main__ -   Step: 4260, LR: 1.9299216835519977e-05, Loss: 0.18451595306396484
[2023-09-05 15:10:13,630] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4261/66600 [1:06:17<249:03:27, 14.38s/it]09/05/2023 15:10:13 - INFO - __main__ -   Step: 4261, LR: 1.9298907131326195e-05, Loss: 0.17746806144714355
[2023-09-05 15:10:28,182] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4262/66600 [1:06:31<249:56:16, 14.43s/it]09/05/2023 15:10:28 - INFO - __main__ -   Step: 4262, LR: 1.9298597427132414e-05, Loss: 0.13868257403373718
[2023-09-05 15:10:42,175] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4263/66600 [1:06:45<247:38:26, 14.30s/it]09/05/2023 15:10:42 - INFO - __main__ -   Step: 4263, LR: 1.9298287722938628e-05, Loss: 0.19462095201015472
[2023-09-05 15:10:56,571] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4264/66600 [1:07:00<248:07:45, 14.33s/it]09/05/2023 15:10:56 - INFO - __main__ -   Step: 4264, LR: 1.9297978018744846e-05, Loss: 0.13609372079372406
[2023-09-05 15:11:11,330] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4265/66600 [1:07:14<250:21:15, 14.46s/it]09/05/2023 15:11:11 - INFO - __main__ -   Step: 4265, LR: 1.9297668314551064e-05, Loss: 0.18123511970043182
[2023-09-05 15:11:25,176] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4266/66600 [1:07:28<247:10:04, 14.27s/it]09/05/2023 15:11:25 - INFO - __main__ -   Step: 4266, LR: 1.9297358610357282e-05, Loss: 0.13800375163555145
[2023-09-05 15:11:39,316] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4267/66600 [1:07:42<246:27:56, 14.23s/it]09/05/2023 15:11:39 - INFO - __main__ -   Step: 4267, LR: 1.9297048906163504e-05, Loss: 0.12659919261932373
[2023-09-05 15:11:52,589] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4268/66600 [1:07:56<241:27:50, 13.95s/it]09/05/2023 15:11:52 - INFO - __main__ -   Step: 4268, LR: 1.9296739201969722e-05, Loss: 0.1738881766796112
[2023-09-05 15:12:06,868] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4269/66600 [1:08:10<243:11:31, 14.05s/it]09/05/2023 15:12:06 - INFO - __main__ -   Step: 4269, LR: 1.929642949777594e-05, Loss: 0.18575918674468994
[2023-09-05 15:12:21,181] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4270/66600 [1:08:24<244:34:39, 14.13s/it]09/05/2023 15:12:21 - INFO - __main__ -   Step: 4270, LR: 1.9296119793582155e-05, Loss: 0.1842394769191742
[2023-09-05 15:12:35,081] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4271/66600 [1:08:38<243:23:46, 14.06s/it]09/05/2023 15:12:35 - INFO - __main__ -   Step: 4271, LR: 1.9295810089388373e-05, Loss: 0.17322701215744019
[2023-09-05 15:12:49,953] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4272/66600 [1:08:53<247:37:15, 14.30s/it]09/05/2023 15:12:49 - INFO - __main__ -   Step: 4272, LR: 1.929550038519459e-05, Loss: 0.14713194966316223
[2023-09-05 15:13:03,496] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4273/66600 [1:09:07<243:40:26, 14.07s/it]09/05/2023 15:13:03 - INFO - __main__ -   Step: 4273, LR: 1.929519068100081e-05, Loss: 0.18831771612167358
[2023-09-05 15:13:17,915] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4274/66600 [1:09:21<245:27:34, 14.18s/it]09/05/2023 15:13:17 - INFO - __main__ -   Step: 4274, LR: 1.929488097680703e-05, Loss: 0.15661419928073883
[2023-09-05 15:13:32,074] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4275/66600 [1:09:35<245:21:30, 14.17s/it]09/05/2023 15:13:32 - INFO - __main__ -   Step: 4275, LR: 1.929457127261325e-05, Loss: 0.14495986700057983
[2023-09-05 15:13:45,187] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4276/66600 [1:09:48<239:51:14, 13.85s/it]09/05/2023 15:13:45 - INFO - __main__ -   Step: 4276, LR: 1.9294261568419466e-05, Loss: 0.12896619737148285
[2023-09-05 15:13:58,359] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4277/66600 [1:10:01<236:18:12, 13.65s/it]09/05/2023 15:13:58 - INFO - __main__ -   Step: 4277, LR: 1.9293951864225684e-05, Loss: 0.1596023440361023
[2023-09-05 15:14:12,115] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4278/66600 [1:10:15<236:50:59, 13.68s/it]09/05/2023 15:14:12 - INFO - __main__ -   Step: 4278, LR: 1.92936421600319e-05, Loss: 0.16855919361114502
[2023-09-05 15:14:26,380] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4279/66600 [1:10:29<239:52:23, 13.86s/it]09/05/2023 15:14:26 - INFO - __main__ -   Step: 4279, LR: 1.9293332455838117e-05, Loss: 0.16622459888458252
[2023-09-05 15:14:39,419] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4280/66600 [1:10:42<235:37:37, 13.61s/it]09/05/2023 15:14:39 - INFO - __main__ -   Step: 4280, LR: 1.9293022751644335e-05, Loss: 0.152889221906662
[2023-09-05 15:14:53,701] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4281/66600 [1:10:57<239:06:20, 13.81s/it]09/05/2023 15:14:53 - INFO - __main__ -   Step: 4281, LR: 1.9292713047450557e-05, Loss: 0.1470569372177124
[2023-09-05 15:15:07,861] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4282/66600 [1:11:11<240:54:14, 13.92s/it]09/05/2023 15:15:07 - INFO - __main__ -   Step: 4282, LR: 1.9292403343256775e-05, Loss: 0.13325563073158264
[2023-09-05 15:15:22,094] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4283/66600 [1:11:25<242:32:58, 14.01s/it]09/05/2023 15:15:22 - INFO - __main__ -   Step: 4283, LR: 1.9292093639062993e-05, Loss: 0.105553038418293
[2023-09-05 15:15:35,288] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4284/66600 [1:11:38<238:17:43, 13.77s/it]09/05/2023 15:15:35 - INFO - __main__ -   Step: 4284, LR: 1.929178393486921e-05, Loss: 0.0983981043100357
[2023-09-05 15:15:50,349] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4285/66600 [1:11:53<245:00:57, 14.15s/it]09/05/2023 15:15:50 - INFO - __main__ -   Step: 4285, LR: 1.929147423067543e-05, Loss: 0.14408010244369507
[2023-09-05 15:16:05,560] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4286/66600 [1:12:09<250:29:48, 14.47s/it]09/05/2023 15:16:05 - INFO - __main__ -   Step: 4286, LR: 1.9291164526481644e-05, Loss: 0.12494970113039017
[2023-09-05 15:16:20,532] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4287/66600 [1:12:24<253:05:19, 14.62s/it]09/05/2023 15:16:20 - INFO - __main__ -   Step: 4287, LR: 1.929085482228786e-05, Loss: 0.19888927042484283
[2023-09-05 15:16:34,896] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4288/66600 [1:12:38<251:44:51, 14.54s/it]09/05/2023 15:16:34 - INFO - __main__ -   Step: 4288, LR: 1.9290545118094083e-05, Loss: 0.13044342398643494
[2023-09-05 15:16:49,187] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4289/66600 [1:12:52<250:25:30, 14.47s/it]09/05/2023 15:16:49 - INFO - __main__ -   Step: 4289, LR: 1.92902354139003e-05, Loss: 0.1353113204240799
[2023-09-05 15:17:03,580] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4290/66600 [1:13:07<250:01:59, 14.45s/it]09/05/2023 15:17:03 - INFO - __main__ -   Step: 4290, LR: 1.928992570970652e-05, Loss: 0.12605106830596924
[2023-09-05 15:17:18,816] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4291/66600 [1:13:22<254:07:41, 14.68s/it]09/05/2023 15:17:18 - INFO - __main__ -   Step: 4291, LR: 1.9289616005512737e-05, Loss: 0.14359746873378754
[2023-09-05 15:17:34,255] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4292/66600 [1:13:37<258:03:23, 14.91s/it]09/05/2023 15:17:34 - INFO - __main__ -   Step: 4292, LR: 1.9289306301318955e-05, Loss: 0.10989788919687271
[2023-09-05 15:17:47,554] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4293/66600 [1:13:51<249:41:03, 14.43s/it]09/05/2023 15:17:47 - INFO - __main__ -   Step: 4293, LR: 1.928899659712517e-05, Loss: 0.11878291517496109
[2023-09-05 15:18:01,960] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4294/66600 [1:14:05<249:34:35, 14.42s/it]09/05/2023 15:18:01 - INFO - __main__ -   Step: 4294, LR: 1.9288686892931388e-05, Loss: 0.16541019082069397
[2023-09-05 15:18:16,578] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4295/66600 [1:14:20<250:36:01, 14.48s/it]09/05/2023 15:18:16 - INFO - __main__ -   Step: 4295, LR: 1.928837718873761e-05, Loss: 0.1423284113407135
[2023-09-05 15:18:30,101] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4296/66600 [1:14:33<245:37:33, 14.19s/it]09/05/2023 15:18:30 - INFO - __main__ -   Step: 4296, LR: 1.9288067484543828e-05, Loss: 0.19359993934631348
[2023-09-05 15:18:44,281] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4297/66600 [1:14:47<245:33:18, 14.19s/it]09/05/2023 15:18:44 - INFO - __main__ -   Step: 4297, LR: 1.9287757780350046e-05, Loss: 0.13556893169879913
[2023-09-05 15:18:58,782] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4298/66600 [1:15:02<247:10:21, 14.28s/it]09/05/2023 15:18:58 - INFO - __main__ -   Step: 4298, LR: 1.9287448076156264e-05, Loss: 0.11472378671169281
[2023-09-05 15:19:13,117] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4299/66600 [1:15:16<247:26:30, 14.30s/it]09/05/2023 15:19:13 - INFO - __main__ -   Step: 4299, LR: 1.9287138371962482e-05, Loss: 0.1088603064417839
[2023-09-05 15:19:27,539] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4300/66600 [1:15:31<248:05:02, 14.34s/it]09/05/2023 15:19:27 - INFO - __main__ -   Step: 4300, LR: 1.92868286677687e-05, Loss: 0.11866261065006256
09/05/2023 15:19:27 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.002637624740600586

Evaluating:   0%|          | 1/228 [00:00<01:34,  2.39it/s][Astep: 1
extend+tolist() time: 0.0013871192932128906

Evaluating:   1%|          | 2/228 [00:00<01:13,  3.06it/s][Astep: 2
extend+tolist() time: 0.0015399456024169922

Evaluating:   1%|▏         | 3/228 [00:01<01:15,  2.99it/s][Astep: 3
extend+tolist() time: 0.002048015594482422

Evaluating:   2%|▏         | 4/228 [00:01<01:12,  3.10it/s][Astep: 4
extend+tolist() time: 0.13545513153076172

Evaluating:   2%|▏         | 5/228 [00:01<01:17,  2.89it/s][Astep: 5
extend+tolist() time: 0.001926422119140625

Evaluating:   3%|▎         | 6/228 [00:02<01:15,  2.94it/s][Astep: 6
extend+tolist() time: 0.0017614364624023438

Evaluating:   3%|▎         | 7/228 [00:02<01:14,  2.95it/s][Astep: 7
extend+tolist() time: 0.0012781620025634766

Evaluating:   4%|▎         | 8/228 [00:02<01:08,  3.21it/s][Astep: 8
extend+tolist() time: 0.0007557868957519531

Evaluating:   4%|▍         | 9/228 [00:02<01:02,  3.50it/s][Astep: 9
extend+tolist() time: 0.0011563301086425781

Evaluating:   4%|▍         | 10/228 [00:03<00:59,  3.68it/s][Astep: 10
extend+tolist() time: 0.0008215904235839844

Evaluating:   5%|▍         | 11/228 [00:03<00:57,  3.80it/s][Astep: 11
extend+tolist() time: 0.0010082721710205078

Evaluating:   5%|▌         | 12/228 [00:03<01:02,  3.43it/s][Astep: 12
extend+tolist() time: 0.0006616115570068359

Evaluating:   6%|▌         | 13/228 [00:03<00:58,  3.66it/s][Astep: 13
extend+tolist() time: 0.0005862712860107422

Evaluating:   6%|▌         | 14/228 [00:04<00:55,  3.87it/s][Astep: 14
extend+tolist() time: 0.0009891986846923828

Evaluating:   7%|▋         | 15/228 [00:04<00:52,  4.03it/s][Astep: 15
extend+tolist() time: 0.0005972385406494141

Evaluating:   7%|▋         | 16/228 [00:04<00:51,  4.15it/s][Astep: 16
extend+tolist() time: 0.0006194114685058594

Evaluating:   7%|▋         | 17/228 [00:04<00:49,  4.22it/s][Astep: 17
extend+tolist() time: 0.0008473396301269531

Evaluating:   8%|▊         | 18/228 [00:05<00:50,  4.19it/s][Astep: 18
extend+tolist() time: 0.001573801040649414

Evaluating:   8%|▊         | 19/228 [00:05<00:52,  3.98it/s][Astep: 19
extend+tolist() time: 0.0009703636169433594

Evaluating:   9%|▉         | 20/228 [00:05<00:53,  3.88it/s][Astep: 20
extend+tolist() time: 0.0011899471282958984

Evaluating:   9%|▉         | 21/228 [00:05<00:51,  4.02it/s][Astep: 21
extend+tolist() time: 0.0006909370422363281

Evaluating:  10%|▉         | 22/228 [00:06<00:50,  4.11it/s][Astep: 22
extend+tolist() time: 0.0011088848114013672

Evaluating:  10%|█         | 23/228 [00:06<00:49,  4.18it/s][Astep: 23
extend+tolist() time: 0.0006608963012695312

Evaluating:  11%|█         | 24/228 [00:06<00:48,  4.24it/s][Astep: 24
extend+tolist() time: 0.0014276504516601562

Evaluating:  11%|█         | 25/228 [00:06<00:50,  4.02it/s][Astep: 25
extend+tolist() time: 0.0018780231475830078

Evaluating:  11%|█▏        | 26/228 [00:07<00:55,  3.63it/s][Astep: 26
extend+tolist() time: 0.0007281303405761719

Evaluating:  12%|█▏        | 27/228 [00:07<01:03,  3.15it/s][Astep: 27
extend+tolist() time: 0.0017328262329101562

Evaluating:  12%|█▏        | 28/228 [00:07<01:02,  3.18it/s][Astep: 28
extend+tolist() time: 0.000339508056640625

Evaluating:  13%|█▎        | 29/228 [00:08<00:57,  3.49it/s][Astep: 29
extend+tolist() time: 0.0010993480682373047

Evaluating:  13%|█▎        | 30/228 [00:08<00:53,  3.72it/s][Astep: 30
extend+tolist() time: 0.0010833740234375

Evaluating:  14%|█▎        | 31/228 [00:08<00:53,  3.66it/s][Astep: 31
extend+tolist() time: 0.0005524158477783203

Evaluating:  14%|█▍        | 32/228 [00:08<00:50,  3.86it/s][Astep: 32
extend+tolist() time: 0.0009884834289550781

Evaluating:  14%|█▍        | 33/228 [00:09<00:51,  3.79it/s][Astep: 33
extend+tolist() time: 0.0011990070343017578

Evaluating:  15%|█▍        | 34/228 [00:09<00:53,  3.63it/s][Astep: 34
extend+tolist() time: 0.15658020973205566

Evaluating:  15%|█▌        | 35/228 [00:09<01:00,  3.19it/s][Astep: 35
extend+tolist() time: 0.0006377696990966797

Evaluating:  16%|█▌        | 36/228 [00:10<00:55,  3.45it/s][Astep: 36
extend+tolist() time: 0.0007295608520507812

Evaluating:  16%|█▌        | 37/228 [00:10<00:51,  3.68it/s][Astep: 37
extend+tolist() time: 0.0015683174133300781

Evaluating:  17%|█▋        | 38/228 [00:10<00:53,  3.55it/s][Astep: 38
extend+tolist() time: 0.0007693767547607422

Evaluating:  17%|█▋        | 39/228 [00:10<00:50,  3.76it/s][Astep: 39
extend+tolist() time: 0.001100301742553711

Evaluating:  18%|█▊        | 40/228 [00:11<00:48,  3.91it/s][Astep: 40
extend+tolist() time: 0.0005910396575927734

Evaluating:  18%|█▊        | 41/228 [00:11<00:46,  4.03it/s][Astep: 41
extend+tolist() time: 0.0012459754943847656

Evaluating:  18%|█▊        | 42/228 [00:11<00:46,  4.04it/s][Astep: 42
extend+tolist() time: 0.001529693603515625

Evaluating:  19%|█▉        | 43/228 [00:11<00:48,  3.79it/s][Astep: 43
extend+tolist() time: 0.0018398761749267578

Evaluating:  19%|█▉        | 44/228 [00:12<00:52,  3.50it/s][Astep: 44
extend+tolist() time: 0.0007135868072509766

Evaluating:  20%|█▉        | 45/228 [00:12<00:57,  3.18it/s][Astep: 45
extend+tolist() time: 0.0016200542449951172

Evaluating:  20%|██        | 46/228 [00:12<00:57,  3.19it/s][Astep: 46
extend+tolist() time: 0.0015218257904052734

Evaluating:  21%|██        | 47/228 [00:13<00:56,  3.21it/s][Astep: 47
extend+tolist() time: 0.0014204978942871094

Evaluating:  21%|██        | 48/228 [00:13<00:54,  3.31it/s][Astep: 48
extend+tolist() time: 0.0012350082397460938

Evaluating:  21%|██▏       | 49/228 [00:13<00:54,  3.31it/s][Astep: 49
extend+tolist() time: 0.001463174819946289

Evaluating:  22%|██▏       | 50/228 [00:14<00:51,  3.48it/s][Astep: 50
extend+tolist() time: 0.0015969276428222656

Evaluating:  22%|██▏       | 51/228 [00:14<00:51,  3.42it/s][Astep: 51
extend+tolist() time: 0.001477956771850586

Evaluating:  23%|██▎       | 52/228 [00:14<00:52,  3.38it/s][Astep: 52
extend+tolist() time: 0.0009737014770507812

Evaluating:  23%|██▎       | 53/228 [00:14<00:50,  3.45it/s][Astep: 53
extend+tolist() time: 0.0017571449279785156

Evaluating:  24%|██▎       | 54/228 [00:15<00:51,  3.39it/s][Astep: 54
extend+tolist() time: 0.0011835098266601562

Evaluating:  24%|██▍       | 55/228 [00:15<00:48,  3.60it/s][Astep: 55
extend+tolist() time: 0.0008080005645751953

Evaluating:  25%|██▍       | 56/228 [00:15<00:45,  3.77it/s][Astep: 56
extend+tolist() time: 0.0016608238220214844

Evaluating:  25%|██▌       | 57/228 [00:15<00:47,  3.62it/s][Astep: 57
extend+tolist() time: 0.000614166259765625

Evaluating:  25%|██▌       | 58/228 [00:16<00:52,  3.21it/s][Astep: 58
extend+tolist() time: 0.0013155937194824219

Evaluating:  26%|██▌       | 59/228 [00:16<00:49,  3.42it/s][Astep: 59
extend+tolist() time: 0.0008978843688964844

Evaluating:  26%|██▋       | 60/228 [00:16<00:47,  3.57it/s][Astep: 60
extend+tolist() time: 0.0011725425720214844

Evaluating:  27%|██▋       | 61/228 [00:17<00:44,  3.77it/s][Astep: 61
extend+tolist() time: 0.0008256435394287109

Evaluating:  27%|██▋       | 62/228 [00:17<00:43,  3.86it/s][Astep: 62
extend+tolist() time: 0.0011718273162841797

Evaluating:  28%|██▊       | 63/228 [00:17<00:41,  3.97it/s][Astep: 63
extend+tolist() time: 0.0008063316345214844

Evaluating:  28%|██▊       | 64/228 [00:17<00:40,  4.05it/s][Astep: 64
extend+tolist() time: 0.0012509822845458984

Evaluating:  29%|██▊       | 65/228 [00:18<00:39,  4.09it/s][Astep: 65
extend+tolist() time: 0.000789642333984375

Evaluating:  29%|██▉       | 66/228 [00:18<00:39,  4.13it/s][Astep: 66
extend+tolist() time: 0.0011429786682128906

Evaluating:  29%|██▉       | 67/228 [00:18<00:38,  4.20it/s][Astep: 67
extend+tolist() time: 0.0008745193481445312

Evaluating:  30%|██▉       | 68/228 [00:18<00:38,  4.16it/s][Astep: 68
extend+tolist() time: 0.17047715187072754

Evaluating:  30%|███       | 69/228 [00:19<00:45,  3.46it/s][Astep: 69
extend+tolist() time: 0.0014958381652832031

Evaluating:  31%|███       | 70/228 [00:19<00:45,  3.49it/s][Astep: 70
extend+tolist() time: 0.0013535022735595703

Evaluating:  31%|███       | 71/228 [00:19<00:44,  3.52it/s][Astep: 71
extend+tolist() time: 0.0009636878967285156

Evaluating:  32%|███▏      | 72/228 [00:19<00:43,  3.57it/s][Astep: 72
extend+tolist() time: 0.00115966796875

Evaluating:  32%|███▏      | 73/228 [00:20<00:41,  3.75it/s][Astep: 73
extend+tolist() time: 0.0005655288696289062

Evaluating:  32%|███▏      | 74/228 [00:20<00:39,  3.94it/s][Astep: 74
extend+tolist() time: 0.0010476112365722656

Evaluating:  33%|███▎      | 75/228 [00:20<00:37,  4.05it/s][Astep: 75
extend+tolist() time: 0.0012555122375488281

Evaluating:  33%|███▎      | 76/228 [00:20<00:40,  3.78it/s][Astep: 76
extend+tolist() time: 0.0010883808135986328

Evaluating:  34%|███▍      | 77/228 [00:21<00:38,  3.93it/s][Astep: 77
extend+tolist() time: 0.0018506050109863281

Evaluating:  34%|███▍      | 78/228 [00:21<00:41,  3.59it/s][Astep: 78
extend+tolist() time: 0.0008320808410644531

Evaluating:  35%|███▍      | 79/228 [00:21<00:40,  3.70it/s][Astep: 79
extend+tolist() time: 0.001361846923828125

Evaluating:  35%|███▌      | 80/228 [00:22<00:39,  3.77it/s][Astep: 80
extend+tolist() time: 0.0012624263763427734

Evaluating:  36%|███▌      | 81/228 [00:22<00:38,  3.84it/s][Astep: 81
extend+tolist() time: 0.0008449554443359375

Evaluating:  36%|███▌      | 82/228 [00:22<00:44,  3.28it/s][Astep: 82
extend+tolist() time: 0.0013232231140136719

Evaluating:  36%|███▋      | 83/228 [00:22<00:41,  3.49it/s][Astep: 83
extend+tolist() time: 0.0006935596466064453

Evaluating:  37%|███▋      | 84/228 [00:23<00:38,  3.71it/s][Astep: 84
extend+tolist() time: 0.0014317035675048828

Evaluating:  37%|███▋      | 85/228 [00:23<00:38,  3.70it/s][Astep: 85
extend+tolist() time: 0.0008924007415771484

Evaluating:  38%|███▊      | 86/228 [00:23<00:37,  3.79it/s][Astep: 86
extend+tolist() time: 0.001669168472290039

Evaluating:  38%|███▊      | 87/228 [00:23<00:36,  3.86it/s][Astep: 87
extend+tolist() time: 0.0009160041809082031

Evaluating:  39%|███▊      | 88/228 [00:24<00:35,  3.91it/s][Astep: 88
extend+tolist() time: 0.001186370849609375

Evaluating:  39%|███▉      | 89/228 [00:24<00:34,  4.04it/s][Astep: 89
extend+tolist() time: 0.0007026195526123047

Evaluating:  39%|███▉      | 90/228 [00:24<00:33,  4.13it/s][Astep: 90
extend+tolist() time: 0.0014066696166992188

Evaluating:  40%|███▉      | 91/228 [00:24<00:33,  4.09it/s][Astep: 91
extend+tolist() time: 0.0007374286651611328

Evaluating:  40%|████      | 92/228 [00:25<00:32,  4.14it/s][Astep: 92
extend+tolist() time: 0.0011906623840332031

Evaluating:  41%|████      | 93/228 [00:25<00:32,  4.18it/s][Astep: 93
extend+tolist() time: 0.0012967586517333984

Evaluating:  41%|████      | 94/228 [00:25<00:33,  4.02it/s][Astep: 94
extend+tolist() time: 0.0007140636444091797

Evaluating:  42%|████▏     | 95/228 [00:25<00:32,  4.10it/s][Astep: 95
extend+tolist() time: 0.0016489028930664062

Evaluating:  42%|████▏     | 96/228 [00:26<00:34,  3.84it/s][Astep: 96
extend+tolist() time: 0.0009908676147460938

Evaluating:  43%|████▎     | 97/228 [00:26<00:33,  3.88it/s][Astep: 97
extend+tolist() time: 0.0012836456298828125

Evaluating:  43%|████▎     | 98/228 [00:26<00:32,  3.99it/s][Astep: 98
extend+tolist() time: 0.0012540817260742188

Evaluating:  43%|████▎     | 99/228 [00:26<00:32,  4.00it/s][Astep: 99
extend+tolist() time: 0.0008916854858398438

Evaluating:  44%|████▍     | 100/228 [00:27<00:31,  4.00it/s][Astep: 100
extend+tolist() time: 0.001155853271484375

Evaluating:  44%|████▍     | 101/228 [00:27<00:38,  3.28it/s][Astep: 101
extend+tolist() time: 0.0008158683776855469

Evaluating:  45%|████▍     | 102/228 [00:27<00:36,  3.50it/s][Astep: 102
extend+tolist() time: 0.0011785030364990234

Evaluating:  45%|████▌     | 103/228 [00:28<00:33,  3.70it/s][Astep: 103
extend+tolist() time: 0.000759124755859375

Evaluating:  46%|████▌     | 104/228 [00:28<00:32,  3.87it/s][Astep: 104
extend+tolist() time: 0.0006814002990722656

Evaluating:  46%|████▌     | 105/228 [00:28<00:30,  4.01it/s][Astep: 105
extend+tolist() time: 0.00131988525390625

Evaluating:  46%|████▋     | 106/228 [00:28<00:30,  4.05it/s][Astep: 106
extend+tolist() time: 0.0017321109771728516

Evaluating:  47%|████▋     | 107/228 [00:29<00:32,  3.73it/s][Astep: 107
extend+tolist() time: 0.0007555484771728516

Evaluating:  47%|████▋     | 108/228 [00:29<00:30,  3.87it/s][Astep: 108
extend+tolist() time: 0.0012502670288085938

Evaluating:  48%|████▊     | 109/228 [00:29<00:29,  3.99it/s][Astep: 109
extend+tolist() time: 0.0009279251098632812

Evaluating:  48%|████▊     | 110/228 [00:29<00:29,  4.04it/s][Astep: 110
extend+tolist() time: 0.0011272430419921875

Evaluating:  49%|████▊     | 111/228 [00:30<00:28,  4.12it/s][Astep: 111
extend+tolist() time: 0.001941680908203125

Evaluating:  49%|████▉     | 112/228 [00:30<00:30,  3.75it/s][Astep: 112
extend+tolist() time: 0.00045299530029296875

Evaluating:  50%|████▉     | 113/228 [00:30<00:29,  3.95it/s][Astep: 113
extend+tolist() time: 0.0008320808410644531

Evaluating:  50%|█████     | 114/228 [00:30<00:28,  4.06it/s][Astep: 114
extend+tolist() time: 0.18388009071350098

Evaluating:  50%|█████     | 115/228 [00:31<00:35,  3.21it/s][Astep: 115
extend+tolist() time: 0.0011353492736816406

Evaluating:  51%|█████     | 116/228 [00:31<00:32,  3.49it/s][Astep: 116
extend+tolist() time: 0.0009593963623046875

Evaluating:  51%|█████▏    | 117/228 [00:31<00:30,  3.69it/s][Astep: 117
extend+tolist() time: 0.001390218734741211

Evaluating:  52%|█████▏    | 118/228 [00:31<00:28,  3.80it/s][Astep: 118
extend+tolist() time: 0.0006442070007324219

Evaluating:  52%|█████▏    | 119/228 [00:32<00:27,  3.97it/s][Astep: 119
extend+tolist() time: 0.0011491775512695312

Evaluating:  53%|█████▎    | 120/228 [00:32<00:26,  4.07it/s][Astep: 120
extend+tolist() time: 0.0007102489471435547

Evaluating:  53%|█████▎    | 121/228 [00:32<00:25,  4.17it/s][Astep: 121
extend+tolist() time: 0.0011594295501708984

Evaluating:  54%|█████▎    | 122/228 [00:32<00:25,  4.23it/s][Astep: 122
extend+tolist() time: 0.0008194446563720703

Evaluating:  54%|█████▍    | 123/228 [00:33<00:24,  4.26it/s][Astep: 123
extend+tolist() time: 0.0006861686706542969

Evaluating:  54%|█████▍    | 124/228 [00:33<00:24,  4.30it/s][Astep: 124
extend+tolist() time: 0.0014600753784179688

Evaluating:  55%|█████▍    | 125/228 [00:33<00:23,  4.30it/s][Astep: 125
extend+tolist() time: 0.0004949569702148438

Evaluating:  55%|█████▌    | 126/228 [00:33<00:23,  4.35it/s][Astep: 126
extend+tolist() time: 0.0019369125366210938

Evaluating:  56%|█████▌    | 127/228 [00:34<00:26,  3.88it/s][Astep: 127
extend+tolist() time: 0.001935720443725586

Evaluating:  56%|█████▌    | 128/228 [00:34<00:27,  3.62it/s][Astep: 128
extend+tolist() time: 0.0008413791656494141

Evaluating:  57%|█████▋    | 129/228 [00:34<00:26,  3.81it/s][Astep: 129
extend+tolist() time: 0.0013475418090820312

Evaluating:  57%|█████▋    | 130/228 [00:34<00:24,  3.94it/s][Astep: 130
extend+tolist() time: 0.0009970664978027344

Evaluating:  57%|█████▋    | 131/228 [00:35<00:24,  3.95it/s][Astep: 131
extend+tolist() time: 0.0008919239044189453

Evaluating:  58%|█████▊    | 132/228 [00:35<00:23,  4.10it/s][Astep: 132
extend+tolist() time: 0.0012202262878417969

Evaluating:  58%|█████▊    | 133/228 [00:35<00:24,  3.92it/s][Astep: 133
extend+tolist() time: 0.0009319782257080078

Evaluating:  59%|█████▉    | 134/228 [00:36<00:28,  3.30it/s][Astep: 134
extend+tolist() time: 0.001163482666015625

Evaluating:  59%|█████▉    | 135/228 [00:36<00:27,  3.42it/s][Astep: 135
extend+tolist() time: 0.0004899501800537109

Evaluating:  60%|█████▉    | 136/228 [00:36<00:24,  3.68it/s][Astep: 136
extend+tolist() time: 0.0013985633850097656

Evaluating:  60%|██████    | 137/228 [00:36<00:23,  3.85it/s][Astep: 137
extend+tolist() time: 0.00043320655822753906

Evaluating:  61%|██████    | 138/228 [00:37<00:22,  4.02it/s][Astep: 138
extend+tolist() time: 0.0012280941009521484

Evaluating:  61%|██████    | 139/228 [00:37<00:21,  4.13it/s][Astep: 139
extend+tolist() time: 0.0005173683166503906

Evaluating:  61%|██████▏   | 140/228 [00:37<00:20,  4.24it/s][Astep: 140
extend+tolist() time: 0.0008745193481445312

Evaluating:  62%|██████▏   | 141/228 [00:37<00:20,  4.27it/s][Astep: 141
extend+tolist() time: 0.0013918876647949219

Evaluating:  62%|██████▏   | 142/228 [00:37<00:20,  4.28it/s][Astep: 142
extend+tolist() time: 0.0006456375122070312

Evaluating:  63%|██████▎   | 143/228 [00:38<00:19,  4.32it/s][Astep: 143
extend+tolist() time: 0.0003719329833984375

Evaluating:  63%|██████▎   | 144/228 [00:38<00:19,  4.38it/s][Astep: 144
extend+tolist() time: 0.0012359619140625

Evaluating:  64%|██████▎   | 145/228 [00:38<00:18,  4.38it/s][Astep: 145
extend+tolist() time: 0.0005555152893066406

Evaluating:  64%|██████▍   | 146/228 [00:38<00:18,  4.42it/s][Astep: 146
extend+tolist() time: 0.00043129920959472656

Evaluating:  64%|██████▍   | 147/228 [00:39<00:18,  4.44it/s][Astep: 147
extend+tolist() time: 0.0012269020080566406

Evaluating:  65%|██████▍   | 148/228 [00:39<00:18,  4.43it/s][Astep: 148
extend+tolist() time: 0.0007383823394775391

Evaluating:  65%|██████▌   | 149/228 [00:39<00:17,  4.42it/s][Astep: 149
extend+tolist() time: 0.0003879070281982422

Evaluating:  66%|██████▌   | 150/228 [00:39<00:17,  4.45it/s][Astep: 150
extend+tolist() time: 0.0013911724090576172

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.41it/s][Astep: 151
extend+tolist() time: 0.0006246566772460938

Evaluating:  67%|██████▋   | 152/228 [00:40<00:17,  4.41it/s][Astep: 152
extend+tolist() time: 0.0012636184692382812

Evaluating:  67%|██████▋   | 153/228 [00:40<00:17,  4.38it/s][Astep: 153
extend+tolist() time: 0.0010416507720947266

Evaluating:  68%|██████▊   | 154/228 [00:40<00:17,  4.30it/s][Astep: 154
extend+tolist() time: 0.0021355152130126953

Evaluating:  68%|██████▊   | 155/228 [00:40<00:19,  3.82it/s][Astep: 155
extend+tolist() time: 0.0006802082061767578

Evaluating:  68%|██████▊   | 156/228 [00:41<00:18,  3.98it/s][Astep: 156
extend+tolist() time: 0.0009102821350097656

Evaluating:  69%|██████▉   | 157/228 [00:41<00:17,  4.12it/s][Astep: 157
extend+tolist() time: 0.0006673336029052734

Evaluating:  69%|██████▉   | 158/228 [00:41<00:16,  4.20it/s][Astep: 158
extend+tolist() time: 0.0006003379821777344

Evaluating:  70%|██████▉   | 159/228 [00:41<00:16,  4.27it/s][Astep: 159
extend+tolist() time: 0.0011813640594482422

Evaluating:  70%|███████   | 160/228 [00:42<00:15,  4.29it/s][Astep: 160
extend+tolist() time: 0.00038623809814453125

Evaluating:  71%|███████   | 161/228 [00:42<00:15,  4.36it/s][Astep: 161
extend+tolist() time: 0.0007967948913574219

Evaluating:  71%|███████   | 162/228 [00:42<00:15,  4.37it/s][Astep: 162
extend+tolist() time: 0.0009684562683105469

Evaluating:  71%|███████▏  | 163/228 [00:42<00:14,  4.40it/s][Astep: 163
extend+tolist() time: 0.0013279914855957031

Evaluating:  72%|███████▏  | 164/228 [00:43<00:14,  4.43it/s][Astep: 164
extend+tolist() time: 0.0005786418914794922

Evaluating:  72%|███████▏  | 165/228 [00:43<00:14,  4.44it/s][Astep: 165
extend+tolist() time: 0.0004565715789794922

Evaluating:  73%|███████▎  | 166/228 [00:43<00:13,  4.45it/s][Astep: 166
extend+tolist() time: 0.0008146762847900391

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.47it/s][Astep: 167
extend+tolist() time: 0.0006172657012939453

Evaluating:  74%|███████▎  | 168/228 [00:43<00:13,  4.49it/s][Astep: 168
extend+tolist() time: 0.0016293525695800781

Evaluating:  74%|███████▍  | 169/228 [00:44<00:14,  4.18it/s][Astep: 169
extend+tolist() time: 0.00036978721618652344

Evaluating:  75%|███████▍  | 170/228 [00:44<00:13,  4.29it/s][Astep: 170
extend+tolist() time: 0.0008685588836669922

Evaluating:  75%|███████▌  | 171/228 [00:44<00:13,  4.29it/s][Astep: 171
extend+tolist() time: 0.0003151893615722656

Evaluating:  75%|███████▌  | 172/228 [00:45<00:16,  3.41it/s][Astep: 172
extend+tolist() time: 0.0012989044189453125

Evaluating:  76%|███████▌  | 173/228 [00:45<00:15,  3.64it/s][Astep: 173
extend+tolist() time: 0.0011653900146484375

Evaluating:  76%|███████▋  | 174/228 [00:45<00:14,  3.65it/s][Astep: 174
extend+tolist() time: 0.0019283294677734375

Evaluating:  77%|███████▋  | 175/228 [00:45<00:14,  3.54it/s][Astep: 175
extend+tolist() time: 0.0012214183807373047

Evaluating:  77%|███████▋  | 176/228 [00:46<00:13,  3.74it/s][Astep: 176
extend+tolist() time: 0.0006825923919677734

Evaluating:  78%|███████▊  | 177/228 [00:46<00:12,  3.93it/s][Astep: 177
extend+tolist() time: 0.0005862712860107422

Evaluating:  78%|███████▊  | 178/228 [00:46<00:12,  4.07it/s][Astep: 178
extend+tolist() time: 0.0017571449279785156

Evaluating:  79%|███████▊  | 179/228 [00:46<00:12,  3.92it/s][Astep: 179
extend+tolist() time: 0.0004100799560546875

Evaluating:  79%|███████▉  | 180/228 [00:47<00:11,  4.07it/s][Astep: 180
extend+tolist() time: 0.00037097930908203125

Evaluating:  79%|███████▉  | 181/228 [00:47<00:11,  4.21it/s][Astep: 181
extend+tolist() time: 0.0010311603546142578

Evaluating:  80%|███████▉  | 182/228 [00:47<00:10,  4.28it/s][Astep: 182
extend+tolist() time: 0.0007786750793457031

Evaluating:  80%|████████  | 183/228 [00:47<00:10,  4.31it/s][Astep: 183
extend+tolist() time: 0.0010509490966796875

Evaluating:  81%|████████  | 184/228 [00:47<00:10,  4.34it/s][Astep: 184
extend+tolist() time: 0.0004360675811767578

Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.38it/s][Astep: 185
extend+tolist() time: 0.0011425018310546875

Evaluating:  82%|████████▏ | 186/228 [00:48<00:10,  4.16it/s][Astep: 186
extend+tolist() time: 0.0015177726745605469

Evaluating:  82%|████████▏ | 187/228 [00:48<00:09,  4.14it/s][Astep: 187
extend+tolist() time: 0.00046324729919433594

Evaluating:  82%|████████▏ | 188/228 [00:48<00:09,  4.25it/s][Astep: 188
extend+tolist() time: 0.0011038780212402344

Evaluating:  83%|████████▎ | 189/228 [00:49<00:09,  4.28it/s][Astep: 189
extend+tolist() time: 0.0003528594970703125

Evaluating:  83%|████████▎ | 190/228 [00:49<00:08,  4.36it/s][Astep: 190
extend+tolist() time: 0.0011444091796875

Evaluating:  84%|████████▍ | 191/228 [00:49<00:08,  4.13it/s][Astep: 191
extend+tolist() time: 0.0012214183807373047

Evaluating:  84%|████████▍ | 192/228 [00:49<00:08,  4.21it/s][Astep: 192
extend+tolist() time: 0.0004482269287109375

Evaluating:  85%|████████▍ | 193/228 [00:50<00:08,  4.28it/s][Astep: 193
extend+tolist() time: 0.0014879703521728516

Evaluating:  85%|████████▌ | 194/228 [00:50<00:08,  4.20it/s][Astep: 194
extend+tolist() time: 0.2088942527770996

Evaluating:  86%|████████▌ | 195/228 [00:50<00:09,  3.36it/s][Astep: 195
extend+tolist() time: 0.0005738735198974609

Evaluating:  86%|████████▌ | 196/228 [00:51<00:08,  3.64it/s][Astep: 196
extend+tolist() time: 0.001033782958984375

Evaluating:  86%|████████▋ | 197/228 [00:51<00:08,  3.84it/s][Astep: 197
extend+tolist() time: 0.0006954669952392578

Evaluating:  87%|████████▋ | 198/228 [00:51<00:07,  4.01it/s][Astep: 198
extend+tolist() time: 0.0006120204925537109

Evaluating:  87%|████████▋ | 199/228 [00:51<00:06,  4.15it/s][Astep: 199
extend+tolist() time: 0.0014827251434326172

Evaluating:  88%|████████▊ | 200/228 [00:51<00:07,  3.84it/s][Astep: 200
extend+tolist() time: 0.0011856555938720703

Evaluating:  88%|████████▊ | 201/228 [00:52<00:06,  3.98it/s][Astep: 201
extend+tolist() time: 0.0005908012390136719

Evaluating:  89%|████████▊ | 202/228 [00:52<00:06,  4.11it/s][Astep: 202
extend+tolist() time: 0.00045013427734375

Evaluating:  89%|████████▉ | 203/228 [00:52<00:05,  4.23it/s][Astep: 203
extend+tolist() time: 0.0009675025939941406

Evaluating:  89%|████████▉ | 204/228 [00:52<00:05,  4.32it/s][Astep: 204
extend+tolist() time: 0.0004189014434814453

Evaluating:  90%|████████▉ | 205/228 [00:53<00:05,  4.37it/s][Astep: 205
extend+tolist() time: 0.0003228187561035156

Evaluating:  90%|█████████ | 206/228 [00:53<00:04,  4.42it/s][Astep: 206
extend+tolist() time: 0.0006501674652099609

Evaluating:  91%|█████████ | 207/228 [00:53<00:04,  4.45it/s][Astep: 207
extend+tolist() time: 0.0006220340728759766

Evaluating:  91%|█████████ | 208/228 [00:53<00:04,  4.46it/s][Astep: 208
extend+tolist() time: 0.0007097721099853516

Evaluating:  92%|█████████▏| 209/228 [00:53<00:04,  4.44it/s][Astep: 209
extend+tolist() time: 0.0006051063537597656

Evaluating:  92%|█████████▏| 210/228 [00:54<00:04,  4.44it/s][Astep: 210
extend+tolist() time: 0.0010144710540771484

Evaluating:  93%|█████████▎| 211/228 [00:54<00:03,  4.44it/s][Astep: 211
extend+tolist() time: 0.0011162757873535156

Evaluating:  93%|█████████▎| 212/228 [00:54<00:03,  4.19it/s][Astep: 212
extend+tolist() time: 0.0013852119445800781

Evaluating:  93%|█████████▎| 213/228 [00:54<00:03,  4.21it/s][Astep: 213
extend+tolist() time: 0.0007064342498779297

Evaluating:  94%|█████████▍| 214/228 [00:55<00:03,  4.25it/s][Astep: 214
extend+tolist() time: 0.0012230873107910156

Evaluating:  94%|█████████▍| 215/228 [00:55<00:03,  4.28it/s][Astep: 215
extend+tolist() time: 0.0006735324859619141

Evaluating:  95%|█████████▍| 216/228 [00:55<00:02,  4.33it/s][Astep: 216
extend+tolist() time: 0.0005743503570556641

Evaluating:  95%|█████████▌| 217/228 [00:55<00:02,  4.38it/s][Astep: 217
extend+tolist() time: 0.003476858139038086

Evaluating:  96%|█████████▌| 218/228 [00:56<00:02,  4.39it/s][Astep: 218
extend+tolist() time: 0.0010597705841064453

Evaluating:  96%|█████████▌| 219/228 [00:56<00:02,  4.29it/s][Astep: 219
extend+tolist() time: 0.0004982948303222656

Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.35it/s][Astep: 220
extend+tolist() time: 0.00084686279296875

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.40it/s][Astep: 221
extend+tolist() time: 0.0007042884826660156

Evaluating:  97%|█████████▋| 222/228 [00:56<00:01,  4.42it/s][Astep: 222
extend+tolist() time: 0.00044345855712890625

Evaluating:  98%|█████████▊| 223/228 [00:57<00:01,  4.43it/s][Astep: 223
extend+tolist() time: 0.0003962516784667969

Evaluating:  98%|█████████▊| 224/228 [00:57<00:00,  4.46it/s][Astep: 224
extend+tolist() time: 0.0008046627044677734

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.47it/s][Astep: 225
extend+tolist() time: 0.00042366981506347656

Evaluating:  99%|█████████▉| 226/228 [00:57<00:00,  4.49it/s][Astep: 226
extend+tolist() time: 0.0006117820739746094

Evaluating: 100%|█████████▉| 227/228 [00:58<00:00,  4.48it/s][Astep: 227
extend+tolist() time: 0.0005071163177490234

Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.89it/s][A09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 15:20:26 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 15:20:27 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 15:20:28 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 15:20:28 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 15:20:28 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:00<00:00,  3.75it/s]
09/05/2023 15:20:28 - INFO - __main__ -   Step: 4300, Validation Metrics: {'pred_1_num': 9570, 'pred_-1_num': 957, 'pred_0_num': 274, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7851124895842978, 'f1_micro': 0.7851124895842978, 'f1_macro': 0.4611649710838144, 'f1_weighted': 0.7583766695173053, 'f1_-1': 0.36781609195402293, 'f1_0': 0.140197152245345, 'f1_1': 0.8754816690520753, 'precision_micro': 0.7851124895842978, 'precision_macro': 0.5164517055783101, 'precision_weighted': 0.7454126832610796, 'precision_-1': 0.48484848484848486, 'precision_0': 0.23357664233576642, 'precision_1': 0.8309299895506792, 'recall_micro': 0.7851124895842978, 'recall_macro': 0.4405114080145813, 'recall_weighted': 0.7851124895842978, 'recall_-1': 0.2962962962962963, 'recall_0': 0.10015649452269171, 'recall_1': 0.9250814332247557, 'roc_auc_micro': 0.909786839373137, 'roc_auc_macro': 0.7310185961643457, 'roc_auc_weighted': 0.7217076394109472, 'roc_auc_-1': 0.8092513073908812, 'roc_auc_0': 0.6745389787169298, 'roc_auc_1': 0.7092655023852259}
[2023-09-05 15:20:41,898] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4301/66600 [1:16:45<559:41:44, 32.34s/it]09/05/2023 15:20:41 - INFO - __main__ -   Step: 4301, LR: 1.9286518963574914e-05, Loss: 0.14920535683631897
[2023-09-05 15:20:55,427] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4302/66600 [1:16:58<462:00:59, 26.70s/it]09/05/2023 15:20:55 - INFO - __main__ -   Step: 4302, LR: 1.9286209259381136e-05, Loss: 0.15277087688446045
[2023-09-05 15:21:09,051] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4303/66600 [1:17:12<394:08:09, 22.78s/it]09/05/2023 15:21:09 - INFO - __main__ -   Step: 4303, LR: 1.9285899555187354e-05, Loss: 0.19317062199115753
[2023-09-05 15:21:23,465] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4304/66600 [1:17:27<350:42:57, 20.27s/it]09/05/2023 15:21:23 - INFO - __main__ -   Step: 4304, LR: 1.9285589850993572e-05, Loss: 0.21944522857666016
[2023-09-05 15:21:39,045] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4305/66600 [1:17:42<326:22:40, 18.86s/it]09/05/2023 15:21:39 - INFO - __main__ -   Step: 4305, LR: 1.928528014679979e-05, Loss: 0.1414823830127716
[2023-09-05 15:21:53,170] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4306/66600 [1:17:56<301:47:07, 17.44s/it]09/05/2023 15:21:53 - INFO - __main__ -   Step: 4306, LR: 1.9284970442606008e-05, Loss: 0.13718920946121216
[2023-09-05 15:22:06,344] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4307/66600 [1:18:09<279:38:01, 16.16s/it]09/05/2023 15:22:06 - INFO - __main__ -   Step: 4307, LR: 1.9284660738412226e-05, Loss: 0.16760395467281342
[2023-09-05 15:22:19,509] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4308/66600 [1:18:23<264:04:55, 15.26s/it]09/05/2023 15:22:19 - INFO - __main__ -   Step: 4308, LR: 1.9284351034218444e-05, Loss: 0.14360001683235168
[2023-09-05 15:22:33,740] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4309/66600 [1:18:37<258:43:35, 14.95s/it]09/05/2023 15:22:33 - INFO - __main__ -   Step: 4309, LR: 1.9284041330024662e-05, Loss: 0.14387986063957214
[2023-09-05 15:22:48,826] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4310/66600 [1:18:52<259:24:46, 14.99s/it]09/05/2023 15:22:48 - INFO - __main__ -   Step: 4310, LR: 1.928373162583088e-05, Loss: 0.10956469923257828
[2023-09-05 15:23:03,675] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4311/66600 [1:19:07<258:40:05, 14.95s/it]09/05/2023 15:23:03 - INFO - __main__ -   Step: 4311, LR: 1.92834219216371e-05, Loss: 0.1541321575641632
[2023-09-05 15:23:18,847] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4312/66600 [1:19:22<259:48:47, 15.02s/it]09/05/2023 15:23:18 - INFO - __main__ -   Step: 4312, LR: 1.9283112217443317e-05, Loss: 0.14602130651474
wandb: Network error (ProxyError), entering retry loop.
[2023-09-05 15:23:33,596] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4313/66600 [1:19:37<258:25:33, 14.94s/it]09/05/2023 15:23:33 - INFO - __main__ -   Step: 4313, LR: 1.9282802513249535e-05, Loss: 0.1570989042520523
[2023-09-05 15:23:48,445] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4314/66600 [1:19:51<257:57:49, 14.91s/it]09/05/2023 15:23:48 - INFO - __main__ -   Step: 4314, LR: 1.9282492809055753e-05, Loss: 0.17530107498168945
[2023-09-05 15:24:02,913] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4315/66600 [1:20:06<255:40:03, 14.78s/it]09/05/2023 15:24:02 - INFO - __main__ -   Step: 4315, LR: 1.928218310486197e-05, Loss: 0.1786770224571228
[2023-09-05 15:24:18,332] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4316/66600 [1:20:21<258:59:37, 14.97s/it]09/05/2023 15:24:18 - INFO - __main__ -   Step: 4316, LR: 1.928187340066819e-05, Loss: 0.08826150000095367
[2023-09-05 15:24:32,339] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4317/66600 [1:20:35<253:59:48, 14.68s/it]09/05/2023 15:24:32 - INFO - __main__ -   Step: 4317, LR: 1.9281563696474407e-05, Loss: 0.20474669337272644
[2023-09-05 15:24:45,868] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4318/66600 [1:20:49<248:00:33, 14.34s/it]09/05/2023 15:24:45 - INFO - __main__ -   Step: 4318, LR: 1.9281253992280625e-05, Loss: 0.18314461410045624
[2023-09-05 15:24:59,571] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4319/66600 [1:21:03<244:43:31, 14.15s/it]09/05/2023 15:24:59 - INFO - __main__ -   Step: 4319, LR: 1.9280944288086843e-05, Loss: 0.13595235347747803
[2023-09-05 15:25:14,403] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4320/66600 [1:21:17<248:16:47, 14.35s/it]09/05/2023 15:25:14 - INFO - __main__ -   Step: 4320, LR: 1.928063458389306e-05, Loss: 0.15347827970981598
[2023-09-05 15:25:28,433] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4321/66600 [1:21:31<246:36:36, 14.26s/it]09/05/2023 15:25:28 - INFO - __main__ -   Step: 4321, LR: 1.928032487969928e-05, Loss: 0.14834384620189667
[2023-09-05 15:25:41,286] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4322/66600 [1:21:44<239:19:33, 13.83s/it]09/05/2023 15:25:41 - INFO - __main__ -   Step: 4322, LR: 1.9280015175505497e-05, Loss: 0.16353751718997955
[2023-09-05 15:25:55,864] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4323/66600 [1:21:59<243:10:55, 14.06s/it]09/05/2023 15:25:55 - INFO - __main__ -   Step: 4323, LR: 1.9279705471311715e-05, Loss: 0.13807249069213867
[2023-09-05 15:26:09,223] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4324/66600 [1:22:12<239:33:11, 13.85s/it]09/05/2023 15:26:09 - INFO - __main__ -   Step: 4324, LR: 1.9279395767117933e-05, Loss: 0.11511695384979248
[2023-09-05 15:26:22,611] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4325/66600 [1:22:26<237:09:51, 13.71s/it]09/05/2023 15:26:22 - INFO - __main__ -   Step: 4325, LR: 1.927908606292415e-05, Loss: 0.12992486357688904
[2023-09-05 15:26:37,296] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4326/66600 [1:22:40<242:13:22, 14.00s/it]09/05/2023 15:26:37 - INFO - __main__ -   Step: 4326, LR: 1.927877635873037e-05, Loss: 0.12951236963272095
[2023-09-05 15:26:51,072] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4327/66600 [1:22:54<241:02:32, 13.93s/it]09/05/2023 15:26:51 - INFO - __main__ -   Step: 4327, LR: 1.9278466654536587e-05, Loss: 0.12854477763175964
[2023-09-05 15:27:04,653] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4328/66600 [1:23:08<239:12:08, 13.83s/it]09/05/2023 15:27:04 - INFO - __main__ -   Step: 4328, LR: 1.9278156950342805e-05, Loss: 0.11049875617027283
[2023-09-05 15:27:19,075] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|▋         | 4329/66600 [1:23:22<242:16:42, 14.01s/it]09/05/2023 15:27:19 - INFO - __main__ -   Step: 4329, LR: 1.9277847246149024e-05, Loss: 0.12946271896362305
[2023-09-05 15:27:33,510] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4330/66600 [1:23:37<244:29:46, 14.14s/it]09/05/2023 15:27:33 - INFO - __main__ -   Step: 4330, LR: 1.927753754195524e-05, Loss: 0.1875195950269699
[2023-09-05 15:27:48,087] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4331/66600 [1:23:51<246:46:59, 14.27s/it]09/05/2023 15:27:48 - INFO - __main__ -   Step: 4331, LR: 1.927722783776146e-05, Loss: 0.20004604756832123
[2023-09-05 15:28:02,668] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4332/66600 [1:24:06<248:24:31, 14.36s/it]09/05/2023 15:28:02 - INFO - __main__ -   Step: 4332, LR: 1.9276918133567678e-05, Loss: 0.1393647938966751
[2023-09-05 15:28:15,629] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4333/66600 [1:24:19<241:08:21, 13.94s/it]09/05/2023 15:28:15 - INFO - __main__ -   Step: 4333, LR: 1.9276608429373896e-05, Loss: 0.12007202953100204
[2023-09-05 15:28:29,626] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4334/66600 [1:24:33<241:25:12, 13.96s/it]09/05/2023 15:28:29 - INFO - __main__ -   Step: 4334, LR: 1.9276298725180114e-05, Loss: 0.214706689119339
[2023-09-05 15:28:43,393] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4335/66600 [1:24:46<240:25:23, 13.90s/it]09/05/2023 15:28:43 - INFO - __main__ -   Step: 4335, LR: 1.9275989020986332e-05, Loss: 0.18581300973892212
[2023-09-05 15:28:57,831] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4336/66600 [1:25:01<243:12:44, 14.06s/it]09/05/2023 15:28:57 - INFO - __main__ -   Step: 4336, LR: 1.927567931679255e-05, Loss: 0.14460301399230957
[2023-09-05 15:29:11,679] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4337/66600 [1:25:15<242:05:36, 14.00s/it]09/05/2023 15:29:11 - INFO - __main__ -   Step: 4337, LR: 1.9275369612598768e-05, Loss: 0.1369955986738205
[2023-09-05 15:29:26,335] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4338/66600 [1:25:29<245:30:29, 14.20s/it]09/05/2023 15:29:26 - INFO - __main__ -   Step: 4338, LR: 1.9275059908404986e-05, Loss: 0.16516557335853577
[2023-09-05 15:29:41,163] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4339/66600 [1:25:44<248:46:58, 14.38s/it]09/05/2023 15:29:41 - INFO - __main__ -   Step: 4339, LR: 1.9274750204211204e-05, Loss: 0.12225951254367828
[2023-09-05 15:29:55,589] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4340/66600 [1:25:59<248:59:32, 14.40s/it]09/05/2023 15:29:55 - INFO - __main__ -   Step: 4340, LR: 1.9274440500017422e-05, Loss: 0.19296479225158691
[2023-09-05 15:30:08,427] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4341/66600 [1:26:11<240:53:55, 13.93s/it]09/05/2023 15:30:08 - INFO - __main__ -   Step: 4341, LR: 1.927413079582364e-05, Loss: 0.1972571313381195
[2023-09-05 15:30:22,389] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4342/66600 [1:26:25<241:03:59, 13.94s/it]09/05/2023 15:30:22 - INFO - __main__ -   Step: 4342, LR: 1.927382109162986e-05, Loss: 0.14638054370880127
[2023-09-05 15:30:37,473] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4343/66600 [1:26:41<247:00:03, 14.28s/it]09/05/2023 15:30:37 - INFO - __main__ -   Step: 4343, LR: 1.9273511387436076e-05, Loss: 0.1544332355260849
[2023-09-05 15:30:51,005] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4344/66600 [1:26:54<243:05:56, 14.06s/it]09/05/2023 15:30:51 - INFO - __main__ -   Step: 4344, LR: 1.9273201683242294e-05, Loss: 0.15551665425300598
[2023-09-05 15:31:05,225] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4345/66600 [1:27:08<243:56:31, 14.11s/it]09/05/2023 15:31:05 - INFO - __main__ -   Step: 4345, LR: 1.9272891979048513e-05, Loss: 0.12724149227142334
[2023-09-05 15:31:19,899] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4346/66600 [1:27:23<246:52:51, 14.28s/it]09/05/2023 15:31:19 - INFO - __main__ -   Step: 4346, LR: 1.927258227485473e-05, Loss: 0.13656097650527954
[2023-09-05 15:31:33,163] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4347/66600 [1:27:36<241:37:32, 13.97s/it]09/05/2023 15:31:33 - INFO - __main__ -   Step: 4347, LR: 1.927227257066095e-05, Loss: 0.09912818670272827
[2023-09-05 15:31:46,909] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4348/66600 [1:27:50<240:26:45, 13.90s/it]09/05/2023 15:31:46 - INFO - __main__ -   Step: 4348, LR: 1.9271962866467167e-05, Loss: 0.12094636261463165
[2023-09-05 15:32:00,079] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4349/66600 [1:28:03<236:37:36, 13.68s/it]09/05/2023 15:32:00 - INFO - __main__ -   Step: 4349, LR: 1.9271653162273385e-05, Loss: 0.15878674387931824
[2023-09-05 15:32:14,810] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4350/66600 [1:28:18<242:03:23, 14.00s/it]09/05/2023 15:32:14 - INFO - __main__ -   Step: 4350, LR: 1.9271343458079603e-05, Loss: 0.14542695879936218
[2023-09-05 15:32:28,828] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4351/66600 [1:28:32<242:09:17, 14.00s/it]09/05/2023 15:32:28 - INFO - __main__ -   Step: 4351, LR: 1.927103375388582e-05, Loss: 0.14785289764404297
[2023-09-05 15:32:42,196] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4352/66600 [1:28:45<238:50:53, 13.81s/it]09/05/2023 15:32:42 - INFO - __main__ -   Step: 4352, LR: 1.927072404969204e-05, Loss: 0.17279572784900665
[2023-09-05 15:32:57,543] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4353/66600 [1:29:01<246:48:01, 14.27s/it]09/05/2023 15:32:57 - INFO - __main__ -   Step: 4353, LR: 1.9270414345498257e-05, Loss: 0.1901155561208725
[2023-09-05 15:33:11,861] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4354/66600 [1:29:15<247:01:28, 14.29s/it]09/05/2023 15:33:11 - INFO - __main__ -   Step: 4354, LR: 1.9270104641304475e-05, Loss: 0.15415920317173004
[2023-09-05 15:33:25,970] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4355/66600 [1:29:29<246:05:52, 14.23s/it]09/05/2023 15:33:25 - INFO - __main__ -   Step: 4355, LR: 1.9269794937110693e-05, Loss: 0.14787521958351135
[2023-09-05 15:33:39,772] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4356/66600 [1:29:43<243:51:30, 14.10s/it]09/05/2023 15:33:39 - INFO - __main__ -   Step: 4356, LR: 1.926948523291691e-05, Loss: 0.15954992175102234
[2023-09-05 15:33:53,484] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4357/66600 [1:29:57<241:49:24, 13.99s/it]09/05/2023 15:33:53 - INFO - __main__ -   Step: 4357, LR: 1.926917552872313e-05, Loss: 0.20367704331874847
[2023-09-05 15:34:08,404] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4358/66600 [1:30:11<246:39:41, 14.27s/it]09/05/2023 15:34:08 - INFO - __main__ -   Step: 4358, LR: 1.9268865824529347e-05, Loss: 0.14513751864433289
[2023-09-05 15:34:22,548] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4359/66600 [1:30:26<246:01:17, 14.23s/it]09/05/2023 15:34:22 - INFO - __main__ -   Step: 4359, LR: 1.9268556120335565e-05, Loss: 0.18239490687847137
[2023-09-05 15:34:37,094] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4360/66600 [1:30:40<247:39:15, 14.32s/it]09/05/2023 15:34:37 - INFO - __main__ -   Step: 4360, LR: 1.9268246416141783e-05, Loss: 0.17689208686351776
[2023-09-05 15:34:51,252] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4361/66600 [1:30:54<246:47:18, 14.27s/it]09/05/2023 15:34:51 - INFO - __main__ -   Step: 4361, LR: 1.9267936711948005e-05, Loss: 0.11476695537567139
[2023-09-05 15:35:04,449] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4362/66600 [1:31:08<241:11:38, 13.95s/it]09/05/2023 15:35:04 - INFO - __main__ -   Step: 4362, LR: 1.926762700775422e-05, Loss: 0.14120995998382568
[2023-09-05 15:35:18,419] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4363/66600 [1:31:21<241:17:23, 13.96s/it]09/05/2023 15:35:18 - INFO - __main__ -   Step: 4363, LR: 1.9267317303560438e-05, Loss: 0.12379205971956253
[2023-09-05 15:35:31,840] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4364/66600 [1:31:35<238:30:23, 13.80s/it]09/05/2023 15:35:31 - INFO - __main__ -   Step: 4364, LR: 1.9267007599366656e-05, Loss: 0.14884154498577118
[2023-09-05 15:35:45,878] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4365/66600 [1:31:49<239:45:21, 13.87s/it]09/05/2023 15:35:45 - INFO - __main__ -   Step: 4365, LR: 1.9266697895172874e-05, Loss: 0.14147037267684937
[2023-09-05 15:35:59,412] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4366/66600 [1:32:02<238:01:00, 13.77s/it]09/05/2023 15:35:59 - INFO - __main__ -   Step: 4366, LR: 1.9266388190979092e-05, Loss: 0.1539231687784195
[2023-09-05 15:36:13,681] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4367/66600 [1:32:17<240:36:15, 13.92s/it]09/05/2023 15:36:13 - INFO - __main__ -   Step: 4367, LR: 1.926607848678531e-05, Loss: 0.15348540246486664
[2023-09-05 15:36:27,589] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4368/66600 [1:32:31<240:33:08, 13.92s/it]09/05/2023 15:36:27 - INFO - __main__ -   Step: 4368, LR: 1.926576878259153e-05, Loss: 0.12773728370666504
[2023-09-05 15:36:41,603] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4369/66600 [1:32:45<241:03:17, 13.94s/it]09/05/2023 15:36:41 - INFO - __main__ -   Step: 4369, LR: 1.926545907839775e-05, Loss: 0.15522436797618866
[2023-09-05 15:36:55,546] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4370/66600 [1:32:59<241:02:29, 13.94s/it]09/05/2023 15:36:55 - INFO - __main__ -   Step: 4370, LR: 1.9265149374203964e-05, Loss: 0.14377647638320923
[2023-09-05 15:37:09,162] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4371/66600 [1:33:12<239:20:20, 13.85s/it]09/05/2023 15:37:09 - INFO - __main__ -   Step: 4371, LR: 1.9264839670010182e-05, Loss: 0.24230509996414185
[2023-09-05 15:37:23,416] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4372/66600 [1:33:26<241:27:03, 13.97s/it]09/05/2023 15:37:23 - INFO - __main__ -   Step: 4372, LR: 1.92645299658164e-05, Loss: 0.1730574369430542
[2023-09-05 15:37:37,291] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4373/66600 [1:33:40<240:57:48, 13.94s/it]09/05/2023 15:37:37 - INFO - __main__ -   Step: 4373, LR: 1.9264220261622618e-05, Loss: 0.1595461219549179
[2023-09-05 15:37:51,817] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4374/66600 [1:33:55<243:59:43, 14.12s/it]09/05/2023 15:37:51 - INFO - __main__ -   Step: 4374, LR: 1.9263910557428836e-05, Loss: 0.16365547478199005
[2023-09-05 15:38:07,076] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4375/66600 [1:34:10<249:55:03, 14.46s/it]09/05/2023 15:38:07 - INFO - __main__ -   Step: 4375, LR: 1.9263600853235058e-05, Loss: 0.123319111764431
[2023-09-05 15:38:20,679] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4376/66600 [1:34:24<245:28:34, 14.20s/it]09/05/2023 15:38:20 - INFO - __main__ -   Step: 4376, LR: 1.9263291149041276e-05, Loss: 0.11578135192394257
[2023-09-05 15:38:34,162] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4377/66600 [1:34:37<241:44:37, 13.99s/it]09/05/2023 15:38:34 - INFO - __main__ -   Step: 4377, LR: 1.926298144484749e-05, Loss: 0.17191699147224426
[2023-09-05 15:38:47,958] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4378/66600 [1:34:51<240:45:00, 13.93s/it]09/05/2023 15:38:47 - INFO - __main__ -   Step: 4378, LR: 1.926267174065371e-05, Loss: 0.12055568397045135
[2023-09-05 15:39:01,682] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4379/66600 [1:35:05<239:40:59, 13.87s/it]09/05/2023 15:39:01 - INFO - __main__ -   Step: 4379, LR: 1.9262362036459927e-05, Loss: 0.12535126507282257
[2023-09-05 15:39:15,226] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4380/66600 [1:35:18<238:00:06, 13.77s/it]09/05/2023 15:39:15 - INFO - __main__ -   Step: 4380, LR: 1.9262052332266145e-05, Loss: 0.14561069011688232
[2023-09-05 15:39:29,949] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4381/66600 [1:35:33<242:56:02, 14.06s/it]09/05/2023 15:39:29 - INFO - __main__ -   Step: 4381, LR: 1.9261742628072363e-05, Loss: 0.126656174659729
[2023-09-05 15:39:43,743] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4382/66600 [1:35:47<241:34:17, 13.98s/it]09/05/2023 15:39:43 - INFO - __main__ -   Step: 4382, LR: 1.9261432923878584e-05, Loss: 0.21005889773368835
[2023-09-05 15:39:58,661] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4383/66600 [1:36:02<246:26:32, 14.26s/it]09/05/2023 15:39:58 - INFO - __main__ -   Step: 4383, LR: 1.9261123219684802e-05, Loss: 0.1614614576101303
[2023-09-05 15:40:13,484] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4384/66600 [1:36:17<249:21:33, 14.43s/it]09/05/2023 15:40:13 - INFO - __main__ -   Step: 4384, LR: 1.926081351549102e-05, Loss: 0.19575725495815277
[2023-09-05 15:40:27,414] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4385/66600 [1:36:30<246:46:20, 14.28s/it]09/05/2023 15:40:27 - INFO - __main__ -   Step: 4385, LR: 1.9260503811297235e-05, Loss: 0.14316809177398682
[2023-09-05 15:40:41,966] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4386/66600 [1:36:45<248:11:04, 14.36s/it]09/05/2023 15:40:41 - INFO - __main__ -   Step: 4386, LR: 1.9260194107103453e-05, Loss: 0.12978330254554749
[2023-09-05 15:40:55,944] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4387/66600 [1:36:59<246:11:37, 14.25s/it]09/05/2023 15:40:55 - INFO - __main__ -   Step: 4387, LR: 1.925988440290967e-05, Loss: 0.1189383789896965
[2023-09-05 15:41:10,075] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4388/66600 [1:37:13<245:35:29, 14.21s/it]09/05/2023 15:41:10 - INFO - __main__ -   Step: 4388, LR: 1.925957469871589e-05, Loss: 0.16053159534931183
[2023-09-05 15:41:23,819] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4389/66600 [1:37:27<243:09:36, 14.07s/it]09/05/2023 15:41:23 - INFO - __main__ -   Step: 4389, LR: 1.925926499452211e-05, Loss: 0.14774534106254578
[2023-09-05 15:41:38,409] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4390/66600 [1:37:41<245:50:48, 14.23s/it]09/05/2023 15:41:38 - INFO - __main__ -   Step: 4390, LR: 1.925895529032833e-05, Loss: 0.18990689516067505
[2023-09-05 15:41:52,574] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4391/66600 [1:37:56<245:31:27, 14.21s/it]09/05/2023 15:41:52 - INFO - __main__ -   Step: 4391, LR: 1.9258645586134547e-05, Loss: 0.1639387011528015
[2023-09-05 15:42:06,882] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4392/66600 [1:38:10<246:02:13, 14.24s/it]09/05/2023 15:42:06 - INFO - __main__ -   Step: 4392, LR: 1.9258335881940765e-05, Loss: 0.14038482308387756
[2023-09-05 15:42:21,720] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4393/66600 [1:38:25<249:08:33, 14.42s/it]09/05/2023 15:42:21 - INFO - __main__ -   Step: 4393, LR: 1.925802617774698e-05, Loss: 0.15927252173423767
[2023-09-05 15:42:34,965] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4394/66600 [1:38:38<243:03:32, 14.07s/it]09/05/2023 15:42:34 - INFO - __main__ -   Step: 4394, LR: 1.9257716473553197e-05, Loss: 0.14736154675483704
[2023-09-05 15:42:48,021] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4395/66600 [1:38:51<237:49:00, 13.76s/it]09/05/2023 15:42:48 - INFO - __main__ -   Step: 4395, LR: 1.9257406769359416e-05, Loss: 0.11500847339630127
[2023-09-05 15:43:02,703] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4396/66600 [1:39:06<242:34:20, 14.04s/it]09/05/2023 15:43:02 - INFO - __main__ -   Step: 4396, LR: 1.9257097065165637e-05, Loss: 0.1588514745235443
[2023-09-05 15:43:16,822] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4397/66600 [1:39:20<242:59:07, 14.06s/it]09/05/2023 15:43:16 - INFO - __main__ -   Step: 4397, LR: 1.9256787360971855e-05, Loss: 0.14256125688552856
[2023-09-05 15:43:30,041] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4398/66600 [1:39:33<238:36:36, 13.81s/it]09/05/2023 15:43:30 - INFO - __main__ -   Step: 4398, LR: 1.9256477656778073e-05, Loss: 0.1851937621831894
[2023-09-05 15:43:43,971] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4399/66600 [1:39:47<239:13:37, 13.85s/it]09/05/2023 15:43:43 - INFO - __main__ -   Step: 4399, LR: 1.925616795258429e-05, Loss: 0.1622840166091919
[2023-09-05 15:43:59,052] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4400/66600 [1:40:02<245:37:46, 14.22s/it]09/05/2023 15:43:59 - INFO - __main__ -   Step: 4400, LR: 1.9255858248390506e-05, Loss: 0.1849837303161621
09/05/2023 15:43:59 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.002716541290283203

Evaluating:   0%|          | 1/228 [00:00<01:24,  2.68it/s][Astep: 1
extend+tolist() time: 0.0010106563568115234

Evaluating:   1%|          | 2/228 [00:00<01:08,  3.30it/s][Astep: 2
extend+tolist() time: 0.0021829605102539062

Evaluating:   1%|▏         | 3/228 [00:00<01:11,  3.13it/s][Astep: 3
extend+tolist() time: 0.0018768310546875

Evaluating:   2%|▏         | 4/228 [00:01<01:10,  3.18it/s][Astep: 4
extend+tolist() time: 0.0013370513916015625

Evaluating:   2%|▏         | 5/228 [00:01<01:16,  2.93it/s][Astep: 5
extend+tolist() time: 0.0018112659454345703

Evaluating:   3%|▎         | 6/228 [00:01<01:15,  2.96it/s][Astep: 6
extend+tolist() time: 0.0014691352844238281

Evaluating:   3%|▎         | 7/228 [00:02<01:25,  2.59it/s][Astep: 7
extend+tolist() time: 0.0009417533874511719

Evaluating:   4%|▎         | 8/228 [00:02<01:15,  2.91it/s][Astep: 8
extend+tolist() time: 0.0012476444244384766

Evaluating:   4%|▍         | 9/228 [00:02<01:07,  3.25it/s][Astep: 9
extend+tolist() time: 0.0007989406585693359

Evaluating:   4%|▍         | 10/228 [00:03<01:02,  3.51it/s][Astep: 10
extend+tolist() time: 0.0013117790222167969

Evaluating:   5%|▍         | 11/228 [00:03<00:59,  3.67it/s][Astep: 11
extend+tolist() time: 0.0005812644958496094

Evaluating:   5%|▌         | 12/228 [00:03<00:55,  3.86it/s][Astep: 12
extend+tolist() time: 0.0006668567657470703

Evaluating:   6%|▌         | 13/228 [00:03<00:53,  4.01it/s][Astep: 13
extend+tolist() time: 0.001077890396118164

Evaluating:   6%|▌         | 14/228 [00:04<00:52,  4.11it/s][Astep: 14
extend+tolist() time: 0.0005412101745605469

Evaluating:   7%|▋         | 15/228 [00:04<00:50,  4.21it/s][Astep: 15
extend+tolist() time: 0.0006234645843505859

Evaluating:   7%|▋         | 16/228 [00:04<00:49,  4.28it/s][Astep: 16
extend+tolist() time: 0.0011441707611083984

Evaluating:   7%|▋         | 17/228 [00:04<00:48,  4.31it/s][Astep: 17
extend+tolist() time: 0.0008594989776611328

Evaluating:   8%|▊         | 18/228 [00:05<00:49,  4.27it/s][Astep: 18
extend+tolist() time: 0.0016210079193115234

Evaluating:   8%|▊         | 19/228 [00:05<00:51,  4.05it/s][Astep: 19
extend+tolist() time: 0.0013747215270996094

Evaluating:   9%|▉         | 20/228 [00:05<00:52,  3.93it/s][Astep: 20
extend+tolist() time: 0.0007669925689697266

Evaluating:   9%|▉         | 21/228 [00:05<00:51,  4.02it/s][Astep: 21
extend+tolist() time: 0.0010864734649658203

Evaluating:  10%|▉         | 22/228 [00:06<00:49,  4.12it/s][Astep: 22
extend+tolist() time: 0.0007262229919433594

Evaluating:  10%|█         | 23/228 [00:06<00:48,  4.20it/s][Astep: 23
extend+tolist() time: 0.0006580352783203125

Evaluating:  11%|█         | 24/228 [00:06<00:48,  4.24it/s][Astep: 24
extend+tolist() time: 0.15522003173828125

Evaluating:  11%|█         | 25/228 [00:06<00:59,  3.39it/s][Astep: 25
extend+tolist() time: 0.002048492431640625

Evaluating:  11%|█▏        | 26/228 [00:07<01:02,  3.24it/s][Astep: 26
extend+tolist() time: 0.0010421276092529297

Evaluating:  12%|█▏        | 27/228 [00:07<00:57,  3.52it/s][Astep: 27
extend+tolist() time: 0.0016179084777832031

Evaluating:  12%|█▏        | 28/228 [00:07<00:58,  3.45it/s][Astep: 28
extend+tolist() time: 0.0003180503845214844

Evaluating:  13%|█▎        | 29/228 [00:08<00:53,  3.72it/s][Astep: 29
extend+tolist() time: 0.0007121562957763672

Evaluating:  13%|█▎        | 30/228 [00:08<00:50,  3.89it/s][Astep: 30
extend+tolist() time: 0.0015082359313964844

Evaluating:  14%|█▎        | 31/228 [00:08<00:52,  3.77it/s][Astep: 31
extend+tolist() time: 0.0009341239929199219

Evaluating:  14%|█▍        | 32/228 [00:08<00:49,  3.94it/s][Astep: 32
extend+tolist() time: 0.000988006591796875

Evaluating:  14%|█▍        | 33/228 [00:09<00:50,  3.83it/s][Astep: 33
extend+tolist() time: 0.001708984375

Evaluating:  15%|█▍        | 34/228 [00:09<00:53,  3.65it/s][Astep: 34
extend+tolist() time: 0.0012433528900146484

Evaluating:  15%|█▌        | 35/228 [00:09<00:59,  3.22it/s][Astep: 35
extend+tolist() time: 0.0006258487701416016

Evaluating:  16%|█▌        | 36/228 [00:09<00:54,  3.52it/s][Astep: 36
extend+tolist() time: 0.0007567405700683594

Evaluating:  16%|█▌        | 37/228 [00:10<00:51,  3.73it/s][Astep: 37
extend+tolist() time: 0.0019371509552001953

Evaluating:  17%|█▋        | 38/228 [00:10<01:01,  3.10it/s][Astep: 38
extend+tolist() time: 0.001119852066040039

Evaluating:  17%|█▋        | 39/228 [00:10<00:55,  3.39it/s][Astep: 39
extend+tolist() time: 0.0006897449493408203

Evaluating:  18%|█▊        | 40/228 [00:11<00:51,  3.63it/s][Astep: 40
extend+tolist() time: 0.0010387897491455078

Evaluating:  18%|█▊        | 41/228 [00:11<00:48,  3.84it/s][Astep: 41
extend+tolist() time: 0.0008382797241210938

Evaluating:  18%|█▊        | 42/228 [00:11<00:47,  3.91it/s][Astep: 42
extend+tolist() time: 0.0015904903411865234

Evaluating:  19%|█▉        | 43/228 [00:11<00:49,  3.71it/s][Astep: 43
extend+tolist() time: 0.0018689632415771484

Evaluating:  19%|█▉        | 44/228 [00:12<00:53,  3.45it/s][Astep: 44
extend+tolist() time: 0.0007243156433105469

Evaluating:  20%|█▉        | 45/228 [00:12<00:49,  3.68it/s][Astep: 45
extend+tolist() time: 0.0017361640930175781

Evaluating:  20%|██        | 46/228 [00:12<00:51,  3.55it/s][Astep: 46
extend+tolist() time: 0.0015947818756103516

Evaluating:  21%|██        | 47/228 [00:13<00:52,  3.46it/s][Astep: 47
extend+tolist() time: 0.001482248306274414

Evaluating:  21%|██        | 48/228 [00:13<00:51,  3.49it/s][Astep: 48
extend+tolist() time: 0.0017366409301757812

Evaluating:  21%|██▏       | 49/228 [00:13<00:52,  3.43it/s][Astep: 49
extend+tolist() time: 0.0009419918060302734

Evaluating:  22%|██▏       | 50/228 [00:13<00:49,  3.57it/s][Astep: 50
extend+tolist() time: 0.0016677379608154297

Evaluating:  22%|██▏       | 51/228 [00:14<00:50,  3.48it/s][Astep: 51
extend+tolist() time: 0.0015463829040527344

Evaluating:  23%|██▎       | 52/228 [00:14<00:51,  3.42it/s][Astep: 52
extend+tolist() time: 0.0010008811950683594

Evaluating:  23%|██▎       | 53/228 [00:14<00:50,  3.48it/s][Astep: 53
extend+tolist() time: 0.0017054080963134766

Evaluating:  24%|██▎       | 54/228 [00:15<00:50,  3.42it/s][Astep: 54
extend+tolist() time: 0.15871715545654297

Evaluating:  24%|██▍       | 55/228 [00:15<00:56,  3.09it/s][Astep: 55
extend+tolist() time: 0.0007455348968505859

Evaluating:  25%|██▍       | 56/228 [00:15<00:51,  3.36it/s][Astep: 56
extend+tolist() time: 0.0011794567108154297

Evaluating:  25%|██▌       | 57/228 [00:16<00:51,  3.35it/s][Astep: 57
extend+tolist() time: 0.0009944438934326172

Evaluating:  25%|██▌       | 58/228 [00:16<00:46,  3.62it/s][Astep: 58
extend+tolist() time: 0.0008261203765869141

Evaluating:  26%|██▌       | 59/228 [00:16<00:45,  3.74it/s][Astep: 59
extend+tolist() time: 0.001355886459350586

Evaluating:  26%|██▋       | 60/228 [00:16<00:44,  3.81it/s][Astep: 60
extend+tolist() time: 0.0007128715515136719

Evaluating:  27%|██▋       | 61/228 [00:16<00:42,  3.97it/s][Astep: 61
extend+tolist() time: 0.0012373924255371094

Evaluating:  27%|██▋       | 62/228 [00:17<00:41,  4.01it/s][Astep: 62
extend+tolist() time: 0.0007460117340087891

Evaluating:  28%|██▊       | 63/228 [00:17<00:40,  4.09it/s][Astep: 63
extend+tolist() time: 0.0008101463317871094

Evaluating:  28%|██▊       | 64/228 [00:17<00:39,  4.14it/s][Astep: 64
extend+tolist() time: 0.0011932849884033203

Evaluating:  29%|██▊       | 65/228 [00:17<00:39,  4.18it/s][Astep: 65
extend+tolist() time: 0.0008215904235839844

Evaluating:  29%|██▉       | 66/228 [00:18<00:38,  4.17it/s][Astep: 66
extend+tolist() time: 0.0011627674102783203

Evaluating:  29%|██▉       | 67/228 [00:18<00:37,  4.25it/s][Astep: 67
extend+tolist() time: 0.0008862018585205078

Evaluating:  30%|██▉       | 68/228 [00:18<00:38,  4.18it/s][Astep: 68
extend+tolist() time: 0.0011436939239501953

Evaluating:  30%|███       | 69/228 [00:19<00:46,  3.42it/s][Astep: 69
extend+tolist() time: 0.0011069774627685547

Evaluating:  31%|███       | 70/228 [00:19<00:45,  3.47it/s][Astep: 70
extend+tolist() time: 0.001516580581665039

Evaluating:  31%|███       | 71/228 [00:19<00:52,  3.00it/s][Astep: 71
extend+tolist() time: 0.0013337135314941406

Evaluating:  32%|███▏      | 72/228 [00:20<00:49,  3.18it/s][Astep: 72
extend+tolist() time: 0.0007822513580322266

Evaluating:  32%|███▏      | 73/228 [00:20<00:44,  3.45it/s][Astep: 73
extend+tolist() time: 0.0005383491516113281

Evaluating:  32%|███▏      | 74/228 [00:20<00:41,  3.68it/s][Astep: 74
extend+tolist() time: 0.0011615753173828125

Evaluating:  33%|███▎      | 75/228 [00:20<00:39,  3.84it/s][Astep: 75
extend+tolist() time: 0.0016512870788574219

Evaluating:  33%|███▎      | 76/228 [00:21<00:41,  3.65it/s][Astep: 76
extend+tolist() time: 0.0006248950958251953

Evaluating:  34%|███▍      | 77/228 [00:21<00:39,  3.84it/s][Astep: 77
extend+tolist() time: 0.001965045928955078

Evaluating:  34%|███▍      | 78/228 [00:21<00:42,  3.53it/s][Astep: 78
extend+tolist() time: 0.001220703125

Evaluating:  35%|███▍      | 79/228 [00:21<00:40,  3.65it/s][Astep: 79
extend+tolist() time: 0.0008494853973388672

Evaluating:  35%|███▌      | 80/228 [00:22<00:39,  3.75it/s][Astep: 80
extend+tolist() time: 0.001363515853881836

Evaluating:  36%|███▌      | 81/228 [00:22<00:38,  3.84it/s][Astep: 81
extend+tolist() time: 0.0008339881896972656

Evaluating:  36%|███▌      | 82/228 [00:22<00:37,  3.92it/s][Astep: 82
extend+tolist() time: 0.001316070556640625

Evaluating:  36%|███▋      | 83/228 [00:22<00:36,  3.97it/s][Astep: 83
extend+tolist() time: 0.0006840229034423828

Evaluating:  37%|███▋      | 84/228 [00:23<00:35,  4.10it/s][Astep: 84
extend+tolist() time: 0.0014615058898925781

Evaluating:  37%|███▋      | 85/228 [00:23<00:36,  3.96it/s][Astep: 85
extend+tolist() time: 0.0012543201446533203

Evaluating:  38%|███▊      | 86/228 [00:23<00:35,  3.97it/s][Astep: 86
extend+tolist() time: 0.0008764266967773438

Evaluating:  38%|███▊      | 87/228 [00:23<00:35,  4.01it/s][Astep: 87
extend+tolist() time: 0.0013163089752197266

Evaluating:  39%|███▊      | 88/228 [00:24<00:34,  4.00it/s][Astep: 88
extend+tolist() time: 0.0007810592651367188

Evaluating:  39%|███▉      | 89/228 [00:24<00:33,  4.09it/s][Astep: 89
extend+tolist() time: 0.0011565685272216797

Evaluating:  39%|███▉      | 90/228 [00:24<00:33,  4.18it/s][Astep: 90
extend+tolist() time: 0.0009357929229736328

Evaluating:  40%|███▉      | 91/228 [00:24<00:33,  4.12it/s][Astep: 91
extend+tolist() time: 0.0012364387512207031

Evaluating:  40%|████      | 92/228 [00:25<00:32,  4.17it/s][Astep: 92
extend+tolist() time: 0.0007944107055664062

Evaluating:  41%|████      | 93/228 [00:25<00:31,  4.22it/s][Astep: 93
extend+tolist() time: 0.001447916030883789

Evaluating:  41%|████      | 94/228 [00:25<00:33,  4.05it/s][Astep: 94
extend+tolist() time: 0.0007240772247314453

Evaluating:  42%|████▏     | 95/228 [00:25<00:32,  4.15it/s][Astep: 95
extend+tolist() time: 0.0016663074493408203

Evaluating:  42%|████▏     | 96/228 [00:26<00:34,  3.88it/s][Astep: 96
extend+tolist() time: 0.001348733901977539

Evaluating:  43%|████▎     | 97/228 [00:26<00:33,  3.90it/s][Astep: 97
extend+tolist() time: 0.0008237361907958984

Evaluating:  43%|████▎     | 98/228 [00:26<00:32,  3.99it/s][Astep: 98
extend+tolist() time: 0.18638300895690918

Evaluating:  43%|████▎     | 99/228 [00:26<00:39,  3.27it/s][Astep: 99
extend+tolist() time: 0.0008754730224609375

Evaluating:  44%|████▍     | 100/228 [00:27<00:36,  3.46it/s][Astep: 100
extend+tolist() time: 0.0007116794586181641

Evaluating:  44%|████▍     | 101/228 [00:27<00:34,  3.70it/s][Astep: 101
extend+tolist() time: 0.0014443397521972656

Evaluating:  45%|████▍     | 102/228 [00:27<00:32,  3.82it/s][Astep: 102
extend+tolist() time: 0.0007190704345703125

Evaluating:  45%|████▌     | 103/228 [00:27<00:31,  3.96it/s][Astep: 103
extend+tolist() time: 0.0012040138244628906

Evaluating:  46%|████▌     | 104/228 [00:28<00:30,  4.07it/s][Astep: 104
extend+tolist() time: 0.0007088184356689453

Evaluating:  46%|████▌     | 105/228 [00:28<00:29,  4.15it/s][Astep: 105
extend+tolist() time: 0.001262664794921875

Evaluating:  46%|████▋     | 106/228 [00:28<00:29,  4.14it/s][Astep: 106
extend+tolist() time: 0.0017285346984863281

Evaluating:  47%|████▋     | 107/228 [00:28<00:32,  3.78it/s][Astep: 107
extend+tolist() time: 0.0007576942443847656

Evaluating:  47%|████▋     | 108/228 [00:29<00:30,  3.91it/s][Astep: 108
extend+tolist() time: 0.001277923583984375

Evaluating:  48%|████▊     | 109/228 [00:29<00:29,  4.01it/s][Astep: 109
extend+tolist() time: 0.0008628368377685547

Evaluating:  48%|████▊     | 110/228 [00:29<00:29,  4.05it/s][Astep: 110
extend+tolist() time: 0.0010426044464111328

Evaluating:  49%|████▊     | 111/228 [00:29<00:28,  4.14it/s][Astep: 111
extend+tolist() time: 0.0014653205871582031

Evaluating:  49%|████▉     | 112/228 [00:30<00:30,  3.77it/s][Astep: 112
extend+tolist() time: 0.0008847713470458984

Evaluating:  50%|████▉     | 113/228 [00:30<00:29,  3.95it/s][Astep: 113
extend+tolist() time: 0.0008330345153808594

Evaluating:  50%|█████     | 114/228 [00:30<00:27,  4.09it/s][Astep: 114
extend+tolist() time: 0.0016994476318359375

Evaluating:  50%|█████     | 115/228 [00:31<00:35,  3.21it/s][Astep: 115
extend+tolist() time: 0.0007498264312744141

Evaluating:  51%|█████     | 116/228 [00:31<00:31,  3.50it/s][Astep: 116
extend+tolist() time: 0.001359701156616211

Evaluating:  51%|█████▏    | 117/228 [00:31<00:29,  3.70it/s][Astep: 117
extend+tolist() time: 0.0009784698486328125

Evaluating:  52%|█████▏    | 118/228 [00:31<00:28,  3.81it/s][Astep: 118
extend+tolist() time: 0.001081705093383789

Evaluating:  52%|█████▏    | 119/228 [00:32<00:33,  3.24it/s][Astep: 119
extend+tolist() time: 0.0008392333984375

Evaluating:  53%|█████▎    | 120/228 [00:32<00:30,  3.51it/s][Astep: 120
extend+tolist() time: 0.0007269382476806641

Evaluating:  53%|█████▎    | 121/228 [00:32<00:28,  3.74it/s][Astep: 121
extend+tolist() time: 0.0011742115020751953

Evaluating:  54%|█████▎    | 122/228 [00:32<00:27,  3.90it/s][Astep: 122
extend+tolist() time: 0.0008032321929931641

Evaluating:  54%|█████▍    | 123/228 [00:33<00:26,  4.03it/s][Astep: 123
extend+tolist() time: 0.0011129379272460938

Evaluating:  54%|█████▍    | 124/228 [00:33<00:25,  4.14it/s][Astep: 124
extend+tolist() time: 0.000972747802734375

Evaluating:  55%|█████▍    | 125/228 [00:33<00:24,  4.17it/s][Astep: 125
extend+tolist() time: 0.00048065185546875

Evaluating:  55%|█████▌    | 126/228 [00:33<00:24,  4.24it/s][Astep: 126
extend+tolist() time: 0.0019617080688476562

Evaluating:  56%|█████▌    | 127/228 [00:34<00:26,  3.82it/s][Astep: 127
extend+tolist() time: 0.0018706321716308594

Evaluating:  56%|█████▌    | 128/228 [00:34<00:27,  3.58it/s][Astep: 128
extend+tolist() time: 0.0014913082122802734

Evaluating:  57%|█████▋    | 129/228 [00:34<00:26,  3.79it/s][Astep: 129
extend+tolist() time: 0.0008835792541503906

Evaluating:  57%|█████▋    | 130/228 [00:34<00:24,  3.94it/s][Astep: 130
extend+tolist() time: 0.0014660358428955078

Evaluating:  57%|█████▋    | 131/228 [00:35<00:24,  3.96it/s][Astep: 131
extend+tolist() time: 0.0004811286926269531

Evaluating:  58%|█████▊    | 132/228 [00:35<00:23,  4.10it/s][Astep: 132
extend+tolist() time: 0.0016224384307861328

Evaluating:  58%|█████▊    | 133/228 [00:35<00:24,  3.93it/s][Astep: 133
extend+tolist() time: 0.0004985332489013672

Evaluating:  59%|█████▉    | 134/228 [00:35<00:23,  4.06it/s][Astep: 134
extend+tolist() time: 0.0011184215545654297

Evaluating:  59%|█████▉    | 135/228 [00:36<00:23,  3.95it/s][Astep: 135
extend+tolist() time: 0.0009448528289794922

Evaluating:  60%|█████▉    | 136/228 [00:36<00:22,  4.10it/s][Astep: 136
extend+tolist() time: 0.0009350776672363281

Evaluating:  60%|██████    | 137/228 [00:36<00:21,  4.17it/s][Astep: 137
extend+tolist() time: 0.0004153251647949219

Evaluating:  61%|██████    | 138/228 [00:36<00:21,  4.25it/s][Astep: 138
extend+tolist() time: 0.0012362003326416016

Evaluating:  61%|██████    | 139/228 [00:37<00:20,  4.30it/s][Astep: 139
extend+tolist() time: 0.0005218982696533203

Evaluating:  61%|██████▏   | 140/228 [00:37<00:20,  4.35it/s][Astep: 140
extend+tolist() time: 0.0012633800506591797

Evaluating:  62%|██████▏   | 141/228 [00:37<00:19,  4.35it/s][Astep: 141
extend+tolist() time: 0.0009076595306396484

Evaluating:  62%|██████▏   | 142/228 [00:37<00:19,  4.32it/s][Astep: 142
extend+tolist() time: 0.0006585121154785156

Evaluating:  63%|██████▎   | 143/228 [00:38<00:19,  4.35it/s][Astep: 143
extend+tolist() time: 0.0007996559143066406

Evaluating:  63%|██████▎   | 144/228 [00:38<00:19,  4.39it/s][Astep: 144
extend+tolist() time: 0.0008087158203125

Evaluating:  64%|██████▎   | 145/228 [00:38<00:18,  4.41it/s][Astep: 145
extend+tolist() time: 0.00055694580078125

Evaluating:  64%|██████▍   | 146/228 [00:38<00:18,  4.43it/s][Astep: 146
extend+tolist() time: 0.0008056163787841797

Evaluating:  64%|██████▍   | 147/228 [00:38<00:18,  4.44it/s][Astep: 147
extend+tolist() time: 0.0008382797241210938

Evaluating:  65%|██████▍   | 148/228 [00:39<00:18,  4.41it/s][Astep: 148
extend+tolist() time: 0.0007200241088867188

Evaluating:  65%|██████▌   | 149/228 [00:39<00:17,  4.41it/s][Astep: 149
extend+tolist() time: 0.0008494853973388672

Evaluating:  66%|██████▌   | 150/228 [00:39<00:17,  4.44it/s][Astep: 150
extend+tolist() time: 0.0009455680847167969

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.39it/s][Astep: 151
extend+tolist() time: 0.0006575584411621094

Evaluating:  67%|██████▋   | 152/228 [00:40<00:17,  4.42it/s][Astep: 152
extend+tolist() time: 0.0014047622680664062

Evaluating:  67%|██████▋   | 153/228 [00:40<00:17,  4.37it/s][Astep: 153
extend+tolist() time: 0.0010330677032470703

Evaluating:  68%|██████▊   | 154/228 [00:40<00:17,  4.30it/s][Astep: 154
extend+tolist() time: 0.0020647048950195312

Evaluating:  68%|██████▊   | 155/228 [00:40<00:19,  3.83it/s][Astep: 155
extend+tolist() time: 0.0010602474212646484

Evaluating:  68%|██████▊   | 156/228 [00:41<00:18,  4.00it/s][Astep: 156
extend+tolist() time: 0.0005578994750976562

Evaluating:  69%|██████▉   | 157/228 [00:41<00:17,  4.14it/s][Astep: 157
extend+tolist() time: 0.0007104873657226562

Evaluating:  69%|██████▉   | 158/228 [00:41<00:16,  4.26it/s][Astep: 158
extend+tolist() time: 0.0009646415710449219

Evaluating:  70%|██████▉   | 159/228 [00:41<00:15,  4.31it/s][Astep: 159
extend+tolist() time: 0.0006771087646484375

Evaluating:  70%|███████   | 160/228 [00:41<00:15,  4.34it/s][Astep: 160
extend+tolist() time: 0.0003848075866699219

Evaluating:  71%|███████   | 161/228 [00:42<00:15,  4.39it/s][Astep: 161
extend+tolist() time: 0.0012531280517578125

Evaluating:  71%|███████   | 162/228 [00:42<00:15,  4.39it/s][Astep: 162
extend+tolist() time: 0.0005083084106445312

Evaluating:  71%|███████▏  | 163/228 [00:42<00:14,  4.42it/s][Astep: 163
extend+tolist() time: 0.0004105567932128906

Evaluating:  72%|███████▏  | 164/228 [00:42<00:14,  4.44it/s][Astep: 164
extend+tolist() time: 0.0005865097045898438

Evaluating:  72%|███████▏  | 165/228 [00:43<00:14,  4.46it/s][Astep: 165
extend+tolist() time: 0.0009026527404785156

Evaluating:  73%|███████▎  | 166/228 [00:43<00:13,  4.47it/s][Astep: 166
extend+tolist() time: 0.0003724098205566406

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.49it/s][Astep: 167
extend+tolist() time: 0.0009126663208007812

Evaluating:  74%|███████▎  | 168/228 [00:43<00:13,  4.46it/s][Astep: 168
extend+tolist() time: 0.21502327919006348

Evaluating:  74%|███████▍  | 169/228 [00:44<00:17,  3.30it/s][Astep: 169
extend+tolist() time: 0.0003650188446044922

Evaluating:  75%|███████▍  | 170/228 [00:44<00:16,  3.58it/s][Astep: 170
extend+tolist() time: 0.0013151168823242188

Evaluating:  75%|███████▌  | 171/228 [00:44<00:15,  3.76it/s][Astep: 171
extend+tolist() time: 0.0002925395965576172

Evaluating:  75%|███████▌  | 172/228 [00:44<00:14,  3.95it/s][Astep: 172
extend+tolist() time: 0.0008082389831542969

Evaluating:  76%|███████▌  | 173/228 [00:45<00:13,  4.08it/s][Astep: 173
extend+tolist() time: 0.0015780925750732422

Evaluating:  76%|███████▋  | 174/228 [00:45<00:13,  3.96it/s][Astep: 174
extend+tolist() time: 0.0017943382263183594

Evaluating:  77%|███████▋  | 175/228 [00:45<00:14,  3.74it/s][Astep: 175
extend+tolist() time: 0.0008802413940429688

Evaluating:  77%|███████▋  | 176/228 [00:45<00:13,  3.90it/s][Astep: 176
extend+tolist() time: 0.0011067390441894531

Evaluating:  78%|███████▊  | 177/228 [00:46<00:12,  4.03it/s][Astep: 177
extend+tolist() time: 0.0005958080291748047

Evaluating:  78%|███████▊  | 178/228 [00:46<00:12,  4.14it/s][Astep: 178
extend+tolist() time: 0.0017066001892089844

Evaluating:  79%|███████▊  | 179/228 [00:46<00:12,  3.97it/s][Astep: 179
extend+tolist() time: 0.0004086494445800781

Evaluating:  79%|███████▉  | 180/228 [00:46<00:11,  4.10it/s][Astep: 180
extend+tolist() time: 0.0004076957702636719

Evaluating:  79%|███████▉  | 181/228 [00:47<00:11,  4.22it/s][Astep: 181
extend+tolist() time: 0.0006222724914550781

Evaluating:  80%|███████▉  | 182/228 [00:47<00:10,  4.28it/s][Astep: 182
extend+tolist() time: 0.0012767314910888672

Evaluating:  80%|████████  | 183/228 [00:47<00:10,  4.32it/s][Astep: 183
extend+tolist() time: 0.000644683837890625

Evaluating:  81%|████████  | 184/228 [00:47<00:10,  4.35it/s][Astep: 184
extend+tolist() time: 0.00043964385986328125

Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.39it/s][Astep: 185
extend+tolist() time: 0.001622915267944336

Evaluating:  82%|████████▏ | 186/228 [00:48<00:10,  4.16it/s][Astep: 186
extend+tolist() time: 0.0014758110046386719

Evaluating:  82%|████████▏ | 187/228 [00:48<00:09,  4.14it/s][Astep: 187
extend+tolist() time: 0.0004336833953857422

Evaluating:  82%|████████▏ | 188/228 [00:48<00:09,  4.24it/s][Astep: 188
extend+tolist() time: 0.0007069110870361328

Evaluating:  83%|████████▎ | 189/228 [00:48<00:09,  4.28it/s][Astep: 189
extend+tolist() time: 0.0003647804260253906

Evaluating:  83%|████████▎ | 190/228 [00:49<00:08,  4.35it/s][Astep: 190
extend+tolist() time: 0.0016629695892333984

Evaluating:  84%|████████▍ | 191/228 [00:49<00:08,  4.13it/s][Astep: 191
extend+tolist() time: 0.0007004737854003906

Evaluating:  84%|████████▍ | 192/228 [00:49<00:08,  4.19it/s][Astep: 192
extend+tolist() time: 0.0008301734924316406

Evaluating:  85%|████████▍ | 193/228 [00:49<00:08,  4.26it/s][Astep: 193
extend+tolist() time: 0.0010571479797363281

Evaluating:  85%|████████▌ | 194/228 [00:50<00:08,  4.19it/s][Astep: 194
extend+tolist() time: 0.0010578632354736328

Evaluating:  86%|████████▌ | 195/228 [00:50<00:09,  3.33it/s][Astep: 195
extend+tolist() time: 0.0005805492401123047

Evaluating:  86%|████████▌ | 196/228 [00:50<00:08,  3.60it/s][Astep: 196
extend+tolist() time: 0.0006351470947265625

Evaluating:  86%|████████▋ | 197/228 [00:51<00:08,  3.81it/s][Astep: 197
extend+tolist() time: 0.0011262893676757812

Evaluating:  87%|████████▋ | 198/228 [00:51<00:07,  3.97it/s][Astep: 198
extend+tolist() time: 0.0006511211395263672

Evaluating:  87%|████████▋ | 199/228 [00:51<00:07,  4.09it/s][Astep: 199
extend+tolist() time: 0.0018534660339355469

Evaluating:  88%|████████▊ | 200/228 [00:51<00:07,  3.82it/s][Astep: 200
extend+tolist() time: 0.0007081031799316406

Evaluating:  88%|████████▊ | 201/228 [00:52<00:08,  3.17it/s][Astep: 201
extend+tolist() time: 0.0005705356597900391

Evaluating:  89%|████████▊ | 202/228 [00:52<00:07,  3.47it/s][Astep: 202
extend+tolist() time: 0.0008528232574462891

Evaluating:  89%|████████▉ | 203/228 [00:52<00:06,  3.72it/s][Astep: 203
extend+tolist() time: 0.0005190372467041016

Evaluating:  89%|████████▉ | 204/228 [00:52<00:06,  3.91it/s][Astep: 204
extend+tolist() time: 0.0003743171691894531

Evaluating:  90%|████████▉ | 205/228 [00:53<00:05,  4.08it/s][Astep: 205
extend+tolist() time: 0.00031876564025878906

Evaluating:  90%|█████████ | 206/228 [00:53<00:05,  4.20it/s][Astep: 206
extend+tolist() time: 0.0006279945373535156

Evaluating:  91%|█████████ | 207/228 [00:53<00:04,  4.26it/s][Astep: 207
extend+tolist() time: 0.0006203651428222656

Evaluating:  91%|█████████ | 208/228 [00:53<00:04,  4.30it/s][Astep: 208
extend+tolist() time: 0.0007154941558837891

Evaluating:  92%|█████████▏| 209/228 [00:54<00:04,  4.34it/s][Astep: 209
extend+tolist() time: 0.0010230541229248047

Evaluating:  92%|█████████▏| 210/228 [00:54<00:04,  4.38it/s][Astep: 210
extend+tolist() time: 0.0005972385406494141

Evaluating:  93%|█████████▎| 211/228 [00:54<00:03,  4.40it/s][Astep: 211
extend+tolist() time: 0.0014653205871582031

Evaluating:  93%|█████████▎| 212/228 [00:54<00:03,  4.17it/s][Astep: 212
extend+tolist() time: 0.0013511180877685547

Evaluating:  93%|█████████▎| 213/228 [00:55<00:03,  4.19it/s][Astep: 213
extend+tolist() time: 0.0007739067077636719

Evaluating:  94%|█████████▍| 214/228 [00:55<00:03,  4.24it/s][Astep: 214
extend+tolist() time: 0.0013117790222167969

Evaluating:  94%|█████████▍| 215/228 [00:55<00:03,  4.29it/s][Astep: 215
extend+tolist() time: 0.0006687641143798828

Evaluating:  95%|█████████▍| 216/228 [00:55<00:02,  4.34it/s][Astep: 216
extend+tolist() time: 0.0010063648223876953

Evaluating:  95%|█████████▌| 217/228 [00:55<00:02,  4.37it/s][Astep: 217
extend+tolist() time: 0.0005145072937011719

Evaluating:  96%|█████████▌| 218/228 [00:56<00:02,  4.40it/s][Astep: 218
extend+tolist() time: 0.0010836124420166016

Evaluating:  96%|█████████▌| 219/228 [00:56<00:02,  4.30it/s][Astep: 219
extend+tolist() time: 0.0009281635284423828

Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.36it/s][Astep: 220
extend+tolist() time: 0.0003974437713623047

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.39it/s][Astep: 221
extend+tolist() time: 0.0007116794586181641

Evaluating:  97%|█████████▋| 222/228 [00:57<00:01,  4.40it/s][Astep: 222
extend+tolist() time: 0.0004620552062988281

Evaluating:  98%|█████████▊| 223/228 [00:57<00:01,  4.43it/s][Astep: 223
extend+tolist() time: 0.000881195068359375

Evaluating:  98%|█████████▊| 224/228 [00:57<00:00,  4.45it/s][Astep: 224
extend+tolist() time: 0.0003895759582519531

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.47it/s][Astep: 225
extend+tolist() time: 0.0004677772521972656

Evaluating:  99%|█████████▉| 226/228 [00:57<00:00,  4.49it/s][Astep: 226
extend+tolist() time: 0.0005555152893066406

Evaluating: 100%|█████████▉| 227/228 [00:58<00:00,  4.48it/s][Astep: 227
extend+tolist() time: 0.0009641647338867188

Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.90it/s][A09/05/2023 15:44:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 15:44:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 15:44:59 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 15:44:59 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 15:44:59 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 15:44:59 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:00<00:00,  3.78it/s]
09/05/2023 15:44:59 - INFO - __main__ -   Step: 4400, Validation Metrics: {'pred_1_num': 9610, 'pred_-1_num': 859, 'pred_0_num': 332, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7838163133043237, 'f1_micro': 0.7838163133043236, 'f1_macro': 0.4555457140276726, 'f1_weighted': 0.7562274750748107, 'f1_-1': 0.3529896907216495, 'f1_0': 0.1380020597322348, 'f1_1': 0.8756453916291332, 'precision_micro': 0.7838163133043237, 'precision_macro': 0.5098365011799532, 'precision_weighted': 0.7442977015238015, 'precision_-1': 0.49825378346915017, 'precision_0': 0.20180722891566266, 'precision_1': 0.8294484911550468, 'recall_micro': 0.7838163133043237, 'recall_macro': 0.4351502947878649, 'recall_weighted': 0.7838163133043237, 'recall_-1': 0.27330779054916987, 'recall_0': 0.10485133020344288, 'recall_1': 0.9272917636109819, 'roc_auc_micro': 0.9189732725337791, 'roc_auc_macro': 0.7556612401881905, 'roc_auc_weighted': 0.76042091098981, 'roc_auc_-1': 0.8439834780919111, 'roc_auc_0': 0.6711677706907103, 'roc_auc_1': 0.75183247178195}
[2023-09-05 15:45:13,059] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4401/66600 [1:41:16<555:31:55, 32.15s/it]09/05/2023 15:45:13 - INFO - __main__ -   Step: 4401, LR: 1.9255548544196724e-05, Loss: 0.15502214431762695
[2023-09-05 15:45:27,563] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4402/66600 [1:41:31<464:02:36, 26.86s/it]09/05/2023 15:45:27 - INFO - __main__ -   Step: 4402, LR: 1.9255238840002942e-05, Loss: 0.24337390065193176
[2023-09-05 15:45:42,436] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4403/66600 [1:41:45<401:54:38, 23.26s/it]09/05/2023 15:45:42 - INFO - __main__ -   Step: 4403, LR: 1.9254929135809163e-05, Loss: 0.14762918651103973
[2023-09-05 15:45:56,309] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4404/66600 [1:41:59<353:14:23, 20.45s/it]09/05/2023 15:45:56 - INFO - __main__ -   Step: 4404, LR: 1.925461943161538e-05, Loss: 0.19511768221855164
[2023-09-05 15:46:10,401] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4405/66600 [1:42:13<320:17:58, 18.54s/it]09/05/2023 15:46:10 - INFO - __main__ -   Step: 4405, LR: 1.92543097274216e-05, Loss: 0.13267795741558075
[2023-09-05 15:46:24,020] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4406/66600 [1:42:27<294:47:26, 17.06s/it]09/05/2023 15:46:24 - INFO - __main__ -   Step: 4406, LR: 1.9254000023227818e-05, Loss: 0.1877860426902771
[2023-09-05 15:46:38,774] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4407/66600 [1:42:42<282:49:01, 16.37s/it]09/05/2023 15:46:38 - INFO - __main__ -   Step: 4407, LR: 1.9253690319034036e-05, Loss: 0.14974357187747955
[2023-09-05 15:46:52,281] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4408/66600 [1:42:55<267:58:25, 15.51s/it]09/05/2023 15:46:52 - INFO - __main__ -   Step: 4408, LR: 1.925338061484025e-05, Loss: 0.149740070104599
[2023-09-05 15:47:05,132] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4409/66600 [1:43:08<254:10:32, 14.71s/it]09/05/2023 15:47:05 - INFO - __main__ -   Step: 4409, LR: 1.925307091064647e-05, Loss: 0.20048178732395172
[2023-09-05 15:47:19,205] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4410/66600 [1:43:22<250:51:25, 14.52s/it]09/05/2023 15:47:19 - INFO - __main__ -   Step: 4410, LR: 1.9252761206452686e-05, Loss: 0.20646311342716217
[2023-09-05 15:47:32,573] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4411/66600 [1:43:36<244:52:34, 14.18s/it]09/05/2023 15:47:32 - INFO - __main__ -   Step: 4411, LR: 1.9252451502258908e-05, Loss: 0.1454385668039322
[2023-09-05 15:47:47,254] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4412/66600 [1:43:50<247:29:19, 14.33s/it]09/05/2023 15:47:47 - INFO - __main__ -   Step: 4412, LR: 1.9252141798065126e-05, Loss: 0.17655938863754272
[2023-09-05 15:48:00,709] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4413/66600 [1:44:04<242:57:59, 14.07s/it]09/05/2023 15:48:00 - INFO - __main__ -   Step: 4413, LR: 1.9251832093871344e-05, Loss: 0.15904878079891205
[2023-09-05 15:48:15,361] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4414/66600 [1:44:18<246:00:17, 14.24s/it]09/05/2023 15:48:15 - INFO - __main__ -   Step: 4414, LR: 1.9251522389677562e-05, Loss: 0.15991562604904175
[2023-09-05 15:48:29,277] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4415/66600 [1:44:32<244:18:56, 14.14s/it]09/05/2023 15:48:29 - INFO - __main__ -   Step: 4415, LR: 1.925121268548378e-05, Loss: 0.16946077346801758
[2023-09-05 15:48:42,786] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4416/66600 [1:44:46<241:01:04, 13.95s/it]09/05/2023 15:48:42 - INFO - __main__ -   Step: 4416, LR: 1.9250902981289995e-05, Loss: 0.16602635383605957
[2023-09-05 15:48:56,196] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4417/66600 [1:44:59<238:11:58, 13.79s/it]09/05/2023 15:48:56 - INFO - __main__ -   Step: 4417, LR: 1.9250593277096213e-05, Loss: 0.17147600650787354
[2023-09-05 15:49:09,874] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4418/66600 [1:45:13<237:37:00, 13.76s/it]09/05/2023 15:49:09 - INFO - __main__ -   Step: 4418, LR: 1.9250283572902434e-05, Loss: 0.13909468054771423
[2023-09-05 15:49:23,581] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4419/66600 [1:45:27<237:21:15, 13.74s/it]09/05/2023 15:49:23 - INFO - __main__ -   Step: 4419, LR: 1.9249973868708652e-05, Loss: 0.18080584704875946
[2023-09-05 15:49:37,148] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4420/66600 [1:45:40<236:26:46, 13.69s/it]09/05/2023 15:49:37 - INFO - __main__ -   Step: 4420, LR: 1.924966416451487e-05, Loss: 0.1340784728527069
[2023-09-05 15:49:52,343] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4421/66600 [1:45:55<244:14:29, 14.14s/it]09/05/2023 15:49:52 - INFO - __main__ -   Step: 4421, LR: 1.924935446032109e-05, Loss: 0.18631738424301147
[2023-09-05 15:50:06,543] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4422/66600 [1:46:10<244:32:49, 14.16s/it]09/05/2023 15:50:06 - INFO - __main__ -   Step: 4422, LR: 1.9249044756127307e-05, Loss: 0.14339187741279602
[2023-09-05 15:50:20,822] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4423/66600 [1:46:24<245:09:56, 14.19s/it]09/05/2023 15:50:20 - INFO - __main__ -   Step: 4423, LR: 1.924873505193352e-05, Loss: 0.17516212165355682
[2023-09-05 15:50:35,134] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4424/66600 [1:46:38<245:45:53, 14.23s/it]09/05/2023 15:50:35 - INFO - __main__ -   Step: 4424, LR: 1.924842534773974e-05, Loss: 0.15761609375476837
[2023-09-05 15:50:48,544] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4425/66600 [1:46:52<241:30:45, 13.98s/it]09/05/2023 15:50:48 - INFO - __main__ -   Step: 4425, LR: 1.924811564354596e-05, Loss: 0.15353095531463623
[2023-09-05 15:51:01,903] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4426/66600 [1:47:05<238:16:26, 13.80s/it]09/05/2023 15:51:01 - INFO - __main__ -   Step: 4426, LR: 1.924780593935218e-05, Loss: 0.24328404664993286
[2023-09-05 15:51:15,707] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4427/66600 [1:47:19<238:18:32, 13.80s/it]09/05/2023 15:51:15 - INFO - __main__ -   Step: 4427, LR: 1.9247496235158397e-05, Loss: 0.13872116804122925
[2023-09-05 15:51:30,096] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4428/66600 [1:47:33<241:21:40, 13.98s/it]09/05/2023 15:51:30 - INFO - __main__ -   Step: 4428, LR: 1.9247186530964615e-05, Loss: 0.14769628643989563
[2023-09-05 15:51:43,573] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4429/66600 [1:47:47<238:46:35, 13.83s/it]09/05/2023 15:51:43 - INFO - __main__ -   Step: 4429, LR: 1.9246876826770833e-05, Loss: 0.18829447031021118
[2023-09-05 15:51:56,504] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4430/66600 [1:48:00<234:07:51, 13.56s/it]09/05/2023 15:51:56 - INFO - __main__ -   Step: 4430, LR: 1.924656712257705e-05, Loss: 0.17605233192443848
[2023-09-05 15:52:10,333] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4431/66600 [1:48:13<235:32:14, 13.64s/it]09/05/2023 15:52:10 - INFO - __main__ -   Step: 4431, LR: 1.9246257418383266e-05, Loss: 0.1854836493730545
[2023-09-05 15:52:23,237] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4432/66600 [1:48:26<231:43:26, 13.42s/it]09/05/2023 15:52:23 - INFO - __main__ -   Step: 4432, LR: 1.9245947714189487e-05, Loss: 0.15521056950092316
[2023-09-05 15:52:37,463] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4433/66600 [1:48:41<235:54:02, 13.66s/it]09/05/2023 15:52:37 - INFO - __main__ -   Step: 4433, LR: 1.9245638009995705e-05, Loss: 0.13870546221733093
[2023-09-05 15:52:50,454] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4434/66600 [1:48:54<232:25:52, 13.46s/it]09/05/2023 15:52:50 - INFO - __main__ -   Step: 4434, LR: 1.9245328305801923e-05, Loss: 0.17620757222175598
[2023-09-05 15:53:03,632] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4435/66600 [1:49:07<230:57:44, 13.38s/it]09/05/2023 15:53:03 - INFO - __main__ -   Step: 4435, LR: 1.924501860160814e-05, Loss: 0.1529957354068756
[2023-09-05 15:53:17,569] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4436/66600 [1:49:21<233:52:21, 13.54s/it]09/05/2023 15:53:17 - INFO - __main__ -   Step: 4436, LR: 1.924470889741436e-05, Loss: 0.16356487572193146
[2023-09-05 15:53:30,707] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4437/66600 [1:49:34<231:45:51, 13.42s/it]09/05/2023 15:53:30 - INFO - __main__ -   Step: 4437, LR: 1.9244399193220577e-05, Loss: 0.13754285871982574
[2023-09-05 15:53:45,525] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4438/66600 [1:49:49<238:59:35, 13.84s/it]09/05/2023 15:53:45 - INFO - __main__ -   Step: 4438, LR: 1.9244089489026795e-05, Loss: 0.12842868268489838
[2023-09-05 15:53:59,729] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4439/66600 [1:50:03<240:52:19, 13.95s/it]09/05/2023 15:53:59 - INFO - __main__ -   Step: 4439, LR: 1.9243779784833014e-05, Loss: 0.13224206864833832
[2023-09-05 15:54:14,270] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4440/66600 [1:50:17<243:55:46, 14.13s/it]09/05/2023 15:54:14 - INFO - __main__ -   Step: 4440, LR: 1.924347008063923e-05, Loss: 0.17243951559066772
[2023-09-05 15:54:27,906] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4441/66600 [1:50:31<241:22:54, 13.98s/it]09/05/2023 15:54:27 - INFO - __main__ -   Step: 4441, LR: 1.924316037644545e-05, Loss: 0.13510340452194214
[2023-09-05 15:54:42,746] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4442/66600 [1:50:46<245:49:55, 14.24s/it]09/05/2023 15:54:42 - INFO - __main__ -   Step: 4442, LR: 1.9242850672251668e-05, Loss: 0.25672999024391174
[2023-09-05 15:54:56,334] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4443/66600 [1:50:59<242:27:39, 14.04s/it]09/05/2023 15:54:56 - INFO - __main__ -   Step: 4443, LR: 1.9242540968057886e-05, Loss: 0.17492246627807617
[2023-09-05 15:55:09,815] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4444/66600 [1:51:13<239:32:46, 13.87s/it]09/05/2023 15:55:09 - INFO - __main__ -   Step: 4444, LR: 1.9242231263864104e-05, Loss: 0.17310485243797302
[2023-09-05 15:55:24,245] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4445/66600 [1:51:27<242:25:11, 14.04s/it]09/05/2023 15:55:24 - INFO - __main__ -   Step: 4445, LR: 1.9241921559670322e-05, Loss: 0.16769355535507202
[2023-09-05 15:55:38,129] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4446/66600 [1:51:41<241:36:16, 13.99s/it]09/05/2023 15:55:38 - INFO - __main__ -   Step: 4446, LR: 1.924161185547654e-05, Loss: 0.2010250985622406
[2023-09-05 15:55:51,743] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4447/66600 [1:51:55<239:38:10, 13.88s/it]09/05/2023 15:55:51 - INFO - __main__ -   Step: 4447, LR: 1.9241302151282758e-05, Loss: 0.20244044065475464
[2023-09-05 15:56:05,282] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4448/66600 [1:52:08<237:51:40, 13.78s/it]09/05/2023 15:56:05 - INFO - __main__ -   Step: 4448, LR: 1.9240992447088976e-05, Loss: 0.16819988191127777
[2023-09-05 15:56:20,279] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4449/66600 [1:52:23<244:10:41, 14.14s/it]09/05/2023 15:56:20 - INFO - __main__ -   Step: 4449, LR: 1.9240682742895194e-05, Loss: 0.15808820724487305
[2023-09-05 15:56:35,466] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4450/66600 [1:52:39<249:34:29, 14.46s/it]09/05/2023 15:56:35 - INFO - __main__ -   Step: 4450, LR: 1.9240373038701412e-05, Loss: 0.13406124711036682
[2023-09-05 15:56:49,418] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4451/66600 [1:52:52<246:57:37, 14.31s/it]09/05/2023 15:56:49 - INFO - __main__ -   Step: 4451, LR: 1.924006333450763e-05, Loss: 0.256813108921051
[2023-09-05 15:57:03,349] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4452/66600 [1:53:06<245:01:11, 14.19s/it]09/05/2023 15:57:03 - INFO - __main__ -   Step: 4452, LR: 1.9239753630313848e-05, Loss: 0.13884535431861877
[2023-09-05 15:57:17,081] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4453/66600 [1:53:20<242:37:36, 14.05s/it]09/05/2023 15:57:17 - INFO - __main__ -   Step: 4453, LR: 1.9239443926120066e-05, Loss: 0.23518584668636322
[2023-09-05 15:57:30,865] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4454/66600 [1:53:34<241:13:00, 13.97s/it]09/05/2023 15:57:30 - INFO - __main__ -   Step: 4454, LR: 1.9239134221926284e-05, Loss: 0.17819027602672577
[2023-09-05 15:57:45,390] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4455/66600 [1:53:48<244:04:15, 14.14s/it]09/05/2023 15:57:45 - INFO - __main__ -   Step: 4455, LR: 1.9238824517732502e-05, Loss: 0.175258606672287
[2023-09-05 15:57:58,708] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4456/66600 [1:54:02<239:48:57, 13.89s/it]09/05/2023 15:57:58 - INFO - __main__ -   Step: 4456, LR: 1.923851481353872e-05, Loss: 0.13833869993686676
[2023-09-05 15:58:12,328] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4457/66600 [1:54:15<238:24:19, 13.81s/it]09/05/2023 15:58:12 - INFO - __main__ -   Step: 4457, LR: 1.923820510934494e-05, Loss: 0.11307860910892487
[2023-09-05 15:58:27,162] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4458/66600 [1:54:30<243:41:43, 14.12s/it]09/05/2023 15:58:27 - INFO - __main__ -   Step: 4458, LR: 1.9237895405151157e-05, Loss: 0.13507229089736938
[2023-09-05 15:58:40,358] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4459/66600 [1:54:43<238:55:14, 13.84s/it]09/05/2023 15:58:40 - INFO - __main__ -   Step: 4459, LR: 1.9237585700957375e-05, Loss: 0.13854199647903442
[2023-09-05 15:58:53,241] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4460/66600 [1:54:56<233:57:01, 13.55s/it]09/05/2023 15:58:53 - INFO - __main__ -   Step: 4460, LR: 1.9237275996763593e-05, Loss: 0.19491487741470337
[2023-09-05 15:59:08,072] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4461/66600 [1:55:11<240:33:43, 13.94s/it]09/05/2023 15:59:08 - INFO - __main__ -   Step: 4461, LR: 1.923696629256981e-05, Loss: 0.11528270691633224
[2023-09-05 15:59:22,151] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4462/66600 [1:55:25<241:17:50, 13.98s/it]09/05/2023 15:59:22 - INFO - __main__ -   Step: 4462, LR: 1.923665658837603e-05, Loss: 0.208095520734787
[2023-09-05 15:59:36,244] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4463/66600 [1:55:39<241:52:50, 14.01s/it]09/05/2023 15:59:36 - INFO - __main__ -   Step: 4463, LR: 1.9236346884182247e-05, Loss: 0.13339635729789734
[2023-09-05 15:59:50,307] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4464/66600 [1:55:53<242:07:46, 14.03s/it]09/05/2023 15:59:50 - INFO - __main__ -   Step: 4464, LR: 1.9236037179988465e-05, Loss: 0.1678370088338852
[2023-09-05 16:00:03,739] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4465/66600 [1:56:07<239:02:07, 13.85s/it]09/05/2023 16:00:03 - INFO - __main__ -   Step: 4465, LR: 1.9235727475794683e-05, Loss: 0.16305166482925415
[2023-09-05 16:00:18,168] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4466/66600 [1:56:21<242:02:02, 14.02s/it]09/05/2023 16:00:18 - INFO - __main__ -   Step: 4466, LR: 1.92354177716009e-05, Loss: 0.17628571391105652
[2023-09-05 16:00:31,362] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4467/66600 [1:56:34<237:44:08, 13.77s/it]09/05/2023 16:00:31 - INFO - __main__ -   Step: 4467, LR: 1.923510806740712e-05, Loss: 0.18318670988082886
[2023-09-05 16:00:44,883] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4468/66600 [1:56:48<236:25:21, 13.70s/it]09/05/2023 16:00:44 - INFO - __main__ -   Step: 4468, LR: 1.9234798363213337e-05, Loss: 0.18190254271030426
[2023-09-05 16:01:00,124] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4469/66600 [1:57:03<244:24:10, 14.16s/it]09/05/2023 16:01:00 - INFO - __main__ -   Step: 4469, LR: 1.9234488659019555e-05, Loss: 0.16858355700969696
[2023-09-05 16:01:14,908] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4470/66600 [1:57:18<247:37:24, 14.35s/it]09/05/2023 16:01:14 - INFO - __main__ -   Step: 4470, LR: 1.9234178954825773e-05, Loss: 0.1460648626089096
[2023-09-05 16:01:29,405] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4471/66600 [1:57:32<248:23:29, 14.39s/it]09/05/2023 16:01:29 - INFO - __main__ -   Step: 4471, LR: 1.923386925063199e-05, Loss: 0.1386021226644516
[2023-09-05 16:01:42,705] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4472/66600 [1:57:46<242:43:54, 14.07s/it]09/05/2023 16:01:42 - INFO - __main__ -   Step: 4472, LR: 1.923355954643821e-05, Loss: 0.1880597621202469
[2023-09-05 16:01:57,664] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4473/66600 [1:58:01<247:21:12, 14.33s/it]09/05/2023 16:01:57 - INFO - __main__ -   Step: 4473, LR: 1.9233249842244428e-05, Loss: 0.17946994304656982
[2023-09-05 16:02:12,109] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4474/66600 [1:58:15<247:55:52, 14.37s/it]09/05/2023 16:02:12 - INFO - __main__ -   Step: 4474, LR: 1.9232940138050646e-05, Loss: 0.15906518697738647
[2023-09-05 16:02:26,582] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4475/66600 [1:58:30<248:28:24, 14.40s/it]09/05/2023 16:02:26 - INFO - __main__ -   Step: 4475, LR: 1.9232630433856864e-05, Loss: 0.1501232087612152
[2023-09-05 16:02:39,750] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4476/66600 [1:58:43<242:06:05, 14.03s/it]09/05/2023 16:02:39 - INFO - __main__ -   Step: 4476, LR: 1.9232320729663082e-05, Loss: 0.15580499172210693
[2023-09-05 16:02:54,295] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4477/66600 [1:58:57<244:45:58, 14.18s/it]09/05/2023 16:02:54 - INFO - __main__ -   Step: 4477, LR: 1.92320110254693e-05, Loss: 0.16194617748260498
[2023-09-05 16:03:08,080] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4478/66600 [1:59:11<242:41:41, 14.06s/it]09/05/2023 16:03:08 - INFO - __main__ -   Step: 4478, LR: 1.9231701321275518e-05, Loss: 0.23947519063949585
[2023-09-05 16:03:22,996] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4479/66600 [1:59:26<247:06:00, 14.32s/it]09/05/2023 16:03:22 - INFO - __main__ -   Step: 4479, LR: 1.9231391617081736e-05, Loss: 0.17270126938819885
[2023-09-05 16:03:37,632] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4480/66600 [1:59:41<248:43:57, 14.41s/it]09/05/2023 16:03:37 - INFO - __main__ -   Step: 4480, LR: 1.9231081912887954e-05, Loss: 0.16050595045089722
[2023-09-05 16:03:51,950] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4481/66600 [1:59:55<248:13:50, 14.39s/it]09/05/2023 16:03:51 - INFO - __main__ -   Step: 4481, LR: 1.9230772208694172e-05, Loss: 0.1539362668991089
[2023-09-05 16:04:05,759] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4482/66600 [2:00:09<245:14:27, 14.21s/it]09/05/2023 16:04:05 - INFO - __main__ -   Step: 4482, LR: 1.923046250450039e-05, Loss: 0.18247494101524353
[2023-09-05 16:04:19,563] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4483/66600 [2:00:23<243:07:20, 14.09s/it]09/05/2023 16:04:19 - INFO - __main__ -   Step: 4483, LR: 1.9230152800306608e-05, Loss: 0.16067364811897278
[2023-09-05 16:04:34,075] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4484/66600 [2:00:37<245:18:06, 14.22s/it]09/05/2023 16:04:34 - INFO - __main__ -   Step: 4484, LR: 1.9229843096112826e-05, Loss: 0.16646528244018555
[2023-09-05 16:04:47,646] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4485/66600 [2:00:51<241:57:19, 14.02s/it]09/05/2023 16:04:47 - INFO - __main__ -   Step: 4485, LR: 1.9229533391919044e-05, Loss: 0.16770653426647186
[2023-09-05 16:05:00,645] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4486/66600 [2:01:04<236:38:51, 13.72s/it]09/05/2023 16:05:00 - INFO - __main__ -   Step: 4486, LR: 1.9229223687725262e-05, Loss: 0.16339486837387085
[2023-09-05 16:05:14,494] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4487/66600 [2:01:18<237:20:06, 13.76s/it]09/05/2023 16:05:14 - INFO - __main__ -   Step: 4487, LR: 1.922891398353148e-05, Loss: 0.1529601812362671
[2023-09-05 16:05:28,865] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4488/66600 [2:01:32<240:30:59, 13.94s/it]09/05/2023 16:05:28 - INFO - __main__ -   Step: 4488, LR: 1.92286042793377e-05, Loss: 0.15629318356513977
[2023-09-05 16:05:42,924] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4489/66600 [2:01:46<241:07:29, 13.98s/it]09/05/2023 16:05:42 - INFO - __main__ -   Step: 4489, LR: 1.9228294575143917e-05, Loss: 0.16475650668144226
[2023-09-05 16:05:56,797] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4490/66600 [2:02:00<240:35:37, 13.95s/it]09/05/2023 16:05:56 - INFO - __main__ -   Step: 4490, LR: 1.9227984870950135e-05, Loss: 0.1640118807554245
[2023-09-05 16:06:10,972] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4491/66600 [2:02:14<241:46:32, 14.01s/it]09/05/2023 16:06:10 - INFO - __main__ -   Step: 4491, LR: 1.9227675166756356e-05, Loss: 0.16328668594360352
[2023-09-05 16:06:23,870] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4492/66600 [2:02:27<235:59:43, 13.68s/it]09/05/2023 16:06:23 - INFO - __main__ -   Step: 4492, LR: 1.922736546256257e-05, Loss: 0.16194824874401093
[2023-09-05 16:06:37,208] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4493/66600 [2:02:40<234:13:37, 13.58s/it]09/05/2023 16:06:37 - INFO - __main__ -   Step: 4493, LR: 1.922705575836879e-05, Loss: 0.20800219476222992
[2023-09-05 16:06:51,625] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4494/66600 [2:02:55<238:34:27, 13.83s/it]09/05/2023 16:06:51 - INFO - __main__ -   Step: 4494, LR: 1.9226746054175007e-05, Loss: 0.15765725076198578
[2023-09-05 16:07:04,978] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4495/66600 [2:03:08<236:06:23, 13.69s/it]09/05/2023 16:07:04 - INFO - __main__ -   Step: 4495, LR: 1.9226436349981225e-05, Loss: 0.15925735235214233
[2023-09-05 16:07:19,396] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4496/66600 [2:03:22<239:53:20, 13.91s/it]09/05/2023 16:07:19 - INFO - __main__ -   Step: 4496, LR: 1.9226126645787443e-05, Loss: 0.14179092645645142
[2023-09-05 16:07:33,383] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4497/66600 [2:03:36<240:18:20, 13.93s/it]09/05/2023 16:07:33 - INFO - __main__ -   Step: 4497, LR: 1.922581694159366e-05, Loss: 0.14950630068778992
[2023-09-05 16:07:46,948] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4498/66600 [2:03:50<238:24:49, 13.82s/it]09/05/2023 16:07:46 - INFO - __main__ -   Step: 4498, LR: 1.9225507237399882e-05, Loss: 0.12776249647140503
[2023-09-05 16:08:00,267] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4499/66600 [2:04:03<235:48:39, 13.67s/it]09/05/2023 16:08:00 - INFO - __main__ -   Step: 4499, LR: 1.92251975332061e-05, Loss: 0.15765409171581268
[2023-09-05 16:08:14,792] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4500/66600 [2:04:18<240:13:49, 13.93s/it]09/05/2023 16:08:14 - INFO - __main__ -   Step: 4500, LR: 1.9224887829012315e-05, Loss: 0.16287320852279663
09/05/2023 16:08:14 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.002712249755859375

Evaluating:   0%|          | 1/228 [00:00<01:32,  2.45it/s][Astep: 1
extend+tolist() time: 0.0014927387237548828

Evaluating:   1%|          | 2/228 [00:00<01:11,  3.16it/s][Astep: 2
extend+tolist() time: 0.002050638198852539

Evaluating:   1%|▏         | 3/228 [00:00<01:13,  3.07it/s][Astep: 3
extend+tolist() time: 0.0013260841369628906

Evaluating:   2%|▏         | 4/228 [00:01<01:11,  3.15it/s][Astep: 4
extend+tolist() time: 0.0015294551849365234

Evaluating:   2%|▏         | 5/228 [00:01<01:05,  3.42it/s][Astep: 5
extend+tolist() time: 0.001976490020751953

Evaluating:   3%|▎         | 6/228 [00:01<01:07,  3.27it/s][Astep: 6
extend+tolist() time: 0.0018510818481445312

Evaluating:   3%|▎         | 7/228 [00:02<01:09,  3.16it/s][Astep: 7
extend+tolist() time: 0.0013456344604492188

Evaluating:   4%|▎         | 8/228 [00:02<01:05,  3.37it/s][Astep: 8
extend+tolist() time: 0.0007610321044921875

Evaluating:   4%|▍         | 9/228 [00:02<01:00,  3.63it/s][Astep: 9
extend+tolist() time: 0.0012590885162353516

Evaluating:   4%|▍         | 10/228 [00:02<00:57,  3.82it/s][Astep: 10
extend+tolist() time: 0.0008649826049804688

Evaluating:   5%|▍         | 11/228 [00:03<00:55,  3.90it/s][Astep: 11
extend+tolist() time: 0.0005881786346435547

Evaluating:   5%|▌         | 12/228 [00:03<01:02,  3.47it/s][Astep: 12
extend+tolist() time: 0.0011532306671142578

Evaluating:   6%|▌         | 13/228 [00:03<00:57,  3.72it/s][Astep: 13
extend+tolist() time: 0.0005743503570556641

Evaluating:   6%|▌         | 14/228 [00:03<00:54,  3.90it/s][Astep: 14
extend+tolist() time: 0.0009684562683105469

Evaluating:   7%|▋         | 15/228 [00:04<00:52,  4.03it/s][Astep: 15
extend+tolist() time: 0.14191269874572754

Evaluating:   7%|▋         | 16/228 [00:04<01:00,  3.53it/s][Astep: 16
extend+tolist() time: 0.0006256103515625

Evaluating:   7%|▋         | 17/228 [00:04<00:56,  3.76it/s][Astep: 17
extend+tolist() time: 0.0008599758148193359

Evaluating:   8%|▊         | 18/228 [00:05<00:54,  3.89it/s][Astep: 18
extend+tolist() time: 0.0015764236450195312

Evaluating:   8%|▊         | 19/228 [00:05<00:55,  3.79it/s][Astep: 19
extend+tolist() time: 0.0009713172912597656

Evaluating:   9%|▉         | 20/228 [00:05<00:55,  3.77it/s][Astep: 20
extend+tolist() time: 0.0007450580596923828

Evaluating:   9%|▉         | 21/228 [00:05<00:52,  3.91it/s][Astep: 21
extend+tolist() time: 0.0006580352783203125

Evaluating:  10%|▉         | 22/228 [00:06<00:51,  4.03it/s][Astep: 22
extend+tolist() time: 0.0012001991271972656

Evaluating:  10%|█         | 23/228 [00:06<00:50,  4.10it/s][Astep: 23
extend+tolist() time: 0.0006883144378662109

Evaluating:  11%|█         | 24/228 [00:06<00:48,  4.20it/s][Astep: 24
extend+tolist() time: 0.001598358154296875

Evaluating:  11%|█         | 25/228 [00:06<00:50,  4.00it/s][Astep: 25
extend+tolist() time: 0.0019161701202392578

Evaluating:  11%|█▏        | 26/228 [00:07<00:55,  3.61it/s][Astep: 26
extend+tolist() time: 0.0010900497436523438

Evaluating:  12%|█▏        | 27/228 [00:07<00:52,  3.82it/s][Astep: 27
extend+tolist() time: 0.0013010501861572266

Evaluating:  12%|█▏        | 28/228 [00:07<01:03,  3.14it/s][Astep: 28
extend+tolist() time: 0.0007948875427246094

Evaluating:  13%|█▎        | 29/228 [00:08<00:57,  3.45it/s][Astep: 29
extend+tolist() time: 0.0007469654083251953

Evaluating:  13%|█▎        | 30/228 [00:08<00:54,  3.66it/s][Astep: 30
extend+tolist() time: 0.0014767646789550781

Evaluating:  14%|█▎        | 31/228 [00:08<00:54,  3.63it/s][Astep: 31
extend+tolist() time: 0.0005738735198974609

Evaluating:  14%|█▍        | 32/228 [00:08<00:50,  3.85it/s][Astep: 32
extend+tolist() time: 0.0014126300811767578

Evaluating:  14%|█▍        | 33/228 [00:09<00:51,  3.79it/s][Astep: 33
extend+tolist() time: 0.0012340545654296875

Evaluating:  15%|█▍        | 34/228 [00:09<00:53,  3.63it/s][Astep: 34
extend+tolist() time: 0.0008656978607177734

Evaluating:  15%|█▌        | 35/228 [00:09<00:51,  3.75it/s][Astep: 35
extend+tolist() time: 0.0006618499755859375

Evaluating:  16%|█▌        | 36/228 [00:09<00:48,  3.95it/s][Astep: 36
extend+tolist() time: 0.0012803077697753906

Evaluating:  16%|█▌        | 37/228 [00:10<00:47,  4.05it/s][Astep: 37
extend+tolist() time: 0.0015954971313476562

Evaluating:  17%|█▋        | 38/228 [00:10<00:50,  3.78it/s][Astep: 38
extend+tolist() time: 0.0007798671722412109

Evaluating:  17%|█▋        | 39/228 [00:10<00:48,  3.92it/s][Astep: 39
extend+tolist() time: 0.0011823177337646484

Evaluating:  18%|█▊        | 40/228 [00:10<00:46,  4.04it/s][Astep: 40
extend+tolist() time: 0.0006392002105712891

Evaluating:  18%|█▊        | 41/228 [00:11<00:45,  4.15it/s][Astep: 41
extend+tolist() time: 0.0012581348419189453

Evaluating:  18%|█▊        | 42/228 [00:11<00:44,  4.14it/s][Astep: 42
extend+tolist() time: 0.0012063980102539062

Evaluating:  19%|█▉        | 43/228 [00:11<00:48,  3.85it/s][Astep: 43
extend+tolist() time: 0.0019371509552001953

Evaluating:  19%|█▉        | 44/228 [00:11<00:52,  3.54it/s][Astep: 44
extend+tolist() time: 0.0011649131774902344

Evaluating:  20%|█▉        | 45/228 [00:12<00:57,  3.19it/s][Astep: 45
extend+tolist() time: 0.0016453266143798828

Evaluating:  20%|██        | 46/228 [00:12<00:56,  3.22it/s][Astep: 46
extend+tolist() time: 0.0016071796417236328

Evaluating:  21%|██        | 47/228 [00:12<00:56,  3.23it/s][Astep: 47
extend+tolist() time: 0.15645384788513184

Evaluating:  21%|██        | 48/228 [00:13<01:02,  2.87it/s][Astep: 48
extend+tolist() time: 0.0017151832580566406

Evaluating:  21%|██▏       | 49/228 [00:13<00:59,  2.99it/s][Astep: 49
extend+tolist() time: 0.001272439956665039

Evaluating:  22%|██▏       | 50/228 [00:13<00:55,  3.22it/s][Astep: 50
extend+tolist() time: 0.0015935897827148438

Evaluating:  22%|██▏       | 51/228 [00:14<00:54,  3.24it/s][Astep: 51
extend+tolist() time: 0.0015120506286621094

Evaluating:  23%|██▎       | 52/228 [00:14<00:53,  3.27it/s][Astep: 52
extend+tolist() time: 0.0009984970092773438

Evaluating:  23%|██▎       | 53/228 [00:14<00:52,  3.36it/s][Astep: 53
extend+tolist() time: 0.0017406940460205078

Evaluating:  24%|██▎       | 54/228 [00:15<00:52,  3.34it/s][Astep: 54
extend+tolist() time: 0.0011439323425292969

Evaluating:  24%|██▍       | 55/228 [00:15<00:48,  3.57it/s][Astep: 55
extend+tolist() time: 0.0007557868957519531

Evaluating:  25%|██▍       | 56/228 [00:15<00:45,  3.77it/s][Astep: 56
extend+tolist() time: 0.001692056655883789

Evaluating:  25%|██▌       | 57/228 [00:15<00:47,  3.61it/s][Astep: 57
extend+tolist() time: 0.0005941390991210938

Evaluating:  25%|██▌       | 58/228 [00:16<00:44,  3.82it/s][Astep: 58
extend+tolist() time: 0.0013153553009033203

Evaluating:  26%|██▌       | 59/228 [00:16<00:51,  3.27it/s][Astep: 59
extend+tolist() time: 0.0008933544158935547

Evaluating:  26%|██▋       | 60/228 [00:16<00:48,  3.47it/s][Astep: 60
extend+tolist() time: 0.0012271404266357422

Evaluating:  27%|██▋       | 61/228 [00:16<00:44,  3.71it/s][Astep: 61
extend+tolist() time: 0.0008571147918701172

Evaluating:  27%|██▋       | 62/228 [00:17<00:43,  3.82it/s][Astep: 62
extend+tolist() time: 0.0012202262878417969

Evaluating:  28%|██▊       | 63/228 [00:17<00:41,  3.97it/s][Astep: 63
extend+tolist() time: 0.0008006095886230469

Evaluating:  28%|██▊       | 64/228 [00:17<00:40,  4.06it/s][Astep: 64
extend+tolist() time: 0.00081634521484375

Evaluating:  29%|██▊       | 65/228 [00:17<00:39,  4.11it/s][Astep: 65
extend+tolist() time: 0.0012524127960205078

Evaluating:  29%|██▉       | 66/228 [00:18<00:39,  4.12it/s][Astep: 66
extend+tolist() time: 0.0007367134094238281

Evaluating:  29%|██▉       | 67/228 [00:18<00:38,  4.20it/s][Astep: 67
extend+tolist() time: 0.0013344287872314453

Evaluating:  30%|██▉       | 68/228 [00:18<00:38,  4.14it/s][Astep: 68
extend+tolist() time: 0.0006906986236572266

Evaluating:  30%|███       | 69/228 [00:18<00:37,  4.19it/s][Astep: 69
extend+tolist() time: 0.0019693374633789062

Evaluating:  31%|███       | 70/228 [00:19<00:39,  3.98it/s][Astep: 70
extend+tolist() time: 0.0010213851928710938

Evaluating:  31%|███       | 71/228 [00:19<00:40,  3.86it/s][Astep: 71
extend+tolist() time: 0.0014007091522216797

Evaluating:  32%|███▏      | 72/228 [00:19<00:40,  3.81it/s][Astep: 72
extend+tolist() time: 0.0011458396911621094

Evaluating:  32%|███▏      | 73/228 [00:19<00:39,  3.96it/s][Astep: 73
extend+tolist() time: 0.0007121562957763672

Evaluating:  32%|███▏      | 74/228 [00:20<00:37,  4.06it/s][Astep: 74
extend+tolist() time: 0.0007243156433105469

Evaluating:  33%|███▎      | 75/228 [00:20<00:36,  4.15it/s][Astep: 75
extend+tolist() time: 0.0017910003662109375

Evaluating:  33%|███▎      | 76/228 [00:20<00:39,  3.85it/s][Astep: 76
extend+tolist() time: 0.0010204315185546875

Evaluating:  34%|███▍      | 77/228 [00:20<00:37,  3.98it/s][Astep: 77
extend+tolist() time: 0.0018320083618164062

Evaluating:  34%|███▍      | 78/228 [00:21<00:41,  3.61it/s][Astep: 78
extend+tolist() time: 0.0008347034454345703

Evaluating:  35%|███▍      | 79/228 [00:21<00:40,  3.72it/s][Astep: 79
extend+tolist() time: 0.0013828277587890625

Evaluating:  35%|███▌      | 80/228 [00:21<00:46,  3.21it/s][Astep: 80
extend+tolist() time: 0.0011954307556152344

Evaluating:  36%|███▌      | 81/228 [00:22<00:43,  3.42it/s][Astep: 81
extend+tolist() time: 0.0012869834899902344

Evaluating:  36%|███▌      | 82/228 [00:22<00:40,  3.60it/s][Astep: 82
extend+tolist() time: 0.0008795261383056641

Evaluating:  36%|███▋      | 83/228 [00:22<00:38,  3.74it/s][Astep: 83
extend+tolist() time: 0.001184701919555664

Evaluating:  37%|███▋      | 84/228 [00:22<00:37,  3.81it/s][Astep: 84
extend+tolist() time: 0.0011789798736572266

Evaluating:  37%|███▋      | 85/228 [00:23<00:37,  3.77it/s][Astep: 85
extend+tolist() time: 0.17299675941467285

Evaluating:  38%|███▊      | 86/228 [00:23<00:44,  3.20it/s][Astep: 86
extend+tolist() time: 0.0014247894287109375

Evaluating:  38%|███▊      | 87/228 [00:23<00:41,  3.42it/s][Astep: 87
extend+tolist() time: 0.0009162425994873047

Evaluating:  39%|███▊      | 88/228 [00:24<00:39,  3.58it/s][Astep: 88
extend+tolist() time: 0.0007407665252685547

Evaluating:  39%|███▉      | 89/228 [00:24<00:37,  3.74it/s][Astep: 89
extend+tolist() time: 0.0007255077362060547

Evaluating:  39%|███▉      | 90/228 [00:24<00:35,  3.92it/s][Astep: 90
extend+tolist() time: 0.0009198188781738281

Evaluating:  40%|███▉      | 91/228 [00:24<00:34,  3.95it/s][Astep: 91
extend+tolist() time: 0.0011785030364990234

Evaluating:  40%|████      | 92/228 [00:25<00:33,  4.03it/s][Astep: 92
extend+tolist() time: 0.0007910728454589844

Evaluating:  41%|████      | 93/228 [00:25<00:32,  4.10it/s][Astep: 93
extend+tolist() time: 0.001422882080078125

Evaluating:  41%|████      | 94/228 [00:25<00:33,  3.98it/s][Astep: 94
extend+tolist() time: 0.0007495880126953125

Evaluating:  42%|████▏     | 95/228 [00:25<00:32,  4.11it/s][Astep: 95
extend+tolist() time: 0.001653909683227539

Evaluating:  42%|████▏     | 96/228 [00:26<00:34,  3.84it/s][Astep: 96
extend+tolist() time: 0.0013425350189208984

Evaluating:  43%|████▎     | 97/228 [00:26<00:33,  3.87it/s][Astep: 97
extend+tolist() time: 0.0008261203765869141

Evaluating:  43%|████▎     | 98/228 [00:26<00:32,  3.96it/s][Astep: 98
extend+tolist() time: 0.0013306140899658203

Evaluating:  43%|████▎     | 99/228 [00:26<00:32,  3.98it/s][Astep: 99
extend+tolist() time: 0.0009102821350097656

Evaluating:  44%|████▍     | 100/228 [00:27<00:31,  4.01it/s][Astep: 100
extend+tolist() time: 0.001161336898803711

Evaluating:  44%|████▍     | 101/228 [00:27<00:30,  4.12it/s][Astep: 101
extend+tolist() time: 0.0008034706115722656

Evaluating:  45%|████▍     | 102/228 [00:27<00:30,  4.14it/s][Astep: 102
extend+tolist() time: 0.001178741455078125

Evaluating:  45%|████▌     | 103/228 [00:27<00:29,  4.19it/s][Astep: 103
extend+tolist() time: 0.001013040542602539

Evaluating:  46%|████▌     | 104/228 [00:28<00:36,  3.44it/s][Astep: 104
extend+tolist() time: 0.0010805130004882812

Evaluating:  46%|████▌     | 105/228 [00:28<00:33,  3.68it/s][Astep: 105
extend+tolist() time: 0.0008399486541748047

Evaluating:  46%|████▋     | 106/228 [00:28<00:32,  3.81it/s][Astep: 106
extend+tolist() time: 0.0018162727355957031

Evaluating:  47%|████▋     | 107/228 [00:28<00:33,  3.57it/s][Astep: 107
extend+tolist() time: 0.0010952949523925781

Evaluating:  47%|████▋     | 108/228 [00:29<00:31,  3.76it/s][Astep: 108
extend+tolist() time: 0.0007977485656738281

Evaluating:  48%|████▊     | 109/228 [00:29<00:30,  3.89it/s][Astep: 109
extend+tolist() time: 0.0013213157653808594

Evaluating:  48%|████▊     | 110/228 [00:29<00:29,  3.96it/s][Astep: 110
extend+tolist() time: 0.0006351470947265625

Evaluating:  49%|████▊     | 111/228 [00:29<00:28,  4.09it/s][Astep: 111
extend+tolist() time: 0.0021364688873291016

Evaluating:  49%|████▉     | 112/228 [00:30<00:31,  3.74it/s][Astep: 112
extend+tolist() time: 0.0003826618194580078

Evaluating:  50%|████▉     | 113/228 [00:30<00:29,  3.94it/s][Astep: 113
extend+tolist() time: 0.0011131763458251953

Evaluating:  50%|█████     | 114/228 [00:30<00:28,  4.07it/s][Astep: 114
extend+tolist() time: 0.0010788440704345703

Evaluating:  50%|█████     | 115/228 [00:30<00:28,  3.91it/s][Astep: 115
extend+tolist() time: 0.0011446475982666016

Evaluating:  51%|█████     | 116/228 [00:31<00:27,  4.05it/s][Astep: 116
extend+tolist() time: 0.0009531974792480469

Evaluating:  51%|█████▏    | 117/228 [00:31<00:26,  4.11it/s][Astep: 117
extend+tolist() time: 0.0014677047729492188

Evaluating:  52%|█████▏    | 118/228 [00:31<00:26,  4.11it/s][Astep: 118
extend+tolist() time: 0.0006554126739501953

Evaluating:  52%|█████▏    | 119/228 [00:31<00:26,  4.19it/s][Astep: 119
extend+tolist() time: 0.0012483596801757812

Evaluating:  53%|█████▎    | 120/228 [00:32<00:25,  4.21it/s][Astep: 120
extend+tolist() time: 0.0007483959197998047

Evaluating:  53%|█████▎    | 121/228 [00:32<00:25,  4.25it/s][Astep: 121
extend+tolist() time: 0.0007376670837402344

Evaluating:  54%|█████▎    | 122/228 [00:32<00:24,  4.27it/s][Astep: 122
extend+tolist() time: 0.0012671947479248047

Evaluating:  54%|█████▍    | 123/228 [00:32<00:24,  4.28it/s][Astep: 123
extend+tolist() time: 0.0007050037384033203

Evaluating:  54%|█████▍    | 124/228 [00:33<00:24,  4.33it/s][Astep: 124
extend+tolist() time: 0.0013666152954101562

Evaluating:  55%|█████▍    | 125/228 [00:33<00:23,  4.29it/s][Astep: 125
extend+tolist() time: 0.0004999637603759766

Evaluating:  55%|█████▌    | 126/228 [00:33<00:23,  4.36it/s][Astep: 126
extend+tolist() time: 0.0019354820251464844

Evaluating:  56%|█████▌    | 127/228 [00:33<00:25,  3.89it/s][Astep: 127
extend+tolist() time: 0.0018231868743896484

Evaluating:  56%|█████▌    | 128/228 [00:34<00:27,  3.58it/s][Astep: 128
extend+tolist() time: 0.0007867813110351562

Evaluating:  57%|█████▋    | 129/228 [00:34<00:26,  3.79it/s][Astep: 129
extend+tolist() time: 0.0012555122375488281

Evaluating:  57%|█████▋    | 130/228 [00:34<00:24,  3.95it/s][Astep: 130
extend+tolist() time: 0.0010745525360107422

Evaluating:  57%|█████▋    | 131/228 [00:34<00:24,  3.97it/s][Astep: 131
extend+tolist() time: 0.0004994869232177734

Evaluating:  58%|█████▊    | 132/228 [00:35<00:28,  3.33it/s][Astep: 132
extend+tolist() time: 0.0017092227935791016

Evaluating:  58%|█████▊    | 133/228 [00:35<00:27,  3.40it/s][Astep: 133
extend+tolist() time: 0.0005118846893310547

Evaluating:  59%|█████▉    | 134/228 [00:35<00:25,  3.66it/s][Astep: 134
extend+tolist() time: 0.0015950202941894531

Evaluating:  59%|█████▉    | 135/228 [00:36<00:25,  3.67it/s][Astep: 135
extend+tolist() time: 0.0005068778991699219

Evaluating:  60%|█████▉    | 136/228 [00:36<00:23,  3.87it/s][Astep: 136
extend+tolist() time: 0.0014045238494873047

Evaluating:  60%|██████    | 137/228 [00:36<00:22,  4.00it/s][Astep: 137
extend+tolist() time: 0.0004265308380126953

Evaluating:  61%|██████    | 138/228 [00:36<00:21,  4.12it/s][Astep: 138
extend+tolist() time: 0.0008397102355957031

Evaluating:  61%|██████    | 139/228 [00:36<00:21,  4.20it/s][Astep: 139
extend+tolist() time: 0.0009691715240478516

Evaluating:  61%|██████▏   | 140/228 [00:37<00:20,  4.28it/s][Astep: 140
extend+tolist() time: 0.0008671283721923828

Evaluating:  62%|██████▏   | 141/228 [00:37<00:20,  4.29it/s][Astep: 141
extend+tolist() time: 0.20204854011535645

Evaluating:  62%|██████▏   | 142/228 [00:37<00:25,  3.41it/s][Astep: 142
extend+tolist() time: 0.000637054443359375

Evaluating:  63%|██████▎   | 143/228 [00:38<00:23,  3.64it/s][Astep: 143
extend+tolist() time: 0.00037860870361328125

Evaluating:  63%|██████▎   | 144/228 [00:38<00:21,  3.84it/s][Astep: 144
extend+tolist() time: 0.0012462139129638672

Evaluating:  64%|██████▎   | 145/228 [00:38<00:20,  4.00it/s][Astep: 145
extend+tolist() time: 0.0005741119384765625

Evaluating:  64%|██████▍   | 146/228 [00:38<00:19,  4.12it/s][Astep: 146
extend+tolist() time: 0.0004296302795410156

Evaluating:  64%|██████▍   | 147/228 [00:38<00:19,  4.20it/s][Astep: 147
extend+tolist() time: 0.0011913776397705078

Evaluating:  65%|██████▍   | 148/228 [00:39<00:18,  4.23it/s][Astep: 148
extend+tolist() time: 0.0007343292236328125

Evaluating:  65%|██████▌   | 149/228 [00:39<00:18,  4.31it/s][Astep: 149
extend+tolist() time: 0.00043320655822753906

Evaluating:  66%|██████▌   | 150/228 [00:39<00:18,  4.33it/s][Astep: 150
extend+tolist() time: 0.0016875267028808594

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.32it/s][Astep: 151
extend+tolist() time: 0.0006678104400634766

Evaluating:  67%|██████▋   | 152/228 [00:40<00:17,  4.35it/s][Astep: 152
extend+tolist() time: 0.0013108253479003906

Evaluating:  67%|██████▋   | 153/228 [00:40<00:17,  4.31it/s][Astep: 153
extend+tolist() time: 0.001026153564453125

Evaluating:  68%|██████▊   | 154/228 [00:40<00:17,  4.26it/s][Astep: 154
extend+tolist() time: 0.0021567344665527344

Evaluating:  68%|██████▊   | 155/228 [00:40<00:19,  3.80it/s][Astep: 155
extend+tolist() time: 0.0010673999786376953

Evaluating:  68%|██████▊   | 156/228 [00:41<00:18,  3.97it/s][Astep: 156
extend+tolist() time: 0.0005769729614257812

Evaluating:  69%|██████▉   | 157/228 [00:41<00:17,  4.09it/s][Astep: 157
extend+tolist() time: 0.0006890296936035156

Evaluating:  69%|██████▉   | 158/228 [00:41<00:16,  4.22it/s][Astep: 158
extend+tolist() time: 0.0010356903076171875

Evaluating:  70%|██████▉   | 159/228 [00:41<00:16,  4.28it/s][Astep: 159
extend+tolist() time: 0.0007486343383789062

Evaluating:  70%|███████   | 160/228 [00:42<00:15,  4.31it/s][Astep: 160
extend+tolist() time: 0.0003898143768310547

Evaluating:  71%|███████   | 161/228 [00:42<00:15,  4.35it/s][Astep: 161
extend+tolist() time: 0.0012679100036621094

Evaluating:  71%|███████   | 162/228 [00:42<00:15,  4.40it/s][Astep: 162
extend+tolist() time: 0.0005447864532470703

Evaluating:  71%|███████▏  | 163/228 [00:42<00:14,  4.41it/s][Astep: 163
extend+tolist() time: 0.0004532337188720703

Evaluating:  72%|███████▏  | 164/228 [00:42<00:14,  4.42it/s][Astep: 164
extend+tolist() time: 0.0005972385406494141

Evaluating:  72%|███████▏  | 165/228 [00:43<00:14,  4.42it/s][Astep: 165
extend+tolist() time: 0.000942230224609375

Evaluating:  73%|███████▎  | 166/228 [00:43<00:14,  4.42it/s][Astep: 166
extend+tolist() time: 0.00040411949157714844

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.44it/s][Astep: 167
extend+tolist() time: 0.0006201267242431641

Evaluating:  74%|███████▎  | 168/228 [00:43<00:13,  4.44it/s][Astep: 168
extend+tolist() time: 0.0016524791717529297

Evaluating:  74%|███████▍  | 169/228 [00:44<00:14,  4.17it/s][Astep: 169
extend+tolist() time: 0.00038623809814453125

Evaluating:  75%|███████▍  | 170/228 [00:44<00:13,  4.24it/s][Astep: 170
extend+tolist() time: 0.0008883476257324219

Evaluating:  75%|███████▌  | 171/228 [00:44<00:13,  4.26it/s][Astep: 171
extend+tolist() time: 0.0007703304290771484

Evaluating:  75%|███████▌  | 172/228 [00:44<00:12,  4.32it/s][Astep: 172
extend+tolist() time: 0.0008420944213867188

Evaluating:  76%|███████▌  | 173/228 [00:45<00:12,  4.33it/s][Astep: 173
extend+tolist() time: 0.0018165111541748047

Evaluating:  76%|███████▋  | 174/228 [00:45<00:13,  4.11it/s][Astep: 174
extend+tolist() time: 0.0018591880798339844

Evaluating:  77%|███████▋  | 175/228 [00:45<00:17,  2.96it/s][Astep: 175
extend+tolist() time: 0.0008244514465332031

Evaluating:  77%|███████▋  | 176/228 [00:46<00:15,  3.26it/s][Astep: 176
extend+tolist() time: 0.0006711483001708984

Evaluating:  78%|███████▊  | 177/228 [00:46<00:14,  3.52it/s][Astep: 177
extend+tolist() time: 0.0010743141174316406

Evaluating:  78%|███████▊  | 178/228 [00:46<00:13,  3.74it/s][Astep: 178
extend+tolist() time: 0.0012664794921875

Evaluating:  79%|███████▊  | 179/228 [00:46<00:13,  3.71it/s][Astep: 179
extend+tolist() time: 0.0004105567932128906

Evaluating:  79%|███████▉  | 180/228 [00:47<00:12,  3.90it/s][Astep: 180
extend+tolist() time: 0.0006854534149169922

Evaluating:  79%|███████▉  | 181/228 [00:47<00:11,  4.03it/s][Astep: 181
extend+tolist() time: 0.0005562305450439453

Evaluating:  80%|███████▉  | 182/228 [00:47<00:11,  4.13it/s][Astep: 182
extend+tolist() time: 0.0007700920104980469

Evaluating:  80%|████████  | 183/228 [00:47<00:10,  4.23it/s][Astep: 183
extend+tolist() time: 0.0011532306671142578

Evaluating:  81%|████████  | 184/228 [00:47<00:10,  4.29it/s][Astep: 184
extend+tolist() time: 0.0004456043243408203

Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.33it/s][Astep: 185
extend+tolist() time: 0.0014007091522216797

Evaluating:  82%|████████▏ | 186/228 [00:48<00:10,  4.10it/s][Astep: 186
extend+tolist() time: 0.0010559558868408203

Evaluating:  82%|████████▏ | 187/228 [00:48<00:10,  4.09it/s][Astep: 187
extend+tolist() time: 0.0004661083221435547

Evaluating:  82%|████████▏ | 188/228 [00:48<00:09,  4.19it/s][Astep: 188
extend+tolist() time: 0.0011641979217529297

Evaluating:  83%|████████▎ | 189/228 [00:49<00:09,  4.25it/s][Astep: 189
extend+tolist() time: 0.0003752708435058594

Evaluating:  83%|████████▎ | 190/228 [00:49<00:08,  4.31it/s][Astep: 190
extend+tolist() time: 0.001584768295288086

Evaluating:  84%|████████▍ | 191/228 [00:49<00:09,  4.11it/s][Astep: 191
extend+tolist() time: 0.0007176399230957031

Evaluating:  84%|████████▍ | 192/228 [00:49<00:08,  4.17it/s][Astep: 192
extend+tolist() time: 0.0004620552062988281

Evaluating:  85%|████████▍ | 193/228 [00:50<00:08,  4.24it/s][Astep: 193
extend+tolist() time: 0.0015370845794677734

Evaluating:  85%|████████▌ | 194/228 [00:50<00:08,  4.16it/s][Astep: 194
extend+tolist() time: 0.0006420612335205078

Evaluating:  86%|████████▌ | 195/228 [00:50<00:07,  4.25it/s][Astep: 195
extend+tolist() time: 0.0005497932434082031

Evaluating:  86%|████████▌ | 196/228 [00:50<00:07,  4.31it/s][Astep: 196
extend+tolist() time: 0.001058816909790039

Evaluating:  86%|████████▋ | 197/228 [00:51<00:07,  4.32it/s][Astep: 197
extend+tolist() time: 0.0007719993591308594

Evaluating:  87%|████████▋ | 198/228 [00:51<00:06,  4.33it/s][Astep: 198
extend+tolist() time: 0.0006005764007568359

Evaluating:  87%|████████▋ | 199/228 [00:51<00:06,  4.34it/s][Astep: 199
extend+tolist() time: 0.0019359588623046875

Evaluating:  88%|████████▊ | 200/228 [00:51<00:07,  3.96it/s][Astep: 200
extend+tolist() time: 0.0010516643524169922

Evaluating:  88%|████████▊ | 201/228 [00:52<00:06,  4.10it/s][Astep: 201
extend+tolist() time: 0.000621795654296875

Evaluating:  89%|████████▊ | 202/228 [00:52<00:06,  4.19it/s][Astep: 202
extend+tolist() time: 0.0004279613494873047

Evaluating:  89%|████████▉ | 203/228 [00:52<00:05,  4.25it/s][Astep: 203
extend+tolist() time: 0.0005159378051757812

Evaluating:  89%|████████▉ | 204/228 [00:52<00:05,  4.31it/s][Astep: 204
extend+tolist() time: 0.0008668899536132812

Evaluating:  90%|████████▉ | 205/228 [00:52<00:05,  4.35it/s][Astep: 205
extend+tolist() time: 0.00031113624572753906

Evaluating:  90%|█████████ | 206/228 [00:53<00:05,  4.38it/s][Astep: 206
extend+tolist() time: 0.0006244182586669922

Evaluating:  91%|█████████ | 207/228 [00:53<00:04,  4.36it/s][Astep: 207
extend+tolist() time: 0.0006387233734130859

Evaluating:  91%|█████████ | 208/228 [00:53<00:04,  4.37it/s][Astep: 208
extend+tolist() time: 0.0011601448059082031

Evaluating:  92%|█████████▏| 209/228 [00:53<00:04,  4.40it/s][Astep: 209
extend+tolist() time: 0.0006296634674072266

Evaluating:  92%|█████████▏| 210/228 [00:54<00:04,  4.41it/s][Astep: 210
extend+tolist() time: 0.0007007122039794922

Evaluating:  93%|█████████▎| 211/228 [00:54<00:03,  4.40it/s][Astep: 211
extend+tolist() time: 0.0016205310821533203

Evaluating:  93%|█████████▎| 212/228 [00:54<00:03,  4.17it/s][Astep: 212
extend+tolist() time: 0.001260519027709961

Evaluating:  93%|█████████▎| 213/228 [00:54<00:03,  4.19it/s][Astep: 213
extend+tolist() time: 0.0007956027984619141

Evaluating:  94%|█████████▍| 214/228 [00:55<00:03,  4.23it/s][Astep: 214
extend+tolist() time: 0.0008885860443115234

Evaluating:  94%|█████████▍| 215/228 [00:55<00:03,  4.27it/s][Astep: 215
extend+tolist() time: 0.0011250972747802734

Evaluating:  95%|█████████▍| 216/228 [00:55<00:02,  4.31it/s][Astep: 216
extend+tolist() time: 0.0005443096160888672

Evaluating:  95%|█████████▌| 217/228 [00:55<00:02,  4.36it/s][Astep: 217
extend+tolist() time: 0.0005719661712646484

Evaluating:  96%|█████████▌| 218/228 [00:55<00:02,  4.38it/s][Astep: 218
extend+tolist() time: 0.0015697479248046875

Evaluating:  96%|█████████▌| 219/228 [00:56<00:02,  4.29it/s][Astep: 219
extend+tolist() time: 0.0005078315734863281

Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.34it/s][Astep: 220
extend+tolist() time: 0.0004532337188720703

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.39it/s][Astep: 221
extend+tolist() time: 0.0011873245239257812

Evaluating:  97%|█████████▋| 222/228 [00:56<00:01,  4.40it/s][Astep: 222
extend+tolist() time: 0.0004532337188720703

Evaluating:  98%|█████████▊| 223/228 [00:57<00:01,  3.38it/s][Astep: 223
extend+tolist() time: 0.00040340423583984375

Evaluating:  98%|█████████▊| 224/228 [00:57<00:01,  3.64it/s][Astep: 224
extend+tolist() time: 0.0003840923309326172

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  3.87it/s][Astep: 225
extend+tolist() time: 0.0009245872497558594

Evaluating:  99%|█████████▉| 226/228 [00:57<00:00,  4.05it/s][Astep: 226
extend+tolist() time: 0.0005643367767333984

Evaluating: 100%|█████████▉| 227/228 [00:58<00:00,  4.16it/s][Astep: 227
extend+tolist() time: 0.0004978179931640625

Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.73it/s][A09/05/2023 16:09:13 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 16:09:13 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:09:13 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:09:13 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:09:13 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:09:13 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:09:13 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:09:13 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:09:14 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:09:14 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:09:14 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:09:14 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:09:14 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:09:15 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 16:09:15 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 16:09:15 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 16:09:15 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:01<00:00,  3.73it/s]
09/05/2023 16:09:15 - INFO - __main__ -   Step: 4500, Validation Metrics: {'pred_1_num': 9561, 'pred_-1_num': 1040, 'pred_0_num': 200, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.788445514304231, 'f1_micro': 0.788445514304231, 'f1_macro': 0.45849140600769694, 'f1_weighted': 0.7608659705079583, 'f1_-1': 0.3806600153491942, 'f1_0': 0.1168057210965435, 'f1_1': 0.8780084815773531, 'precision_micro': 0.788445514304231, 'precision_macro': 0.5185408269170427, 'precision_weighted': 0.7471434694720367, 'precision_-1': 0.47692307692307695, 'precision_0': 0.245, 'precision_1': 0.8336994038280514, 'recall_micro': 0.788445514304231, 'recall_macro': 0.4402348677856644, 'recall_weighted': 0.788445514304231, 'recall_-1': 0.3167305236270754, 'recall_0': 0.07668231611893583, 'recall_1': 0.9272917636109819, 'roc_auc_micro': 0.9063444963351737, 'roc_auc_macro': 0.7151278474395694, 'roc_auc_weighted': 0.7007637523474132, 'roc_auc_-1': 0.7774038670973122, 'roc_auc_0': 0.6796051847396126, 'roc_auc_1': 0.688374490481783}
[2023-09-05 16:09:30,152] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4501/66600 [2:05:33<558:08:30, 32.36s/it]09/05/2023 16:09:30 - INFO - __main__ -   Step: 4501, LR: 1.9224578124818533e-05, Loss: 0.18132063746452332
[2023-09-05 16:09:43,624] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4502/66600 [2:05:47<460:24:35, 26.69s/it]09/05/2023 16:09:43 - INFO - __main__ -   Step: 4502, LR: 1.922426842062475e-05, Loss: 0.14193010330200195
[2023-09-05 16:09:57,644] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4503/66600 [2:06:01<394:49:51, 22.89s/it]09/05/2023 16:09:57 - INFO - __main__ -   Step: 4503, LR: 1.922395871643097e-05, Loss: 0.18964238464832306
[2023-09-05 16:10:11,805] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4504/66600 [2:06:15<349:39:18, 20.27s/it]09/05/2023 16:10:11 - INFO - __main__ -   Step: 4504, LR: 1.9223649012237187e-05, Loss: 0.18365582823753357
[2023-09-05 16:10:25,358] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4505/66600 [2:06:28<314:53:17, 18.26s/it]09/05/2023 16:10:25 - INFO - __main__ -   Step: 4505, LR: 1.922333930804341e-05, Loss: 0.143228217959404
[2023-09-05 16:10:39,739] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4506/66600 [2:06:43<294:49:50, 17.09s/it]09/05/2023 16:10:39 - INFO - __main__ -   Step: 4506, LR: 1.9223029603849627e-05, Loss: 0.15986087918281555
[2023-09-05 16:10:54,182] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4507/66600 [2:06:57<281:06:45, 16.30s/it]09/05/2023 16:10:54 - INFO - __main__ -   Step: 4507, LR: 1.922271989965584e-05, Loss: 0.13577501475811005
[2023-09-05 16:11:07,941] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4508/66600 [2:07:11<267:58:13, 15.54s/it]09/05/2023 16:11:07 - INFO - __main__ -   Step: 4508, LR: 1.922241019546206e-05, Loss: 0.1847684383392334
[2023-09-05 16:11:22,848] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4509/66600 [2:07:26<264:42:21, 15.35s/it]09/05/2023 16:11:22 - INFO - __main__ -   Step: 4509, LR: 1.9222100491268278e-05, Loss: 0.13518421351909637
[2023-09-05 16:11:36,628] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4510/66600 [2:07:40<256:35:28, 14.88s/it]09/05/2023 16:11:36 - INFO - __main__ -   Step: 4510, LR: 1.9221790787074496e-05, Loss: 0.16961240768432617
[2023-09-05 16:11:49,804] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4511/66600 [2:07:53<247:47:04, 14.37s/it]09/05/2023 16:11:49 - INFO - __main__ -   Step: 4511, LR: 1.9221481082880714e-05, Loss: 0.14973978698253632
[2023-09-05 16:12:03,951] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4512/66600 [2:08:07<246:38:31, 14.30s/it]09/05/2023 16:12:03 - INFO - __main__ -   Step: 4512, LR: 1.9221171378686935e-05, Loss: 0.13471104204654694
[2023-09-05 16:12:17,830] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4513/66600 [2:08:21<244:27:33, 14.17s/it]09/05/2023 16:12:17 - INFO - __main__ -   Step: 4513, LR: 1.9220861674493153e-05, Loss: 0.16241729259490967
[2023-09-05 16:12:31,622] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4514/66600 [2:08:35<242:28:29, 14.06s/it]09/05/2023 16:12:31 - INFO - __main__ -   Step: 4514, LR: 1.922055197029937e-05, Loss: 0.2099911868572235
[2023-09-05 16:12:45,055] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4515/66600 [2:08:48<239:13:33, 13.87s/it]09/05/2023 16:12:45 - INFO - __main__ -   Step: 4515, LR: 1.9220242266105586e-05, Loss: 0.13732445240020752
[2023-09-05 16:12:59,328] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4516/66600 [2:09:02<241:18:14, 13.99s/it]09/05/2023 16:12:59 - INFO - __main__ -   Step: 4516, LR: 1.9219932561911804e-05, Loss: 0.22140106558799744
[2023-09-05 16:13:14,754] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4517/66600 [2:09:18<248:42:59, 14.42s/it]09/05/2023 16:13:14 - INFO - __main__ -   Step: 4517, LR: 1.9219622857718022e-05, Loss: 0.17845597863197327
[2023-09-05 16:13:28,963] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4518/66600 [2:09:32<247:36:30, 14.36s/it]09/05/2023 16:13:28 - INFO - __main__ -   Step: 4518, LR: 1.921931315352424e-05, Loss: 0.14912168681621552
[2023-09-05 16:13:42,857] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4519/66600 [2:09:46<245:12:09, 14.22s/it]09/05/2023 16:13:42 - INFO - __main__ -   Step: 4519, LR: 1.921900344933046e-05, Loss: 0.10563092678785324
[2023-09-05 16:13:57,044] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4520/66600 [2:10:00<245:01:58, 14.21s/it]09/05/2023 16:13:57 - INFO - __main__ -   Step: 4520, LR: 1.921869374513668e-05, Loss: 0.16411352157592773
[2023-09-05 16:14:10,843] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4521/66600 [2:10:14<242:54:19, 14.09s/it]09/05/2023 16:14:10 - INFO - __main__ -   Step: 4521, LR: 1.9218384040942898e-05, Loss: 0.12703116238117218
[2023-09-05 16:14:25,124] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4522/66600 [2:10:28<243:54:41, 14.14s/it]09/05/2023 16:14:25 - INFO - __main__ -   Step: 4522, LR: 1.9218074336749116e-05, Loss: 0.18079812824726105
[2023-09-05 16:14:40,739] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4523/66600 [2:10:44<251:30:45, 14.59s/it]09/05/2023 16:14:40 - INFO - __main__ -   Step: 4523, LR: 1.921776463255533e-05, Loss: 0.15890273451805115
[2023-09-05 16:14:54,236] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4524/66600 [2:10:57<245:52:28, 14.26s/it]09/05/2023 16:14:54 - INFO - __main__ -   Step: 4524, LR: 1.921745492836155e-05, Loss: 0.16709938645362854
[2023-09-05 16:15:07,760] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4525/66600 [2:11:11<242:03:59, 14.04s/it]09/05/2023 16:15:07 - INFO - __main__ -   Step: 4525, LR: 1.9217145224167767e-05, Loss: 0.1563854217529297
[2023-09-05 16:15:21,886] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4526/66600 [2:11:25<242:31:06, 14.06s/it]09/05/2023 16:15:21 - INFO - __main__ -   Step: 4526, LR: 1.9216835519973988e-05, Loss: 0.21566420793533325
[2023-09-05 16:15:35,203] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4527/66600 [2:11:38<238:38:32, 13.84s/it]09/05/2023 16:15:35 - INFO - __main__ -   Step: 4527, LR: 1.9216525815780206e-05, Loss: 0.1577005833387375
[2023-09-05 16:15:49,229] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4528/66600 [2:11:52<239:35:58, 13.90s/it]09/05/2023 16:15:49 - INFO - __main__ -   Step: 4528, LR: 1.9216216111586424e-05, Loss: 0.17789576947689056
[2023-09-05 16:16:03,257] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4529/66600 [2:12:06<240:16:42, 13.94s/it]09/05/2023 16:16:03 - INFO - __main__ -   Step: 4529, LR: 1.9215906407392642e-05, Loss: 0.15792763233184814
[2023-09-05 16:16:16,560] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4530/66600 [2:12:20<237:00:14, 13.75s/it]09/05/2023 16:16:16 - INFO - __main__ -   Step: 4530, LR: 1.9215596703198857e-05, Loss: 0.21265612542629242
[2023-09-05 16:16:29,728] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4531/66600 [2:12:33<234:00:25, 13.57s/it]09/05/2023 16:16:29 - INFO - __main__ -   Step: 4531, LR: 1.9215286999005075e-05, Loss: 0.15425466001033783
[2023-09-05 16:16:43,512] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4532/66600 [2:12:47<235:06:03, 13.64s/it]09/05/2023 16:16:43 - INFO - __main__ -   Step: 4532, LR: 1.9214977294811293e-05, Loss: 0.13737504184246063
[2023-09-05 16:16:58,059] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4533/66600 [2:13:01<239:48:27, 13.91s/it]09/05/2023 16:16:58 - INFO - __main__ -   Step: 4533, LR: 1.9214667590617515e-05, Loss: 0.16660931706428528
[2023-09-05 16:17:12,837] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4534/66600 [2:13:16<244:17:53, 14.17s/it]09/05/2023 16:17:12 - INFO - __main__ -   Step: 4534, LR: 1.9214357886423733e-05, Loss: 0.20321014523506165
[2023-09-05 16:17:26,521] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4535/66600 [2:13:30<241:46:54, 14.02s/it]09/05/2023 16:17:26 - INFO - __main__ -   Step: 4535, LR: 1.921404818222995e-05, Loss: 0.15135544538497925
[2023-09-05 16:17:39,995] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4536/66600 [2:13:43<238:55:50, 13.86s/it]09/05/2023 16:17:39 - INFO - __main__ -   Step: 4536, LR: 1.921373847803617e-05, Loss: 0.15529398620128632
[2023-09-05 16:17:53,837] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4537/66600 [2:13:57<238:50:15, 13.85s/it]09/05/2023 16:17:53 - INFO - __main__ -   Step: 4537, LR: 1.9213428773842387e-05, Loss: 0.20730805397033691
[2023-09-05 16:18:07,724] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4538/66600 [2:14:11<239:00:43, 13.86s/it]09/05/2023 16:18:07 - INFO - __main__ -   Step: 4538, LR: 1.92131190696486e-05, Loss: 0.15634459257125854
[2023-09-05 16:18:21,244] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4539/66600 [2:14:24<237:13:18, 13.76s/it]09/05/2023 16:18:21 - INFO - __main__ -   Step: 4539, LR: 1.921280936545482e-05, Loss: 0.16801215708255768
[2023-09-05 16:18:34,075] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4540/66600 [2:14:37<232:24:21, 13.48s/it]09/05/2023 16:18:34 - INFO - __main__ -   Step: 4540, LR: 1.921249966126104e-05, Loss: 0.18485119938850403
[2023-09-05 16:18:48,405] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4541/66600 [2:14:51<236:47:37, 13.74s/it]09/05/2023 16:18:48 - INFO - __main__ -   Step: 4541, LR: 1.921218995706726e-05, Loss: 0.14966538548469543
[2023-09-05 16:19:02,328] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4542/66600 [2:15:05<237:45:12, 13.79s/it]09/05/2023 16:19:02 - INFO - __main__ -   Step: 4542, LR: 1.9211880252873477e-05, Loss: 0.19075876474380493
[2023-09-05 16:19:16,092] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4543/66600 [2:15:19<237:36:21, 13.78s/it]09/05/2023 16:19:16 - INFO - __main__ -   Step: 4543, LR: 1.9211570548679695e-05, Loss: 0.1833307147026062
[2023-09-05 16:19:30,818] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4544/66600 [2:15:34<242:28:26, 14.07s/it]09/05/2023 16:19:30 - INFO - __main__ -   Step: 4544, LR: 1.9211260844485913e-05, Loss: 0.1419735550880432
[2023-09-05 16:19:45,111] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4545/66600 [2:15:48<243:38:21, 14.13s/it]09/05/2023 16:19:45 - INFO - __main__ -   Step: 4545, LR: 1.921095114029213e-05, Loss: 0.17784737050533295
[2023-09-05 16:19:59,635] [WARNING] [stage3.py:1898:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4546/66600 [2:16:03<245:39:15, 14.25s/it]09/05/2023 16:19:59 - INFO - __main__ -   Step: 4546, LR: 1.9210641436098346e-05, Loss: 0.15052509307861328
[2023-09-05 16:20:12,530] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4547/66600 [2:16:16<238:38:03, 13.84s/it]09/05/2023 16:20:12 - INFO - __main__ -   Step: 4547, LR: 1.9210331731904567e-05, Loss: 0.18911898136138916
[2023-09-05 16:20:27,300] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4548/66600 [2:16:30<243:24:53, 14.12s/it]09/05/2023 16:20:27 - INFO - __main__ -   Step: 4548, LR: 1.9210022027710785e-05, Loss: 0.16358250379562378
[2023-09-05 16:20:40,383] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4549/66600 [2:16:43<238:02:25, 13.81s/it]09/05/2023 16:20:40 - INFO - __main__ -   Step: 4549, LR: 1.9209712323517003e-05, Loss: 0.15102876722812653
[2023-09-05 16:20:54,789] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4550/66600 [2:16:58<241:07:11, 13.99s/it]09/05/2023 16:20:54 - INFO - __main__ -   Step: 4550, LR: 1.920940261932322e-05, Loss: 0.14260035753250122
[2023-09-05 16:21:08,726] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4551/66600 [2:17:12<240:50:35, 13.97s/it]09/05/2023 16:21:08 - INFO - __main__ -   Step: 4551, LR: 1.920909291512944e-05, Loss: 0.1760980188846588
[2023-09-05 16:21:23,264] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4552/66600 [2:17:26<243:45:25, 14.14s/it]09/05/2023 16:21:23 - INFO - __main__ -   Step: 4552, LR: 1.9208783210935658e-05, Loss: 0.12458552420139313
[2023-09-05 16:21:37,249] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4553/66600 [2:17:40<242:56:21, 14.10s/it]09/05/2023 16:21:37 - INFO - __main__ -   Step: 4553, LR: 1.9208473506741872e-05, Loss: 0.14427652955055237
[2023-09-05 16:21:52,894] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4554/66600 [2:17:56<250:56:59, 14.56s/it]09/05/2023 16:21:52 - INFO - __main__ -   Step: 4554, LR: 1.9208163802548094e-05, Loss: 0.14944159984588623
[2023-09-05 16:22:07,020] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4555/66600 [2:18:10<248:41:53, 14.43s/it]09/05/2023 16:22:07 - INFO - __main__ -   Step: 4555, LR: 1.9207854098354312e-05, Loss: 0.22530359029769897
[2023-09-05 16:22:20,713] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4556/66600 [2:18:24<244:52:58, 14.21s/it]09/05/2023 16:22:20 - INFO - __main__ -   Step: 4556, LR: 1.920754439416053e-05, Loss: 0.18280071020126343
[2023-09-05 16:22:34,852] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4557/66600 [2:18:38<244:30:54, 14.19s/it]09/05/2023 16:22:34 - INFO - __main__ -   Step: 4557, LR: 1.9207234689966748e-05, Loss: 0.17987284064292908
[2023-09-05 16:22:49,054] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4558/66600 [2:18:52<244:35:18, 14.19s/it]09/05/2023 16:22:49 - INFO - __main__ -   Step: 4558, LR: 1.9206924985772966e-05, Loss: 0.16052982211112976
[2023-09-05 16:23:03,435] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4559/66600 [2:19:06<245:33:26, 14.25s/it]09/05/2023 16:23:03 - INFO - __main__ -   Step: 4559, LR: 1.9206615281579184e-05, Loss: 0.17466989159584045
[2023-09-05 16:23:16,704] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4560/66600 [2:19:20<240:29:11, 13.95s/it]09/05/2023 16:23:16 - INFO - __main__ -   Step: 4560, LR: 1.9206305577385402e-05, Loss: 0.15829671919345856
[2023-09-05 16:23:30,783] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4561/66600 [2:19:34<241:07:44, 13.99s/it]09/05/2023 16:23:30 - INFO - __main__ -   Step: 4561, LR: 1.920599587319162e-05, Loss: 0.18118999898433685
[2023-09-05 16:23:45,218] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4562/66600 [2:19:48<243:24:53, 14.13s/it]09/05/2023 16:23:45 - INFO - __main__ -   Step: 4562, LR: 1.9205686168997838e-05, Loss: 0.1503266990184784
[2023-09-05 16:24:00,283] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4563/66600 [2:20:03<248:15:55, 14.41s/it]09/05/2023 16:24:00 - INFO - __main__ -   Step: 4563, LR: 1.9205376464804056e-05, Loss: 0.12696507573127747
[2023-09-05 16:24:15,282] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4564/66600 [2:20:18<251:19:34, 14.58s/it]09/05/2023 16:24:15 - INFO - __main__ -   Step: 4564, LR: 1.9205066760610274e-05, Loss: 0.14548690617084503
[2023-09-05 16:24:30,593] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4565/66600 [2:20:34<255:04:21, 14.80s/it]09/05/2023 16:24:30 - INFO - __main__ -   Step: 4565, LR: 1.9204757056416492e-05, Loss: 0.15399038791656494
[2023-09-05 16:24:44,538] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4566/66600 [2:20:48<250:38:31, 14.55s/it]09/05/2023 16:24:44 - INFO - __main__ -   Step: 4566, LR: 1.920444735222271e-05, Loss: 0.14968357980251312
[2023-09-05 16:24:57,931] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4567/66600 [2:21:01<244:40:47, 14.20s/it]09/05/2023 16:24:57 - INFO - __main__ -   Step: 4567, LR: 1.920413764802893e-05, Loss: 0.23392866551876068
[2023-09-05 16:25:11,784] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4568/66600 [2:21:15<242:52:52, 14.10s/it]09/05/2023 16:25:11 - INFO - __main__ -   Step: 4568, LR: 1.9203827943835143e-05, Loss: 0.1598663628101349
[2023-09-05 16:25:26,005] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4569/66600 [2:21:29<243:31:43, 14.13s/it]09/05/2023 16:25:26 - INFO - __main__ -   Step: 4569, LR: 1.9203518239641365e-05, Loss: 0.1830769032239914
[2023-09-05 16:25:41,061] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4570/66600 [2:21:44<248:17:34, 14.41s/it]09/05/2023 16:25:41 - INFO - __main__ -   Step: 4570, LR: 1.9203208535447583e-05, Loss: 0.17737479507923126
[2023-09-05 16:25:55,040] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4571/66600 [2:21:58<246:03:40, 14.28s/it]09/05/2023 16:25:55 - INFO - __main__ -   Step: 4571, LR: 1.92028988312538e-05, Loss: 0.21065929532051086
[2023-09-05 16:26:09,258] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4572/66600 [2:22:12<245:44:00, 14.26s/it]09/05/2023 16:26:09 - INFO - __main__ -   Step: 4572, LR: 1.920258912706002e-05, Loss: 0.21912342309951782
[2023-09-05 16:26:22,464] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4573/66600 [2:22:26<240:16:10, 13.95s/it]09/05/2023 16:26:22 - INFO - __main__ -   Step: 4573, LR: 1.9202279422866237e-05, Loss: 0.2096010446548462
[2023-09-05 16:26:37,652] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4574/66600 [2:22:41<246:41:34, 14.32s/it]09/05/2023 16:26:37 - INFO - __main__ -   Step: 4574, LR: 1.9201969718672455e-05, Loss: 0.13481229543685913
[2023-09-05 16:26:51,326] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4575/66600 [2:22:54<243:21:22, 14.12s/it]09/05/2023 16:26:51 - INFO - __main__ -   Step: 4575, LR: 1.9201660014478673e-05, Loss: 0.22889675199985504
[2023-09-05 16:27:06,083] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4576/66600 [2:23:09<246:37:21, 14.31s/it]09/05/2023 16:27:06 - INFO - __main__ -   Step: 4576, LR: 1.920135031028489e-05, Loss: 0.13651970028877258
[2023-09-05 16:27:19,872] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4577/66600 [2:23:23<243:54:13, 14.16s/it]09/05/2023 16:27:19 - INFO - __main__ -   Step: 4577, LR: 1.920104060609111e-05, Loss: 0.17541487514972687
[2023-09-05 16:27:34,285] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4578/66600 [2:23:37<245:13:10, 14.23s/it]09/05/2023 16:27:34 - INFO - __main__ -   Step: 4578, LR: 1.9200730901897327e-05, Loss: 0.12957702577114105
[2023-09-05 16:27:48,347] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4579/66600 [2:23:51<244:19:48, 14.18s/it]09/05/2023 16:27:48 - INFO - __main__ -   Step: 4579, LR: 1.9200421197703545e-05, Loss: 0.1841984987258911
[2023-09-05 16:28:04,637] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4580/66600 [2:24:08<255:13:18, 14.81s/it]09/05/2023 16:28:04 - INFO - __main__ -   Step: 4580, LR: 1.9200111493509763e-05, Loss: 0.13116276264190674
[2023-09-05 16:28:18,775] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4581/66600 [2:24:22<251:43:14, 14.61s/it]09/05/2023 16:28:18 - INFO - __main__ -   Step: 4581, LR: 1.919980178931598e-05, Loss: 0.15926894545555115
[2023-09-05 16:28:33,445] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4582/66600 [2:24:36<252:00:59, 14.63s/it]09/05/2023 16:28:33 - INFO - __main__ -   Step: 4582, LR: 1.91994920851222e-05, Loss: 0.1092400997877121
[2023-09-05 16:28:46,608] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4583/66600 [2:24:50<244:26:12, 14.19s/it]09/05/2023 16:28:46 - INFO - __main__ -   Step: 4583, LR: 1.9199182380928418e-05, Loss: 0.18207043409347534
[2023-09-05 16:29:00,951] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4584/66600 [2:25:04<245:13:45, 14.24s/it]09/05/2023 16:29:00 - INFO - __main__ -   Step: 4584, LR: 1.9198872676734636e-05, Loss: 0.18622374534606934
[2023-09-05 16:29:14,529] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4585/66600 [2:25:18<241:49:40, 14.04s/it]09/05/2023 16:29:14 - INFO - __main__ -   Step: 4585, LR: 1.9198562972540854e-05, Loss: 0.19582819938659668
[2023-09-05 16:29:27,911] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4586/66600 [2:25:31<238:25:53, 13.84s/it]09/05/2023 16:29:27 - INFO - __main__ -   Step: 4586, LR: 1.9198253268347072e-05, Loss: 0.1392817497253418
[2023-09-05 16:29:42,580] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4587/66600 [2:25:46<242:42:14, 14.09s/it]09/05/2023 16:29:42 - INFO - __main__ -   Step: 4587, LR: 1.919794356415329e-05, Loss: 0.16656652092933655
[2023-09-05 16:29:57,421] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4588/66600 [2:26:00<246:35:00, 14.31s/it]09/05/2023 16:29:57 - INFO - __main__ -   Step: 4588, LR: 1.9197633859959508e-05, Loss: 0.19777372479438782
[2023-09-05 16:30:11,350] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4589/66600 [2:26:14<244:35:13, 14.20s/it]09/05/2023 16:30:11 - INFO - __main__ -   Step: 4589, LR: 1.9197324155765726e-05, Loss: 0.1248854547739029
[2023-09-05 16:30:24,979] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4590/66600 [2:26:28<241:38:21, 14.03s/it]09/05/2023 16:30:24 - INFO - __main__ -   Step: 4590, LR: 1.9197014451571944e-05, Loss: 0.15868979692459106
[2023-09-05 16:30:40,317] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4591/66600 [2:26:43<248:24:00, 14.42s/it]09/05/2023 16:30:40 - INFO - __main__ -   Step: 4591, LR: 1.9196704747378162e-05, Loss: 0.13605742156505585
[2023-09-05 16:30:54,951] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4592/66600 [2:26:58<249:29:35, 14.48s/it]09/05/2023 16:30:54 - INFO - __main__ -   Step: 4592, LR: 1.919639504318438e-05, Loss: 0.15625491738319397
[2023-09-05 16:31:09,885] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4593/66600 [2:27:13<251:48:41, 14.62s/it]09/05/2023 16:31:09 - INFO - __main__ -   Step: 4593, LR: 1.9196085338990598e-05, Loss: 0.18058252334594727
[2023-09-05 16:31:25,874] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4594/66600 [2:27:29<258:53:06, 15.03s/it]09/05/2023 16:31:25 - INFO - __main__ -   Step: 4594, LR: 1.9195775634796816e-05, Loss: 0.17803537845611572
[2023-09-05 16:31:39,913] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4595/66600 [2:27:43<253:45:18, 14.73s/it]09/05/2023 16:31:39 - INFO - __main__ -   Step: 4595, LR: 1.9195465930603034e-05, Loss: 0.1150047704577446
[2023-09-05 16:31:53,944] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4596/66600 [2:27:57<250:07:30, 14.52s/it]09/05/2023 16:31:53 - INFO - __main__ -   Step: 4596, LR: 1.9195156226409252e-05, Loss: 0.23296591639518738
[2023-09-05 16:32:09,595] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4597/66600 [2:28:13<255:57:14, 14.86s/it]09/05/2023 16:32:09 - INFO - __main__ -   Step: 4597, LR: 1.919484652221547e-05, Loss: 0.1721213012933731
[2023-09-05 16:32:23,146] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4598/66600 [2:28:26<249:10:45, 14.47s/it]09/05/2023 16:32:23 - INFO - __main__ -   Step: 4598, LR: 1.919453681802169e-05, Loss: 0.12852314114570618
[2023-09-05 16:32:37,896] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4599/66600 [2:28:41<250:37:48, 14.55s/it]09/05/2023 16:32:37 - INFO - __main__ -   Step: 4599, LR: 1.9194227113827906e-05, Loss: 0.13985967636108398
[2023-09-05 16:32:53,125] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4600/66600 [2:28:56<254:07:23, 14.76s/it]09/05/2023 16:32:53 - INFO - __main__ -   Step: 4600, LR: 1.9193917409634125e-05, Loss: 0.15514324605464935
09/05/2023 16:32:53 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.002308368682861328

Evaluating:   0%|          | 1/228 [00:00<01:34,  2.40it/s][Astep: 1
extend+tolist() time: 0.0010232925415039062

Evaluating:   1%|          | 2/228 [00:00<01:13,  3.09it/s][Astep: 2
extend+tolist() time: 0.002132415771484375

Evaluating:   1%|▏         | 3/228 [00:01<01:14,  3.02it/s][Astep: 3
extend+tolist() time: 0.0019237995147705078

Evaluating:   2%|▏         | 4/228 [00:01<01:12,  3.10it/s][Astep: 4
extend+tolist() time: 0.0014507770538330078

Evaluating:   2%|▏         | 5/228 [00:01<01:06,  3.37it/s][Astep: 5
extend+tolist() time: 0.002169370651245117

Evaluating:   3%|▎         | 6/228 [00:01<01:08,  3.24it/s][Astep: 6
extend+tolist() time: 0.0018661022186279297

Evaluating:   3%|▎         | 7/228 [00:02<01:10,  3.15it/s][Astep: 7
extend+tolist() time: 0.0009374618530273438

Evaluating:   4%|▎         | 8/228 [00:02<01:05,  3.36it/s][Astep: 8
extend+tolist() time: 0.0012178421020507812

Evaluating:   4%|▍         | 9/228 [00:02<01:11,  3.08it/s][Astep: 9
extend+tolist() time: 0.0008056163787841797

Evaluating:   4%|▍         | 10/228 [00:03<01:04,  3.38it/s][Astep: 10
extend+tolist() time: 0.0013196468353271484

Evaluating:   5%|▍         | 11/228 [00:03<01:00,  3.58it/s][Astep: 11
extend+tolist() time: 0.0005688667297363281

Evaluating:   5%|▌         | 12/228 [00:03<00:56,  3.82it/s][Astep: 12
extend+tolist() time: 0.0013420581817626953

Evaluating:   6%|▌         | 13/228 [00:03<00:53,  4.00it/s][Astep: 13
extend+tolist() time: 0.0005812644958496094

Evaluating:   6%|▌         | 14/228 [00:04<00:51,  4.12it/s][Astep: 14
extend+tolist() time: 0.0005550384521484375

Evaluating:   7%|▋         | 15/228 [00:04<00:50,  4.26it/s][Astep: 15
extend+tolist() time: 0.0010826587677001953

Evaluating:   7%|▋         | 16/228 [00:04<00:48,  4.34it/s][Astep: 16
extend+tolist() time: 0.0005953311920166016

Evaluating:   7%|▋         | 17/228 [00:04<00:47,  4.40it/s][Astep: 17
extend+tolist() time: 0.0015897750854492188

Evaluating:   8%|▊         | 18/228 [00:04<00:48,  4.33it/s][Astep: 18
extend+tolist() time: 0.0011768341064453125

Evaluating:   8%|▊         | 19/228 [00:05<00:51,  4.09it/s][Astep: 19
extend+tolist() time: 0.0014955997467041016

Evaluating:   9%|▉         | 20/228 [00:05<00:52,  3.96it/s][Astep: 20
extend+tolist() time: 0.0007784366607666016

Evaluating:   9%|▉         | 21/228 [00:05<00:58,  3.51it/s][Astep: 21
extend+tolist() time: 0.001102447509765625

Evaluating:  10%|▉         | 22/228 [00:06<00:54,  3.75it/s][Astep: 22
extend+tolist() time: 0.0007288455963134766

Evaluating:  10%|█         | 23/228 [00:06<00:52,  3.93it/s][Astep: 23
extend+tolist() time: 0.0011293888092041016

Evaluating:  11%|█         | 24/228 [00:06<00:49,  4.08it/s][Astep: 24
extend+tolist() time: 0.0010693073272705078

Evaluating:  11%|█         | 25/228 [00:06<00:51,  3.93it/s][Astep: 25
extend+tolist() time: 0.0019898414611816406

Evaluating:  11%|█▏        | 26/228 [00:07<00:56,  3.58it/s][Astep: 26
extend+tolist() time: 0.16889691352844238

Evaluating:  12%|█▏        | 27/228 [00:07<01:02,  3.20it/s][Astep: 27
extend+tolist() time: 0.0017290115356445312

Evaluating:  12%|█▏        | 28/228 [00:07<01:01,  3.23it/s][Astep: 28
extend+tolist() time: 0.0003414154052734375

Evaluating:  13%|█▎        | 29/228 [00:08<00:55,  3.56it/s][Astep: 29
extend+tolist() time: 0.00103759765625

Evaluating:  13%|█▎        | 30/228 [00:08<00:52,  3.78it/s][Astep: 30
extend+tolist() time: 0.0010561943054199219

Evaluating:  14%|█▎        | 31/228 [00:08<00:53,  3.71it/s][Astep: 31
extend+tolist() time: 0.0009746551513671875

Evaluating:  14%|█▍        | 32/228 [00:08<00:49,  3.92it/s][Astep: 32
extend+tolist() time: 0.0009846687316894531

Evaluating:  14%|█▍        | 33/228 [00:09<00:50,  3.84it/s][Astep: 33
extend+tolist() time: 0.001749277114868164

Evaluating:  15%|█▍        | 34/228 [00:09<00:52,  3.67it/s][Astep: 34
extend+tolist() time: 0.0012755393981933594

Evaluating:  15%|█▌        | 35/228 [00:09<00:51,  3.78it/s][Astep: 35
extend+tolist() time: 0.0006678104400634766

Evaluating:  16%|█▌        | 36/228 [00:09<00:48,  3.99it/s][Astep: 36
extend+tolist() time: 0.0011749267578125

Evaluating:  16%|█▌        | 37/228 [00:10<00:46,  4.09it/s][Astep: 37
extend+tolist() time: 0.0016219615936279297

Evaluating:  17%|█▋        | 38/228 [00:10<00:49,  3.82it/s][Astep: 38
extend+tolist() time: 0.0007600784301757812

Evaluating:  17%|█▋        | 39/228 [00:10<00:47,  3.97it/s][Astep: 39
extend+tolist() time: 0.0007491111755371094

Evaluating:  18%|█▊        | 40/228 [00:10<00:45,  4.10it/s][Astep: 40
extend+tolist() time: 0.0011432170867919922

Evaluating:  18%|█▊        | 41/228 [00:10<00:44,  4.22it/s][Astep: 41
extend+tolist() time: 0.000835418701171875

Evaluating:  18%|█▊        | 42/228 [00:11<00:53,  3.51it/s][Astep: 42
extend+tolist() time: 0.0016977787017822266

Evaluating:  19%|█▉        | 43/228 [00:11<00:53,  3.45it/s][Astep: 43
extend+tolist() time: 0.0018672943115234375

Evaluating:  19%|█▉        | 44/228 [00:12<00:55,  3.29it/s][Astep: 44
extend+tolist() time: 0.001096963882446289

Evaluating:  20%|█▉        | 45/228 [00:12<00:51,  3.56it/s][Astep: 45
extend+tolist() time: 0.0012547969818115234

Evaluating:  20%|██        | 46/228 [00:12<00:52,  3.47it/s][Astep: 46
extend+tolist() time: 0.0016491413116455078

Evaluating:  21%|██        | 47/228 [00:12<00:53,  3.41it/s][Astep: 47
extend+tolist() time: 0.0015323162078857422

Evaluating:  21%|██        | 48/228 [00:13<00:52,  3.45it/s][Astep: 48
extend+tolist() time: 0.001687765121459961

Evaluating:  21%|██▏       | 49/228 [00:13<00:52,  3.41it/s][Astep: 49
extend+tolist() time: 0.0009772777557373047

Evaluating:  22%|██▏       | 50/228 [00:13<00:50,  3.55it/s][Astep: 50
extend+tolist() time: 0.0017518997192382812

Evaluating:  22%|██▏       | 51/228 [00:14<00:50,  3.48it/s][Astep: 51
extend+tolist() time: 0.0015506744384765625

Evaluating:  23%|██▎       | 52/228 [00:14<01:01,  2.88it/s][Astep: 52
extend+tolist() time: 0.0013108253479003906

Evaluating:  23%|██▎       | 53/228 [00:14<00:56,  3.08it/s][Astep: 53
extend+tolist() time: 0.0012989044189453125

Evaluating:  24%|██▎       | 54/228 [00:15<00:55,  3.14it/s][Astep: 54
extend+tolist() time: 0.0012831687927246094

Evaluating:  24%|██▍       | 55/228 [00:15<00:50,  3.40it/s][Astep: 55
extend+tolist() time: 0.001125335693359375

Evaluating:  25%|██▍       | 56/228 [00:15<00:47,  3.62it/s][Astep: 56
extend+tolist() time: 0.0011792182922363281

Evaluating:  25%|██▌       | 57/228 [00:15<00:48,  3.53it/s][Astep: 57
extend+tolist() time: 0.1625509262084961

Evaluating:  25%|██▌       | 58/228 [00:16<00:53,  3.20it/s][Astep: 58
extend+tolist() time: 0.0008525848388671875

Evaluating:  26%|██▌       | 59/228 [00:16<00:49,  3.42it/s][Astep: 59
extend+tolist() time: 0.0013186931610107422

Evaluating:  26%|██▋       | 60/228 [00:16<00:46,  3.59it/s][Astep: 60
extend+tolist() time: 0.001071929931640625

Evaluating:  27%|██▋       | 61/228 [00:16<00:43,  3.82it/s][Astep: 61
extend+tolist() time: 0.0008423328399658203

Evaluating:  27%|██▋       | 62/228 [00:17<00:42,  3.91it/s][Astep: 62
extend+tolist() time: 0.0012345314025878906

Evaluating:  28%|██▊       | 63/228 [00:17<00:41,  4.02it/s][Astep: 63
extend+tolist() time: 0.0007848739624023438

Evaluating:  28%|██▊       | 64/228 [00:17<00:40,  4.09it/s][Astep: 64
extend+tolist() time: 0.0012044906616210938

Evaluating:  29%|██▊       | 65/228 [00:17<00:39,  4.14it/s][Astep: 65
extend+tolist() time: 0.0008037090301513672

Evaluating:  29%|██▉       | 66/228 [00:18<00:38,  4.16it/s][Astep: 66
extend+tolist() time: 0.0011777877807617188

Evaluating:  29%|██▉       | 67/228 [00:18<00:37,  4.25it/s][Astep: 67
extend+tolist() time: 0.0008859634399414062

Evaluating:  30%|██▉       | 68/228 [00:18<00:38,  4.18it/s][Astep: 68
extend+tolist() time: 0.0011141300201416016

Evaluating:  30%|███       | 69/228 [00:18<00:37,  4.25it/s][Astep: 69
extend+tolist() time: 0.0011227130889892578

Evaluating:  31%|███       | 70/228 [00:19<00:39,  4.00it/s][Astep: 70
extend+tolist() time: 0.0015637874603271484

Evaluating:  31%|███       | 71/228 [00:19<00:40,  3.88it/s][Astep: 71
extend+tolist() time: 0.0013451576232910156

Evaluating:  32%|███▏      | 72/228 [00:19<00:40,  3.82it/s][Astep: 72
extend+tolist() time: 0.0007724761962890625

Evaluating:  32%|███▏      | 73/228 [00:19<00:39,  3.96it/s][Astep: 73
extend+tolist() time: 0.0010077953338623047

Evaluating:  32%|███▏      | 74/228 [00:20<00:37,  4.09it/s][Astep: 74
extend+tolist() time: 0.0007450580596923828

Evaluating:  33%|███▎      | 75/228 [00:20<00:36,  4.19it/s][Astep: 75
extend+tolist() time: 0.0017817020416259766

Evaluating:  33%|███▎      | 76/228 [00:20<00:39,  3.86it/s][Astep: 76
extend+tolist() time: 0.0006406307220458984

Evaluating:  34%|███▍      | 77/228 [00:21<00:45,  3.34it/s][Astep: 77
extend+tolist() time: 0.0019693374633789062

Evaluating:  34%|███▍      | 78/228 [00:21<00:46,  3.22it/s][Astep: 78
extend+tolist() time: 0.0012278556823730469

Evaluating:  35%|███▍      | 79/228 [00:21<00:43,  3.42it/s][Astep: 79
extend+tolist() time: 0.0008783340454101562

Evaluating:  35%|███▌      | 80/228 [00:21<00:41,  3.59it/s][Astep: 80
extend+tolist() time: 0.0013625621795654297

Evaluating:  36%|███▌      | 81/228 [00:22<00:39,  3.71it/s][Astep: 81
extend+tolist() time: 0.001220703125

Evaluating:  36%|███▌      | 82/228 [00:22<00:38,  3.83it/s][Astep: 82
extend+tolist() time: 0.0008575916290283203

Evaluating:  36%|███▋      | 83/228 [00:22<00:36,  3.92it/s][Astep: 83
extend+tolist() time: 0.0010843276977539062

Evaluating:  37%|███▋      | 84/228 [00:22<00:35,  4.08it/s][Astep: 84
extend+tolist() time: 0.0010156631469726562

Evaluating:  37%|███▋      | 85/228 [00:23<00:36,  3.95it/s][Astep: 85
extend+tolist() time: 0.0013835430145263672

Evaluating:  38%|███▊      | 86/228 [00:23<00:35,  3.96it/s][Astep: 86
extend+tolist() time: 0.0009024143218994141

Evaluating:  38%|███▊      | 87/228 [00:23<00:35,  4.00it/s][Astep: 87
extend+tolist() time: 0.0013508796691894531

Evaluating:  39%|███▊      | 88/228 [00:23<00:34,  4.01it/s][Astep: 88
extend+tolist() time: 0.0007827281951904297

Evaluating:  39%|███▉      | 89/228 [00:24<00:33,  4.11it/s][Astep: 89
extend+tolist() time: 0.0011713504791259766

Evaluating:  39%|███▉      | 90/228 [00:24<00:32,  4.21it/s][Astep: 90
extend+tolist() time: 0.0009310245513916016

Evaluating:  40%|███▉      | 91/228 [00:24<00:32,  4.16it/s][Astep: 91
extend+tolist() time: 0.0011928081512451172

Evaluating:  40%|████      | 92/228 [00:24<00:32,  4.23it/s][Astep: 92
extend+tolist() time: 0.0007824897766113281

Evaluating:  41%|████      | 93/228 [00:25<00:38,  3.47it/s][Astep: 93
extend+tolist() time: 0.0014107227325439453

Evaluating:  41%|████      | 94/228 [00:25<00:37,  3.54it/s][Astep: 94
extend+tolist() time: 0.0010521411895751953

Evaluating:  42%|████▏     | 95/228 [00:25<00:35,  3.78it/s][Astep: 95
extend+tolist() time: 0.0011866092681884766

Evaluating:  42%|████▏     | 96/228 [00:25<00:36,  3.65it/s][Astep: 96
extend+tolist() time: 0.0014579296112060547

Evaluating:  43%|████▎     | 97/228 [00:26<00:35,  3.74it/s][Astep: 97
extend+tolist() time: 0.0008351802825927734

Evaluating:  43%|████▎     | 98/228 [00:26<00:33,  3.88it/s][Astep: 98
extend+tolist() time: 0.0012674331665039062

Evaluating:  43%|████▎     | 99/228 [00:26<00:32,  3.93it/s][Astep: 99
extend+tolist() time: 0.0012025833129882812

Evaluating:  44%|████▍     | 100/228 [00:26<00:32,  3.96it/s][Astep: 100
extend+tolist() time: 0.0007321834564208984

Evaluating:  44%|████▍     | 101/228 [00:27<00:30,  4.10it/s][Astep: 101
extend+tolist() time: 0.0012733936309814453

Evaluating:  45%|████▍     | 102/228 [00:27<00:30,  4.11it/s][Astep: 102
extend+tolist() time: 0.0007259845733642578

Evaluating:  45%|████▌     | 103/228 [00:27<00:36,  3.43it/s][Astep: 103
extend+tolist() time: 0.0010988712310791016

Evaluating:  46%|████▌     | 104/228 [00:28<00:33,  3.68it/s][Astep: 104
extend+tolist() time: 0.0006732940673828125

Evaluating:  46%|████▌     | 105/228 [00:28<00:31,  3.89it/s][Astep: 105
extend+tolist() time: 0.0012502670288085938

Evaluating:  46%|████▋     | 106/228 [00:28<00:30,  3.96it/s][Astep: 106
extend+tolist() time: 0.0017135143280029297

Evaluating:  47%|████▋     | 107/228 [00:28<00:32,  3.67it/s][Astep: 107
extend+tolist() time: 0.0007550716400146484

Evaluating:  47%|████▋     | 108/228 [00:29<00:31,  3.84it/s][Astep: 108
extend+tolist() time: 0.0012402534484863281

Evaluating:  48%|████▊     | 109/228 [00:29<00:30,  3.96it/s][Astep: 109
extend+tolist() time: 0.0008845329284667969

Evaluating:  48%|████▊     | 110/228 [00:29<00:29,  4.02it/s][Astep: 110
extend+tolist() time: 0.0010228157043457031

Evaluating:  49%|████▊     | 111/228 [00:29<00:28,  4.15it/s][Astep: 111
extend+tolist() time: 0.0017676353454589844

Evaluating:  49%|████▉     | 112/228 [00:30<00:30,  3.78it/s][Astep: 112
extend+tolist() time: 0.0004210472106933594

Evaluating:  50%|████▉     | 113/228 [00:30<00:28,  3.98it/s][Astep: 113
extend+tolist() time: 0.0007240772247314453

Evaluating:  50%|█████     | 114/228 [00:30<00:27,  4.12it/s][Astep: 114
extend+tolist() time: 0.001585245132446289

Evaluating:  50%|█████     | 115/228 [00:30<00:28,  3.95it/s][Astep: 115
extend+tolist() time: 0.0007357597351074219

Evaluating:  51%|█████     | 116/228 [00:31<00:27,  4.10it/s][Astep: 116
extend+tolist() time: 0.0013866424560546875

Evaluating:  51%|█████▏    | 117/228 [00:31<00:26,  4.15it/s][Astep: 117
extend+tolist() time: 0.0009915828704833984

Evaluating:  52%|█████▏    | 118/228 [00:31<00:26,  4.14it/s][Astep: 118
extend+tolist() time: 0.001104593276977539

Evaluating:  52%|█████▏    | 119/228 [00:31<00:25,  4.23it/s][Astep: 119
extend+tolist() time: 0.0008356571197509766

Evaluating:  53%|█████▎    | 120/228 [00:31<00:25,  4.29it/s][Astep: 120
extend+tolist() time: 0.0011875629425048828

Evaluating:  53%|█████▎    | 121/228 [00:32<00:24,  4.34it/s][Astep: 121
extend+tolist() time: 0.0007140636444091797

Evaluating:  54%|█████▎    | 122/228 [00:32<00:24,  4.36it/s][Astep: 122
extend+tolist() time: 0.0008101463317871094

Evaluating:  54%|█████▍    | 123/228 [00:32<00:23,  4.38it/s][Astep: 123
extend+tolist() time: 0.0011527538299560547

Evaluating:  54%|█████▍    | 124/228 [00:32<00:23,  4.44it/s][Astep: 124
extend+tolist() time: 0.0009937286376953125

Evaluating:  55%|█████▍    | 125/228 [00:33<00:23,  4.37it/s][Astep: 125
extend+tolist() time: 0.0009312629699707031

Evaluating:  55%|█████▌    | 126/228 [00:33<00:23,  4.42it/s][Astep: 126
extend+tolist() time: 0.0015070438385009766

Evaluating:  56%|█████▌    | 127/228 [00:33<00:25,  3.93it/s][Astep: 127
extend+tolist() time: 0.002015829086303711

Evaluating:  56%|█████▌    | 128/228 [00:34<00:33,  2.97it/s][Astep: 128
extend+tolist() time: 0.0012063980102539062

Evaluating:  57%|█████▋    | 129/228 [00:34<00:29,  3.31it/s][Astep: 129
extend+tolist() time: 0.0009207725524902344

Evaluating:  57%|█████▋    | 130/228 [00:34<00:27,  3.58it/s][Astep: 130
extend+tolist() time: 0.0014922618865966797

Evaluating:  57%|█████▋    | 131/228 [00:34<00:26,  3.70it/s][Astep: 131
extend+tolist() time: 0.0005283355712890625

Evaluating:  58%|█████▊    | 132/228 [00:35<00:24,  3.91it/s][Astep: 132
extend+tolist() time: 0.001672983169555664

Evaluating:  58%|█████▊    | 133/228 [00:35<00:24,  3.81it/s][Astep: 133
extend+tolist() time: 0.0005102157592773438

Evaluating:  59%|█████▉    | 134/228 [00:35<00:23,  4.00it/s][Astep: 134
extend+tolist() time: 0.0015306472778320312

Evaluating:  59%|█████▉    | 135/228 [00:35<00:23,  3.91it/s][Astep: 135
extend+tolist() time: 0.0005271434783935547

Evaluating:  60%|█████▉    | 136/228 [00:36<00:22,  4.09it/s][Astep: 136
extend+tolist() time: 0.0009653568267822266

Evaluating:  60%|██████    | 137/228 [00:36<00:21,  4.16it/s][Astep: 137
extend+tolist() time: 0.0008611679077148438

Evaluating:  61%|██████    | 138/228 [00:36<00:21,  4.27it/s][Astep: 138
extend+tolist() time: 0.0008397102355957031

Evaluating:  61%|██████    | 139/228 [00:36<00:20,  4.34it/s][Astep: 139
extend+tolist() time: 0.0005242824554443359

Evaluating:  61%|██████▏   | 140/228 [00:36<00:19,  4.42it/s][Astep: 140
extend+tolist() time: 0.0013689994812011719

Evaluating:  62%|██████▏   | 141/228 [00:37<00:19,  4.41it/s][Astep: 141
extend+tolist() time: 0.0008900165557861328

Evaluating:  62%|██████▏   | 142/228 [00:37<00:19,  4.39it/s][Astep: 142
extend+tolist() time: 0.0010366439819335938

Evaluating:  63%|██████▎   | 143/228 [00:37<00:19,  4.42it/s][Astep: 143
extend+tolist() time: 0.00039315223693847656

Evaluating:  63%|██████▎   | 144/228 [00:37<00:18,  4.47it/s][Astep: 144
extend+tolist() time: 0.0008394718170166016

Evaluating:  64%|██████▎   | 145/228 [00:38<00:18,  4.48it/s][Astep: 145
extend+tolist() time: 0.0009751319885253906

Evaluating:  64%|██████▍   | 146/228 [00:38<00:18,  4.49it/s][Astep: 146
extend+tolist() time: 0.0004210472106933594

Evaluating:  64%|██████▍   | 147/228 [00:38<00:17,  4.51it/s][Astep: 147
extend+tolist() time: 0.0008287429809570312

Evaluating:  65%|██████▍   | 148/228 [00:38<00:17,  4.49it/s][Astep: 148
extend+tolist() time: 0.0012159347534179688

Evaluating:  65%|██████▌   | 149/228 [00:38<00:17,  4.51it/s][Astep: 149
extend+tolist() time: 0.00043845176696777344

Evaluating:  66%|██████▌   | 150/228 [00:39<00:17,  4.50it/s][Astep: 150
extend+tolist() time: 0.0009505748748779297

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.46it/s][Astep: 151
extend+tolist() time: 0.0011157989501953125

Evaluating:  67%|██████▋   | 152/228 [00:39<00:16,  4.50it/s][Astep: 152
extend+tolist() time: 0.0008985996246337891

Evaluating:  67%|██████▋   | 153/228 [00:39<00:16,  4.45it/s][Astep: 153
extend+tolist() time: 0.0014781951904296875

Evaluating:  68%|██████▊   | 154/228 [00:40<00:17,  4.35it/s][Astep: 154
extend+tolist() time: 0.002027750015258789

Evaluating:  68%|██████▊   | 155/228 [00:40<00:18,  3.86it/s][Astep: 155
extend+tolist() time: 0.0006856918334960938

Evaluating:  68%|██████▊   | 156/228 [00:40<00:22,  3.27it/s][Astep: 156
extend+tolist() time: 0.0005829334259033203

Evaluating:  69%|██████▉   | 157/228 [00:41<00:19,  3.58it/s][Astep: 157
extend+tolist() time: 0.0011489391326904297

Evaluating:  69%|██████▉   | 158/228 [00:41<00:18,  3.82it/s][Astep: 158
extend+tolist() time: 0.0005340576171875

Evaluating:  70%|██████▉   | 159/228 [00:41<00:17,  4.02it/s][Astep: 159
extend+tolist() time: 0.0007381439208984375

Evaluating:  70%|███████   | 160/228 [00:41<00:16,  4.15it/s][Astep: 160
extend+tolist() time: 0.00040221214294433594

Evaluating:  71%|███████   | 161/228 [00:41<00:15,  4.27it/s][Astep: 161
extend+tolist() time: 0.0012907981872558594

Evaluating:  71%|███████   | 162/228 [00:42<00:15,  4.32it/s][Astep: 162
extend+tolist() time: 0.0005252361297607422

Evaluating:  71%|███████▏  | 163/228 [00:42<00:14,  4.39it/s][Astep: 163
extend+tolist() time: 0.000423431396484375

Evaluating:  72%|███████▏  | 164/228 [00:42<00:14,  4.46it/s][Astep: 164
extend+tolist() time: 0.0009601116180419922

Evaluating:  72%|███████▏  | 165/228 [00:42<00:14,  4.49it/s][Astep: 165
extend+tolist() time: 0.00047779083251953125

Evaluating:  73%|███████▎  | 166/228 [00:43<00:13,  4.51it/s][Astep: 166
extend+tolist() time: 0.0003998279571533203

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.51it/s][Astep: 167
extend+tolist() time: 0.0006186962127685547

Evaluating:  74%|███████▎  | 168/228 [00:43<00:13,  4.51it/s][Astep: 168
extend+tolist() time: 0.0017604827880859375

Evaluating:  74%|███████▍  | 169/228 [00:43<00:13,  4.22it/s][Astep: 169
extend+tolist() time: 0.0004820823669433594

Evaluating:  75%|███████▍  | 170/228 [00:43<00:13,  4.31it/s][Astep: 170
extend+tolist() time: 0.0013356208801269531

Evaluating:  75%|███████▌  | 171/228 [00:44<00:13,  4.30it/s][Astep: 171
extend+tolist() time: 0.0003077983856201172

Evaluating:  75%|███████▌  | 172/228 [00:44<00:12,  4.38it/s][Astep: 172
extend+tolist() time: 0.0008180141448974609

Evaluating:  76%|███████▌  | 173/228 [00:44<00:12,  4.41it/s][Astep: 173
extend+tolist() time: 0.20625758171081543

Evaluating:  76%|███████▋  | 174/228 [00:45<00:16,  3.32it/s][Astep: 174
extend+tolist() time: 0.001994609832763672

Evaluating:  77%|███████▋  | 175/228 [00:45<00:15,  3.32it/s][Astep: 175
extend+tolist() time: 0.0008568763732910156

Evaluating:  77%|███████▋  | 176/228 [00:45<00:14,  3.60it/s][Astep: 176
extend+tolist() time: 0.0006434917449951172

Evaluating:  78%|███████▊  | 177/228 [00:45<00:13,  3.82it/s][Astep: 177
extend+tolist() time: 0.0009143352508544922

Evaluating:  78%|███████▊  | 178/228 [00:46<00:12,  4.01it/s][Astep: 178
extend+tolist() time: 0.0017046928405761719

Evaluating:  79%|███████▊  | 179/228 [00:46<00:12,  3.88it/s][Astep: 179
extend+tolist() time: 0.0003972053527832031

Evaluating:  79%|███████▉  | 180/228 [00:46<00:11,  4.07it/s][Astep: 180
extend+tolist() time: 0.00037360191345214844

Evaluating:  79%|███████▉  | 181/228 [00:46<00:11,  4.22it/s][Astep: 181
extend+tolist() time: 0.0010411739349365234

Evaluating:  80%|███████▉  | 182/228 [00:47<00:10,  4.30it/s][Astep: 182
extend+tolist() time: 0.0007853507995605469

Evaluating:  80%|████████  | 183/228 [00:47<00:10,  4.35it/s][Astep: 183
extend+tolist() time: 0.0006709098815917969

Evaluating:  81%|████████  | 184/228 [00:47<00:09,  4.42it/s][Astep: 184
extend+tolist() time: 0.0009057521820068359

Evaluating:  81%|████████  | 185/228 [00:47<00:09,  4.44it/s][Astep: 185
extend+tolist() time: 0.0011615753173828125

Evaluating:  82%|████████▏ | 186/228 [00:47<00:10,  4.19it/s][Astep: 186
extend+tolist() time: 0.0015420913696289062

Evaluating:  82%|████████▏ | 187/228 [00:48<00:09,  4.15it/s][Astep: 187
extend+tolist() time: 0.0004477500915527344

Evaluating:  82%|████████▏ | 188/228 [00:48<00:09,  4.27it/s][Astep: 188
extend+tolist() time: 0.0007195472717285156

Evaluating:  83%|████████▎ | 189/228 [00:48<00:08,  4.35it/s][Astep: 189
extend+tolist() time: 0.0008208751678466797

Evaluating:  83%|████████▎ | 190/228 [00:48<00:08,  4.42it/s][Astep: 190
extend+tolist() time: 0.0011837482452392578

Evaluating:  84%|████████▍ | 191/228 [00:49<00:08,  4.17it/s][Astep: 191
extend+tolist() time: 0.0011615753173828125

Evaluating:  84%|████████▍ | 192/228 [00:49<00:08,  4.24it/s][Astep: 192
extend+tolist() time: 0.0004589557647705078

Evaluating:  85%|████████▍ | 193/228 [00:49<00:08,  4.34it/s][Astep: 193
extend+tolist() time: 0.001096963882446289

Evaluating:  85%|████████▌ | 194/228 [00:49<00:08,  4.24it/s][Astep: 194
extend+tolist() time: 0.001094818115234375

Evaluating:  86%|████████▌ | 195/228 [00:50<00:07,  4.33it/s][Astep: 195
extend+tolist() time: 0.0005645751953125

Evaluating:  86%|████████▌ | 196/228 [00:50<00:07,  4.37it/s][Astep: 196
extend+tolist() time: 0.0006320476531982422

Evaluating:  86%|████████▋ | 197/228 [00:50<00:07,  4.41it/s][Astep: 197
extend+tolist() time: 0.0011444091796875

Evaluating:  87%|████████▋ | 198/228 [00:50<00:06,  4.43it/s][Astep: 198
extend+tolist() time: 0.0006401538848876953

Evaluating:  87%|████████▋ | 199/228 [00:50<00:06,  4.45it/s][Astep: 199
extend+tolist() time: 0.0019333362579345703

Evaluating:  88%|████████▊ | 200/228 [00:51<00:06,  4.02it/s][Astep: 200
extend+tolist() time: 0.0007195472717285156

Evaluating:  88%|████████▊ | 201/228 [00:51<00:06,  4.16it/s][Astep: 201
extend+tolist() time: 0.001031637191772461

Evaluating:  89%|████████▊ | 202/228 [00:51<00:06,  4.28it/s][Astep: 202
extend+tolist() time: 0.00043487548828125

Evaluating:  89%|████████▉ | 203/228 [00:51<00:05,  4.37it/s][Astep: 203
extend+tolist() time: 0.0007479190826416016

Evaluating:  89%|████████▉ | 204/228 [00:52<00:05,  4.41it/s][Astep: 204
extend+tolist() time: 0.00039839744567871094

Evaluating:  90%|████████▉ | 205/228 [00:52<00:05,  4.47it/s][Astep: 205
extend+tolist() time: 0.0003001689910888672

Evaluating:  90%|█████████ | 206/228 [00:52<00:04,  4.51it/s][Astep: 206
extend+tolist() time: 0.0011031627655029297

Evaluating:  91%|█████████ | 207/228 [00:52<00:04,  4.51it/s][Astep: 207
extend+tolist() time: 0.0006315708160400391

Evaluating:  91%|█████████ | 208/228 [00:52<00:04,  4.39it/s][Astep: 208
extend+tolist() time: 0.0007119178771972656

Evaluating:  92%|█████████▏| 209/228 [00:53<00:04,  4.42it/s][Astep: 209
extend+tolist() time: 0.0010557174682617188

Evaluating:  92%|█████████▏| 210/228 [00:53<00:04,  4.44it/s][Astep: 210
extend+tolist() time: 0.0006299018859863281

Evaluating:  93%|█████████▎| 211/228 [00:53<00:03,  4.47it/s][Astep: 211
extend+tolist() time: 0.0015811920166015625

Evaluating:  93%|█████████▎| 212/228 [00:53<00:03,  4.21it/s][Astep: 212
extend+tolist() time: 0.0009398460388183594

Evaluating:  93%|█████████▎| 213/228 [00:54<00:03,  4.23it/s][Astep: 213
extend+tolist() time: 0.0014767646789550781

Evaluating:  94%|█████████▍| 214/228 [00:54<00:04,  3.33it/s][Astep: 214
extend+tolist() time: 0.0008788108825683594

Evaluating:  94%|█████████▍| 215/228 [00:54<00:03,  3.52it/s][Astep: 215
extend+tolist() time: 0.0006854534149169922

Evaluating:  95%|█████████▍| 216/228 [00:55<00:03,  3.77it/s][Astep: 216
extend+tolist() time: 0.0010535717010498047

Evaluating:  95%|█████████▌| 217/228 [00:55<00:02,  3.97it/s][Astep: 217
extend+tolist() time: 0.0005702972412109375

Evaluating:  96%|█████████▌| 218/228 [00:55<00:02,  4.13it/s][Astep: 218
extend+tolist() time: 0.0014603137969970703

Evaluating:  96%|█████████▌| 219/228 [00:55<00:02,  4.12it/s][Astep: 219
extend+tolist() time: 0.0005059242248535156

Evaluating:  96%|█████████▋| 220/228 [00:55<00:01,  4.26it/s][Astep: 220
extend+tolist() time: 0.00042176246643066406

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.34it/s][Astep: 221
extend+tolist() time: 0.0007283687591552734

Evaluating:  97%|█████████▋| 222/228 [00:56<00:01,  4.39it/s][Astep: 222
extend+tolist() time: 0.0008578300476074219

Evaluating:  98%|█████████▊| 223/228 [00:56<00:01,  4.45it/s][Astep: 223
extend+tolist() time: 0.0003914833068847656

Evaluating:  98%|█████████▊| 224/228 [00:56<00:00,  4.50it/s][Astep: 224
extend+tolist() time: 0.0003795623779296875

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.54it/s][Astep: 225
extend+tolist() time: 0.00044274330139160156

Evaluating:  99%|█████████▉| 226/228 [00:57<00:00,  4.59it/s][Astep: 226
extend+tolist() time: 0.0010223388671875

Evaluating: 100%|█████████▉| 227/228 [00:57<00:00,  4.58it/s][Astep: 227
extend+tolist() time: 0.0004940032958984375

Evaluating: 100%|██████████| 228/228 [00:57<00:00,  3.96it/s][A09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:33:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:33:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 16:33:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 16:33:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 16:33:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [00:59<00:00,  3.82it/s]
09/05/2023 16:33:52 - INFO - __main__ -   Step: 4600, Validation Metrics: {'pred_1_num': 9726, 'pred_-1_num': 885, 'pred_0_num': 190, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7918711230441626, 'f1_micro': 0.7918711230441626, 'f1_macro': 0.446062760079944, 'f1_weighted': 0.7591609743084694, 'f1_-1': 0.36066911464708284, 'f1_0': 0.09650180940892641, 'f1_1': 0.8810173561838228, 'precision_micro': 0.7918711230441626, 'precision_macro': 0.513266297625409, 'precision_weighted': 0.7452944301305097, 'precision_-1': 0.4994350282485876, 'precision_0': 0.21052631578947367, 'precision_1': 0.8298375488381657, 'recall_micro': 0.7918711230441626, 'recall_macro': 0.4279235518387643, 'recall_weighted': 0.7918711230441626, 'recall_-1': 0.2822477650063857, 'recall_0': 0.06259780907668232, 'recall_1': 0.9389250814332247, 'roc_auc_micro': 0.914772959441899, 'roc_auc_macro': 0.7400964079213465, 'roc_auc_weighted': 0.7311894137480742, 'roc_auc_-1': 0.8159479560586669, 'roc_auc_0': 0.6851721978748653, 'roc_auc_1': 0.7191690698305071}
[2023-09-05 16:34:06,991] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4601/66600 [2:30:10<559:31:11, 32.49s/it]09/05/2023 16:34:06 - INFO - __main__ -   Step: 4601, LR: 1.9193607705440343e-05, Loss: 0.20196957886219025
[2023-09-05 16:34:21,028] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4602/66600 [2:30:24<464:10:44, 26.95s/it]09/05/2023 16:34:21 - INFO - __main__ -   Step: 4602, LR: 1.919329800124656e-05, Loss: 0.16989043354988098
[2023-09-05 16:34:35,657] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4603/66600 [2:30:39<400:30:01, 23.26s/it]09/05/2023 16:34:35 - INFO - __main__ -   Step: 4603, LR: 1.919298829705278e-05, Loss: 0.15186646580696106
[2023-09-05 16:34:48,994] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4604/66600 [2:30:52<349:14:39, 20.28s/it]09/05/2023 16:34:48 - INFO - __main__ -   Step: 4604, LR: 1.9192678592858997e-05, Loss: 0.1564694046974182
[2023-09-05 16:35:04,399] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4605/66600 [2:31:07<324:03:25, 18.82s/it]09/05/2023 16:35:04 - INFO - __main__ -   Step: 4605, LR: 1.9192368888665215e-05, Loss: 0.1768859326839447
[2023-09-05 16:35:19,959] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4606/66600 [2:31:23<307:13:12, 17.84s/it]09/05/2023 16:35:19 - INFO - __main__ -   Step: 4606, LR: 1.9192059184471433e-05, Loss: 0.12725254893302917
[2023-09-05 16:35:33,957] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4607/66600 [2:31:37<287:21:51, 16.69s/it]09/05/2023 16:35:33 - INFO - __main__ -   Step: 4607, LR: 1.919174948027765e-05, Loss: 0.163275346159935
[2023-09-05 16:35:49,172] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4608/66600 [2:31:52<279:45:19, 16.25s/it]09/05/2023 16:35:49 - INFO - __main__ -   Step: 4608, LR: 1.919143977608387e-05, Loss: 0.1371667981147766
[2023-09-05 16:36:02,942] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4609/66600 [2:32:06<266:57:26, 15.50s/it]09/05/2023 16:36:02 - INFO - __main__ -   Step: 4609, LR: 1.9191130071890087e-05, Loss: 0.19878125190734863
[2023-09-05 16:36:17,015] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4610/66600 [2:32:20<259:34:14, 15.07s/it]09/05/2023 16:36:17 - INFO - __main__ -   Step: 4610, LR: 1.9190820367696305e-05, Loss: 0.2036541849374771
[2023-09-05 16:36:31,211] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4611/66600 [2:32:34<255:01:32, 14.81s/it]09/05/2023 16:36:31 - INFO - __main__ -   Step: 4611, LR: 1.9190510663502523e-05, Loss: 0.17554207146167755
[2023-09-05 16:36:45,489] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4612/66600 [2:32:49<252:16:17, 14.65s/it]09/05/2023 16:36:45 - INFO - __main__ -   Step: 4612, LR: 1.919020095930874e-05, Loss: 0.1558522880077362
[2023-09-05 16:36:59,451] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4613/66600 [2:33:03<248:42:39, 14.44s/it]09/05/2023 16:36:59 - INFO - __main__ -   Step: 4613, LR: 1.918989125511496e-05, Loss: 0.15821920335292816
[2023-09-05 16:37:12,920] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4614/66600 [2:33:16<243:39:53, 14.15s/it]09/05/2023 16:37:12 - INFO - __main__ -   Step: 4614, LR: 1.9189581550921177e-05, Loss: 0.17264506220817566
[2023-09-05 16:37:27,531] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4615/66600 [2:33:31<246:02:14, 14.29s/it]09/05/2023 16:37:27 - INFO - __main__ -   Step: 4615, LR: 1.9189271846727395e-05, Loss: 0.17199571430683136
[2023-09-05 16:37:41,928] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4616/66600 [2:33:45<246:35:21, 14.32s/it]09/05/2023 16:37:41 - INFO - __main__ -   Step: 4616, LR: 1.9188962142533613e-05, Loss: 0.15522965788841248
[2023-09-05 16:37:56,737] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4617/66600 [2:34:00<249:06:04, 14.47s/it]09/05/2023 16:37:56 - INFO - __main__ -   Step: 4617, LR: 1.918865243833983e-05, Loss: 0.16716688871383667
[2023-09-05 16:38:11,492] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4618/66600 [2:34:15<250:34:47, 14.55s/it]09/05/2023 16:38:11 - INFO - __main__ -   Step: 4618, LR: 1.918834273414605e-05, Loss: 0.18372498452663422
[2023-09-05 16:38:26,019] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4619/66600 [2:34:29<250:26:12, 14.55s/it]09/05/2023 16:38:26 - INFO - __main__ -   Step: 4619, LR: 1.9188033029952268e-05, Loss: 0.20778845250606537
[2023-09-05 16:38:40,620] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4620/66600 [2:34:44<250:42:49, 14.56s/it]09/05/2023 16:38:40 - INFO - __main__ -   Step: 4620, LR: 1.9187723325758486e-05, Loss: 0.18439266085624695
[2023-09-05 16:38:54,253] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4621/66600 [2:34:57<245:54:44, 14.28s/it]09/05/2023 16:38:54 - INFO - __main__ -   Step: 4621, LR: 1.9187413621564707e-05, Loss: 0.14434152841567993
[2023-09-05 16:39:08,273] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4622/66600 [2:35:11<244:32:47, 14.20s/it]09/05/2023 16:39:08 - INFO - __main__ -   Step: 4622, LR: 1.9187103917370922e-05, Loss: 0.13321346044540405
[2023-09-05 16:39:20,852] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4623/66600 [2:35:24<236:08:55, 13.72s/it]09/05/2023 16:39:20 - INFO - __main__ -   Step: 4623, LR: 1.918679421317714e-05, Loss: 0.17841219902038574
[2023-09-05 16:39:33,894] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4624/66600 [2:35:37<232:39:21, 13.51s/it]09/05/2023 16:39:33 - INFO - __main__ -   Step: 4624, LR: 1.9186484508983358e-05, Loss: 0.16526111960411072
[2023-09-05 16:39:49,126] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4625/66600 [2:35:52<241:31:21, 14.03s/it]09/05/2023 16:39:49 - INFO - __main__ -   Step: 4625, LR: 1.9186174804789576e-05, Loss: 0.1455463469028473
[2023-09-05 16:40:03,068] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4626/66600 [2:36:06<241:04:05, 14.00s/it]09/05/2023 16:40:03 - INFO - __main__ -   Step: 4626, LR: 1.9185865100595794e-05, Loss: 0.17199814319610596
[2023-09-05 16:40:16,814] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4627/66600 [2:36:20<239:44:05, 13.93s/it]09/05/2023 16:40:16 - INFO - __main__ -   Step: 4627, LR: 1.9185555396402012e-05, Loss: 0.1639980673789978
[2023-09-05 16:40:30,825] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4628/66600 [2:36:34<240:10:03, 13.95s/it]09/05/2023 16:40:30 - INFO - __main__ -   Step: 4628, LR: 1.9185245692208234e-05, Loss: 0.19653165340423584
[2023-09-05 16:40:45,365] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4629/66600 [2:36:48<243:12:25, 14.13s/it]09/05/2023 16:40:45 - INFO - __main__ -   Step: 4629, LR: 1.918493598801445e-05, Loss: 0.16768506169319153
[2023-09-05 16:40:58,900] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4630/66600 [2:37:02<240:08:14, 13.95s/it]09/05/2023 16:40:58 - INFO - __main__ -   Step: 4630, LR: 1.9184626283820666e-05, Loss: 0.17459005117416382
[2023-09-05 16:41:12,024] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4631/66600 [2:37:15<235:52:06, 13.70s/it]09/05/2023 16:41:12 - INFO - __main__ -   Step: 4631, LR: 1.9184316579626884e-05, Loss: 0.15434826910495758
[2023-09-05 16:41:26,070] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4632/66600 [2:37:29<237:38:06, 13.81s/it]09/05/2023 16:41:26 - INFO - __main__ -   Step: 4632, LR: 1.9184006875433102e-05, Loss: 0.1672530472278595
[2023-09-05 16:41:40,521] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4633/66600 [2:37:44<240:58:12, 14.00s/it]09/05/2023 16:41:40 - INFO - __main__ -   Step: 4633, LR: 1.918369717123932e-05, Loss: 0.1967911422252655
[2023-09-05 16:41:54,100] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4634/66600 [2:37:57<238:47:34, 13.87s/it]09/05/2023 16:41:54 - INFO - __main__ -   Step: 4634, LR: 1.918338746704554e-05, Loss: 0.222185879945755
[2023-09-05 16:42:09,043] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4635/66600 [2:38:12<244:19:01, 14.19s/it]09/05/2023 16:42:09 - INFO - __main__ -   Step: 4635, LR: 1.918307776285176e-05, Loss: 0.15899309515953064
[2023-09-05 16:42:24,086] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4636/66600 [2:38:27<248:41:36, 14.45s/it]09/05/2023 16:42:24 - INFO - __main__ -   Step: 4636, LR: 1.9182768058657978e-05, Loss: 0.11465558409690857
[2023-09-05 16:42:38,278] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4637/66600 [2:38:41<247:22:01, 14.37s/it]09/05/2023 16:42:38 - INFO - __main__ -   Step: 4637, LR: 1.9182458354464193e-05, Loss: 0.15358613431453705
[2023-09-05 16:42:53,238] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4638/66600 [2:38:56<250:23:55, 14.55s/it]09/05/2023 16:42:53 - INFO - __main__ -   Step: 4638, LR: 1.918214865027041e-05, Loss: 0.12137481570243835
[2023-09-05 16:43:06,865] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4639/66600 [2:39:10<245:38:19, 14.27s/it]09/05/2023 16:43:06 - INFO - __main__ -   Step: 4639, LR: 1.918183894607663e-05, Loss: 0.1145755872130394
[2023-09-05 16:43:22,304] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4640/66600 [2:39:25<251:39:39, 14.62s/it]09/05/2023 16:43:22 - INFO - __main__ -   Step: 4640, LR: 1.9181529241882847e-05, Loss: 0.10556995123624802
[2023-09-05 16:43:36,363] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4641/66600 [2:39:39<248:45:01, 14.45s/it]09/05/2023 16:43:36 - INFO - __main__ -   Step: 4641, LR: 1.9181219537689065e-05, Loss: 0.13609077036380768
[2023-09-05 16:43:50,883] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4642/66600 [2:39:54<249:05:29, 14.47s/it]09/05/2023 16:43:50 - INFO - __main__ -   Step: 4642, LR: 1.9180909833495286e-05, Loss: 0.19885535538196564
[2023-09-05 16:44:04,282] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4643/66600 [2:40:07<243:32:15, 14.15s/it]09/05/2023 16:44:04 - INFO - __main__ -   Step: 4643, LR: 1.9180600129301504e-05, Loss: 0.21807578206062317
[2023-09-05 16:44:18,612] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4644/66600 [2:40:22<244:27:36, 14.20s/it]09/05/2023 16:44:18 - INFO - __main__ -   Step: 4644, LR: 1.9180290425107723e-05, Loss: 0.12017975002527237
[2023-09-05 16:44:32,314] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4645/66600 [2:40:35<241:51:39, 14.05s/it]09/05/2023 16:44:32 - INFO - __main__ -   Step: 4645, LR: 1.9179980720913937e-05, Loss: 0.14861106872558594
[2023-09-05 16:44:45,817] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4646/66600 [2:40:49<239:00:52, 13.89s/it]09/05/2023 16:44:45 - INFO - __main__ -   Step: 4646, LR: 1.9179671016720155e-05, Loss: 0.17616990208625793
[2023-09-05 16:45:00,420] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4647/66600 [2:41:03<242:42:03, 14.10s/it]09/05/2023 16:45:00 - INFO - __main__ -   Step: 4647, LR: 1.9179361312526373e-05, Loss: 0.19531434774398804
[2023-09-05 16:45:14,896] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4648/66600 [2:41:18<244:37:13, 14.21s/it]09/05/2023 16:45:14 - INFO - __main__ -   Step: 4648, LR: 1.917905160833259e-05, Loss: 0.13170091807842255
[2023-09-05 16:45:29,586] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4649/66600 [2:41:33<247:04:24, 14.36s/it]09/05/2023 16:45:29 - INFO - __main__ -   Step: 4649, LR: 1.9178741904138813e-05, Loss: 0.17723575234413147
[2023-09-05 16:45:44,170] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4650/66600 [2:41:47<248:14:12, 14.43s/it]09/05/2023 16:45:44 - INFO - __main__ -   Step: 4650, LR: 1.917843219994503e-05, Loss: 0.1777198612689972
[2023-09-05 16:45:57,881] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4651/66600 [2:42:01<244:32:47, 14.21s/it]09/05/2023 16:45:57 - INFO - __main__ -   Step: 4651, LR: 1.917812249575125e-05, Loss: 0.2369772046804428
[2023-09-05 16:46:11,115] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4652/66600 [2:42:14<239:29:53, 13.92s/it]09/05/2023 16:46:11 - INFO - __main__ -   Step: 4652, LR: 1.9177812791557467e-05, Loss: 0.1405056118965149
[2023-09-05 16:46:25,172] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4653/66600 [2:42:28<240:12:41, 13.96s/it]09/05/2023 16:46:25 - INFO - __main__ -   Step: 4653, LR: 1.9177503087363682e-05, Loss: 0.15074481070041656
[2023-09-05 16:46:39,407] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4654/66600 [2:42:42<241:37:33, 14.04s/it]09/05/2023 16:46:39 - INFO - __main__ -   Step: 4654, LR: 1.91771933831699e-05, Loss: 0.1670919954776764
[2023-09-05 16:46:53,178] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4655/66600 [2:42:56<240:13:30, 13.96s/it]09/05/2023 16:46:53 - INFO - __main__ -   Step: 4655, LR: 1.9176883678976118e-05, Loss: 0.1414211094379425
[2023-09-05 16:47:07,917] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4656/66600 [2:43:11<244:14:07, 14.19s/it]09/05/2023 16:47:07 - INFO - __main__ -   Step: 4656, LR: 1.917657397478234e-05, Loss: 0.1530407965183258
[2023-09-05 16:47:21,453] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4657/66600 [2:43:25<240:50:04, 14.00s/it]09/05/2023 16:47:21 - INFO - __main__ -   Step: 4657, LR: 1.9176264270588557e-05, Loss: 0.16567224264144897
[2023-09-05 16:47:34,745] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4658/66600 [2:43:38<237:11:27, 13.79s/it]09/05/2023 16:47:34 - INFO - __main__ -   Step: 4658, LR: 1.9175954566394775e-05, Loss: 0.18543654680252075
[2023-09-05 16:47:49,057] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4659/66600 [2:43:52<239:54:28, 13.94s/it]09/05/2023 16:47:49 - INFO - __main__ -   Step: 4659, LR: 1.9175644862200993e-05, Loss: 0.17838183045387268
[2023-09-05 16:48:03,803] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4660/66600 [2:44:07<244:02:51, 14.18s/it]09/05/2023 16:48:03 - INFO - __main__ -   Step: 4660, LR: 1.9175335158007208e-05, Loss: 0.18258917331695557
[2023-09-05 16:48:19,032] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4661/66600 [2:44:22<249:25:57, 14.50s/it]09/05/2023 16:48:19 - INFO - __main__ -   Step: 4661, LR: 1.9175025453813426e-05, Loss: 0.20401915907859802
  7%|▋         | 4662/66600 [2:44:30<213:17:47, 12.40s/it]09/05/2023 16:48:26 - INFO - __main__ -   Step: 4662, LR: 1.9174715749619644e-05, Loss: 0.06738488376140594
09/05/2023 16:48:26 - INFO - accelerate.accelerator - Saving current state to /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6
09/05/2023 16:48:26 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2023-09-05 16:48:26,539] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-09-05 16:48:26,546] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt
[2023-09-05 16:48:26,546] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2023-09-05 16:48:26,546] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2023-09-05 16:48:26,546] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2023-09-05 16:48:26,548] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt
[2023-09-05 16:48:26,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-09-05 16:48:26,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2023-09-05 16:48:26,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2023-09-05 16:48:26,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2023-09-05 16:48:26,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-09-05 16:48:26,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2023-09-05 16:48:26,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2023-09-05 16:48:26,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-09-05 16:48:26,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2023-09-05 16:49:20,648] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2023-09-05 16:49:20,648] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2023-09-05 16:49:22,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2023-09-05 16:49:22,974] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2023-09-05 16:49:24,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2023-09-05 16:49:24,829] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2023-09-05 16:49:25,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-09-05 16:49:25,685] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2023-09-05 16:49:25,689] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-09-05 16:49:25,689] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-09-05 16:49:25,689] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2023-09-05 16:49:25,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
09/05/2023 16:49:25 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/pytorch_model
09/05/2023 16:49:25 - INFO - accelerate.checkpointing - Scheduler state saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/scheduler.bin
09/05/2023 16:49:25 - INFO - accelerate.checkpointing - Random states saved in /data/users/zhangjunlei/tyx/reward-by-prm800k/models/wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16/epoch_6/random_states_0.pkl
09/05/2023 16:49:25 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.0020971298217773438

Evaluating:   0%|          | 1/228 [00:00<01:15,  2.99it/s][Astep: 1
extend+tolist() time: 0.0013871192932128906

Evaluating:   1%|          | 2/228 [00:00<01:03,  3.53it/s][Astep: 2
extend+tolist() time: 0.0019049644470214844

Evaluating:   1%|▏         | 3/228 [00:00<01:08,  3.27it/s][Astep: 3
extend+tolist() time: 0.0013637542724609375

Evaluating:   2%|▏         | 4/228 [00:01<01:08,  3.28it/s][Astep: 4
extend+tolist() time: 0.0016148090362548828

Evaluating:   2%|▏         | 5/228 [00:01<01:03,  3.51it/s][Astep: 5
extend+tolist() time: 0.0019664764404296875

Evaluating:   3%|▎         | 6/228 [00:01<01:06,  3.33it/s][Astep: 6
extend+tolist() time: 0.001895904541015625

Evaluating:   3%|▎         | 7/228 [00:02<01:08,  3.21it/s][Astep: 7
extend+tolist() time: 0.0009672641754150391

Evaluating:   4%|▎         | 8/228 [00:02<01:04,  3.43it/s][Astep: 8
extend+tolist() time: 0.13144230842590332

Evaluating:   4%|▍         | 9/228 [00:02<01:08,  3.21it/s][Astep: 9
extend+tolist() time: 0.0012011528015136719

Evaluating:   4%|▍         | 10/228 [00:02<01:04,  3.41it/s][Astep: 10
extend+tolist() time: 0.0008225440979003906

Evaluating:   5%|▍         | 11/228 [00:03<01:00,  3.61it/s][Astep: 11
extend+tolist() time: 0.0009355545043945312

Evaluating:   5%|▌         | 12/228 [00:03<00:56,  3.84it/s][Astep: 12
extend+tolist() time: 0.0006654262542724609

Evaluating:   6%|▌         | 13/228 [00:03<00:53,  4.02it/s][Astep: 13
extend+tolist() time: 0.0005784034729003906

Evaluating:   6%|▌         | 14/228 [00:03<00:51,  4.15it/s][Astep: 14
extend+tolist() time: 0.0009765625

Evaluating:   7%|▋         | 15/228 [00:04<01:00,  3.53it/s][Astep: 15
extend+tolist() time: 0.0006046295166015625

Evaluating:   7%|▋         | 16/228 [00:04<00:56,  3.78it/s][Astep: 16
extend+tolist() time: 0.0010950565338134766

Evaluating:   7%|▋         | 17/228 [00:04<00:53,  3.97it/s][Astep: 17
extend+tolist() time: 0.0008375644683837891

Evaluating:   8%|▊         | 18/228 [00:04<00:52,  4.02it/s][Astep: 18
extend+tolist() time: 0.0016095638275146484

Evaluating:   8%|▊         | 19/228 [00:05<00:53,  3.90it/s][Astep: 19
extend+tolist() time: 0.0009915828704833984

Evaluating:   9%|▉         | 20/228 [00:05<00:54,  3.84it/s][Astep: 20
extend+tolist() time: 0.0007753372192382812

Evaluating:   9%|▉         | 21/228 [00:05<00:51,  3.98it/s][Astep: 21
extend+tolist() time: 0.0006966590881347656

Evaluating:  10%|▉         | 22/228 [00:05<00:50,  4.12it/s][Astep: 22
extend+tolist() time: 0.00122833251953125

Evaluating:  10%|█         | 23/228 [00:06<00:48,  4.22it/s][Astep: 23
extend+tolist() time: 0.0006821155548095703

Evaluating:  11%|█         | 24/228 [00:06<00:47,  4.30it/s][Astep: 24
extend+tolist() time: 0.001550436019897461

Evaluating:  11%|█         | 25/228 [00:06<00:49,  4.06it/s][Astep: 25
extend+tolist() time: 0.0018763542175292969

Evaluating:  11%|█▏        | 26/228 [00:07<00:55,  3.66it/s][Astep: 26
extend+tolist() time: 0.0007028579711914062

Evaluating:  12%|█▏        | 27/228 [00:07<00:51,  3.88it/s][Astep: 27
extend+tolist() time: 0.0017385482788085938

Evaluating:  12%|█▏        | 28/228 [00:07<00:54,  3.69it/s][Astep: 28
extend+tolist() time: 0.00033736228942871094

Evaluating:  13%|█▎        | 29/228 [00:07<00:50,  3.90it/s][Astep: 29
extend+tolist() time: 0.0011146068572998047

Evaluating:  13%|█▎        | 30/228 [00:07<00:48,  4.05it/s][Astep: 30
extend+tolist() time: 0.0014569759368896484

Evaluating:  14%|█▎        | 31/228 [00:08<00:50,  3.88it/s][Astep: 31
extend+tolist() time: 0.0005857944488525391

Evaluating:  14%|█▍        | 32/228 [00:08<00:48,  4.07it/s][Astep: 32
extend+tolist() time: 0.001415252685546875

Evaluating:  14%|█▍        | 33/228 [00:08<00:49,  3.95it/s][Astep: 33
extend+tolist() time: 0.00122833251953125

Evaluating:  15%|█▍        | 34/228 [00:09<00:51,  3.73it/s][Astep: 34
extend+tolist() time: 0.001300811767578125

Evaluating:  15%|█▌        | 35/228 [00:09<00:50,  3.83it/s][Astep: 35
extend+tolist() time: 0.0006983280181884766

Evaluating:  16%|█▌        | 36/228 [00:09<00:47,  4.01it/s][Astep: 36
extend+tolist() time: 0.0012445449829101562

Evaluating:  16%|█▌        | 37/228 [00:09<00:46,  4.12it/s][Astep: 37
extend+tolist() time: 0.0016434192657470703

Evaluating:  17%|█▋        | 38/228 [00:10<00:49,  3.83it/s][Astep: 38
extend+tolist() time: 0.0007863044738769531

Evaluating:  17%|█▋        | 39/228 [00:10<00:47,  3.99it/s][Astep: 39
extend+tolist() time: 0.001132965087890625

Evaluating:  18%|█▊        | 40/228 [00:10<00:45,  4.12it/s][Astep: 40
extend+tolist() time: 0.000644683837890625

Evaluating:  18%|█▊        | 41/228 [00:10<00:44,  4.23it/s][Astep: 41
extend+tolist() time: 0.0012574195861816406

Evaluating:  18%|█▊        | 42/228 [00:10<00:44,  4.19it/s][Astep: 42
extend+tolist() time: 0.1770470142364502

Evaluating:  19%|█▉        | 43/228 [00:11<00:57,  3.22it/s][Astep: 43
extend+tolist() time: 0.002016782760620117

Evaluating:  19%|█▉        | 44/228 [00:11<00:58,  3.15it/s][Astep: 44
extend+tolist() time: 0.0007269382476806641

Evaluating:  20%|█▉        | 45/228 [00:12<00:53,  3.45it/s][Astep: 45
extend+tolist() time: 0.0016887187957763672

Evaluating:  20%|██        | 46/228 [00:12<00:53,  3.40it/s][Astep: 46
extend+tolist() time: 0.0015816688537597656

Evaluating:  21%|██        | 47/228 [00:12<01:02,  2.91it/s][Astep: 47
extend+tolist() time: 0.0014412403106689453

Evaluating:  21%|██        | 48/228 [00:13<00:58,  3.08it/s][Astep: 48
extend+tolist() time: 0.0012047290802001953

Evaluating:  21%|██▏       | 49/228 [00:13<00:56,  3.15it/s][Astep: 49
extend+tolist() time: 0.0014405250549316406

Evaluating:  22%|██▏       | 50/228 [00:13<00:53,  3.35it/s][Astep: 50
extend+tolist() time: 0.001653432846069336

Evaluating:  22%|██▏       | 51/228 [00:13<00:53,  3.34it/s][Astep: 51
extend+tolist() time: 0.001556396484375

Evaluating:  23%|██▎       | 52/228 [00:14<00:52,  3.33it/s][Astep: 52
extend+tolist() time: 0.0009975433349609375

Evaluating:  23%|██▎       | 53/228 [00:14<00:51,  3.41it/s][Astep: 53
extend+tolist() time: 0.0038673877716064453

Evaluating:  24%|██▎       | 54/228 [00:14<00:51,  3.37it/s][Astep: 54
extend+tolist() time: 0.0012044906616210938

Evaluating:  24%|██▍       | 55/228 [00:15<00:48,  3.57it/s][Astep: 55
extend+tolist() time: 0.0007805824279785156

Evaluating:  25%|██▍       | 56/228 [00:15<00:45,  3.76it/s][Astep: 56
extend+tolist() time: 0.0016300678253173828

Evaluating:  25%|██▌       | 57/228 [00:15<00:47,  3.62it/s][Astep: 57
extend+tolist() time: 0.0005877017974853516

Evaluating:  25%|██▌       | 58/228 [00:15<00:44,  3.85it/s][Astep: 58
extend+tolist() time: 0.0012729167938232422

Evaluating:  26%|██▌       | 59/228 [00:16<00:43,  3.90it/s][Astep: 59
extend+tolist() time: 0.0009195804595947266

Evaluating:  26%|██▋       | 60/228 [00:16<00:42,  3.94it/s][Astep: 60
extend+tolist() time: 0.0011563301086425781

Evaluating:  27%|██▋       | 61/228 [00:16<00:40,  4.08it/s][Astep: 61
extend+tolist() time: 0.001171112060546875

Evaluating:  27%|██▋       | 62/228 [00:16<00:40,  4.10it/s][Astep: 62
extend+tolist() time: 0.0012385845184326172

Evaluating:  28%|██▊       | 63/228 [00:16<00:39,  4.15it/s][Astep: 63
extend+tolist() time: 0.0008151531219482422

Evaluating:  28%|██▊       | 64/228 [00:17<00:39,  4.19it/s][Astep: 64
extend+tolist() time: 0.0012574195861816406

Evaluating:  29%|██▊       | 65/228 [00:17<00:38,  4.20it/s][Astep: 65
extend+tolist() time: 0.0008101463317871094

Evaluating:  29%|██▉       | 66/228 [00:17<00:38,  4.23it/s][Astep: 66
extend+tolist() time: 0.0011637210845947266

Evaluating:  29%|██▉       | 67/228 [00:17<00:37,  4.28it/s][Astep: 67
extend+tolist() time: 0.0012443065643310547

Evaluating:  30%|██▉       | 68/228 [00:18<00:38,  4.21it/s][Astep: 68
extend+tolist() time: 0.0007302761077880859

Evaluating:  30%|███       | 69/228 [00:18<00:37,  4.28it/s][Astep: 69
extend+tolist() time: 0.0015823841094970703

Evaluating:  31%|███       | 70/228 [00:18<00:39,  4.02it/s][Astep: 70
extend+tolist() time: 0.0010859966278076172

Evaluating:  31%|███       | 71/228 [00:18<00:40,  3.89it/s][Astep: 71
extend+tolist() time: 0.001422882080078125

Evaluating:  32%|███▏      | 72/228 [00:19<00:40,  3.82it/s][Astep: 72
extend+tolist() time: 0.0007648468017578125

Evaluating:  32%|███▏      | 73/228 [00:19<00:39,  3.96it/s][Astep: 73
extend+tolist() time: 0.00054168701171875

Evaluating:  32%|███▏      | 74/228 [00:19<00:37,  4.11it/s][Astep: 74
extend+tolist() time: 0.0007443428039550781

Evaluating:  33%|███▎      | 75/228 [00:19<00:36,  4.19it/s][Astep: 75
extend+tolist() time: 0.0017893314361572266

Evaluating:  33%|███▎      | 76/228 [00:20<00:39,  3.88it/s][Astep: 76
extend+tolist() time: 0.0010097026824951172

Evaluating:  34%|███▍      | 77/228 [00:20<00:37,  4.04it/s][Astep: 77
extend+tolist() time: 0.1672806739807129

Evaluating:  34%|███▍      | 78/228 [00:20<00:48,  3.09it/s][Astep: 78
extend+tolist() time: 0.0008232593536376953

Evaluating:  35%|███▍      | 79/228 [00:21<00:46,  3.22it/s][Astep: 79
extend+tolist() time: 0.0012471675872802734

Evaluating:  35%|███▌      | 80/228 [00:21<00:43,  3.43it/s][Astep: 80
extend+tolist() time: 0.0012593269348144531

Evaluating:  36%|███▌      | 81/228 [00:21<00:40,  3.59it/s][Astep: 81
extend+tolist() time: 0.0008380413055419922

Evaluating:  36%|███▌      | 82/228 [00:21<00:39,  3.74it/s][Astep: 82
extend+tolist() time: 0.001279592514038086

Evaluating:  36%|███▋      | 83/228 [00:22<00:37,  3.85it/s][Astep: 83
extend+tolist() time: 0.0006787776947021484

Evaluating:  37%|███▋      | 84/228 [00:22<00:44,  3.25it/s][Astep: 84
extend+tolist() time: 0.001443624496459961

Evaluating:  37%|███▋      | 85/228 [00:22<00:42,  3.37it/s][Astep: 85
extend+tolist() time: 0.0008916854858398438

Evaluating:  38%|███▊      | 86/228 [00:23<00:40,  3.53it/s][Astep: 86
extend+tolist() time: 0.0013210773468017578

Evaluating:  38%|███▊      | 87/228 [00:23<00:38,  3.68it/s][Astep: 87
extend+tolist() time: 0.0012464523315429688

Evaluating:  39%|███▊      | 88/228 [00:23<00:37,  3.78it/s][Astep: 88
extend+tolist() time: 0.0007770061492919922

Evaluating:  39%|███▉      | 89/228 [00:23<00:35,  3.94it/s][Astep: 89
extend+tolist() time: 0.0011370182037353516

Evaluating:  39%|███▉      | 90/228 [00:24<00:33,  4.08it/s][Astep: 90
extend+tolist() time: 0.0009207725524902344

Evaluating:  40%|███▉      | 91/228 [00:24<00:33,  4.06it/s][Astep: 91
extend+tolist() time: 0.0012021064758300781

Evaluating:  40%|████      | 92/228 [00:24<00:32,  4.16it/s][Astep: 92
extend+tolist() time: 0.0010924339294433594

Evaluating:  41%|████      | 93/228 [00:24<00:32,  4.20it/s][Astep: 93
extend+tolist() time: 0.0013666152954101562

Evaluating:  41%|████      | 94/228 [00:25<00:33,  4.04it/s][Astep: 94
extend+tolist() time: 0.0007336139678955078

Evaluating:  42%|████▏     | 95/228 [00:25<00:31,  4.17it/s][Astep: 95
extend+tolist() time: 0.001631021499633789

Evaluating:  42%|████▏     | 96/228 [00:25<00:33,  3.90it/s][Astep: 96
extend+tolist() time: 0.001275777816772461

Evaluating:  43%|████▎     | 97/228 [00:25<00:33,  3.92it/s][Astep: 97
extend+tolist() time: 0.0008606910705566406

Evaluating:  43%|████▎     | 98/228 [00:26<00:32,  4.02it/s][Astep: 98
extend+tolist() time: 0.001352071762084961

Evaluating:  43%|████▎     | 99/228 [00:26<00:32,  4.02it/s][Astep: 99
extend+tolist() time: 0.0008778572082519531

Evaluating:  44%|████▍     | 100/228 [00:26<00:31,  4.03it/s][Astep: 100
extend+tolist() time: 0.0011327266693115234

Evaluating:  44%|████▍     | 101/228 [00:26<00:30,  4.15it/s][Astep: 101
extend+tolist() time: 0.0008220672607421875

Evaluating:  45%|████▍     | 102/228 [00:27<00:30,  4.14it/s][Astep: 102
extend+tolist() time: 0.0012078285217285156

Evaluating:  45%|████▌     | 103/228 [00:27<00:29,  4.24it/s][Astep: 103
extend+tolist() time: 0.0007860660552978516

Evaluating:  46%|████▌     | 104/228 [00:27<00:28,  4.29it/s][Astep: 104
extend+tolist() time: 0.0011088848114013672

Evaluating:  46%|████▌     | 105/228 [00:27<00:28,  4.33it/s][Astep: 105
extend+tolist() time: 0.0008497238159179688

Evaluating:  46%|████▋     | 106/228 [00:27<00:28,  4.29it/s][Astep: 106
extend+tolist() time: 0.0018215179443359375

Evaluating:  47%|████▋     | 107/228 [00:28<00:31,  3.87it/s][Astep: 107
extend+tolist() time: 0.0007867813110351562

Evaluating:  47%|████▋     | 108/228 [00:28<00:30,  4.00it/s][Astep: 108
extend+tolist() time: 0.0008056163787841797

Evaluating:  48%|████▊     | 109/228 [00:28<00:29,  4.03it/s][Astep: 109
extend+tolist() time: 0.0012614727020263672

Evaluating:  48%|████▊     | 110/228 [00:28<00:29,  4.06it/s][Astep: 110
extend+tolist() time: 0.0006322860717773438

Evaluating:  49%|████▊     | 111/228 [00:29<00:28,  4.17it/s][Astep: 111
extend+tolist() time: 0.0018620491027832031

Evaluating:  49%|████▉     | 112/228 [00:29<00:30,  3.79it/s][Astep: 112
extend+tolist() time: 0.0003781318664550781

Evaluating:  50%|████▉     | 113/228 [00:29<00:28,  4.00it/s][Astep: 113
extend+tolist() time: 0.0006883144378662109

Evaluating:  50%|█████     | 114/228 [00:29<00:27,  4.12it/s][Astep: 114
extend+tolist() time: 0.0010879039764404297

Evaluating:  50%|█████     | 115/228 [00:30<00:28,  3.95it/s][Astep: 115
extend+tolist() time: 0.0011119842529296875

Evaluating:  51%|█████     | 116/228 [00:30<00:27,  4.09it/s][Astep: 116
extend+tolist() time: 0.0008115768432617188

Evaluating:  51%|█████▏    | 117/228 [00:30<00:26,  4.15it/s][Astep: 117
extend+tolist() time: 0.0012447834014892578

Evaluating:  52%|█████▏    | 118/228 [00:30<00:26,  4.13it/s][Astep: 118
extend+tolist() time: 0.0005924701690673828

Evaluating:  52%|█████▏    | 119/228 [00:31<00:25,  4.24it/s][Astep: 119
extend+tolist() time: 0.0007171630859375

Evaluating:  53%|█████▎    | 120/228 [00:31<00:25,  4.31it/s][Astep: 120
extend+tolist() time: 0.0011157989501953125

Evaluating:  53%|█████▎    | 121/228 [00:31<00:24,  4.36it/s][Astep: 121
extend+tolist() time: 0.0007522106170654297

Evaluating:  54%|█████▎    | 122/228 [00:31<00:24,  4.39it/s][Astep: 122
extend+tolist() time: 0.0012218952178955078

Evaluating:  54%|█████▍    | 123/228 [00:32<00:23,  4.42it/s][Astep: 123
extend+tolist() time: 0.0007233619689941406

Evaluating:  54%|█████▍    | 124/228 [00:32<00:23,  4.45it/s][Astep: 124
extend+tolist() time: 0.0013964176177978516

Evaluating:  55%|█████▍    | 125/228 [00:32<00:23,  4.39it/s][Astep: 125
extend+tolist() time: 0.0004837512969970703

Evaluating:  55%|█████▌    | 126/228 [00:32<00:22,  4.45it/s][Astep: 126
extend+tolist() time: 0.0019335746765136719

Evaluating:  56%|█████▌    | 127/228 [00:33<00:25,  3.95it/s][Astep: 127
extend+tolist() time: 0.0018961429595947266

Evaluating:  56%|█████▌    | 128/228 [00:33<00:27,  3.63it/s][Astep: 128
extend+tolist() time: 0.1861896514892578

Evaluating:  57%|█████▋    | 129/228 [00:33<00:31,  3.17it/s][Astep: 129
extend+tolist() time: 0.0013058185577392578

Evaluating:  57%|█████▋    | 130/228 [00:34<00:28,  3.40it/s][Astep: 130
extend+tolist() time: 0.0010340213775634766

Evaluating:  57%|█████▋    | 131/228 [00:34<00:27,  3.57it/s][Astep: 131
extend+tolist() time: 0.0008687973022460938

Evaluating:  58%|█████▊    | 132/228 [00:34<00:25,  3.82it/s][Astep: 132
extend+tolist() time: 0.0012717247009277344

Evaluating:  58%|█████▊    | 133/228 [00:34<00:25,  3.75it/s][Astep: 133
extend+tolist() time: 0.0008640289306640625

Evaluating:  59%|█████▉    | 134/228 [00:34<00:23,  3.96it/s][Astep: 134
extend+tolist() time: 0.0011768341064453125

Evaluating:  59%|█████▉    | 135/228 [00:35<00:23,  3.90it/s][Astep: 135
extend+tolist() time: 0.0009343624114990234

Evaluating:  60%|█████▉    | 136/228 [00:35<00:22,  4.09it/s][Astep: 136
extend+tolist() time: 0.0009615421295166016

Evaluating:  60%|██████    | 137/228 [00:35<00:27,  3.36it/s][Astep: 137
extend+tolist() time: 0.0004456043243408203

Evaluating:  61%|██████    | 138/228 [00:36<00:24,  3.66it/s][Astep: 138
extend+tolist() time: 0.0012974739074707031

Evaluating:  61%|██████    | 139/228 [00:36<00:22,  3.87it/s][Astep: 139
extend+tolist() time: 0.0005218982696533203

Evaluating:  61%|██████▏   | 140/228 [00:36<00:21,  4.07it/s][Astep: 140
extend+tolist() time: 0.0008878707885742188

Evaluating:  62%|██████▏   | 141/228 [00:36<00:20,  4.17it/s][Astep: 141
extend+tolist() time: 0.001331329345703125

Evaluating:  62%|██████▏   | 142/228 [00:37<00:20,  4.23it/s][Astep: 142
extend+tolist() time: 0.0006961822509765625

Evaluating:  63%|██████▎   | 143/228 [00:37<00:19,  4.33it/s][Astep: 143
extend+tolist() time: 0.0005049705505371094

Evaluating:  63%|██████▎   | 144/228 [00:37<00:19,  4.30it/s][Astep: 144
extend+tolist() time: 0.0012359619140625

Evaluating:  64%|██████▎   | 145/228 [00:37<00:19,  4.28it/s][Astep: 145
extend+tolist() time: 0.0005540847778320312

Evaluating:  64%|██████▍   | 146/228 [00:37<00:18,  4.37it/s][Astep: 146
extend+tolist() time: 0.00044989585876464844

Evaluating:  64%|██████▍   | 147/228 [00:38<00:18,  4.44it/s][Astep: 147
extend+tolist() time: 0.0012683868408203125

Evaluating:  65%|██████▍   | 148/228 [00:38<00:18,  4.44it/s][Astep: 148
extend+tolist() time: 0.0007433891296386719

Evaluating:  65%|██████▌   | 149/228 [00:38<00:17,  4.46it/s][Astep: 149
extend+tolist() time: 0.0003941059112548828

Evaluating:  66%|██████▌   | 150/228 [00:38<00:17,  4.47it/s][Astep: 150
extend+tolist() time: 0.0013873577117919922

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.41it/s][Astep: 151
extend+tolist() time: 0.0006816387176513672

Evaluating:  67%|██████▋   | 152/228 [00:39<00:17,  4.44it/s][Astep: 152
extend+tolist() time: 0.0013022422790527344

Evaluating:  67%|██████▋   | 153/228 [00:39<00:17,  4.39it/s][Astep: 153
extend+tolist() time: 0.0010533332824707031

Evaluating:  68%|██████▊   | 154/228 [00:39<00:17,  4.30it/s][Astep: 154
extend+tolist() time: 0.0021088123321533203

Evaluating:  68%|██████▊   | 155/228 [00:40<00:19,  3.83it/s][Astep: 155
extend+tolist() time: 0.0010149478912353516

Evaluating:  68%|██████▊   | 156/228 [00:40<00:17,  4.01it/s][Astep: 156
extend+tolist() time: 0.0005631446838378906

Evaluating:  69%|██████▉   | 157/228 [00:40<00:17,  4.16it/s][Astep: 157
extend+tolist() time: 0.0007197856903076172

Evaluating:  69%|██████▉   | 158/228 [00:40<00:16,  4.27it/s][Astep: 158
extend+tolist() time: 0.0005679130554199219

Evaluating:  70%|██████▉   | 159/228 [00:40<00:15,  4.34it/s][Astep: 159
extend+tolist() time: 0.0011734962463378906

Evaluating:  70%|███████   | 160/228 [00:41<00:15,  4.38it/s][Astep: 160
extend+tolist() time: 0.0004017353057861328

Evaluating:  71%|███████   | 161/228 [00:41<00:15,  4.44it/s][Astep: 161
extend+tolist() time: 0.0008409023284912109

Evaluating:  71%|███████   | 162/228 [00:41<00:14,  4.46it/s][Astep: 162
extend+tolist() time: 0.0005125999450683594

Evaluating:  71%|███████▏  | 163/228 [00:41<00:14,  4.49it/s][Astep: 163
extend+tolist() time: 0.00042319297790527344

Evaluating:  72%|███████▏  | 164/228 [00:42<00:14,  4.50it/s][Astep: 164
extend+tolist() time: 0.0005967617034912109

Evaluating:  72%|███████▏  | 165/228 [00:42<00:13,  4.52it/s][Astep: 165
extend+tolist() time: 0.0004515647888183594

Evaluating:  73%|███████▎  | 166/228 [00:42<00:13,  4.54it/s][Astep: 166
extend+tolist() time: 0.0008952617645263672

Evaluating:  73%|███████▎  | 167/228 [00:42<00:13,  4.56it/s][Astep: 167
extend+tolist() time: 0.0007236003875732422

Evaluating:  74%|███████▎  | 168/228 [00:42<00:13,  4.54it/s][Astep: 168
extend+tolist() time: 0.0016307830810546875

Evaluating:  74%|███████▍  | 169/228 [00:43<00:13,  4.24it/s][Astep: 169
extend+tolist() time: 0.0003707408905029297

Evaluating:  75%|███████▍  | 170/228 [00:43<00:13,  4.33it/s][Astep: 170
extend+tolist() time: 0.0009300708770751953

Evaluating:  75%|███████▌  | 171/228 [00:43<00:13,  4.34it/s][Astep: 171
extend+tolist() time: 0.0003185272216796875

Evaluating:  75%|███████▌  | 172/228 [00:43<00:12,  4.41it/s][Astep: 172
extend+tolist() time: 0.0012612342834472656

Evaluating:  76%|███████▌  | 173/228 [00:44<00:12,  4.42it/s][Astep: 173
extend+tolist() time: 0.001543283462524414

Evaluating:  76%|███████▋  | 174/228 [00:44<00:12,  4.18it/s][Astep: 174
extend+tolist() time: 0.0014934539794921875

Evaluating:  77%|███████▋  | 175/228 [00:44<00:13,  3.88it/s][Astep: 175
extend+tolist() time: 0.0012941360473632812

Evaluating:  77%|███████▋  | 176/228 [00:44<00:12,  4.01it/s][Astep: 176
extend+tolist() time: 0.0006957054138183594

Evaluating:  78%|███████▊  | 177/228 [00:45<00:12,  4.15it/s][Astep: 177
extend+tolist() time: 0.0005884170532226562

Evaluating:  78%|███████▊  | 178/228 [00:45<00:11,  4.25it/s][Astep: 178
extend+tolist() time: 0.001741647720336914

Evaluating:  79%|███████▊  | 179/228 [00:45<00:12,  4.04it/s][Astep: 179
extend+tolist() time: 0.00040793418884277344

Evaluating:  79%|███████▉  | 180/228 [00:45<00:11,  4.19it/s][Astep: 180
extend+tolist() time: 0.0007812976837158203

Evaluating:  79%|███████▉  | 181/228 [00:46<00:11,  4.27it/s][Astep: 181
extend+tolist() time: 0.0006685256958007812

Evaluating:  80%|███████▉  | 182/228 [00:46<00:10,  4.34it/s][Astep: 182
extend+tolist() time: 0.0007648468017578125

Evaluating:  80%|████████  | 183/228 [00:46<00:10,  4.38it/s][Astep: 183
extend+tolist() time: 0.0011255741119384766

Evaluating:  81%|████████  | 184/228 [00:46<00:09,  4.43it/s][Astep: 184
extend+tolist() time: 0.0005242824554443359

Evaluating:  81%|████████  | 185/228 [00:46<00:09,  4.43it/s][Astep: 185
extend+tolist() time: 0.0011534690856933594

Evaluating:  82%|████████▏ | 186/228 [00:47<00:10,  4.19it/s][Astep: 186
extend+tolist() time: 0.0015742778778076172

Evaluating:  82%|████████▏ | 187/228 [00:47<00:09,  4.16it/s][Astep: 187
extend+tolist() time: 0.0004730224609375

Evaluating:  82%|████████▏ | 188/228 [00:47<00:09,  4.26it/s][Astep: 188
extend+tolist() time: 0.0010831356048583984

Evaluating:  83%|████████▎ | 189/228 [00:47<00:09,  4.32it/s][Astep: 189
extend+tolist() time: 0.0003566741943359375

Evaluating:  83%|████████▎ | 190/228 [00:48<00:08,  4.40it/s][Astep: 190
extend+tolist() time: 0.001155853271484375

Evaluating:  84%|████████▍ | 191/228 [00:48<00:08,  4.16it/s][Astep: 191
extend+tolist() time: 0.0012204647064208984

Evaluating:  84%|████████▍ | 192/228 [00:48<00:08,  4.23it/s][Astep: 192
extend+tolist() time: 0.0004410743713378906

Evaluating:  85%|████████▍ | 193/228 [00:48<00:08,  4.31it/s][Astep: 193
extend+tolist() time: 0.0014636516571044922

Evaluating:  85%|████████▌ | 194/228 [00:49<00:08,  4.21it/s][Astep: 194
extend+tolist() time: 0.0006036758422851562

Evaluating:  86%|████████▌ | 195/228 [00:49<00:07,  4.31it/s][Astep: 195
extend+tolist() time: 0.0005600452423095703

Evaluating:  86%|████████▌ | 196/228 [00:49<00:07,  4.39it/s][Astep: 196
extend+tolist() time: 0.0010673999786376953

Evaluating:  86%|████████▋ | 197/228 [00:49<00:07,  4.43it/s][Astep: 197
extend+tolist() time: 0.0007066726684570312

Evaluating:  87%|████████▋ | 198/228 [00:49<00:06,  4.44it/s][Astep: 198
extend+tolist() time: 0.0006537437438964844

Evaluating:  87%|████████▋ | 199/228 [00:50<00:06,  4.47it/s][Astep: 199
extend+tolist() time: 0.0019114017486572266

Evaluating:  88%|████████▊ | 200/228 [00:50<00:06,  4.04it/s][Astep: 200
extend+tolist() time: 0.0007197856903076172

Evaluating:  88%|████████▊ | 201/228 [00:50<00:06,  4.18it/s][Astep: 201
extend+tolist() time: 0.0009865760803222656

Evaluating:  89%|████████▊ | 202/228 [00:50<00:06,  4.29it/s][Astep: 202
extend+tolist() time: 0.0004165172576904297

Evaluating:  89%|████████▉ | 203/228 [00:51<00:05,  4.39it/s][Astep: 203
extend+tolist() time: 0.0005321502685546875

Evaluating:  89%|████████▉ | 204/228 [00:51<00:05,  4.45it/s][Astep: 204
extend+tolist() time: 0.00041961669921875

Evaluating:  90%|████████▉ | 205/228 [00:51<00:05,  4.50it/s][Astep: 205
extend+tolist() time: 0.00074005126953125

Evaluating:  90%|█████████ | 206/228 [00:51<00:04,  4.54it/s][Astep: 206
extend+tolist() time: 0.0006661415100097656

Evaluating:  91%|█████████ | 207/228 [00:52<00:04,  4.53it/s][Astep: 207
extend+tolist() time: 0.0006785392761230469

Evaluating:  91%|█████████ | 208/228 [00:52<00:04,  4.55it/s][Astep: 208
extend+tolist() time: 0.0010986328125

Evaluating:  92%|█████████▏| 209/228 [00:52<00:04,  4.54it/s][Astep: 209
extend+tolist() time: 0.0006279945373535156

Evaluating:  92%|█████████▏| 210/228 [00:52<00:03,  4.55it/s][Astep: 210
extend+tolist() time: 0.0008573532104492188

Evaluating:  93%|█████████▎| 211/228 [00:52<00:03,  4.55it/s][Astep: 211
extend+tolist() time: 0.001562356948852539

Evaluating:  93%|█████████▎| 212/228 [00:53<00:03,  4.26it/s][Astep: 212
extend+tolist() time: 0.0009205341339111328

Evaluating:  93%|█████████▎| 213/228 [00:53<00:03,  4.28it/s][Astep: 213
extend+tolist() time: 0.0012233257293701172

Evaluating:  94%|█████████▍| 214/228 [00:53<00:03,  4.33it/s][Astep: 214
extend+tolist() time: 0.0008738040924072266

Evaluating:  94%|█████████▍| 215/228 [00:53<00:02,  4.37it/s][Astep: 215
extend+tolist() time: 0.0010461807250976562

Evaluating:  95%|█████████▍| 216/228 [00:54<00:02,  4.40it/s][Astep: 216
extend+tolist() time: 0.0005764961242675781

Evaluating:  95%|█████████▌| 217/228 [00:54<00:03,  3.46it/s][Astep: 217
extend+tolist() time: 0.0005843639373779297

Evaluating:  96%|█████████▌| 218/228 [00:54<00:02,  3.73it/s][Astep: 218
extend+tolist() time: 0.22362351417541504

Evaluating:  96%|█████████▌| 219/228 [00:55<00:02,  3.05it/s][Astep: 219
extend+tolist() time: 0.0004897117614746094

Evaluating:  96%|█████████▋| 220/228 [00:55<00:02,  3.39it/s][Astep: 220
extend+tolist() time: 0.0004165172576904297

Evaluating:  97%|█████████▋| 221/228 [00:55<00:01,  3.68it/s][Astep: 221
extend+tolist() time: 0.0006866455078125

Evaluating:  97%|█████████▋| 222/228 [00:55<00:01,  3.89it/s][Astep: 222
extend+tolist() time: 0.0004432201385498047

Evaluating:  98%|█████████▊| 223/228 [00:56<00:01,  4.07it/s][Astep: 223
extend+tolist() time: 0.00038170814514160156

Evaluating:  98%|█████████▊| 224/228 [00:56<00:00,  4.22it/s][Astep: 224
extend+tolist() time: 0.0007908344268798828

Evaluating:  99%|█████████▊| 225/228 [00:56<00:00,  4.34it/s][Astep: 225
extend+tolist() time: 0.0004322528839111328

Evaluating:  99%|█████████▉| 226/228 [00:56<00:00,  4.43it/s][Astep: 226
extend+tolist() time: 0.0005826950073242188

Evaluating: 100%|█████████▉| 227/228 [00:56<00:00,  4.46it/s][Astep: 227
extend+tolist() time: 0.0005011558532714844

Evaluating: 100%|██████████| 228/228 [00:57<00:00,  3.89it/s][A09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 16:50:23 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 16:50:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 16:50:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 16:50:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.89it/s]
09/05/2023 16:50:24 - INFO - __main__ -   Step: 4662, Validation Metrics: {'pred_1_num': 9866, 'pred_-1_num': 764, 'pred_0_num': 171, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7918711230441626, 'f1_micro': 0.7918711230441626, 'f1_macro': 0.43730435704648984, 'f1_weighted': 0.7542613838465445, 'f1_-1': 0.3167381974248927, 'f1_0': 0.11358024691358025, 'f1_1': 0.8815946268009967, 'precision_micro': 0.7918711230441626, 'precision_macro': 0.5256143905857034, 'precision_weighted': 0.7424020730588411, 'precision_-1': 0.4829842931937173, 'precision_0': 0.26900584795321636, 'precision_1': 0.8248530306101763, 'recall_micro': 0.7918711230441626, 'recall_macro': 0.41811302290678604, 'recall_weighted': 0.7918711230441626, 'recall_-1': 0.23563218390804597, 'recall_0': 0.07198748043818466, 'recall_1': 0.9467194043741275, 'roc_auc_micro': 0.9047082338600856, 'roc_auc_macro': 0.7102553233259988, 'roc_auc_weighted': 0.6868941530662364, 'roc_auc_-1': 0.7653717567613354, 'roc_auc_0': 0.6932709665238473, 'roc_auc_1': 0.6721232466928139}
[2023-09-05 16:50:31,450] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4663/66600 [2:46:42<832:55:28, 48.41s/it]09/05/2023 16:50:38 - INFO - __main__ -   Step: 4663, LR: 1.9174406045425866e-05, Loss: 0.11609680950641632
[2023-09-05 16:50:45,455] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4664/66600 [2:46:56<655:12:48, 38.08s/it]09/05/2023 16:50:52 - INFO - __main__ -   Step: 4664, LR: 1.9174096341232084e-05, Loss: 0.13508528470993042
[2023-09-05 16:50:59,771] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4665/66600 [2:47:10<532:46:19, 30.97s/it]09/05/2023 16:51:07 - INFO - __main__ -   Step: 4665, LR: 1.9173786637038302e-05, Loss: 0.10148483514785767
[2023-09-05 16:51:13,155] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4666/66600 [2:47:25<449:33:43, 26.13s/it]09/05/2023 16:51:22 - INFO - __main__ -   Step: 4666, LR: 1.917347693284452e-05, Loss: 0.0881631076335907
[2023-09-05 16:51:28,475] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4667/66600 [2:47:38<382:56:55, 22.26s/it]09/05/2023 16:51:35 - INFO - __main__ -   Step: 4667, LR: 1.9173167228650738e-05, Loss: 0.09401798993349075
[2023-09-05 16:51:41,922] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4668/66600 [2:47:53<343:53:26, 19.99s/it]09/05/2023 16:51:50 - INFO - __main__ -   Step: 4668, LR: 1.9172857524456953e-05, Loss: 0.15870171785354614
[2023-09-05 16:51:56,886] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4669/66600 [2:48:09<320:14:36, 18.62s/it]09/05/2023 16:52:05 - INFO - __main__ -   Step: 4669, LR: 1.917254782026317e-05, Loss: 0.15959835052490234
[2023-09-05 16:52:12,643] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4670/66600 [2:48:23<298:48:59, 17.37s/it]09/05/2023 16:52:19 - INFO - __main__ -   Step: 4670, LR: 1.9172238116069392e-05, Loss: 0.10446900129318237
[2023-09-05 16:52:26,738] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4671/66600 [2:48:38<285:32:27, 16.60s/it]09/05/2023 16:52:34 - INFO - __main__ -   Step: 4671, LR: 1.917192841187561e-05, Loss: 0.15078473091125488
[2023-09-05 16:52:40,976] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4672/66600 [2:48:51<269:40:17, 15.68s/it]09/05/2023 16:52:48 - INFO - __main__ -   Step: 4672, LR: 1.9171618707681828e-05, Loss: 0.10026465356349945
[2023-09-05 16:52:54,741] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4673/66600 [2:49:05<260:41:41, 15.15s/it]09/05/2023 16:53:02 - INFO - __main__ -   Step: 4673, LR: 1.9171309003488046e-05, Loss: 0.12691441178321838
[2023-09-05 16:53:08,324] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4674/66600 [2:49:19<252:47:39, 14.70s/it]09/05/2023 16:53:15 - INFO - __main__ -   Step: 4674, LR: 1.9170999299294264e-05, Loss: 0.0828717052936554
[2023-09-05 16:53:22,005] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4675/66600 [2:49:32<246:38:43, 14.34s/it]09/05/2023 16:53:29 - INFO - __main__ -   Step: 4675, LR: 1.9170689595100482e-05, Loss: 0.11956284940242767
[2023-09-05 16:53:35,489] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4676/66600 [2:49:47<247:31:02, 14.39s/it]09/05/2023 16:53:43 - INFO - __main__ -   Step: 4676, LR: 1.9170379890906697e-05, Loss: 0.14114803075790405
[2023-09-05 16:53:50,906] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4677/66600 [2:50:02<250:15:55, 14.55s/it]09/05/2023 16:53:58 - INFO - __main__ -   Step: 4677, LR: 1.917007018671292e-05, Loss: 0.09269192069768906
[2023-09-05 16:54:05,883] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4678/66600 [2:50:17<251:38:25, 14.63s/it]09/05/2023 16:54:13 - INFO - __main__ -   Step: 4678, LR: 1.9169760482519137e-05, Loss: 0.09341832250356674
[2023-09-05 16:54:20,540] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4679/66600 [2:50:31<251:19:48, 14.61s/it]09/05/2023 16:54:28 - INFO - __main__ -   Step: 4679, LR: 1.9169450778325355e-05, Loss: 0.09001311659812927
[2023-09-05 16:54:34,981] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4680/66600 [2:50:46<251:22:12, 14.61s/it]09/05/2023 16:54:42 - INFO - __main__ -   Step: 4680, LR: 1.9169141074131573e-05, Loss: 0.09395641088485718
[2023-09-05 16:54:49,797] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4681/66600 [2:51:01<254:17:01, 14.78s/it]09/05/2023 16:54:57 - INFO - __main__ -   Step: 4681, LR: 1.916883136993779e-05, Loss: 0.13946709036827087
[2023-09-05 16:55:04,279] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4682/66600 [2:51:15<250:34:15, 14.57s/it]09/05/2023 16:55:12 - INFO - __main__ -   Step: 4682, LR: 1.916852166574401e-05, Loss: 0.15357452630996704
[2023-09-05 16:55:17,671] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4683/66600 [2:51:28<243:40:54, 14.17s/it]09/05/2023 16:55:25 - INFO - __main__ -   Step: 4683, LR: 1.9168211961550223e-05, Loss: 0.12708663940429688
[2023-09-05 16:55:31,831] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4684/66600 [2:51:43<244:51:18, 14.24s/it]09/05/2023 16:55:39 - INFO - __main__ -   Step: 4684, LR: 1.9167902257356445e-05, Loss: 0.09947320073843002
[2023-09-05 16:55:46,811] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4685/66600 [2:51:58<252:01:42, 14.65s/it]09/05/2023 16:55:55 - INFO - __main__ -   Step: 4685, LR: 1.9167592553162663e-05, Loss: 0.08450940251350403
[2023-09-05 16:56:01,747] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4686/66600 [2:52:13<250:53:31, 14.59s/it]09/05/2023 16:56:09 - INFO - __main__ -   Step: 4686, LR: 1.916728284896888e-05, Loss: 0.1283746063709259
[2023-09-05 16:56:16,579] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4687/66600 [2:52:28<254:15:12, 14.78s/it]09/05/2023 16:56:24 - INFO - __main__ -   Step: 4687, LR: 1.91669731447751e-05, Loss: 0.09437575936317444
[2023-09-05 16:56:32,383] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4688/66600 [2:52:43<254:54:26, 14.82s/it]09/05/2023 16:56:39 - INFO - __main__ -   Step: 4688, LR: 1.9166663440581317e-05, Loss: 0.1391848474740982
[2023-09-05 16:56:46,907] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4689/66600 [2:52:57<250:45:43, 14.58s/it]09/05/2023 16:56:53 - INFO - __main__ -   Step: 4689, LR: 1.9166353736387535e-05, Loss: 0.14852793514728546
[2023-09-05 16:56:59,634] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4690/66600 [2:53:10<242:25:25, 14.10s/it]09/05/2023 16:57:06 - INFO - __main__ -   Step: 4690, LR: 1.9166044032193753e-05, Loss: 0.08113274723291397
[2023-09-05 16:57:13,205] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4691/66600 [2:53:24<241:34:36, 14.05s/it]09/05/2023 16:57:20 - INFO - __main__ -   Step: 4691, LR: 1.916573432799997e-05, Loss: 0.11606582999229431
[2023-09-05 16:57:27,516] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4692/66600 [2:53:38<240:08:58, 13.96s/it]09/05/2023 16:57:34 - INFO - __main__ -   Step: 4692, LR: 1.916542462380619e-05, Loss: 0.07979407906532288
[2023-09-05 16:57:41,695] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4693/66600 [2:53:53<246:31:05, 14.34s/it]09/05/2023 16:57:49 - INFO - __main__ -   Step: 4693, LR: 1.9165114919612407e-05, Loss: 0.09017662703990936
[2023-09-05 16:57:56,530] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4694/66600 [2:54:07<246:00:09, 14.31s/it]09/05/2023 16:58:04 - INFO - __main__ -   Step: 4694, LR: 1.9164805215418626e-05, Loss: 0.1040385290980339
[2023-09-05 16:58:10,255] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4695/66600 [2:54:20<241:22:55, 14.04s/it]09/05/2023 16:58:17 - INFO - __main__ -   Step: 4695, LR: 1.9164495511224844e-05, Loss: 0.11825864762067795
[2023-09-05 16:58:24,061] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4696/66600 [2:54:35<242:23:44, 14.10s/it]09/05/2023 16:58:31 - INFO - __main__ -   Step: 4696, LR: 1.916418580703106e-05, Loss: 0.09528808295726776
[2023-09-05 16:58:37,782] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4697/66600 [2:54:48<238:19:00, 13.86s/it]09/05/2023 16:58:44 - INFO - __main__ -   Step: 4697, LR: 1.916387610283728e-05, Loss: 0.09937524795532227
[2023-09-05 16:58:50,527] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4698/66600 [2:55:01<234:56:33, 13.66s/it]09/05/2023 16:58:58 - INFO - __main__ -   Step: 4698, LR: 1.9163566398643498e-05, Loss: 0.11275404691696167
[2023-09-05 16:59:04,271] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4699/66600 [2:55:16<238:57:42, 13.90s/it]09/05/2023 16:59:12 - INFO - __main__ -   Step: 4699, LR: 1.9163256694449716e-05, Loss: 0.12127110362052917
[2023-09-05 16:59:18,098] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4700/66600 [2:55:29<236:26:05, 13.75s/it]09/05/2023 16:59:26 - INFO - __main__ -   Step: 4700, LR: 1.9162946990255934e-05, Loss: 0.0957954078912735
09/05/2023 16:59:26 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.0022878646850585938

Evaluating:   0%|          | 1/228 [00:00<01:16,  2.97it/s][Astep: 1
extend+tolist() time: 0.0010008811950683594

Evaluating:   1%|          | 2/228 [00:00<01:04,  3.52it/s][Astep: 2
extend+tolist() time: 0.0022046566009521484

Evaluating:   1%|▏         | 3/228 [00:00<01:09,  3.24it/s][Astep: 3
extend+tolist() time: 0.0018963813781738281

Evaluating:   2%|▏         | 4/228 [00:01<01:08,  3.25it/s][Astep: 4
extend+tolist() time: 0.001432657241821289

Evaluating:   2%|▏         | 5/228 [00:01<01:04,  3.48it/s][Astep: 5
extend+tolist() time: 0.001886606216430664

Evaluating:   3%|▎         | 6/228 [00:01<01:07,  3.31it/s][Astep: 6
extend+tolist() time: 0.0018792152404785156

Evaluating:   3%|▎         | 7/228 [00:02<01:19,  2.78it/s][Astep: 7
extend+tolist() time: 0.0009446144104003906

Evaluating:   4%|▎         | 8/228 [00:02<01:11,  3.07it/s][Astep: 8
extend+tolist() time: 0.0011730194091796875

Evaluating:   4%|▍         | 9/228 [00:02<01:04,  3.40it/s][Astep: 9
extend+tolist() time: 0.0007984638214111328

Evaluating:   4%|▍         | 10/228 [00:02<00:59,  3.65it/s][Astep: 10
extend+tolist() time: 0.0013229846954345703

Evaluating:   5%|▍         | 11/228 [00:03<00:57,  3.77it/s][Astep: 11
extend+tolist() time: 0.0005693435668945312

Evaluating:   5%|▌         | 12/228 [00:03<00:54,  3.98it/s][Astep: 12
extend+tolist() time: 0.0006344318389892578

Evaluating:   6%|▌         | 13/228 [00:03<00:51,  4.14it/s][Astep: 13
extend+tolist() time: 0.0010426044464111328

Evaluating:   6%|▌         | 14/228 [00:03<00:50,  4.25it/s][Astep: 14
extend+tolist() time: 0.0005600452423095703

Evaluating:   7%|▋         | 15/228 [00:04<00:49,  4.34it/s][Astep: 15
extend+tolist() time: 0.0006005764007568359

Evaluating:   7%|▋         | 16/228 [00:04<00:48,  4.40it/s][Astep: 16
extend+tolist() time: 0.0010762214660644531

Evaluating:   7%|▋         | 17/228 [00:04<00:47,  4.43it/s][Astep: 17
extend+tolist() time: 0.0008602142333984375

Evaluating:   8%|▊         | 18/228 [00:04<00:48,  4.36it/s][Astep: 18
extend+tolist() time: 0.0016188621520996094

Evaluating:   8%|▊         | 19/228 [00:05<00:51,  4.09it/s][Astep: 19
extend+tolist() time: 0.16266727447509766

Evaluating:   9%|▉         | 20/228 [00:05<01:11,  2.92it/s][Astep: 20
extend+tolist() time: 0.0007750988006591797

Evaluating:   9%|▉         | 21/228 [00:05<01:03,  3.25it/s][Astep: 21
extend+tolist() time: 0.0011234283447265625

Evaluating:  10%|▉         | 22/228 [00:06<00:58,  3.53it/s][Astep: 22
extend+tolist() time: 0.0007262229919433594

Evaluating:  10%|█         | 23/228 [00:06<00:54,  3.77it/s][Astep: 23
extend+tolist() time: 0.001055002212524414

Evaluating:  11%|█         | 24/228 [00:06<00:51,  3.95it/s][Astep: 24
extend+tolist() time: 0.0014715194702148438

Evaluating:  11%|█         | 25/228 [00:06<00:52,  3.83it/s][Astep: 25
extend+tolist() time: 0.001844644546508789

Evaluating:  11%|█▏        | 26/228 [00:07<00:57,  3.52it/s][Astep: 26
extend+tolist() time: 0.0007193088531494141

Evaluating:  12%|█▏        | 27/228 [00:07<00:53,  3.76it/s][Astep: 27
extend+tolist() time: 0.0016944408416748047

Evaluating:  12%|█▏        | 28/228 [00:07<00:55,  3.61it/s][Astep: 28
extend+tolist() time: 0.00032329559326171875

Evaluating:  13%|█▎        | 29/228 [00:07<00:51,  3.88it/s][Astep: 29
extend+tolist() time: 0.0011072158813476562

Evaluating:  13%|█▎        | 30/228 [00:08<00:49,  4.03it/s][Astep: 30
extend+tolist() time: 0.0011036396026611328

Evaluating:  14%|█▎        | 31/228 [00:08<00:50,  3.87it/s][Astep: 31
extend+tolist() time: 0.0010182857513427734

Evaluating:  14%|█▍        | 32/228 [00:08<00:48,  4.06it/s][Astep: 32
extend+tolist() time: 0.0009851455688476562

Evaluating:  14%|█▍        | 33/228 [00:08<00:49,  3.94it/s][Astep: 33
extend+tolist() time: 0.0016939640045166016

Evaluating:  15%|█▍        | 34/228 [00:09<00:52,  3.73it/s][Astep: 34
extend+tolist() time: 0.0012161731719970703

Evaluating:  15%|█▌        | 35/228 [00:09<00:50,  3.82it/s][Astep: 35
extend+tolist() time: 0.0006346702575683594

Evaluating:  16%|█▌        | 36/228 [00:09<00:47,  4.02it/s][Astep: 36
extend+tolist() time: 0.0011265277862548828

Evaluating:  16%|█▌        | 37/228 [00:09<00:46,  4.12it/s][Astep: 37
extend+tolist() time: 0.0012307167053222656

Evaluating:  17%|█▋        | 38/228 [00:10<00:58,  3.23it/s][Astep: 38
extend+tolist() time: 0.0012354850769042969

Evaluating:  17%|█▋        | 39/228 [00:10<00:53,  3.51it/s][Astep: 39
extend+tolist() time: 0.0006849765777587891

Evaluating:  18%|█▊        | 40/228 [00:10<00:50,  3.75it/s][Astep: 40
extend+tolist() time: 0.0013401508331298828

Evaluating:  18%|█▊        | 41/228 [00:11<00:47,  3.94it/s][Astep: 41
extend+tolist() time: 0.0008139610290527344

Evaluating:  18%|█▊        | 42/228 [00:11<00:46,  4.00it/s][Astep: 42
extend+tolist() time: 0.001688241958618164

Evaluating:  19%|█▉        | 43/228 [00:11<00:49,  3.77it/s][Astep: 43
extend+tolist() time: 0.0018107891082763672

Evaluating:  19%|█▉        | 44/228 [00:11<00:52,  3.49it/s][Astep: 44
extend+tolist() time: 0.0007450580596923828

Evaluating:  20%|█▉        | 45/228 [00:12<00:49,  3.72it/s][Astep: 45
extend+tolist() time: 0.0016965866088867188

Evaluating:  20%|██        | 46/228 [00:12<00:50,  3.58it/s][Astep: 46
extend+tolist() time: 0.0015933513641357422

Evaluating:  21%|██        | 47/228 [00:12<00:52,  3.48it/s][Astep: 47
extend+tolist() time: 0.0014264583587646484

Evaluating:  21%|██        | 48/228 [00:13<00:51,  3.51it/s][Astep: 48
extend+tolist() time: 0.0015475749969482422

Evaluating:  21%|██▏       | 49/228 [00:13<00:51,  3.45it/s][Astep: 49
extend+tolist() time: 0.0009582042694091797

Evaluating:  22%|██▏       | 50/228 [00:13<00:49,  3.59it/s][Astep: 50
extend+tolist() time: 0.16390442848205566

Evaluating:  22%|██▏       | 51/228 [00:14<01:07,  2.62it/s][Astep: 51
extend+tolist() time: 0.0016636848449707031

Evaluating:  23%|██▎       | 52/228 [00:14<01:02,  2.80it/s][Astep: 52
extend+tolist() time: 0.001277923583984375

Evaluating:  23%|██▎       | 53/228 [00:14<00:58,  3.01it/s][Astep: 53
extend+tolist() time: 0.0016367435455322266

Evaluating:  24%|██▎       | 54/228 [00:15<00:56,  3.09it/s][Astep: 54
extend+tolist() time: 0.0008230209350585938

Evaluating:  24%|██▍       | 55/228 [00:15<00:51,  3.35it/s][Astep: 55
extend+tolist() time: 0.0011813640594482422

Evaluating:  25%|██▍       | 56/228 [00:15<00:47,  3.58it/s][Astep: 56
extend+tolist() time: 0.0015385150909423828

Evaluating:  25%|██▌       | 57/228 [00:15<00:48,  3.51it/s][Astep: 57
extend+tolist() time: 0.0005626678466796875

Evaluating:  25%|██▌       | 58/228 [00:16<00:45,  3.77it/s][Astep: 58
extend+tolist() time: 0.0008265972137451172

Evaluating:  26%|██▌       | 59/228 [00:16<00:43,  3.85it/s][Astep: 59
extend+tolist() time: 0.0013737678527832031

Evaluating:  26%|██▋       | 60/228 [00:16<00:43,  3.90it/s][Astep: 60
extend+tolist() time: 0.0010464191436767578

Evaluating:  27%|██▋       | 61/228 [00:16<00:41,  4.06it/s][Astep: 61
extend+tolist() time: 0.0008449554443359375

Evaluating:  27%|██▋       | 62/228 [00:17<00:40,  4.08it/s][Astep: 62
extend+tolist() time: 0.0012009143829345703

Evaluating:  28%|██▊       | 63/228 [00:17<00:39,  4.15it/s][Astep: 63
extend+tolist() time: 0.0008029937744140625

Evaluating:  28%|██▊       | 64/228 [00:17<00:39,  4.19it/s][Astep: 64
extend+tolist() time: 0.0012507438659667969

Evaluating:  29%|██▊       | 65/228 [00:17<00:38,  4.21it/s][Astep: 65
extend+tolist() time: 0.0007994174957275391

Evaluating:  29%|██▉       | 66/228 [00:17<00:38,  4.21it/s][Astep: 66
extend+tolist() time: 0.0011737346649169922

Evaluating:  29%|██▉       | 67/228 [00:18<00:37,  4.28it/s][Astep: 67
extend+tolist() time: 0.0008983612060546875

Evaluating:  30%|██▉       | 68/228 [00:18<00:38,  4.20it/s][Astep: 68
extend+tolist() time: 0.0011663436889648438

Evaluating:  30%|███       | 69/228 [00:18<00:37,  4.28it/s][Astep: 69
extend+tolist() time: 0.0011129379272460938

Evaluating:  31%|███       | 70/228 [00:18<00:39,  4.04it/s][Astep: 70
extend+tolist() time: 0.0015418529510498047

Evaluating:  31%|███       | 71/228 [00:19<00:47,  3.29it/s][Astep: 71
extend+tolist() time: 0.001354217529296875

Evaluating:  32%|███▏      | 72/228 [00:19<00:45,  3.39it/s][Astep: 72
extend+tolist() time: 0.0007374286651611328

Evaluating:  32%|███▏      | 73/228 [00:19<00:42,  3.64it/s][Astep: 73
extend+tolist() time: 0.0005564689636230469

Evaluating:  32%|███▏      | 74/228 [00:20<00:40,  3.85it/s][Astep: 74
extend+tolist() time: 0.0011715888977050781

Evaluating:  33%|███▎      | 75/228 [00:20<00:38,  4.01it/s][Astep: 75
extend+tolist() time: 0.0016558170318603516

Evaluating:  33%|███▎      | 76/228 [00:20<00:40,  3.74it/s][Astep: 76
extend+tolist() time: 0.0006284713745117188

Evaluating:  34%|███▍      | 77/228 [00:20<00:38,  3.92it/s][Astep: 77
extend+tolist() time: 0.0019741058349609375

Evaluating:  34%|███▍      | 78/228 [00:21<00:41,  3.58it/s][Astep: 78
extend+tolist() time: 0.0012178421020507812

Evaluating:  35%|███▍      | 79/228 [00:21<00:40,  3.69it/s][Astep: 79
extend+tolist() time: 0.0008842945098876953

Evaluating:  35%|███▌      | 80/228 [00:21<00:39,  3.78it/s][Astep: 80
extend+tolist() time: 0.0013840198516845703

Evaluating:  36%|███▌      | 81/228 [00:21<00:38,  3.84it/s][Astep: 81
extend+tolist() time: 0.0008378028869628906

Evaluating:  36%|███▌      | 82/228 [00:22<00:37,  3.93it/s][Astep: 82
extend+tolist() time: 0.0013098716735839844

Evaluating:  36%|███▋      | 83/228 [00:22<00:36,  4.00it/s][Astep: 83
extend+tolist() time: 0.0006797313690185547

Evaluating:  37%|███▋      | 84/228 [00:22<00:34,  4.13it/s][Astep: 84
extend+tolist() time: 0.0014164447784423828

Evaluating:  37%|███▋      | 85/228 [00:22<00:35,  3.99it/s][Astep: 85
extend+tolist() time: 0.001262664794921875

Evaluating:  38%|███▊      | 86/228 [00:23<00:35,  3.98it/s][Astep: 86
extend+tolist() time: 0.0008957386016845703

Evaluating:  38%|███▊      | 87/228 [00:23<00:35,  4.02it/s][Astep: 87
extend+tolist() time: 0.0013158321380615234

Evaluating:  39%|███▊      | 88/228 [00:23<00:34,  4.02it/s][Astep: 88
extend+tolist() time: 0.0008029937744140625

Evaluating:  39%|███▉      | 89/228 [00:23<00:33,  4.14it/s][Astep: 89
extend+tolist() time: 0.00110626220703125

Evaluating:  39%|███▉      | 90/228 [00:24<00:32,  4.22it/s][Astep: 90
extend+tolist() time: 0.0009322166442871094

Evaluating:  40%|███▉      | 91/228 [00:24<00:40,  3.42it/s][Astep: 91
extend+tolist() time: 0.20736408233642578

Evaluating:  40%|████      | 92/228 [00:24<00:45,  2.99it/s][Astep: 92
extend+tolist() time: 0.0007679462432861328

Evaluating:  41%|████      | 93/228 [00:25<00:40,  3.30it/s][Astep: 93
extend+tolist() time: 0.0009520053863525391

Evaluating:  41%|████      | 94/228 [00:25<00:39,  3.42it/s][Astep: 94
extend+tolist() time: 0.0011305809020996094

Evaluating:  42%|████▏     | 95/228 [00:25<00:36,  3.68it/s][Astep: 95
extend+tolist() time: 0.0011379718780517578

Evaluating:  42%|████▏     | 96/228 [00:26<00:36,  3.59it/s][Astep: 96
extend+tolist() time: 0.0009672641754150391

Evaluating:  43%|████▎     | 97/228 [00:26<00:35,  3.70it/s][Astep: 97
extend+tolist() time: 0.0012753009796142578

Evaluating:  43%|████▎     | 98/228 [00:26<00:33,  3.86it/s][Astep: 98
extend+tolist() time: 0.0008695125579833984

Evaluating:  43%|████▎     | 99/228 [00:26<00:32,  3.91it/s][Astep: 99
extend+tolist() time: 0.0013053417205810547

Evaluating:  44%|████▍     | 100/228 [00:26<00:32,  3.95it/s][Astep: 100
extend+tolist() time: 0.0007145404815673828

Evaluating:  44%|████▍     | 101/228 [00:27<00:30,  4.10it/s][Astep: 101
extend+tolist() time: 0.0012831687927246094

Evaluating:  45%|████▍     | 102/228 [00:27<00:30,  4.12it/s][Astep: 102
extend+tolist() time: 0.0007495880126953125

Evaluating:  45%|████▌     | 103/228 [00:27<00:29,  4.21it/s][Astep: 103
extend+tolist() time: 0.0011608600616455078

Evaluating:  46%|████▌     | 104/228 [00:27<00:29,  4.28it/s][Astep: 104
extend+tolist() time: 0.0006849765777587891

Evaluating:  46%|████▌     | 105/228 [00:28<00:28,  4.31it/s][Astep: 105
extend+tolist() time: 0.0012936592102050781

Evaluating:  46%|████▋     | 106/228 [00:28<00:28,  4.26it/s][Astep: 106
extend+tolist() time: 0.0017066001892089844

Evaluating:  47%|████▋     | 107/228 [00:28<00:31,  3.85it/s][Astep: 107
extend+tolist() time: 0.0007722377777099609

Evaluating:  47%|████▋     | 108/228 [00:28<00:30,  3.98it/s][Astep: 108
extend+tolist() time: 0.0012369155883789062

Evaluating:  48%|████▊     | 109/228 [00:29<00:29,  4.07it/s][Astep: 109
extend+tolist() time: 0.0008976459503173828

Evaluating:  48%|████▊     | 110/228 [00:29<00:28,  4.10it/s][Astep: 110
extend+tolist() time: 0.0010581016540527344

Evaluating:  49%|████▊     | 111/228 [00:29<00:27,  4.19it/s][Astep: 111
extend+tolist() time: 0.0013594627380371094

Evaluating:  49%|████▉     | 112/228 [00:29<00:30,  3.80it/s][Astep: 112
extend+tolist() time: 0.0008070468902587891

Evaluating:  50%|████▉     | 113/228 [00:30<00:28,  3.98it/s][Astep: 113
extend+tolist() time: 0.0006954669952392578

Evaluating:  50%|█████     | 114/228 [00:30<00:27,  4.11it/s][Astep: 114
extend+tolist() time: 0.0015423297882080078

Evaluating:  50%|█████     | 115/228 [00:30<00:28,  3.94it/s][Astep: 115
extend+tolist() time: 0.0006392002105712891

Evaluating:  51%|█████     | 116/228 [00:30<00:27,  4.09it/s][Astep: 116
extend+tolist() time: 0.001537322998046875

Evaluating:  51%|█████▏    | 117/228 [00:31<00:26,  4.14it/s][Astep: 117
extend+tolist() time: 0.0008213520050048828

Evaluating:  52%|█████▏    | 118/228 [00:31<00:26,  4.14it/s][Astep: 118
extend+tolist() time: 0.0010466575622558594

Evaluating:  52%|█████▏    | 119/228 [00:31<00:32,  3.40it/s][Astep: 119
extend+tolist() time: 0.0007109642028808594

Evaluating:  53%|█████▎    | 120/228 [00:32<00:29,  3.66it/s][Astep: 120
extend+tolist() time: 0.0006079673767089844

Evaluating:  53%|█████▎    | 121/228 [00:32<00:27,  3.87it/s][Astep: 121
extend+tolist() time: 0.0006568431854248047

Evaluating:  54%|█████▎    | 122/228 [00:32<00:26,  4.04it/s][Astep: 122
extend+tolist() time: 0.0007433891296386719

Evaluating:  54%|█████▍    | 123/228 [00:32<00:25,  4.17it/s][Astep: 123
extend+tolist() time: 0.0010561943054199219

Evaluating:  54%|█████▍    | 124/228 [00:32<00:24,  4.26it/s][Astep: 124
extend+tolist() time: 0.0043947696685791016

Evaluating:  55%|█████▍    | 125/228 [00:33<00:24,  4.25it/s][Astep: 125
extend+tolist() time: 0.0004360675811767578

Evaluating:  55%|█████▌    | 126/228 [00:33<00:23,  4.34it/s][Astep: 126
extend+tolist() time: 0.0017549991607666016

Evaluating:  56%|█████▌    | 127/228 [00:33<00:26,  3.88it/s][Astep: 127
extend+tolist() time: 0.001730203628540039

Evaluating:  56%|█████▌    | 128/228 [00:34<00:27,  3.58it/s][Astep: 128
extend+tolist() time: 0.0010378360748291016

Evaluating:  57%|█████▋    | 129/228 [00:34<00:26,  3.80it/s][Astep: 129
extend+tolist() time: 0.0007755756378173828

Evaluating:  57%|█████▋    | 130/228 [00:34<00:24,  3.97it/s][Astep: 130
extend+tolist() time: 0.001298666000366211

Evaluating:  57%|█████▋    | 131/228 [00:34<00:24,  3.97it/s][Astep: 131
extend+tolist() time: 0.0004458427429199219

Evaluating:  58%|█████▊    | 132/228 [00:34<00:23,  4.16it/s][Astep: 132
extend+tolist() time: 0.0014986991882324219

Evaluating:  58%|█████▊    | 133/228 [00:35<00:23,  3.97it/s][Astep: 133
extend+tolist() time: 0.0004305839538574219

Evaluating:  59%|█████▉    | 134/228 [00:35<00:22,  4.15it/s][Astep: 134
extend+tolist() time: 0.0014040470123291016

Evaluating:  59%|█████▉    | 135/228 [00:35<00:23,  4.02it/s][Astep: 135
extend+tolist() time: 0.0004277229309082031

Evaluating:  60%|█████▉    | 136/228 [00:35<00:22,  4.18it/s][Astep: 136
extend+tolist() time: 0.0010693073272705078

Evaluating:  60%|██████    | 137/228 [00:36<00:23,  3.95it/s][Astep: 137
extend+tolist() time: 0.0005223751068115234

Evaluating:  61%|██████    | 138/228 [00:36<00:23,  3.88it/s][Astep: 138
extend+tolist() time: 0.0012962818145751953

Evaluating:  61%|██████    | 139/228 [00:36<00:23,  3.79it/s][Astep: 139
extend+tolist() time: 0.0005588531494140625

Evaluating:  61%|██████▏   | 140/228 [00:37<00:23,  3.72it/s][Astep: 140
extend+tolist() time: 0.0012476444244384766

Evaluating:  62%|██████▏   | 141/228 [00:37<00:23,  3.65it/s][Astep: 141
extend+tolist() time: 0.0009326934814453125

Evaluating:  62%|██████▏   | 142/228 [00:37<00:23,  3.70it/s][Astep: 142
extend+tolist() time: 0.0006144046783447266

Evaluating:  63%|██████▎   | 143/228 [00:37<00:21,  3.92it/s][Astep: 143
extend+tolist() time: 0.0007786750793457031

Evaluating:  63%|██████▎   | 144/228 [00:37<00:20,  4.11it/s][Astep: 144
extend+tolist() time: 0.0007023811340332031

Evaluating:  64%|██████▎   | 145/228 [00:38<00:19,  4.18it/s][Astep: 145
extend+tolist() time: 0.0005102157592773438

Evaluating:  64%|██████▍   | 146/228 [00:38<00:19,  4.31it/s][Astep: 146
extend+tolist() time: 0.0008616447448730469

Evaluating:  64%|██████▍   | 147/228 [00:38<00:18,  4.40it/s][Astep: 147
extend+tolist() time: 0.0007398128509521484

Evaluating:  65%|██████▍   | 148/228 [00:38<00:18,  4.43it/s][Astep: 148
extend+tolist() time: 0.0006489753723144531

Evaluating:  65%|██████▌   | 149/228 [00:39<00:17,  4.47it/s][Astep: 149
extend+tolist() time: 0.0008196830749511719

Evaluating:  66%|██████▌   | 150/228 [00:39<00:17,  4.50it/s][Astep: 150
extend+tolist() time: 0.0007905960083007812

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.46it/s][Astep: 151
extend+tolist() time: 0.0005731582641601562

Evaluating:  67%|██████▋   | 152/228 [00:39<00:21,  3.56it/s][Astep: 152
extend+tolist() time: 0.0013904571533203125

Evaluating:  67%|██████▋   | 153/228 [00:40<00:19,  3.77it/s][Astep: 153
extend+tolist() time: 0.0008842945098876953

Evaluating:  68%|██████▊   | 154/228 [00:40<00:19,  3.88it/s][Astep: 154
extend+tolist() time: 0.19900751113891602

Evaluating:  68%|██████▊   | 155/228 [00:40<00:24,  2.96it/s][Astep: 155
extend+tolist() time: 0.0005946159362792969

Evaluating:  68%|██████▊   | 156/228 [00:41<00:21,  3.30it/s][Astep: 156
extend+tolist() time: 0.0004725456237792969

Evaluating:  69%|██████▉   | 157/228 [00:41<00:19,  3.61it/s][Astep: 157
extend+tolist() time: 0.000965118408203125

Evaluating:  69%|██████▉   | 158/228 [00:41<00:18,  3.85it/s][Astep: 158
extend+tolist() time: 0.0004711151123046875

Evaluating:  70%|██████▉   | 159/228 [00:41<00:17,  4.06it/s][Astep: 159
extend+tolist() time: 0.0006387233734130859

Evaluating:  70%|███████   | 160/228 [00:42<00:16,  4.18it/s][Astep: 160
extend+tolist() time: 0.0007789134979248047

Evaluating:  71%|███████   | 161/228 [00:42<00:15,  4.29it/s][Astep: 161
extend+tolist() time: 0.0008304119110107422

Evaluating:  71%|███████   | 162/228 [00:42<00:15,  4.38it/s][Astep: 162
extend+tolist() time: 0.0004963874816894531

Evaluating:  71%|███████▏  | 163/228 [00:42<00:14,  4.46it/s][Astep: 163
extend+tolist() time: 0.0008394718170166016

Evaluating:  72%|███████▏  | 164/228 [00:42<00:14,  4.51it/s][Astep: 164
extend+tolist() time: 0.0005795955657958984

Evaluating:  72%|███████▏  | 165/228 [00:43<00:13,  4.57it/s][Astep: 165
extend+tolist() time: 0.00043892860412597656

Evaluating:  73%|███████▎  | 166/228 [00:43<00:13,  4.58it/s][Astep: 166
extend+tolist() time: 0.0003840923309326172

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.61it/s][Astep: 167
extend+tolist() time: 0.0010433197021484375

Evaluating:  74%|███████▎  | 168/228 [00:43<00:13,  4.61it/s][Astep: 168
extend+tolist() time: 0.0012199878692626953

Evaluating:  74%|███████▍  | 169/228 [00:44<00:13,  4.29it/s][Astep: 169
extend+tolist() time: 0.00037670135498046875

Evaluating:  75%|███████▍  | 170/228 [00:44<00:13,  4.37it/s][Astep: 170
extend+tolist() time: 0.001379251480102539

Evaluating:  75%|███████▌  | 171/228 [00:44<00:13,  4.37it/s][Astep: 171
extend+tolist() time: 0.0002853870391845703

Evaluating:  75%|███████▌  | 172/228 [00:44<00:12,  4.46it/s][Astep: 172
extend+tolist() time: 0.0011816024780273438

Evaluating:  76%|███████▌  | 173/228 [00:44<00:12,  4.47it/s][Astep: 173
extend+tolist() time: 0.0015106201171875

Evaluating:  76%|███████▋  | 174/228 [00:45<00:12,  4.21it/s][Astep: 174
extend+tolist() time: 0.0019512176513671875

Evaluating:  77%|███████▋  | 175/228 [00:45<00:13,  3.89it/s][Astep: 175
extend+tolist() time: 0.0008068084716796875

Evaluating:  77%|███████▋  | 176/228 [00:45<00:12,  4.05it/s][Astep: 176
extend+tolist() time: 0.0010547637939453125

Evaluating:  78%|███████▊  | 177/228 [00:45<00:12,  4.19it/s][Astep: 177
extend+tolist() time: 0.0005834102630615234

Evaluating:  78%|███████▊  | 178/228 [00:46<00:11,  4.31it/s][Astep: 178
extend+tolist() time: 0.0016679763793945312

Evaluating:  79%|███████▊  | 179/228 [00:46<00:12,  4.08it/s][Astep: 179
extend+tolist() time: 0.00039124488830566406

Evaluating:  79%|███████▉  | 180/228 [00:46<00:11,  4.22it/s][Astep: 180
extend+tolist() time: 0.00037479400634765625

Evaluating:  79%|███████▉  | 181/228 [00:46<00:10,  4.35it/s][Astep: 181
extend+tolist() time: 0.0010459423065185547

Evaluating:  80%|███████▉  | 182/228 [00:47<00:10,  4.41it/s][Astep: 182
extend+tolist() time: 0.0007567405700683594

Evaluating:  80%|████████  | 183/228 [00:47<00:10,  4.44it/s][Astep: 183
extend+tolist() time: 0.0006504058837890625

Evaluating:  81%|████████  | 184/228 [00:47<00:09,  4.48it/s][Astep: 184
extend+tolist() time: 0.00044608116149902344

Evaluating:  81%|████████  | 185/228 [00:47<00:09,  4.51it/s][Astep: 185
extend+tolist() time: 0.0011610984802246094

Evaluating:  82%|████████▏ | 186/228 [00:48<00:09,  4.24it/s][Astep: 186
extend+tolist() time: 0.0015587806701660156

Evaluating:  82%|████████▏ | 187/228 [00:48<00:09,  4.18it/s][Astep: 187
extend+tolist() time: 0.00044846534729003906

Evaluating:  82%|████████▏ | 188/228 [00:48<00:09,  4.30it/s][Astep: 188
extend+tolist() time: 0.0007283687591552734

Evaluating:  83%|████████▎ | 189/228 [00:48<00:08,  4.38it/s][Astep: 189
extend+tolist() time: 0.00036597251892089844

Evaluating:  83%|████████▎ | 190/228 [00:48<00:08,  4.45it/s][Astep: 190
extend+tolist() time: 0.0016417503356933594

Evaluating:  84%|████████▍ | 191/228 [00:49<00:08,  4.18it/s][Astep: 191
extend+tolist() time: 0.0007660388946533203

Evaluating:  84%|████████▍ | 192/228 [00:49<00:08,  4.27it/s][Astep: 192
extend+tolist() time: 0.0004413127899169922

Evaluating:  85%|████████▍ | 193/228 [00:49<00:08,  4.36it/s][Astep: 193
extend+tolist() time: 0.0011277198791503906

Evaluating:  85%|████████▌ | 194/228 [00:49<00:08,  4.24it/s][Astep: 194
extend+tolist() time: 0.0010559558868408203

Evaluating:  86%|████████▌ | 195/228 [00:50<00:07,  4.32it/s][Astep: 195
extend+tolist() time: 0.0005605220794677734

Evaluating:  86%|████████▌ | 196/228 [00:50<00:07,  4.39it/s][Astep: 196
extend+tolist() time: 0.0006453990936279297

Evaluating:  86%|████████▋ | 197/228 [00:50<00:06,  4.43it/s][Astep: 197
extend+tolist() time: 0.0011398792266845703

Evaluating:  87%|████████▋ | 198/228 [00:50<00:06,  4.47it/s][Astep: 198
extend+tolist() time: 0.0006470680236816406

Evaluating:  87%|████████▋ | 199/228 [00:50<00:06,  4.50it/s][Astep: 199
extend+tolist() time: 0.0019407272338867188

Evaluating:  88%|████████▊ | 200/228 [00:51<00:06,  4.05it/s][Astep: 200
extend+tolist() time: 0.0006840229034423828

Evaluating:  88%|████████▊ | 201/228 [00:51<00:08,  3.33it/s][Astep: 201
extend+tolist() time: 0.0005869865417480469

Evaluating:  89%|████████▊ | 202/228 [00:51<00:07,  3.64it/s][Astep: 202
extend+tolist() time: 0.0008618831634521484

Evaluating:  89%|████████▉ | 203/228 [00:52<00:06,  3.90it/s][Astep: 203
extend+tolist() time: 0.0005438327789306641

Evaluating:  89%|████████▉ | 204/228 [00:52<00:05,  4.09it/s][Astep: 204
extend+tolist() time: 0.0003917217254638672

Evaluating:  90%|████████▉ | 205/228 [00:52<00:05,  4.24it/s][Astep: 205
extend+tolist() time: 0.0002892017364501953

Evaluating:  90%|█████████ | 206/228 [00:52<00:05,  4.35it/s][Astep: 206
extend+tolist() time: 0.0010991096496582031

Evaluating:  91%|█████████ | 207/228 [00:52<00:04,  4.41it/s][Astep: 207
extend+tolist() time: 0.0006453990936279297

Evaluating:  91%|█████████ | 208/228 [00:53<00:04,  4.45it/s][Astep: 208
extend+tolist() time: 0.0007333755493164062

Evaluating:  92%|█████████▏| 209/228 [00:53<00:04,  4.45it/s][Astep: 209
extend+tolist() time: 0.0010442733764648438

Evaluating:  92%|█████████▏| 210/228 [00:53<00:04,  4.48it/s][Astep: 210
extend+tolist() time: 0.0006034374237060547

Evaluating:  93%|█████████▎| 211/228 [00:53<00:03,  4.52it/s][Astep: 211
extend+tolist() time: 0.0011725425720214844

Evaluating:  93%|█████████▎| 212/228 [00:54<00:03,  4.25it/s][Astep: 212
extend+tolist() time: 0.0009474754333496094

Evaluating:  93%|█████████▎| 213/228 [00:54<00:03,  4.26it/s][Astep: 213
extend+tolist() time: 0.0007805824279785156

Evaluating:  94%|█████████▍| 214/228 [00:54<00:03,  4.32it/s][Astep: 214
extend+tolist() time: 0.001341104507446289

Evaluating:  94%|█████████▍| 215/228 [00:54<00:02,  4.37it/s][Astep: 215
extend+tolist() time: 0.0006897449493408203

Evaluating:  95%|█████████▍| 216/228 [00:55<00:02,  4.41it/s][Astep: 216
extend+tolist() time: 0.0009400844573974609

Evaluating:  95%|█████████▌| 217/228 [00:55<00:02,  4.46it/s][Astep: 217
extend+tolist() time: 0.0005693435668945312

Evaluating:  96%|█████████▌| 218/228 [00:55<00:02,  4.50it/s][Astep: 218
extend+tolist() time: 0.0010859966278076172

Evaluating:  96%|█████████▌| 219/228 [00:55<00:02,  4.37it/s][Astep: 219
extend+tolist() time: 0.0009169578552246094

Evaluating:  96%|█████████▋| 220/228 [00:55<00:01,  4.44it/s][Astep: 220
extend+tolist() time: 0.00038886070251464844

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.50it/s][Astep: 221
extend+tolist() time: 0.0006814002990722656

Evaluating:  97%|█████████▋| 222/228 [00:56<00:01,  4.51it/s][Astep: 222
extend+tolist() time: 0.0004324913024902344

Evaluating:  98%|█████████▊| 223/228 [00:56<00:01,  4.54it/s][Astep: 223
extend+tolist() time: 0.0008156299591064453

Evaluating:  98%|█████████▊| 224/228 [00:56<00:00,  4.57it/s][Astep: 224
extend+tolist() time: 0.0003757476806640625

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.59it/s][Astep: 225
extend+tolist() time: 0.0005412101745605469

Evaluating:  99%|█████████▉| 226/228 [00:57<00:00,  4.61it/s][Astep: 226
extend+tolist() time: 0.0006241798400878906

Evaluating: 100%|█████████▉| 227/228 [00:57<00:00,  4.56it/s][Astep: 227
extend+tolist() time: 0.0009553432464599609

Evaluating: 100%|██████████| 228/228 [00:57<00:00,  3.94it/s][A09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 17:00:24 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 17:00:25 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 17:00:25 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [00:59<00:00,  3.85it/s]
09/05/2023 17:00:25 - INFO - __main__ -   Step: 4700, Validation Metrics: {'pred_1_num': 9753, 'pred_-1_num': 722, 'pred_0_num': 326, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7860383297842792, 'f1_micro': 0.7860383297842792, 'f1_macro': 0.4491893347136479, 'f1_weighted': 0.7537334750470555, 'f1_-1': 0.32080419580419584, 'f1_0': 0.14922279792746115, 'f1_1': 0.8775410104092867, 'precision_micro': 0.7860383297842792, 'precision_macro': 0.5182195793195871, 'precision_weighted': 0.7437321753294229, 'precision_-1': 0.5083102493074793, 'precision_0': 0.22085889570552147, 'precision_1': 0.8254895929457603, 'recall_micro': 0.7860383297842792, 'recall_macro': 0.42787650630222557, 'recall_weighted': 0.7860383297842792, 'recall_-1': 0.23435504469987228, 'recall_0': 0.11267605633802817, 'recall_1': 0.9365984178687762, 'roc_auc_micro': 0.9090587077576624, 'roc_auc_macro': 0.7308769864779129, 'roc_auc_weighted': 0.7191725375419803, 'roc_auc_-1': 0.813519524602735, 'roc_auc_0': 0.6737502691145231, 'roc_auc_1': 0.7053611657164804}
[2023-09-05 17:00:32,945] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4701/66600 [2:56:44<549:41:12, 31.97s/it]09/05/2023 17:00:40 - INFO - __main__ -   Step: 4701, LR: 1.9162637286062152e-05, Loss: 0.1343647539615631
[2023-09-05 17:00:47,225] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4702/66600 [2:56:58<459:56:12, 26.75s/it]09/05/2023 17:00:55 - INFO - __main__ -   Step: 4702, LR: 1.916232758186837e-05, Loss: 0.08913081884384155
[2023-09-05 17:01:02,539] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4703/66600 [2:57:13<399:51:08, 23.26s/it]09/05/2023 17:01:10 - INFO - __main__ -   Step: 4703, LR: 1.9162017877674588e-05, Loss: 0.15021999180316925
[2023-09-05 17:01:16,398] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4704/66600 [2:57:27<349:30:39, 20.33s/it]09/05/2023 17:01:23 - INFO - __main__ -   Step: 4704, LR: 1.9161708173480806e-05, Loss: 0.0718877911567688
[2023-09-05 17:01:30,541] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4705/66600 [2:57:41<317:59:52, 18.50s/it]09/05/2023 17:01:37 - INFO - __main__ -   Step: 4705, LR: 1.9161398469287024e-05, Loss: 0.09202359616756439
[2023-09-05 17:01:44,232] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4706/66600 [2:57:55<292:34:39, 17.02s/it]09/05/2023 17:01:51 - INFO - __main__ -   Step: 4706, LR: 1.9161088765093242e-05, Loss: 0.13512897491455078
[2023-09-05 17:01:57,879] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4707/66600 [2:58:08<275:16:33, 16.01s/it]09/05/2023 17:02:05 - INFO - __main__ -   Step: 4707, LR: 1.916077906089946e-05, Loss: 0.07335302978754044
[2023-09-05 17:02:11,913] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4708/66600 [2:58:22<266:31:05, 15.50s/it]09/05/2023 17:02:19 - INFO - __main__ -   Step: 4708, LR: 1.916046935670568e-05, Loss: 0.11712503433227539
[2023-09-05 17:02:25,636] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4709/66600 [2:58:37<258:59:28, 15.06s/it]09/05/2023 17:02:33 - INFO - __main__ -   Step: 4709, LR: 1.9160159652511896e-05, Loss: 0.11731946468353271
[2023-09-05 17:02:39,596] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4710/66600 [2:58:50<251:53:05, 14.65s/it]09/05/2023 17:02:47 - INFO - __main__ -   Step: 4710, LR: 1.9159849948318115e-05, Loss: 0.13051195442676544
[2023-09-05 17:02:53,834] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4711/66600 [2:59:04<247:03:26, 14.37s/it]09/05/2023 17:03:00 - INFO - __main__ -   Step: 4711, LR: 1.9159540244124333e-05, Loss: 0.11368078738451004
[2023-09-05 17:03:08,239] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4712/66600 [2:59:20<253:56:29, 14.77s/it]09/05/2023 17:03:16 - INFO - __main__ -   Step: 4712, LR: 1.915923053993055e-05, Loss: 0.1051107794046402
[2023-09-05 17:03:24,451] [WARNING] [stage3.py:1898:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4713/66600 [2:59:35<254:48:19, 14.82s/it]09/05/2023 17:03:31 - INFO - __main__ -   Step: 4713, LR: 1.915892083573677e-05, Loss: 0.1416284739971161
[2023-09-05 17:03:38,349] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4714/66600 [2:59:49<252:39:35, 14.70s/it]09/05/2023 17:03:45 - INFO - __main__ -   Step: 4714, LR: 1.9158611131542987e-05, Loss: 0.12524276971817017
[2023-09-05 17:03:52,340] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4715/66600 [3:00:03<246:40:50, 14.35s/it]09/05/2023 17:03:59 - INFO - __main__ -   Step: 4715, LR: 1.9158301427349205e-05, Loss: 0.21894213557243347
[2023-09-05 17:04:05,654] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4716/66600 [3:00:17<246:23:51, 14.33s/it]09/05/2023 17:04:13 - INFO - __main__ -   Step: 4716, LR: 1.9157991723155423e-05, Loss: 0.1316453367471695
[2023-09-05 17:04:20,626] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4717/66600 [3:00:31<248:06:52, 14.43s/it]09/05/2023 17:04:28 - INFO - __main__ -   Step: 4717, LR: 1.915768201896164e-05, Loss: 0.12647578120231628
[2023-09-05 17:04:34,649] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4718/66600 [3:00:45<245:12:28, 14.27s/it]09/05/2023 17:04:42 - INFO - __main__ -   Step: 4718, LR: 1.915737231476786e-05, Loss: 0.12751731276512146
[2023-09-05 17:04:48,737] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4719/66600 [3:00:59<244:29:47, 14.22s/it]09/05/2023 17:04:56 - INFO - __main__ -   Step: 4719, LR: 1.9157062610574077e-05, Loss: 0.09056626260280609
[2023-09-05 17:05:02,717] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4720/66600 [3:01:14<244:05:35, 14.20s/it]09/05/2023 17:05:10 - INFO - __main__ -   Step: 4720, LR: 1.9156752906380295e-05, Loss: 0.1241564154624939
[2023-09-05 17:05:17,631] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4721/66600 [3:01:29<248:05:02, 14.43s/it]09/05/2023 17:05:25 - INFO - __main__ -   Step: 4721, LR: 1.9156443202186513e-05, Loss: 0.12491148710250854
[2023-09-05 17:05:31,982] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4722/66600 [3:01:42<244:30:38, 14.23s/it]09/05/2023 17:05:39 - INFO - __main__ -   Step: 4722, LR: 1.915613349799273e-05, Loss: 0.1514405906200409
[2023-09-05 17:05:45,597] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4723/66600 [3:01:56<240:52:42, 14.01s/it]09/05/2023 17:05:52 - INFO - __main__ -   Step: 4723, LR: 1.915582379379895e-05, Loss: 0.14735418558120728
[2023-09-05 17:05:59,364] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4724/66600 [3:02:10<240:04:40, 13.97s/it]09/05/2023 17:06:06 - INFO - __main__ -   Step: 4724, LR: 1.9155514089605167e-05, Loss: 0.10297878086566925
[2023-09-05 17:06:13,169] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4725/66600 [3:02:24<240:26:57, 13.99s/it]09/05/2023 17:06:20 - INFO - __main__ -   Step: 4725, LR: 1.9155204385411385e-05, Loss: 0.14082035422325134
[2023-09-05 17:06:26,903] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4726/66600 [3:02:38<241:29:58, 14.05s/it]09/05/2023 17:06:34 - INFO - __main__ -   Step: 4726, LR: 1.9154894681217603e-05, Loss: 0.10172905772924423
[2023-09-05 17:06:41,651] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4727/66600 [3:02:52<243:55:45, 14.19s/it]09/05/2023 17:06:49 - INFO - __main__ -   Step: 4727, LR: 1.915458497702382e-05, Loss: 0.09825297445058823
[2023-09-05 17:06:55,390] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4728/66600 [3:03:06<242:05:09, 14.09s/it]09/05/2023 17:07:03 - INFO - __main__ -   Step: 4728, LR: 1.915427527283004e-05, Loss: 0.1430976837873459
[2023-09-05 17:07:10,304] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4729/66600 [3:03:22<247:53:32, 14.42s/it]09/05/2023 17:07:18 - INFO - __main__ -   Step: 4729, LR: 1.9153965568636258e-05, Loss: 0.14952650666236877
[2023-09-05 17:07:24,908] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4730/66600 [3:03:36<246:51:32, 14.36s/it]09/05/2023 17:07:32 - INFO - __main__ -   Step: 4730, LR: 1.9153655864442476e-05, Loss: 0.13831879198551178
[2023-09-05 17:07:39,363] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4731/66600 [3:03:50<246:37:35, 14.35s/it]09/05/2023 17:07:47 - INFO - __main__ -   Step: 4731, LR: 1.9153346160248694e-05, Loss: 0.12948590517044067
[2023-09-05 17:07:53,463] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4732/66600 [3:04:05<249:46:50, 14.53s/it]09/05/2023 17:08:01 - INFO - __main__ -   Step: 4732, LR: 1.9153036456054912e-05, Loss: 0.11100241541862488
[2023-09-05 17:08:08,708] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4733/66600 [3:04:19<248:33:02, 14.46s/it]09/05/2023 17:08:16 - INFO - __main__ -   Step: 4733, LR: 1.915272675186113e-05, Loss: 0.09987764060497284
[2023-09-05 17:08:23,144] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4734/66600 [3:04:33<246:50:13, 14.36s/it]09/05/2023 17:08:30 - INFO - __main__ -   Step: 4734, LR: 1.9152417047667348e-05, Loss: 0.10970363765954971
[2023-09-05 17:08:36,294] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4735/66600 [3:04:47<240:47:19, 14.01s/it]09/05/2023 17:08:43 - INFO - __main__ -   Step: 4735, LR: 1.9152107343473566e-05, Loss: 0.11815784871578217
[2023-09-05 17:08:50,538] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4736/66600 [3:05:01<244:00:21, 14.20s/it]09/05/2023 17:08:58 - INFO - __main__ -   Step: 4736, LR: 1.9151797639279784e-05, Loss: 0.11474981904029846
[2023-09-05 17:09:04,830] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4737/66600 [3:05:16<244:31:39, 14.23s/it]09/05/2023 17:09:12 - INFO - __main__ -   Step: 4737, LR: 1.9151487935086002e-05, Loss: 0.08179474622011185
[2023-09-05 17:09:19,420] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4738/66600 [3:05:31<250:04:30, 14.55s/it]09/05/2023 17:09:27 - INFO - __main__ -   Step: 4738, LR: 1.915117823089222e-05, Loss: 0.12995943427085876
[2023-09-05 17:09:34,557] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4739/66600 [3:05:45<245:08:06, 14.27s/it]09/05/2023 17:09:41 - INFO - __main__ -   Step: 4739, LR: 1.9150868526698438e-05, Loss: 0.208674818277359
[2023-09-05 17:09:47,426] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4740/66600 [3:05:57<237:11:39, 13.80s/it]09/05/2023 17:09:54 - INFO - __main__ -   Step: 4740, LR: 1.9150558822504656e-05, Loss: 0.15334203839302063
[2023-09-05 17:10:00,201] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4741/66600 [3:06:11<236:00:29, 13.73s/it]09/05/2023 17:10:07 - INFO - __main__ -   Step: 4741, LR: 1.9150249118310874e-05, Loss: 0.10760216414928436
[2023-09-05 17:10:14,528] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4742/66600 [3:06:25<236:57:10, 13.79s/it]09/05/2023 17:10:21 - INFO - __main__ -   Step: 4742, LR: 1.9149939414117092e-05, Loss: 0.15118268132209778
[2023-09-05 17:10:27,589] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4743/66600 [3:06:39<238:52:46, 13.90s/it]09/05/2023 17:10:35 - INFO - __main__ -   Step: 4743, LR: 1.914962970992331e-05, Loss: 0.12164723128080368
[2023-09-05 17:10:43,083] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4744/66600 [3:06:54<243:30:38, 14.17s/it]09/05/2023 17:10:50 - INFO - __main__ -   Step: 4744, LR: 1.914932000572953e-05, Loss: 0.14071056246757507
[2023-09-05 17:10:57,981] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4745/66600 [3:07:08<245:49:42, 14.31s/it]09/05/2023 17:11:05 - INFO - __main__ -   Step: 4745, LR: 1.9149010301535747e-05, Loss: 0.16565510630607605
[2023-09-05 17:11:11,487] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4746/66600 [3:07:22<242:00:50, 14.09s/it]09/05/2023 17:11:18 - INFO - __main__ -   Step: 4746, LR: 1.9148700597341965e-05, Loss: 0.08789848536252975
[2023-09-05 17:11:24,926] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4747/66600 [3:07:36<244:21:19, 14.22s/it]09/05/2023 17:11:33 - INFO - __main__ -   Step: 4747, LR: 1.9148390893148183e-05, Loss: 0.14383941888809204
[2023-09-05 17:11:39,474] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4748/66600 [3:07:50<241:32:18, 14.06s/it]09/05/2023 17:11:47 - INFO - __main__ -   Step: 4748, LR: 1.91480811889544e-05, Loss: 0.12416066229343414
[2023-09-05 17:11:54,287] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4749/66600 [3:08:05<247:51:03, 14.43s/it]09/05/2023 17:12:02 - INFO - __main__ -   Step: 4749, LR: 1.914777148476062e-05, Loss: 0.1325121819972992
[2023-09-05 17:12:08,370] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4750/66600 [3:08:19<242:04:30, 14.09s/it]09/05/2023 17:12:15 - INFO - __main__ -   Step: 4750, LR: 1.9147461780566837e-05, Loss: 0.15090608596801758
[2023-09-05 17:12:21,899] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4751/66600 [3:08:33<241:02:47, 14.03s/it]09/05/2023 17:12:29 - INFO - __main__ -   Step: 4751, LR: 1.914715207637306e-05, Loss: 0.1501457244157791
[2023-09-05 17:12:36,506] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4752/66600 [3:08:47<245:36:12, 14.30s/it]09/05/2023 17:12:44 - INFO - __main__ -   Step: 4752, LR: 1.9146842372179273e-05, Loss: 0.11379154771566391
[2023-09-05 17:12:51,412] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4753/66600 [3:09:02<249:02:40, 14.50s/it]09/05/2023 17:12:59 - INFO - __main__ -   Step: 4753, LR: 1.914653266798549e-05, Loss: 0.10391637682914734
[2023-09-05 17:13:05,990] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4754/66600 [3:09:17<249:29:38, 14.52s/it]09/05/2023 17:13:13 - INFO - __main__ -   Step: 4754, LR: 1.914622296379171e-05, Loss: 0.12814322113990784
[2023-09-05 17:13:20,341] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4755/66600 [3:09:31<246:49:19, 14.37s/it]09/05/2023 17:13:28 - INFO - __main__ -   Step: 4755, LR: 1.9145913259597927e-05, Loss: 0.1360609084367752
[2023-09-05 17:13:34,883] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4756/66600 [3:09:45<244:24:37, 14.23s/it]09/05/2023 17:13:41 - INFO - __main__ -   Step: 4756, LR: 1.9145603555404145e-05, Loss: 0.09696198999881744
[2023-09-05 17:13:48,567] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4757/66600 [3:09:59<243:09:35, 14.15s/it]09/05/2023 17:13:55 - INFO - __main__ -   Step: 4757, LR: 1.9145293851210363e-05, Loss: 0.11692102998495102
[2023-09-05 17:14:01,818] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4758/66600 [3:10:12<238:34:05, 13.89s/it]09/05/2023 17:14:09 - INFO - __main__ -   Step: 4758, LR: 1.9144984147016585e-05, Loss: 0.12788666784763336
[2023-09-05 17:14:15,820] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4759/66600 [3:10:26<239:20:04, 13.93s/it]09/05/2023 17:14:23 - INFO - __main__ -   Step: 4759, LR: 1.9144674442822803e-05, Loss: 0.08747685700654984
[2023-09-05 17:14:29,573] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4760/66600 [3:10:40<240:21:39, 13.99s/it]09/05/2023 17:14:37 - INFO - __main__ -   Step: 4760, LR: 1.9144364738629017e-05, Loss: 0.1391156017780304
[2023-09-05 17:14:43,582] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4761/66600 [3:10:54<239:25:42, 13.94s/it]09/05/2023 17:14:51 - INFO - __main__ -   Step: 4761, LR: 1.9144055034435236e-05, Loss: 0.15983711183071136
[2023-09-05 17:14:57,054] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4762/66600 [3:11:08<238:37:13, 13.89s/it]09/05/2023 17:15:04 - INFO - __main__ -   Step: 4762, LR: 1.9143745330241454e-05, Loss: 0.09583529829978943
[2023-09-05 17:15:11,428] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4763/66600 [3:11:22<241:19:05, 14.05s/it]09/05/2023 17:15:19 - INFO - __main__ -   Step: 4763, LR: 1.914343562604767e-05, Loss: 0.12731465697288513
[2023-09-05 17:15:25,510] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4764/66600 [3:11:37<242:19:20, 14.11s/it]09/05/2023 17:15:33 - INFO - __main__ -   Step: 4764, LR: 1.914312592185389e-05, Loss: 0.1333920955657959
[2023-09-05 17:15:40,223] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4765/66600 [3:11:51<243:08:55, 14.16s/it]09/05/2023 17:15:47 - INFO - __main__ -   Step: 4765, LR: 1.914281621766011e-05, Loss: 0.13730335235595703
[2023-09-05 17:15:54,494] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4766/66600 [3:12:05<241:25:26, 14.06s/it]09/05/2023 17:16:01 - INFO - __main__ -   Step: 4766, LR: 1.914250651346633e-05, Loss: 0.1198229193687439
[2023-09-05 17:16:08,009] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4767/66600 [3:12:19<242:00:32, 14.09s/it]09/05/2023 17:16:15 - INFO - __main__ -   Step: 4767, LR: 1.9142196809272544e-05, Loss: 0.16982480883598328
[2023-09-05 17:16:21,750] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4768/66600 [3:12:33<240:02:32, 13.98s/it]09/05/2023 17:16:29 - INFO - __main__ -   Step: 4768, LR: 1.9141887105078762e-05, Loss: 0.13756322860717773
[2023-09-05 17:16:35,895] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4769/66600 [3:12:47<244:42:56, 14.25s/it]09/05/2023 17:16:44 - INFO - __main__ -   Step: 4769, LR: 1.914157740088498e-05, Loss: 0.12121836096048355
[2023-09-05 17:16:51,090] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4770/66600 [3:13:02<244:49:10, 14.25s/it]09/05/2023 17:16:58 - INFO - __main__ -   Step: 4770, LR: 1.9141267696691198e-05, Loss: 0.10135780274868011
[2023-09-05 17:17:05,845] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4771/66600 [3:13:17<249:47:46, 14.54s/it]09/05/2023 17:17:13 - INFO - __main__ -   Step: 4771, LR: 1.9140957992497416e-05, Loss: 0.14845852553844452
[2023-09-05 17:17:20,971] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4772/66600 [3:13:32<250:53:38, 14.61s/it]09/05/2023 17:17:28 - INFO - __main__ -   Step: 4772, LR: 1.9140648288303638e-05, Loss: 0.11683320999145508
[2023-09-05 17:17:34,829] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4773/66600 [3:13:46<246:52:47, 14.38s/it]09/05/2023 17:17:42 - INFO - __main__ -   Step: 4773, LR: 1.9140338584109856e-05, Loss: 0.11363186687231064
[2023-09-05 17:17:48,526] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4774/66600 [3:13:59<241:20:39, 14.05s/it]09/05/2023 17:17:55 - INFO - __main__ -   Step: 4774, LR: 1.9140028879916074e-05, Loss: 0.1294536590576172
[2023-09-05 17:18:02,189] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4775/66600 [3:14:13<241:54:31, 14.09s/it]09/05/2023 17:18:09 - INFO - __main__ -   Step: 4775, LR: 1.913971917572229e-05, Loss: 0.1062304675579071
[2023-09-05 17:18:16,116] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4776/66600 [3:14:27<240:18:40, 13.99s/it]09/05/2023 17:18:23 - INFO - __main__ -   Step: 4776, LR: 1.9139409471528506e-05, Loss: 0.1253969669342041
[2023-09-05 17:18:29,974] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4777/66600 [3:14:41<243:28:44, 14.18s/it]09/05/2023 17:18:38 - INFO - __main__ -   Step: 4777, LR: 1.9139099767334725e-05, Loss: 0.1275235265493393
[2023-09-05 17:18:44,327] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4778/66600 [3:14:55<240:45:19, 14.02s/it]09/05/2023 17:18:52 - INFO - __main__ -   Step: 4778, LR: 1.9138790063140943e-05, Loss: 0.14417830109596252
[2023-09-05 17:18:59,171] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4779/66600 [3:15:10<244:12:47, 14.22s/it]09/05/2023 17:19:06 - INFO - __main__ -   Step: 4779, LR: 1.9138480358947164e-05, Loss: 0.1058611124753952
[2023-09-05 17:19:14,007] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4780/66600 [3:15:24<244:07:02, 14.22s/it]09/05/2023 17:19:20 - INFO - __main__ -   Step: 4780, LR: 1.9138170654753382e-05, Loss: 0.15107855200767517
[2023-09-05 17:19:27,125] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4781/66600 [3:15:38<241:12:24, 14.05s/it]09/05/2023 17:19:34 - INFO - __main__ -   Step: 4781, LR: 1.91378609505596e-05, Loss: 0.10184948146343231
[2023-09-05 17:19:41,330] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4782/66600 [3:15:52<242:55:16, 14.15s/it]09/05/2023 17:19:48 - INFO - __main__ -   Step: 4782, LR: 1.9137551246365818e-05, Loss: 0.19556662440299988
[2023-09-05 17:19:55,507] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4783/66600 [3:16:06<240:49:45, 14.03s/it]09/05/2023 17:20:02 - INFO - __main__ -   Step: 4783, LR: 1.9137241542172033e-05, Loss: 0.10530819743871689
[2023-09-05 17:20:10,265] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4784/66600 [3:16:20<244:21:07, 14.23s/it]09/05/2023 17:20:17 - INFO - __main__ -   Step: 4784, LR: 1.913693183797825e-05, Loss: 0.11389102786779404
[2023-09-05 17:20:24,092] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4785/66600 [3:16:35<245:29:44, 14.30s/it]09/05/2023 17:20:31 - INFO - __main__ -   Step: 4785, LR: 1.913662213378447e-05, Loss: 0.13417202234268188
[2023-09-05 17:20:38,436] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4786/66600 [3:16:49<244:28:37, 14.24s/it]09/05/2023 17:20:45 - INFO - __main__ -   Step: 4786, LR: 1.913631242959069e-05, Loss: 0.124651700258255
[2023-09-05 17:20:52,789] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4787/66600 [3:17:03<244:29:14, 14.24s/it]09/05/2023 17:21:00 - INFO - __main__ -   Step: 4787, LR: 1.913600272539691e-05, Loss: 0.16021640598773956
[2023-09-05 17:21:06,403] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4788/66600 [3:17:17<241:34:32, 14.07s/it]09/05/2023 17:21:13 - INFO - __main__ -   Step: 4788, LR: 1.9135693021203127e-05, Loss: 0.1903853714466095
[2023-09-05 17:21:20,537] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4789/66600 [3:17:31<240:07:01, 13.98s/it]09/05/2023 17:21:27 - INFO - __main__ -   Step: 4789, LR: 1.9135383317009345e-05, Loss: 0.13734140992164612
[2023-09-05 17:21:34,098] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4790/66600 [3:17:45<241:59:05, 14.09s/it]09/05/2023 17:21:41 - INFO - __main__ -   Step: 4790, LR: 1.913507361281556e-05, Loss: 0.09866377711296082
[2023-09-05 17:21:48,957] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4791/66600 [3:17:59<243:30:15, 14.18s/it]09/05/2023 17:21:56 - INFO - __main__ -   Step: 4791, LR: 1.9134763908621777e-05, Loss: 0.10491666942834854
[2023-09-05 17:22:02,503] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4792/66600 [3:18:13<241:00:19, 14.04s/it]09/05/2023 17:22:10 - INFO - __main__ -   Step: 4792, LR: 1.9134454204427995e-05, Loss: 0.1488465666770935
[2023-09-05 17:22:17,726] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4793/66600 [3:18:28<246:03:17, 14.33s/it]09/05/2023 17:22:25 - INFO - __main__ -   Step: 4793, LR: 1.9134144500234217e-05, Loss: 0.1090136170387268
[2023-09-05 17:22:32,258] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4794/66600 [3:18:43<250:21:33, 14.58s/it]09/05/2023 17:22:40 - INFO - __main__ -   Step: 4794, LR: 1.9133834796040435e-05, Loss: 0.1378249228000641
[2023-09-05 17:22:47,032] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4795/66600 [3:18:59<254:04:21, 14.80s/it]09/05/2023 17:22:55 - INFO - __main__ -   Step: 4795, LR: 1.9133525091846653e-05, Loss: 0.12997156381607056
[2023-09-05 17:23:02,294] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4796/66600 [3:19:13<251:17:31, 14.64s/it]09/05/2023 17:23:09 - INFO - __main__ -   Step: 4796, LR: 1.913321538765287e-05, Loss: 0.13814759254455566
[2023-09-05 17:23:16,467] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4797/66600 [3:19:27<248:36:26, 14.48s/it]09/05/2023 17:23:23 - INFO - __main__ -   Step: 4797, LR: 1.913290568345909e-05, Loss: 0.12833808362483978
[2023-09-05 17:23:30,302] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4798/66600 [3:19:41<247:24:54, 14.41s/it]09/05/2023 17:23:38 - INFO - __main__ -   Step: 4798, LR: 1.9132595979265304e-05, Loss: 0.14557331800460815
[2023-09-05 17:23:44,764] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4799/66600 [3:19:55<244:22:13, 14.23s/it]09/05/2023 17:23:52 - INFO - __main__ -   Step: 4799, LR: 1.9132286275071522e-05, Loss: 0.1547921597957611
[2023-09-05 17:23:58,207] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4800/66600 [3:20:09<240:30:31, 14.01s/it]09/05/2023 17:24:05 - INFO - __main__ -   Step: 4800, LR: 1.9131976570877743e-05, Loss: 0.08251791447401047
09/05/2023 17:24:05 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.002627849578857422

Evaluating:   0%|          | 1/228 [00:00<01:16,  2.98it/s][Astep: 1
extend+tolist() time: 0.0015344619750976562

Evaluating:   1%|          | 2/228 [00:00<01:20,  2.80it/s][Astep: 2
extend+tolist() time: 0.002054929733276367

Evaluating:   1%|▏         | 3/228 [00:01<01:31,  2.47it/s][Astep: 3
extend+tolist() time: 0.0013899803161621094

Evaluating:   2%|▏         | 4/228 [00:01<01:21,  2.74it/s][Astep: 4
extend+tolist() time: 0.13797354698181152

Evaluating:   2%|▏         | 5/228 [00:01<01:23,  2.67it/s][Astep: 5
extend+tolist() time: 0.0020923614501953125

Evaluating:   3%|▎         | 6/228 [00:02<01:19,  2.78it/s][Astep: 6
extend+tolist() time: 0.001903533935546875

Evaluating:   3%|▎         | 7/228 [00:02<01:17,  2.83it/s][Astep: 7
extend+tolist() time: 0.0013263225555419922

Evaluating:   4%|▎         | 8/228 [00:02<01:10,  3.11it/s][Astep: 8
extend+tolist() time: 0.0007698535919189453

Evaluating:   4%|▍         | 9/228 [00:03<01:04,  3.41it/s][Astep: 9
extend+tolist() time: 0.001239776611328125

Evaluating:   4%|▍         | 10/228 [00:03<00:59,  3.64it/s][Astep: 10
extend+tolist() time: 0.0008406639099121094

Evaluating:   5%|▍         | 11/228 [00:03<00:57,  3.76it/s][Astep: 11
extend+tolist() time: 0.0005555152893066406

Evaluating:   5%|▌         | 12/228 [00:03<00:54,  3.93it/s][Astep: 12
extend+tolist() time: 0.0006716251373291016

Evaluating:   6%|▌         | 13/228 [00:03<00:52,  4.08it/s][Astep: 13
extend+tolist() time: 0.0010972023010253906

Evaluating:   6%|▌         | 14/228 [00:04<00:51,  4.16it/s][Astep: 14
extend+tolist() time: 0.00057220458984375

Evaluating:   7%|▋         | 15/228 [00:04<00:50,  4.25it/s][Astep: 15
extend+tolist() time: 0.0005993843078613281

Evaluating:   7%|▋         | 16/228 [00:04<00:49,  4.32it/s][Astep: 16
extend+tolist() time: 0.0011112689971923828

Evaluating:   7%|▋         | 17/228 [00:04<00:48,  4.33it/s][Astep: 17
extend+tolist() time: 0.0008485317230224609

Evaluating:   8%|▊         | 18/228 [00:05<00:49,  4.27it/s][Astep: 18
extend+tolist() time: 0.0016379356384277344

Evaluating:   8%|▊         | 19/228 [00:05<00:51,  4.04it/s][Astep: 19
extend+tolist() time: 0.001859903335571289

Evaluating:   9%|▉         | 20/228 [00:05<00:53,  3.91it/s][Astep: 20
extend+tolist() time: 0.0007987022399902344

Evaluating:   9%|▉         | 21/228 [00:06<00:59,  3.47it/s][Astep: 21
extend+tolist() time: 0.000698089599609375

Evaluating:  10%|▉         | 22/228 [00:06<00:55,  3.68it/s][Astep: 22
extend+tolist() time: 0.0012297630310058594

Evaluating:  10%|█         | 23/228 [00:06<00:53,  3.86it/s][Astep: 23
extend+tolist() time: 0.0006945133209228516

Evaluating:  11%|█         | 24/228 [00:06<00:51,  3.99it/s][Astep: 24
extend+tolist() time: 0.0016188621520996094

Evaluating:  11%|█         | 25/228 [00:06<00:52,  3.86it/s][Astep: 25
extend+tolist() time: 0.0019495487213134766

Evaluating:  11%|█▏        | 26/228 [00:07<00:57,  3.54it/s][Astep: 26
extend+tolist() time: 0.0007259845733642578

Evaluating:  12%|█▏        | 27/228 [00:07<00:53,  3.75it/s][Astep: 27
extend+tolist() time: 0.0013151168823242188

Evaluating:  12%|█▏        | 28/228 [00:07<00:55,  3.60it/s][Astep: 28
extend+tolist() time: 0.0008294582366943359

Evaluating:  13%|█▎        | 29/228 [00:08<01:00,  3.27it/s][Astep: 29
extend+tolist() time: 0.0007548332214355469

Evaluating:  13%|█▎        | 30/228 [00:08<00:56,  3.52it/s][Astep: 30
extend+tolist() time: 0.0015244483947753906

Evaluating:  14%|█▎        | 31/228 [00:08<00:55,  3.52it/s][Astep: 31
extend+tolist() time: 0.0005922317504882812

Evaluating:  14%|█▍        | 32/228 [00:08<00:52,  3.77it/s][Astep: 32
extend+tolist() time: 0.0014357566833496094

Evaluating:  14%|█▍        | 33/228 [00:09<00:52,  3.71it/s][Astep: 33
extend+tolist() time: 0.0012173652648925781

Evaluating:  15%|█▍        | 34/228 [00:09<00:54,  3.59it/s][Astep: 34
extend+tolist() time: 0.1535325050354004

Evaluating:  15%|█▌        | 35/228 [00:09<01:00,  3.17it/s][Astep: 35
extend+tolist() time: 0.0011212825775146484

Evaluating:  16%|█▌        | 36/228 [00:10<00:55,  3.48it/s][Astep: 36
extend+tolist() time: 0.0007650852203369141

Evaluating:  16%|█▌        | 37/228 [00:10<00:51,  3.70it/s][Astep: 37
extend+tolist() time: 0.0016407966613769531

Evaluating:  17%|█▋        | 38/228 [00:10<00:53,  3.56it/s][Astep: 38
extend+tolist() time: 0.0011589527130126953

Evaluating:  17%|█▋        | 39/228 [00:10<00:50,  3.75it/s][Astep: 39
extend+tolist() time: 0.0006952285766601562

Evaluating:  18%|█▊        | 40/228 [00:11<00:48,  3.91it/s][Astep: 40
extend+tolist() time: 0.0006325244903564453

Evaluating:  18%|█▊        | 41/228 [00:11<00:45,  4.07it/s][Astep: 41
extend+tolist() time: 0.0008227825164794922

Evaluating:  18%|█▊        | 42/228 [00:11<00:45,  4.09it/s][Astep: 42
extend+tolist() time: 0.0016739368438720703

Evaluating:  19%|█▉        | 43/228 [00:11<00:48,  3.81it/s][Astep: 43
extend+tolist() time: 0.0019032955169677734

Evaluating:  19%|█▉        | 44/228 [00:12<00:52,  3.51it/s][Astep: 44
extend+tolist() time: 0.0007359981536865234

Evaluating:  20%|█▉        | 45/228 [00:12<00:49,  3.73it/s][Astep: 45
extend+tolist() time: 0.0017743110656738281

Evaluating:  20%|██        | 46/228 [00:12<00:50,  3.59it/s][Astep: 46
extend+tolist() time: 0.0016276836395263672

Evaluating:  21%|██        | 47/228 [00:13<00:51,  3.49it/s][Astep: 47
extend+tolist() time: 0.0014917850494384766

Evaluating:  21%|██        | 48/228 [00:13<00:51,  3.51it/s][Astep: 48
extend+tolist() time: 0.0012309551239013672

Evaluating:  21%|██▏       | 49/228 [00:13<00:51,  3.45it/s][Astep: 49
extend+tolist() time: 0.0014929771423339844

Evaluating:  22%|██▏       | 50/228 [00:13<00:49,  3.58it/s][Astep: 50
extend+tolist() time: 0.0016663074493408203

Evaluating:  22%|██▏       | 51/228 [00:14<00:58,  3.02it/s][Astep: 51
extend+tolist() time: 0.0020020008087158203

Evaluating:  23%|██▎       | 52/228 [00:14<00:56,  3.10it/s][Astep: 52
extend+tolist() time: 0.001010894775390625

Evaluating:  23%|██▎       | 53/228 [00:14<00:53,  3.25it/s][Astep: 53
extend+tolist() time: 0.0017766952514648438

Evaluating:  24%|██▎       | 54/228 [00:15<00:53,  3.26it/s][Astep: 54
extend+tolist() time: 0.0012166500091552734

Evaluating:  24%|██▍       | 55/228 [00:15<00:49,  3.50it/s][Astep: 55
extend+tolist() time: 0.0008072853088378906

Evaluating:  25%|██▍       | 56/228 [00:15<00:46,  3.70it/s][Astep: 56
extend+tolist() time: 0.0017309188842773438

Evaluating:  25%|██▌       | 57/228 [00:16<00:48,  3.56it/s][Astep: 57
extend+tolist() time: 0.0006189346313476562

Evaluating:  25%|██▌       | 58/228 [00:16<00:44,  3.79it/s][Astep: 58
extend+tolist() time: 0.0013170242309570312

Evaluating:  26%|██▌       | 59/228 [00:16<00:43,  3.88it/s][Astep: 59
extend+tolist() time: 0.0009179115295410156

Evaluating:  26%|██▋       | 60/228 [00:16<00:50,  3.32it/s][Astep: 60
extend+tolist() time: 0.001188039779663086

Evaluating:  27%|██▋       | 61/228 [00:17<00:54,  3.05it/s][Astep: 61
extend+tolist() time: 0.0008308887481689453

Evaluating:  27%|██▋       | 62/228 [00:17<00:50,  3.31it/s][Astep: 62
extend+tolist() time: 0.0012187957763671875

Evaluating:  28%|██▊       | 63/228 [00:17<00:46,  3.56it/s][Astep: 63
extend+tolist() time: 0.0008320808410644531

Evaluating:  28%|██▊       | 64/228 [00:18<00:43,  3.75it/s][Astep: 64
extend+tolist() time: 0.0008244514465332031

Evaluating:  29%|██▊       | 65/228 [00:18<00:41,  3.88it/s][Astep: 65
extend+tolist() time: 0.001249551773071289

Evaluating:  29%|██▉       | 66/228 [00:18<00:40,  3.95it/s][Astep: 66
extend+tolist() time: 0.0007638931274414062

Evaluating:  29%|██▉       | 67/228 [00:18<00:39,  4.08it/s][Astep: 67
extend+tolist() time: 0.16802215576171875

Evaluating:  30%|██▉       | 68/228 [00:19<00:47,  3.37it/s][Astep: 68
extend+tolist() time: 0.0007412433624267578

Evaluating:  30%|███       | 69/228 [00:19<00:44,  3.61it/s][Astep: 69
extend+tolist() time: 0.0014874935150146484

Evaluating:  31%|███       | 70/228 [00:19<00:44,  3.58it/s][Astep: 70
extend+tolist() time: 0.0014743804931640625

Evaluating:  31%|███       | 71/228 [00:19<00:43,  3.59it/s][Astep: 71
extend+tolist() time: 0.0009558200836181641

Evaluating:  32%|███▏      | 72/228 [00:20<00:43,  3.61it/s][Astep: 72
extend+tolist() time: 0.0011997222900390625

Evaluating:  32%|███▏      | 73/228 [00:20<00:40,  3.81it/s][Astep: 73
extend+tolist() time: 0.000579833984375

Evaluating:  32%|███▏      | 74/228 [00:20<00:38,  3.97it/s][Astep: 74
extend+tolist() time: 0.001161813735961914

Evaluating:  33%|███▎      | 75/228 [00:20<00:37,  4.06it/s][Astep: 75
extend+tolist() time: 0.0016705989837646484

Evaluating:  33%|███▎      | 76/228 [00:21<00:40,  3.78it/s][Astep: 76
extend+tolist() time: 0.0006461143493652344

Evaluating:  34%|███▍      | 77/228 [00:21<00:38,  3.93it/s][Astep: 77
extend+tolist() time: 0.001967906951904297

Evaluating:  34%|███▍      | 78/228 [00:21<00:41,  3.58it/s][Astep: 78
extend+tolist() time: 0.001222848892211914

Evaluating:  35%|███▍      | 79/228 [00:22<00:40,  3.69it/s][Astep: 79
extend+tolist() time: 0.0008943080902099609

Evaluating:  35%|███▌      | 80/228 [00:22<00:39,  3.79it/s][Astep: 80
extend+tolist() time: 0.0013763904571533203

Evaluating:  36%|███▌      | 81/228 [00:22<00:38,  3.85it/s][Astep: 81
extend+tolist() time: 0.0008661746978759766

Evaluating:  36%|███▌      | 82/228 [00:22<00:37,  3.93it/s][Astep: 82
extend+tolist() time: 0.001310110092163086

Evaluating:  36%|███▋      | 83/228 [00:23<00:36,  3.98it/s][Astep: 83
extend+tolist() time: 0.0007040500640869141

Evaluating:  37%|███▋      | 84/228 [00:23<00:35,  4.11it/s][Astep: 84
extend+tolist() time: 0.0014748573303222656

Evaluating:  37%|███▋      | 85/228 [00:23<00:36,  3.97it/s][Astep: 85
extend+tolist() time: 0.0009505748748779297

Evaluating:  38%|███▊      | 86/228 [00:23<00:35,  3.97it/s][Astep: 86
extend+tolist() time: 0.0013582706451416016

Evaluating:  38%|███▊      | 87/228 [00:24<00:35,  4.00it/s][Astep: 87
extend+tolist() time: 0.0012848377227783203

Evaluating:  39%|███▊      | 88/228 [00:24<00:34,  4.00it/s][Astep: 88
extend+tolist() time: 0.0008106231689453125

Evaluating:  39%|███▉      | 89/228 [00:24<00:34,  4.09it/s][Astep: 89
extend+tolist() time: 0.00152587890625

Evaluating:  39%|███▉      | 90/228 [00:24<00:33,  4.16it/s][Astep: 90
extend+tolist() time: 0.0009522438049316406

Evaluating:  40%|███▉      | 91/228 [00:24<00:33,  4.05it/s][Astep: 91
extend+tolist() time: 0.0012204647064208984

Evaluating:  40%|████      | 92/228 [00:25<00:39,  3.42it/s][Astep: 92
extend+tolist() time: 0.000812530517578125

Evaluating:  41%|████      | 93/228 [00:25<00:36,  3.66it/s][Astep: 93
extend+tolist() time: 0.0014643669128417969

Evaluating:  41%|████      | 94/228 [00:25<00:36,  3.67it/s][Astep: 94
extend+tolist() time: 0.000736236572265625

Evaluating:  42%|████▏     | 95/228 [00:26<00:34,  3.88it/s][Astep: 95
extend+tolist() time: 0.001626729965209961

Evaluating:  42%|████▏     | 96/228 [00:26<00:35,  3.70it/s][Astep: 96
extend+tolist() time: 0.001338958740234375

Evaluating:  43%|████▎     | 97/228 [00:26<00:34,  3.78it/s][Astep: 97
extend+tolist() time: 0.0008218288421630859

Evaluating:  43%|████▎     | 98/228 [00:26<00:33,  3.89it/s][Astep: 98
extend+tolist() time: 0.0013263225555419922

Evaluating:  43%|████▎     | 99/228 [00:27<00:32,  3.92it/s][Astep: 99
extend+tolist() time: 0.0008656978607177734

Evaluating:  44%|████▍     | 100/228 [00:27<00:32,  3.96it/s][Astep: 100
extend+tolist() time: 0.00119781494140625

Evaluating:  44%|████▍     | 101/228 [00:27<00:31,  4.10it/s][Astep: 101
extend+tolist() time: 0.0008356571197509766

Evaluating:  45%|████▍     | 102/228 [00:27<00:30,  4.11it/s][Astep: 102
extend+tolist() time: 0.0011436939239501953

Evaluating:  45%|████▌     | 103/228 [00:28<00:29,  4.20it/s][Astep: 103
extend+tolist() time: 0.0007565021514892578

Evaluating:  46%|████▌     | 104/228 [00:28<00:36,  3.42it/s][Astep: 104
extend+tolist() time: 0.0011146068572998047

Evaluating:  46%|████▌     | 105/228 [00:28<00:39,  3.08it/s][Astep: 105
extend+tolist() time: 0.0008547306060791016

Evaluating:  46%|████▋     | 106/228 [00:29<00:36,  3.34it/s][Astep: 106
extend+tolist() time: 0.0017955303192138672

Evaluating:  47%|████▋     | 107/228 [00:29<00:36,  3.28it/s][Astep: 107
extend+tolist() time: 0.0011115074157714844

Evaluating:  47%|████▋     | 108/228 [00:29<00:33,  3.53it/s][Astep: 108
extend+tolist() time: 0.0008087158203125

Evaluating:  48%|████▊     | 109/228 [00:29<00:31,  3.73it/s][Astep: 109
extend+tolist() time: 0.0012657642364501953

Evaluating:  48%|████▊     | 110/228 [00:30<00:30,  3.84it/s][Astep: 110
extend+tolist() time: 0.0006268024444580078

Evaluating:  49%|████▊     | 111/228 [00:30<00:29,  4.03it/s][Astep: 111
extend+tolist() time: 0.0018029212951660156

Evaluating:  49%|████▉     | 112/228 [00:30<00:31,  3.70it/s][Astep: 112
extend+tolist() time: 0.00040841102600097656

Evaluating:  50%|████▉     | 113/228 [00:30<00:29,  3.92it/s][Astep: 113
extend+tolist() time: 0.0010879039764404297

Evaluating:  50%|█████     | 114/228 [00:31<00:27,  4.08it/s][Astep: 114
extend+tolist() time: 0.1823129653930664

Evaluating:  50%|█████     | 115/228 [00:31<00:34,  3.23it/s][Astep: 115
extend+tolist() time: 0.0011250972747802734

Evaluating:  51%|█████     | 116/228 [00:31<00:31,  3.52it/s][Astep: 116
extend+tolist() time: 0.00080108642578125

Evaluating:  51%|█████▏    | 117/228 [00:32<00:29,  3.73it/s][Astep: 117
extend+tolist() time: 0.0012192726135253906

Evaluating:  52%|█████▏    | 118/228 [00:32<00:28,  3.84it/s][Astep: 118
extend+tolist() time: 0.0005621910095214844

Evaluating:  52%|█████▏    | 119/228 [00:32<00:27,  4.02it/s][Astep: 119
extend+tolist() time: 0.0010707378387451172

Evaluating:  53%|█████▎    | 120/228 [00:32<00:26,  4.14it/s][Astep: 120
extend+tolist() time: 0.0006282329559326172

Evaluating:  53%|█████▎    | 121/228 [00:32<00:25,  4.24it/s][Astep: 121
extend+tolist() time: 0.0010344982147216797

Evaluating:  54%|█████▎    | 122/228 [00:33<00:24,  4.31it/s][Astep: 122
extend+tolist() time: 0.0007216930389404297

Evaluating:  54%|█████▍    | 123/228 [00:33<00:24,  4.36it/s][Astep: 123
extend+tolist() time: 0.001024484634399414

Evaluating:  54%|█████▍    | 124/228 [00:33<00:23,  4.40it/s][Astep: 124
extend+tolist() time: 0.0008609294891357422

Evaluating:  55%|█████▍    | 125/228 [00:33<00:23,  4.34it/s][Astep: 125
extend+tolist() time: 0.0004451274871826172

Evaluating:  55%|█████▌    | 126/228 [00:34<00:23,  4.42it/s][Astep: 126
extend+tolist() time: 0.0018100738525390625

Evaluating:  56%|█████▌    | 127/228 [00:34<00:25,  3.93it/s][Astep: 127
extend+tolist() time: 0.0016977787017822266

Evaluating:  56%|█████▌    | 128/228 [00:34<00:27,  3.61it/s][Astep: 128
extend+tolist() time: 0.0007359981536865234

Evaluating:  57%|█████▋    | 129/228 [00:34<00:25,  3.84it/s][Astep: 129
extend+tolist() time: 0.0012013912200927734

Evaluating:  57%|█████▋    | 130/228 [00:35<00:24,  4.00it/s][Astep: 130
extend+tolist() time: 0.0012691020965576172

Evaluating:  57%|█████▋    | 131/228 [00:35<00:24,  4.01it/s][Astep: 131
extend+tolist() time: 0.0004487037658691406

Evaluating:  58%|█████▊    | 132/228 [00:35<00:23,  4.17it/s][Astep: 132
extend+tolist() time: 0.0010645389556884766

Evaluating:  58%|█████▊    | 133/228 [00:35<00:23,  3.98it/s][Astep: 133
extend+tolist() time: 0.0008780956268310547

Evaluating:  59%|█████▉    | 134/228 [00:36<00:22,  4.14it/s][Astep: 134
extend+tolist() time: 0.0009872913360595703

Evaluating:  59%|█████▉    | 135/228 [00:36<00:23,  4.01it/s][Astep: 135
extend+tolist() time: 0.0008492469787597656

Evaluating:  60%|█████▉    | 136/228 [00:36<00:21,  4.19it/s][Astep: 136
extend+tolist() time: 0.0007951259613037109

Evaluating:  60%|██████    | 137/228 [00:36<00:21,  4.22it/s][Astep: 137
extend+tolist() time: 0.0003712177276611328

Evaluating:  61%|██████    | 138/228 [00:37<00:20,  4.33it/s][Astep: 138
extend+tolist() time: 0.0012145042419433594

Evaluating:  61%|██████    | 139/228 [00:37<00:20,  4.39it/s][Astep: 139
extend+tolist() time: 0.0004477500915527344

Evaluating:  61%|██████▏   | 140/228 [00:37<00:19,  4.45it/s][Astep: 140
extend+tolist() time: 0.0007538795471191406

Evaluating:  62%|██████▏   | 141/228 [00:37<00:19,  4.45it/s][Astep: 141
extend+tolist() time: 0.0008046627044677734

Evaluating:  62%|██████▏   | 142/228 [00:37<00:19,  4.43it/s][Astep: 142
extend+tolist() time: 0.0005865097045898438

Evaluating:  63%|██████▎   | 143/228 [00:38<00:19,  4.46it/s][Astep: 143
extend+tolist() time: 0.0003552436828613281

Evaluating:  63%|██████▎   | 144/228 [00:38<00:18,  4.51it/s][Astep: 144
extend+tolist() time: 0.0012099742889404297

Evaluating:  64%|██████▎   | 145/228 [00:38<00:18,  4.51it/s][Astep: 145
extend+tolist() time: 0.00048089027404785156

Evaluating:  64%|██████▍   | 146/228 [00:38<00:18,  4.51it/s][Astep: 146
extend+tolist() time: 0.0003879070281982422

Evaluating:  64%|██████▍   | 147/228 [00:39<00:17,  4.54it/s][Astep: 147
extend+tolist() time: 0.0011599063873291016

Evaluating:  65%|██████▍   | 148/228 [00:39<00:17,  4.50it/s][Astep: 148
extend+tolist() time: 0.0006427764892578125

Evaluating:  65%|██████▌   | 149/228 [00:39<00:17,  4.51it/s][Astep: 149
extend+tolist() time: 0.0003478527069091797

Evaluating:  66%|██████▌   | 150/228 [00:39<00:17,  4.53it/s][Astep: 150
extend+tolist() time: 0.001264333724975586

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.47it/s][Astep: 151
extend+tolist() time: 0.0005676746368408203

Evaluating:  67%|██████▋   | 152/228 [00:40<00:16,  4.52it/s][Astep: 152
extend+tolist() time: 0.0012137889862060547

Evaluating:  67%|██████▋   | 153/228 [00:40<00:16,  4.48it/s][Astep: 153
extend+tolist() time: 0.0008862018585205078

Evaluating:  68%|██████▊   | 154/228 [00:40<00:16,  4.38it/s][Astep: 154
extend+tolist() time: 0.0018801689147949219

Evaluating:  68%|██████▊   | 155/228 [00:41<00:23,  3.10it/s][Astep: 155
extend+tolist() time: 0.0009582042694091797

Evaluating:  68%|██████▊   | 156/228 [00:41<00:21,  3.43it/s][Astep: 156
extend+tolist() time: 0.000492095947265625

Evaluating:  69%|██████▉   | 157/228 [00:41<00:19,  3.71it/s][Astep: 157
extend+tolist() time: 0.0006008148193359375

Evaluating:  69%|██████▉   | 158/228 [00:41<00:17,  3.93it/s][Astep: 158
extend+tolist() time: 0.0008859634399414062

Evaluating:  70%|██████▉   | 159/228 [00:42<00:16,  4.10it/s][Astep: 159
extend+tolist() time: 0.0006542205810546875

Evaluating:  70%|███████   | 160/228 [00:42<00:16,  4.22it/s][Astep: 160
extend+tolist() time: 0.0003712177276611328

Evaluating:  71%|███████   | 161/228 [00:42<00:15,  4.33it/s][Astep: 161
extend+tolist() time: 0.0011322498321533203

Evaluating:  71%|███████   | 162/228 [00:42<00:15,  4.38it/s][Astep: 162
extend+tolist() time: 0.00044274330139160156

Evaluating:  71%|███████▏  | 163/228 [00:42<00:14,  4.44it/s][Astep: 163
extend+tolist() time: 0.0003886222839355469

Evaluating:  72%|███████▏  | 164/228 [00:43<00:14,  4.50it/s][Astep: 164
extend+tolist() time: 0.0005125999450683594

Evaluating:  72%|███████▏  | 165/228 [00:43<00:13,  4.54it/s][Astep: 165
extend+tolist() time: 0.000888824462890625

Evaluating:  73%|███████▎  | 166/228 [00:43<00:13,  4.56it/s][Astep: 166
extend+tolist() time: 0.0003409385681152344

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.57it/s][Astep: 167
extend+tolist() time: 0.0005204677581787109

Evaluating:  74%|███████▎  | 168/228 [00:44<00:13,  4.58it/s][Astep: 168
extend+tolist() time: 0.001481771469116211

Evaluating:  74%|███████▍  | 169/228 [00:44<00:13,  4.27it/s][Astep: 169
extend+tolist() time: 0.00035953521728515625

Evaluating:  75%|███████▍  | 170/228 [00:44<00:13,  4.37it/s][Astep: 170
extend+tolist() time: 0.0009045600891113281

Evaluating:  75%|███████▌  | 171/228 [00:44<00:13,  4.36it/s][Astep: 171
extend+tolist() time: 0.0007276535034179688

Evaluating:  75%|███████▌  | 172/228 [00:44<00:12,  4.43it/s][Astep: 172
extend+tolist() time: 0.0007965564727783203

Evaluating:  76%|███████▌  | 173/228 [00:45<00:12,  4.45it/s][Astep: 173
extend+tolist() time: 0.0016160011291503906

Evaluating:  76%|███████▋  | 174/228 [00:45<00:12,  4.20it/s][Astep: 174
extend+tolist() time: 0.0014650821685791016

Evaluating:  77%|███████▋  | 175/228 [00:45<00:16,  3.13it/s][Astep: 175
extend+tolist() time: 0.001291036605834961

Evaluating:  77%|███████▋  | 176/228 [00:46<00:18,  2.85it/s][Astep: 176
extend+tolist() time: 0.0006842613220214844

Evaluating:  78%|███████▊  | 177/228 [00:46<00:15,  3.21it/s][Astep: 177
extend+tolist() time: 0.0009734630584716797

Evaluating:  78%|███████▊  | 178/228 [00:46<00:14,  3.51it/s][Astep: 178
extend+tolist() time: 0.0011949539184570312

Evaluating:  79%|███████▊  | 179/228 [00:47<00:13,  3.55it/s][Astep: 179
extend+tolist() time: 0.00040149688720703125

Evaluating:  79%|███████▉  | 180/228 [00:47<00:12,  3.82it/s][Astep: 180
extend+tolist() time: 0.0008251667022705078

Evaluating:  79%|███████▉  | 181/228 [00:47<00:11,  4.03it/s][Astep: 181
extend+tolist() time: 0.0006396770477294922

Evaluating:  80%|███████▉  | 182/228 [00:47<00:11,  4.17it/s][Astep: 182
extend+tolist() time: 0.0007534027099609375

Evaluating:  80%|████████  | 183/228 [00:47<00:10,  4.26it/s][Astep: 183
extend+tolist() time: 0.001096963882446289

Evaluating:  81%|████████  | 184/228 [00:48<00:10,  4.34it/s][Astep: 184
extend+tolist() time: 0.00044918060302734375

Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.42it/s][Astep: 185
extend+tolist() time: 0.0015156269073486328

Evaluating:  82%|████████▏ | 186/228 [00:48<00:10,  4.18it/s][Astep: 186
extend+tolist() time: 0.0010619163513183594

Evaluating:  82%|████████▏ | 187/228 [00:48<00:09,  4.15it/s][Astep: 187
extend+tolist() time: 0.000438690185546875

Evaluating:  82%|████████▏ | 188/228 [00:49<00:09,  4.28it/s][Astep: 188
extend+tolist() time: 0.001161813735961914

Evaluating:  83%|████████▎ | 189/228 [00:49<00:08,  4.35it/s][Astep: 189
extend+tolist() time: 0.0003705024719238281

Evaluating:  83%|████████▎ | 190/228 [00:49<00:08,  4.43it/s][Astep: 190
extend+tolist() time: 0.0015480518341064453

Evaluating:  84%|████████▍ | 191/228 [00:49<00:08,  4.17it/s][Astep: 191
extend+tolist() time: 0.0006978511810302734

Evaluating:  84%|████████▍ | 192/228 [00:50<00:08,  4.26it/s][Astep: 192
extend+tolist() time: 0.00045299530029296875

Evaluating:  85%|████████▍ | 193/228 [00:50<00:08,  4.35it/s][Astep: 193
extend+tolist() time: 0.20804619789123535

Evaluating:  85%|████████▌ | 194/228 [00:50<00:10,  3.37it/s][Astep: 194
extend+tolist() time: 0.000568389892578125

Evaluating:  86%|████████▌ | 195/228 [00:50<00:09,  3.65it/s][Astep: 195
extend+tolist() time: 0.0009300708770751953

Evaluating:  86%|████████▌ | 196/228 [00:51<00:08,  3.88it/s][Astep: 196
extend+tolist() time: 0.0006456375122070312

Evaluating:  86%|████████▋ | 197/228 [00:51<00:07,  4.05it/s][Astep: 197
extend+tolist() time: 0.0006721019744873047

Evaluating:  87%|████████▋ | 198/228 [00:51<00:07,  4.18it/s][Astep: 198
extend+tolist() time: 0.0010297298431396484

Evaluating:  87%|████████▋ | 199/228 [00:51<00:06,  4.27it/s][Astep: 199
extend+tolist() time: 0.0018076896667480469

Evaluating:  88%|████████▊ | 200/228 [00:52<00:07,  3.92it/s][Astep: 200
extend+tolist() time: 0.0007262229919433594

Evaluating:  88%|████████▊ | 201/228 [00:52<00:06,  4.09it/s][Astep: 201
extend+tolist() time: 0.0006244182586669922

Evaluating:  89%|████████▊ | 202/228 [00:52<00:06,  4.22it/s][Astep: 202
extend+tolist() time: 0.003091096878051758

Evaluating:  89%|████████▉ | 203/228 [00:52<00:05,  4.31it/s][Astep: 203
extend+tolist() time: 0.0009634494781494141

Evaluating:  89%|████████▉ | 204/228 [00:53<00:05,  4.40it/s][Astep: 204
extend+tolist() time: 0.00040411949157714844

Evaluating:  90%|████████▉ | 205/228 [00:53<00:05,  4.47it/s][Astep: 205
extend+tolist() time: 0.0002875328063964844

Evaluating:  90%|█████████ | 206/228 [00:53<00:04,  4.52it/s][Astep: 206
extend+tolist() time: 0.0006401538848876953

Evaluating:  91%|█████████ | 207/228 [00:53<00:04,  4.52it/s][Astep: 207
extend+tolist() time: 0.0010526180267333984

Evaluating:  91%|█████████ | 208/228 [00:53<00:04,  4.50it/s][Astep: 208
extend+tolist() time: 0.0006814002990722656

Evaluating:  92%|█████████▏| 209/228 [00:54<00:04,  4.51it/s][Astep: 209
extend+tolist() time: 0.0006189346313476562

Evaluating:  92%|█████████▏| 210/228 [00:54<00:03,  4.52it/s][Astep: 210
extend+tolist() time: 0.0010573863983154297

Evaluating:  93%|█████████▎| 211/228 [00:54<00:03,  4.53it/s][Astep: 211
extend+tolist() time: 0.0015263557434082031

Evaluating:  93%|█████████▎| 212/228 [00:54<00:03,  4.24it/s][Astep: 212
extend+tolist() time: 0.0013535022735595703

Evaluating:  93%|█████████▎| 213/228 [00:55<00:03,  4.27it/s][Astep: 213
extend+tolist() time: 0.0007901191711425781

Evaluating:  94%|█████████▍| 214/228 [00:55<00:03,  4.31it/s][Astep: 214
extend+tolist() time: 0.0012981891632080078

Evaluating:  94%|█████████▍| 215/228 [00:55<00:02,  4.36it/s][Astep: 215
extend+tolist() time: 0.0006961822509765625

Evaluating:  95%|█████████▍| 216/228 [00:55<00:02,  4.42it/s][Astep: 216
extend+tolist() time: 0.0005965232849121094

Evaluating:  95%|█████████▌| 217/228 [00:55<00:02,  4.45it/s][Astep: 217
extend+tolist() time: 0.0010116100311279297

Evaluating:  96%|█████████▌| 218/228 [00:56<00:02,  4.48it/s][Astep: 218
extend+tolist() time: 0.0010833740234375

Evaluating:  96%|█████████▌| 219/228 [00:56<00:02,  4.36it/s][Astep: 219
extend+tolist() time: 0.0009300708770751953

Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.43it/s][Astep: 220
extend+tolist() time: 0.00042891502380371094

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.44it/s][Astep: 221
extend+tolist() time: 0.0006885528564453125

Evaluating:  97%|█████████▋| 222/228 [00:57<00:01,  4.46it/s][Astep: 222
extend+tolist() time: 0.00044465065002441406

Evaluating:  98%|█████████▊| 223/228 [00:57<00:01,  4.50it/s][Astep: 223
extend+tolist() time: 0.0003845691680908203

Evaluating:  98%|█████████▊| 224/228 [00:57<00:00,  4.53it/s][Astep: 224
extend+tolist() time: 0.00037169456481933594

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.55it/s][Astep: 225
extend+tolist() time: 0.00046634674072265625

Evaluating:  99%|█████████▉| 226/228 [00:57<00:00,  4.56it/s][Astep: 226
extend+tolist() time: 0.0006024837493896484

Evaluating: 100%|█████████▉| 227/228 [00:58<00:00,  4.56it/s][Astep: 227
extend+tolist() time: 0.0005095005035400391

Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.95it/s][A09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:25:04 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:25:05 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:25:05 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 17:25:06 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 17:25:06 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 17:25:06 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:00<00:00,  3.75it/s]
09/05/2023 17:25:06 - INFO - __main__ -   Step: 4800, Validation Metrics: {'pred_1_num': 9797, 'pred_-1_num': 819, 'pred_0_num': 185, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7897416905842052, 'f1_micro': 0.7897416905842052, 'f1_macro': 0.44260928863065957, 'f1_weighted': 0.7547901715541264, 'f1_-1': 0.33207547169811324, 'f1_0': 0.11650485436893206, 'f1_1': 0.8792475398249333, 'precision_micro': 0.7897416905842052, 'precision_macro': 0.5227768811314796, 'precision_weighted': 0.7423137128579571, 'precision_-1': 0.4835164835164835, 'precision_0': 0.2594594594594595, 'precision_1': 0.8253547004184955, 'recall_micro': 0.7897416905842052, 'recall_macro': 0.42288700440565696, 'recall_weighted': 0.7897416905842052, 'recall_-1': 0.25287356321839083, 'recall_0': 0.07511737089201878, 'recall_1': 0.9406700791065612, 'roc_auc_micro': 0.9041910992632443, 'roc_auc_macro': 0.7084730469644301, 'roc_auc_weighted': 0.6920406375283754, 'roc_auc_-1': 0.7665818236884084, 'roc_auc_0': 0.6794397120328303, 'roc_auc_1': 0.6793976051720517}
[2023-09-05 17:25:12,997] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4801/66600 [3:21:24<555:15:51, 32.35s/it]09/05/2023 17:25:20 - INFO - __main__ -   Step: 4801, LR: 1.913166686668396e-05, Loss: 0.09305405616760254
[2023-09-05 17:25:27,390] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4802/66600 [3:21:38<463:23:43, 26.99s/it]09/05/2023 17:25:35 - INFO - __main__ -   Step: 4802, LR: 1.913135716249018e-05, Loss: 0.11804753541946411
[2023-09-05 17:25:41,383] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4803/66600 [3:21:52<396:15:42, 23.08s/it]09/05/2023 17:25:49 - INFO - __main__ -   Step: 4803, LR: 1.9131047458296397e-05, Loss: 0.10347673296928406
[2023-09-05 17:25:55,182] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4804/66600 [3:22:06<348:19:21, 20.29s/it]09/05/2023 17:26:02 - INFO - __main__ -   Step: 4804, LR: 1.9130737754102616e-05, Loss: 0.11265299469232559
[2023-09-05 17:26:09,943] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4805/66600 [3:22:20<316:38:18, 18.45s/it]09/05/2023 17:26:17 - INFO - __main__ -   Step: 4805, LR: 1.913042804990883e-05, Loss: 0.14152756333351135
[2023-09-05 17:26:23,412] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4806/66600 [3:22:34<295:55:36, 17.24s/it]09/05/2023 17:26:31 - INFO - __main__ -   Step: 4806, LR: 1.9130118345715048e-05, Loss: 0.13268564641475677
[2023-09-05 17:26:37,414] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4807/66600 [3:22:49<283:31:24, 16.52s/it]09/05/2023 17:26:46 - INFO - __main__ -   Step: 4807, LR: 1.912980864152127e-05, Loss: 0.11579816788434982
[2023-09-05 17:26:53,177] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4808/66600 [3:23:03<271:19:43, 15.81s/it]09/05/2023 17:27:00 - INFO - __main__ -   Step: 4808, LR: 1.9129498937327488e-05, Loss: 0.11169657111167908
[2023-09-05 17:27:07,422] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4809/66600 [3:23:19<267:30:51, 15.59s/it]09/05/2023 17:27:15 - INFO - __main__ -   Step: 4809, LR: 1.9129189233133706e-05, Loss: 0.09920110553503036
[2023-09-05 17:27:22,281] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4810/66600 [3:23:33<260:03:51, 15.15s/it]09/05/2023 17:27:29 - INFO - __main__ -   Step: 4810, LR: 1.9128879528939924e-05, Loss: 0.12856554985046387
[2023-09-05 17:27:36,330] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4811/66600 [3:23:47<253:54:09, 14.79s/it]09/05/2023 17:27:43 - INFO - __main__ -   Step: 4811, LR: 1.9128569824746142e-05, Loss: 0.14275941252708435
[2023-09-05 17:27:49,786] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4812/66600 [3:24:00<247:18:43, 14.41s/it]09/05/2023 17:27:57 - INFO - __main__ -   Step: 4812, LR: 1.912826012055236e-05, Loss: 0.13022735714912415
[2023-09-05 17:28:03,571] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4813/66600 [3:24:14<243:03:31, 14.16s/it]09/05/2023 17:28:10 - INFO - __main__ -   Step: 4813, LR: 1.9127950416358575e-05, Loss: 0.13920807838439941
[2023-09-05 17:28:16,649] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4814/66600 [3:24:28<241:28:20, 14.07s/it]09/05/2023 17:28:24 - INFO - __main__ -   Step: 4814, LR: 1.9127640712164796e-05, Loss: 0.12115895748138428
[2023-09-05 17:28:30,390] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4815/66600 [3:24:41<240:12:58, 14.00s/it]09/05/2023 17:28:38 - INFO - __main__ -   Step: 4815, LR: 1.9127331007971014e-05, Loss: 0.11894914507865906
[2023-09-05 17:28:44,885] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4816/66600 [3:24:55<239:44:42, 13.97s/it]09/05/2023 17:28:52 - INFO - __main__ -   Step: 4816, LR: 1.9127021303777232e-05, Loss: 0.10388818383216858
[2023-09-05 17:28:58,400] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4817/66600 [3:25:09<237:23:30, 13.83s/it]09/05/2023 17:29:05 - INFO - __main__ -   Step: 4817, LR: 1.912671159958345e-05, Loss: 0.13568803668022156
[2023-09-05 17:29:12,540] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4818/66600 [3:25:23<240:12:50, 14.00s/it]09/05/2023 17:29:20 - INFO - __main__ -   Step: 4818, LR: 1.912640189538967e-05, Loss: 0.15430517494678497
[2023-09-05 17:29:26,713] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4819/66600 [3:25:37<238:47:51, 13.91s/it]09/05/2023 17:29:33 - INFO - __main__ -   Step: 4819, LR: 1.9126092191195886e-05, Loss: 0.07888559252023697
[2023-09-05 17:29:39,767] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4820/66600 [3:25:51<237:09:03, 13.82s/it]09/05/2023 17:29:47 - INFO - __main__ -   Step: 4820, LR: 1.9125782487002104e-05, Loss: 0.14069388806819916
[2023-09-05 17:29:53,882] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4821/66600 [3:26:05<241:47:42, 14.09s/it]09/05/2023 17:30:02 - INFO - __main__ -   Step: 4821, LR: 1.9125472782808323e-05, Loss: 0.1289963573217392
[2023-09-05 17:30:08,593] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4822/66600 [3:26:19<240:00:19, 13.99s/it]09/05/2023 17:30:15 - INFO - __main__ -   Step: 4822, LR: 1.912516307861454e-05, Loss: 0.1590101569890976
[2023-09-05 17:30:22,873] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4823/66600 [3:26:34<245:11:47, 14.29s/it]09/05/2023 17:30:30 - INFO - __main__ -   Step: 4823, LR: 1.912485337442076e-05, Loss: 0.12113281339406967
[2023-09-05 17:30:37,715] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4824/66600 [3:26:49<247:10:11, 14.40s/it]09/05/2023 17:30:45 - INFO - __main__ -   Step: 4824, LR: 1.9124543670226977e-05, Loss: 0.1102646142244339
[2023-09-05 17:30:52,135] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4825/66600 [3:27:03<245:25:05, 14.30s/it]09/05/2023 17:30:59 - INFO - __main__ -   Step: 4825, LR: 1.9124233966033195e-05, Loss: 0.10873304307460785
[2023-09-05 17:31:06,223] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4826/66600 [3:27:17<246:37:13, 14.37s/it]09/05/2023 17:31:14 - INFO - __main__ -   Step: 4826, LR: 1.9123924261839413e-05, Loss: 0.11314816772937775
[2023-09-05 17:31:20,883] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4827/66600 [3:27:32<246:20:23, 14.36s/it]09/05/2023 17:31:28 - INFO - __main__ -   Step: 4827, LR: 1.912361455764563e-05, Loss: 0.15699627995491028
[2023-09-05 17:31:35,404] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4828/66600 [3:27:46<244:47:48, 14.27s/it]09/05/2023 17:31:42 - INFO - __main__ -   Step: 4828, LR: 1.912330485345185e-05, Loss: 0.17620104551315308
[2023-09-05 17:31:49,139] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4829/66600 [3:28:00<247:07:58, 14.40s/it]09/05/2023 17:31:57 - INFO - __main__ -   Step: 4829, LR: 1.9122995149258067e-05, Loss: 0.13100925087928772
[2023-09-05 17:32:03,666] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4830/66600 [3:28:14<243:47:12, 14.21s/it]09/05/2023 17:32:11 - INFO - __main__ -   Step: 4830, LR: 1.9122685445064285e-05, Loss: 0.17991280555725098
[2023-09-05 17:32:18,086] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4831/66600 [3:28:29<246:29:55, 14.37s/it]09/05/2023 17:32:25 - INFO - __main__ -   Step: 4831, LR: 1.9122375740870503e-05, Loss: 0.12927162647247314
[2023-09-05 17:32:32,019] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4832/66600 [3:28:42<241:41:06, 14.09s/it]09/05/2023 17:32:39 - INFO - __main__ -   Step: 4832, LR: 1.912206603667672e-05, Loss: 0.15185518562793732
[2023-09-05 17:32:45,723] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4833/66600 [3:28:57<244:01:18, 14.22s/it]09/05/2023 17:32:53 - INFO - __main__ -   Step: 4833, LR: 1.912175633248294e-05, Loss: 0.08201023936271667
[2023-09-05 17:32:59,876] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4834/66600 [3:29:11<243:00:41, 14.16s/it]09/05/2023 17:33:07 - INFO - __main__ -   Step: 4834, LR: 1.9121446628289157e-05, Loss: 0.12868382036685944
[2023-09-05 17:33:14,572] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4835/66600 [3:29:25<241:12:42, 14.06s/it]09/05/2023 17:33:21 - INFO - __main__ -   Step: 4835, LR: 1.9121136924095375e-05, Loss: 0.1997252106666565
[2023-09-05 17:33:28,926] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4836/66600 [3:29:40<245:54:33, 14.33s/it]09/05/2023 17:33:36 - INFO - __main__ -   Step: 4836, LR: 1.9120827219901593e-05, Loss: 0.12721893191337585
[2023-09-05 17:33:43,633] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4837/66600 [3:29:54<247:11:57, 14.41s/it]09/05/2023 17:33:51 - INFO - __main__ -   Step: 4837, LR: 1.912051751570781e-05, Loss: 0.10987745225429535
[2023-09-05 17:33:58,469] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4838/66600 [3:30:10<251:38:23, 14.67s/it]09/05/2023 17:34:06 - INFO - __main__ -   Step: 4838, LR: 1.912020781151403e-05, Loss: 0.10502508282661438
[2023-09-05 17:34:13,600] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4839/66600 [3:30:24<253:00:09, 14.75s/it]09/05/2023 17:34:21 - INFO - __main__ -   Step: 4839, LR: 1.9119898107320248e-05, Loss: 0.1278080940246582
[2023-09-05 17:34:28,161] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4840/66600 [3:30:38<248:11:27, 14.47s/it]09/05/2023 17:34:35 - INFO - __main__ -   Step: 4840, LR: 1.9119588403126466e-05, Loss: 0.11910925805568695
[2023-09-05 17:34:42,393] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4841/66600 [3:30:53<247:31:33, 14.43s/it]09/05/2023 17:34:49 - INFO - __main__ -   Step: 4841, LR: 1.9119278698932684e-05, Loss: 0.15774565935134888
[2023-09-05 17:34:55,704] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4842/66600 [3:31:06<242:41:21, 14.15s/it]09/05/2023 17:35:03 - INFO - __main__ -   Step: 4842, LR: 1.9118968994738902e-05, Loss: 0.11505016684532166
[2023-09-05 17:35:09,770] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4843/66600 [3:31:20<243:16:33, 14.18s/it]09/05/2023 17:35:17 - INFO - __main__ -   Step: 4843, LR: 1.911865929054512e-05, Loss: 0.09884437918663025
[2023-09-05 17:35:23,579] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4844/66600 [3:31:35<243:26:50, 14.19s/it]09/05/2023 17:35:31 - INFO - __main__ -   Step: 4844, LR: 1.9118349586351338e-05, Loss: 0.12367520481348038
[2023-09-05 17:35:37,111] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4845/66600 [3:31:48<240:28:12, 14.02s/it]09/05/2023 17:35:45 - INFO - __main__ -   Step: 4845, LR: 1.9118039882157556e-05, Loss: 0.11654320359230042
[2023-09-05 17:35:52,184] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4846/66600 [3:32:03<244:05:27, 14.23s/it]09/05/2023 17:35:59 - INFO - __main__ -   Step: 4846, LR: 1.9117730177963774e-05, Loss: 0.08679176867008209
[2023-09-05 17:36:06,115] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4847/66600 [3:32:17<242:31:34, 14.14s/it]09/05/2023 17:36:13 - INFO - __main__ -   Step: 4847, LR: 1.9117420473769992e-05, Loss: 0.10279785841703415
[2023-09-05 17:36:20,171] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4848/66600 [3:32:31<241:13:51, 14.06s/it]09/05/2023 17:36:27 - INFO - __main__ -   Step: 4848, LR: 1.911711076957621e-05, Loss: 0.10113926231861115
[2023-09-05 17:36:33,853] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4849/66600 [3:32:45<239:51:47, 13.98s/it]09/05/2023 17:36:41 - INFO - __main__ -   Step: 4849, LR: 1.9116801065382428e-05, Loss: 0.11036285758018494
[2023-09-05 17:36:47,902] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4850/66600 [3:33:00<246:39:07, 14.38s/it]09/05/2023 17:36:56 - INFO - __main__ -   Step: 4850, LR: 1.9116491361188646e-05, Loss: 0.12158867716789246
[2023-09-05 17:37:03,341] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4851/66600 [3:33:15<249:26:24, 14.54s/it]09/05/2023 17:37:11 - INFO - __main__ -   Step: 4851, LR: 1.9116181656994864e-05, Loss: 0.1481655091047287
[2023-09-05 17:37:18,076] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4852/66600 [3:33:28<244:57:48, 14.28s/it]09/05/2023 17:37:25 - INFO - __main__ -   Step: 4852, LR: 1.9115871952801082e-05, Loss: 0.12387111037969589
[2023-09-05 17:37:32,780] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4853/66600 [3:33:43<248:21:32, 14.48s/it]09/05/2023 17:37:40 - INFO - __main__ -   Step: 4853, LR: 1.91155622486073e-05, Loss: 0.09392089396715164
[2023-09-05 17:37:46,934] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4854/66600 [3:33:57<246:34:55, 14.38s/it]09/05/2023 17:37:54 - INFO - __main__ -   Step: 4854, LR: 1.911525254441352e-05, Loss: 0.13795584440231323
[2023-09-05 17:38:01,227] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4855/66600 [3:34:12<247:59:12, 14.46s/it]09/05/2023 17:38:09 - INFO - __main__ -   Step: 4855, LR: 1.9114942840219737e-05, Loss: 0.13745130598545074
[2023-09-05 17:38:15,612] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4856/66600 [3:34:26<244:48:44, 14.27s/it]09/05/2023 17:38:22 - INFO - __main__ -   Step: 4856, LR: 1.9114633136025955e-05, Loss: 0.10862386226654053
[2023-09-05 17:38:29,535] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4857/66600 [3:34:39<240:51:37, 14.04s/it]09/05/2023 17:38:36 - INFO - __main__ -   Step: 4857, LR: 1.9114323431832173e-05, Loss: 0.10276907682418823
[2023-09-05 17:38:42,893] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4858/66600 [3:34:53<238:29:00, 13.91s/it]09/05/2023 17:38:50 - INFO - __main__ -   Step: 4858, LR: 1.911401372763839e-05, Loss: 0.10444508492946625
[2023-09-05 17:38:55,747] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4859/66600 [3:35:06<235:48:56, 13.75s/it]09/05/2023 17:39:03 - INFO - __main__ -   Step: 4859, LR: 1.911370402344461e-05, Loss: 0.0791921392083168
[2023-09-05 17:39:10,597] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4860/66600 [3:35:21<239:58:34, 13.99s/it]09/05/2023 17:39:17 - INFO - __main__ -   Step: 4860, LR: 1.9113394319250827e-05, Loss: 0.16631141304969788
[2023-09-05 17:39:24,269] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4861/66600 [3:35:35<239:52:51, 13.99s/it]09/05/2023 17:39:31 - INFO - __main__ -   Step: 4861, LR: 1.9113084615057045e-05, Loss: 0.16674798727035522
[2023-09-05 17:39:38,079] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4862/66600 [3:35:49<239:33:07, 13.97s/it]09/05/2023 17:39:45 - INFO - __main__ -   Step: 4862, LR: 1.9112774910863263e-05, Loss: 0.12589602172374725
[2023-09-05 17:39:52,897] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4863/66600 [3:36:04<243:28:32, 14.20s/it]09/05/2023 17:40:00 - INFO - __main__ -   Step: 4863, LR: 1.911246520666948e-05, Loss: 0.10652817785739899
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
[2023-09-05 17:40:06,701] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4864/66600 [3:36:18<244:17:17, 14.25s/it]09/05/2023 17:40:14 - INFO - __main__ -   Step: 4864, LR: 1.91121555024757e-05, Loss: 0.10941293835639954
[2023-09-05 17:40:21,721] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4865/66600 [3:36:33<248:41:05, 14.50s/it]09/05/2023 17:40:30 - INFO - __main__ -   Step: 4865, LR: 1.9111845798281917e-05, Loss: 0.152005136013031
[2023-09-05 17:40:36,364] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4866/66600 [3:36:48<248:29:30, 14.49s/it]09/05/2023 17:40:44 - INFO - __main__ -   Step: 4866, LR: 1.9111536094088135e-05, Loss: 0.08972522616386414
[2023-09-05 17:40:51,355] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4867/66600 [3:37:02<249:33:44, 14.55s/it]09/05/2023 17:40:59 - INFO - __main__ -   Step: 4867, LR: 1.9111226389894353e-05, Loss: 0.1247578114271164
[2023-09-05 17:41:05,714] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4868/66600 [3:37:16<245:59:31, 14.35s/it]09/05/2023 17:41:13 - INFO - __main__ -   Step: 4868, LR: 1.911091668570057e-05, Loss: 0.12325368821620941
[2023-09-05 17:41:19,878] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4869/66600 [3:37:31<246:15:14, 14.36s/it]09/05/2023 17:41:27 - INFO - __main__ -   Step: 4869, LR: 1.911060698150679e-05, Loss: 0.10438869893550873
[2023-09-05 17:41:34,104] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4870/66600 [3:37:44<244:09:07, 14.24s/it]09/05/2023 17:41:41 - INFO - __main__ -   Step: 4870, LR: 1.9110297277313007e-05, Loss: 0.11317165195941925
[2023-09-05 17:41:47,762] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4871/66600 [3:37:59<243:30:58, 14.20s/it]09/05/2023 17:41:55 - INFO - __main__ -   Step: 4871, LR: 1.9109987573119226e-05, Loss: 0.1458161473274231
[2023-09-05 17:42:02,286] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4872/66600 [3:38:12<241:51:13, 14.10s/it]09/05/2023 17:42:09 - INFO - __main__ -   Step: 4872, LR: 1.9109677868925444e-05, Loss: 0.11572584509849548
[2023-09-05 17:42:16,892] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4873/66600 [3:38:27<246:14:55, 14.36s/it]09/05/2023 17:42:24 - INFO - __main__ -   Step: 4873, LR: 1.910936816473166e-05, Loss: 0.11467857658863068
[2023-09-05 17:42:31,510] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4874/66600 [3:38:42<249:21:25, 14.54s/it]09/05/2023 17:42:39 - INFO - __main__ -   Step: 4874, LR: 1.910905846053788e-05, Loss: 0.11636817455291748
[2023-09-05 17:42:45,599] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4875/66600 [3:38:56<246:28:38, 14.38s/it]09/05/2023 17:42:53 - INFO - __main__ -   Step: 4875, LR: 1.9108748756344098e-05, Loss: 0.12282454967498779
[2023-09-05 17:42:59,414] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4876/66600 [3:39:10<241:36:52, 14.09s/it]09/05/2023 17:43:06 - INFO - __main__ -   Step: 4876, LR: 1.9108439052150316e-05, Loss: 0.1583438515663147
[2023-09-05 17:43:12,828] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4877/66600 [3:39:23<238:28:03, 13.91s/it]09/05/2023 17:43:20 - INFO - __main__ -   Step: 4877, LR: 1.9108129347956534e-05, Loss: 0.15834936499595642
[2023-09-05 17:43:27,244] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4878/66600 [3:39:39<246:10:31, 14.36s/it]09/05/2023 17:43:35 - INFO - __main__ -   Step: 4878, LR: 1.9107819643762752e-05, Loss: 0.10979759693145752
[2023-09-05 17:43:42,876] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4879/66600 [3:39:54<248:32:13, 14.50s/it]09/05/2023 17:43:50 - INFO - __main__ -   Step: 4879, LR: 1.910750993956897e-05, Loss: 0.15282121300697327
[2023-09-05 17:43:57,044] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4880/66600 [3:40:08<247:19:16, 14.43s/it]09/05/2023 17:44:04 - INFO - __main__ -   Step: 4880, LR: 1.9107200235375188e-05, Loss: 0.12475456297397614
[2023-09-05 17:44:10,853] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4881/66600 [3:40:22<244:47:55, 14.28s/it]09/05/2023 17:44:18 - INFO - __main__ -   Step: 4881, LR: 1.910689053118141e-05, Loss: 0.13927622139453888
[2023-09-05 17:44:25,055] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4882/66600 [3:40:36<243:41:41, 14.21s/it]09/05/2023 17:44:32 - INFO - __main__ -   Step: 4882, LR: 1.9106580826987624e-05, Loss: 0.1316990852355957
[2023-09-05 17:44:39,455] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4883/66600 [3:40:50<244:39:52, 14.27s/it]09/05/2023 17:44:47 - INFO - __main__ -   Step: 4883, LR: 1.9106271122793842e-05, Loss: 0.10004667192697525
[2023-09-05 17:44:53,823] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4884/66600 [3:41:05<245:54:37, 14.34s/it]09/05/2023 17:45:01 - INFO - __main__ -   Step: 4884, LR: 1.910596141860006e-05, Loss: 0.12239568680524826
[2023-09-05 17:45:08,573] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4885/66600 [3:41:20<250:44:35, 14.63s/it]09/05/2023 17:45:16 - INFO - __main__ -   Step: 4885, LR: 1.910565171440628e-05, Loss: 0.11363264918327332
[2023-09-05 17:45:24,488] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4886/66600 [3:41:35<252:12:57, 14.71s/it]09/05/2023 17:45:31 - INFO - __main__ -   Step: 4886, LR: 1.9105342010212496e-05, Loss: 0.14991596341133118
[2023-09-05 17:45:38,772] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4887/66600 [3:41:49<248:03:52, 14.47s/it]09/05/2023 17:45:45 - INFO - __main__ -   Step: 4887, LR: 1.9105032306018714e-05, Loss: 0.11595609784126282
[2023-09-05 17:45:52,393] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4888/66600 [3:42:03<246:31:07, 14.38s/it]09/05/2023 17:45:59 - INFO - __main__ -   Step: 4888, LR: 1.9104722601824936e-05, Loss: 0.10661038756370544
[2023-09-05 17:46:06,765] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4889/66600 [3:42:18<247:30:14, 14.44s/it]09/05/2023 17:46:14 - INFO - __main__ -   Step: 4889, LR: 1.9104412897631154e-05, Loss: 0.09742705523967743
[2023-09-05 17:46:21,205] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4890/66600 [3:42:32<247:33:35, 14.44s/it]09/05/2023 17:46:28 - INFO - __main__ -   Step: 4890, LR: 1.910410319343737e-05, Loss: 0.10271656513214111
[2023-09-05 17:46:35,379] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4891/66600 [3:42:46<245:27:56, 14.32s/it]09/05/2023 17:46:42 - INFO - __main__ -   Step: 4891, LR: 1.9103793489243587e-05, Loss: 0.17523014545440674
[2023-09-05 17:46:48,853] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4892/66600 [3:43:00<241:47:15, 14.11s/it]09/05/2023 17:46:56 - INFO - __main__ -   Step: 4892, LR: 1.9103483785049805e-05, Loss: 0.13508406281471252
[2023-09-05 17:47:02,763] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4893/66600 [3:43:14<241:28:14, 14.09s/it]09/05/2023 17:47:10 - INFO - __main__ -   Step: 4893, LR: 1.9103174080856023e-05, Loss: 0.11608956009149551
[2023-09-05 17:47:16,757] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4894/66600 [3:43:28<240:32:29, 14.03s/it]09/05/2023 17:47:24 - INFO - __main__ -   Step: 4894, LR: 1.910286437666224e-05, Loss: 0.16848957538604736
[2023-09-05 17:47:31,033] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4895/66600 [3:43:42<241:26:46, 14.09s/it]09/05/2023 17:47:38 - INFO - __main__ -   Step: 4895, LR: 1.9102554672468462e-05, Loss: 0.11301637440919876
[2023-09-05 17:47:45,448] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4896/66600 [3:43:56<242:03:53, 14.12s/it]09/05/2023 17:47:52 - INFO - __main__ -   Step: 4896, LR: 1.910224496827468e-05, Loss: 0.12763336300849915
[2023-09-05 17:47:59,897] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4897/66600 [3:44:11<244:42:35, 14.28s/it]09/05/2023 17:48:07 - INFO - __main__ -   Step: 4897, LR: 1.9101935264080895e-05, Loss: 0.14863106608390808
[2023-09-05 17:48:14,468] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4898/66600 [3:44:25<246:06:04, 14.36s/it]09/05/2023 17:48:22 - INFO - __main__ -   Step: 4898, LR: 1.9101625559887113e-05, Loss: 0.13119079172611237
[2023-09-05 17:48:28,269] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4899/66600 [3:44:39<244:33:08, 14.27s/it]09/05/2023 17:48:36 - INFO - __main__ -   Step: 4899, LR: 1.910131585569333e-05, Loss: 0.11094804853200912
[2023-09-05 17:48:43,068] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4900/66600 [3:44:53<243:40:46, 14.22s/it]09/05/2023 17:48:50 - INFO - __main__ -   Step: 4900, LR: 1.910100615149955e-05, Loss: 0.13973745703697205
09/05/2023 17:48:50 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.0026068687438964844

Evaluating:   0%|          | 1/228 [00:00<01:16,  2.96it/s][Astep: 1
extend+tolist() time: 0.0011439323425292969

Evaluating:   1%|          | 2/228 [00:00<01:04,  3.51it/s][Astep: 2
extend+tolist() time: 0.0016841888427734375

Evaluating:   1%|▏         | 3/228 [00:00<01:09,  3.23it/s][Astep: 3
extend+tolist() time: 0.002309083938598633

Evaluating:   2%|▏         | 4/228 [00:01<01:20,  2.77it/s][Astep: 4
extend+tolist() time: 0.001554250717163086

Evaluating:   2%|▏         | 5/228 [00:01<01:21,  2.74it/s][Astep: 5
extend+tolist() time: 0.16021227836608887

Evaluating:   3%|▎         | 6/228 [00:02<01:30,  2.45it/s][Astep: 6
extend+tolist() time: 0.00194549560546875

Evaluating:   3%|▎         | 7/228 [00:02<01:25,  2.60it/s][Astep: 7
extend+tolist() time: 0.001283884048461914

Evaluating:   4%|▎         | 8/228 [00:02<01:15,  2.92it/s][Astep: 8
extend+tolist() time: 0.00077056884765625

Evaluating:   4%|▍         | 9/228 [00:03<01:07,  3.24it/s][Astep: 9
extend+tolist() time: 0.0012531280517578125

Evaluating:   4%|▍         | 10/228 [00:03<01:02,  3.50it/s][Astep: 10
extend+tolist() time: 0.0008871555328369141

Evaluating:   5%|▍         | 11/228 [00:03<01:07,  3.20it/s][Astep: 11
extend+tolist() time: 0.0010335445404052734

Evaluating:   5%|▌         | 12/228 [00:03<01:02,  3.48it/s][Astep: 12
extend+tolist() time: 0.0006704330444335938

Evaluating:   6%|▌         | 13/228 [00:04<00:57,  3.74it/s][Astep: 13
extend+tolist() time: 0.0005960464477539062

Evaluating:   6%|▌         | 14/228 [00:04<00:54,  3.91it/s][Astep: 14
extend+tolist() time: 0.0010585784912109375

Evaluating:   7%|▋         | 15/228 [00:04<00:52,  4.06it/s][Astep: 15
extend+tolist() time: 0.000614166259765625

Evaluating:   7%|▋         | 16/228 [00:04<00:50,  4.17it/s][Astep: 16
extend+tolist() time: 0.0006051063537597656

Evaluating:   7%|▋         | 17/228 [00:05<00:49,  4.23it/s][Astep: 17
extend+tolist() time: 0.0008416175842285156

Evaluating:   8%|▊         | 18/228 [00:05<00:49,  4.22it/s][Astep: 18
extend+tolist() time: 0.0016252994537353516

Evaluating:   8%|▊         | 19/228 [00:05<00:52,  4.01it/s][Astep: 19
extend+tolist() time: 0.001003265380859375

Evaluating:   9%|▉         | 20/228 [00:05<00:53,  3.92it/s][Astep: 20
extend+tolist() time: 0.0012314319610595703

Evaluating:   9%|▉         | 21/228 [00:06<00:51,  4.01it/s][Astep: 21
extend+tolist() time: 0.000675201416015625

Evaluating:  10%|▉         | 22/228 [00:06<00:50,  4.12it/s][Astep: 22
extend+tolist() time: 0.0011563301086425781

Evaluating:  10%|█         | 23/228 [00:06<00:48,  4.19it/s][Astep: 23
extend+tolist() time: 0.0007040500640869141

Evaluating:  11%|█         | 24/228 [00:06<00:47,  4.27it/s][Astep: 24
extend+tolist() time: 0.00185394287109375

Evaluating:  11%|█         | 25/228 [00:06<00:50,  4.04it/s][Astep: 25
extend+tolist() time: 0.0018277168273925781

Evaluating:  11%|█▏        | 26/228 [00:07<00:55,  3.62it/s][Astep: 26
extend+tolist() time: 0.0007281303405761719

Evaluating:  12%|█▏        | 27/228 [00:07<00:52,  3.82it/s][Astep: 27
extend+tolist() time: 0.001973867416381836

Evaluating:  12%|█▏        | 28/228 [00:07<00:55,  3.63it/s][Astep: 28
extend+tolist() time: 0.0003361701965332031

Evaluating:  13%|█▎        | 29/228 [00:08<00:51,  3.86it/s][Astep: 29
extend+tolist() time: 0.001157522201538086

Evaluating:  13%|█▎        | 30/228 [00:08<00:49,  3.99it/s][Astep: 30
extend+tolist() time: 0.001056671142578125

Evaluating:  14%|█▎        | 31/228 [00:08<00:51,  3.84it/s][Astep: 31
extend+tolist() time: 0.000579833984375

Evaluating:  14%|█▍        | 32/228 [00:08<00:49,  3.99it/s][Astep: 32
extend+tolist() time: 0.0010013580322265625

Evaluating:  14%|█▍        | 33/228 [00:09<00:50,  3.85it/s][Astep: 33
extend+tolist() time: 0.0012745857238769531

Evaluating:  15%|█▍        | 34/228 [00:09<01:00,  3.18it/s][Astep: 34
extend+tolist() time: 0.0013778209686279297

Evaluating:  15%|█▌        | 35/228 [00:09<01:04,  2.98it/s][Astep: 35
extend+tolist() time: 0.0006551742553710938

Evaluating:  16%|█▌        | 36/228 [00:10<00:57,  3.32it/s][Astep: 36
extend+tolist() time: 0.17309331893920898

Evaluating:  16%|█▌        | 37/228 [00:10<01:03,  3.02it/s][Astep: 37
extend+tolist() time: 0.0016961097717285156

Evaluating:  17%|█▋        | 38/228 [00:10<01:01,  3.08it/s][Astep: 38
extend+tolist() time: 0.0007770061492919922

Evaluating:  17%|█▋        | 39/228 [00:11<00:56,  3.37it/s][Astep: 39
extend+tolist() time: 0.0010712146759033203

Evaluating:  18%|█▊        | 40/228 [00:11<00:51,  3.62it/s][Astep: 40
extend+tolist() time: 0.0006096363067626953

Evaluating:  18%|█▊        | 41/228 [00:11<00:48,  3.85it/s][Astep: 41
extend+tolist() time: 0.0012547969818115234

Evaluating:  18%|█▊        | 42/228 [00:11<00:47,  3.93it/s][Astep: 42
extend+tolist() time: 0.001569509506225586

Evaluating:  19%|█▉        | 43/228 [00:12<00:49,  3.72it/s][Astep: 43
extend+tolist() time: 0.0018689632415771484

Evaluating:  19%|█▉        | 44/228 [00:12<01:02,  2.96it/s][Astep: 44
extend+tolist() time: 0.0007569789886474609

Evaluating:  20%|█▉        | 45/228 [00:12<00:55,  3.27it/s][Astep: 45
extend+tolist() time: 0.0017330646514892578

Evaluating:  20%|██        | 46/228 [00:13<00:55,  3.28it/s][Astep: 46
extend+tolist() time: 0.0016314983367919922

Evaluating:  21%|██        | 47/228 [00:13<00:55,  3.28it/s][Astep: 47
extend+tolist() time: 0.0014939308166503906

Evaluating:  21%|██        | 48/228 [00:13<00:53,  3.36it/s][Astep: 48
extend+tolist() time: 0.0012013912200927734

Evaluating:  21%|██▏       | 49/228 [00:14<00:53,  3.35it/s][Astep: 49
extend+tolist() time: 0.0021715164184570312

Evaluating:  22%|██▏       | 50/228 [00:14<00:51,  3.49it/s][Astep: 50
extend+tolist() time: 0.0025665760040283203

Evaluating:  22%|██▏       | 51/228 [00:14<00:53,  3.33it/s][Astep: 51
extend+tolist() time: 0.0016901493072509766

Evaluating:  23%|██▎       | 52/228 [00:14<00:53,  3.32it/s][Astep: 52
extend+tolist() time: 0.0010037422180175781

Evaluating:  23%|██▎       | 53/228 [00:15<00:51,  3.40it/s][Astep: 53
extend+tolist() time: 0.0017375946044921875

Evaluating:  24%|██▎       | 54/228 [00:15<00:51,  3.37it/s][Astep: 54
extend+tolist() time: 0.0011875629425048828

Evaluating:  24%|██▍       | 55/228 [00:15<00:48,  3.58it/s][Astep: 55
extend+tolist() time: 0.0008046627044677734

Evaluating:  25%|██▍       | 56/228 [00:15<00:45,  3.75it/s][Astep: 56
extend+tolist() time: 0.0016565322875976562

Evaluating:  25%|██▌       | 57/228 [00:16<00:47,  3.62it/s][Astep: 57
extend+tolist() time: 0.0006008148193359375

Evaluating:  25%|██▌       | 58/228 [00:16<00:44,  3.85it/s][Astep: 58
extend+tolist() time: 0.0012905597686767578

Evaluating:  26%|██▌       | 59/228 [00:16<00:43,  3.92it/s][Astep: 59
extend+tolist() time: 0.0008997917175292969

Evaluating:  26%|██▋       | 60/228 [00:16<00:42,  3.94it/s][Astep: 60
extend+tolist() time: 0.0011398792266845703

Evaluating:  27%|██▋       | 61/228 [00:17<00:41,  4.06it/s][Astep: 61
extend+tolist() time: 0.0008497238159179688

Evaluating:  27%|██▋       | 62/228 [00:17<00:40,  4.07it/s][Astep: 62
extend+tolist() time: 0.0012447834014892578

Evaluating:  28%|██▊       | 63/228 [00:17<00:39,  4.15it/s][Astep: 63
extend+tolist() time: 0.0008401870727539062

Evaluating:  28%|██▊       | 64/228 [00:17<00:39,  4.20it/s][Astep: 64
extend+tolist() time: 0.0013003349304199219

Evaluating:  29%|██▊       | 65/228 [00:18<00:38,  4.19it/s][Astep: 65
extend+tolist() time: 0.00080108642578125

Evaluating:  29%|██▉       | 66/228 [00:18<00:38,  4.19it/s][Astep: 66
extend+tolist() time: 0.0012488365173339844

Evaluating:  29%|██▉       | 67/228 [00:18<00:47,  3.37it/s][Astep: 67
extend+tolist() time: 0.0009202957153320312

Evaluating:  30%|██▉       | 68/228 [00:19<00:45,  3.54it/s][Astep: 68
extend+tolist() time: 0.0011823177337646484

Evaluating:  30%|███       | 69/228 [00:19<00:49,  3.20it/s][Astep: 69
extend+tolist() time: 0.0015282630920410156

Evaluating:  31%|███       | 70/228 [00:19<00:47,  3.30it/s][Astep: 70
extend+tolist() time: 0.0010640621185302734

Evaluating:  31%|███       | 71/228 [00:20<00:46,  3.37it/s][Astep: 71
extend+tolist() time: 0.18699383735656738

Evaluating:  32%|███▏      | 72/228 [00:20<00:53,  2.90it/s][Astep: 72
extend+tolist() time: 0.001232147216796875

Evaluating:  32%|███▏      | 73/228 [00:20<00:48,  3.22it/s][Astep: 73
extend+tolist() time: 0.0010304450988769531

Evaluating:  32%|███▏      | 74/228 [00:20<00:43,  3.50it/s][Astep: 74
extend+tolist() time: 0.0007264614105224609

Evaluating:  33%|███▎      | 75/228 [00:21<00:41,  3.73it/s][Astep: 75
extend+tolist() time: 0.0012629032135009766

Evaluating:  33%|███▎      | 76/228 [00:21<00:42,  3.58it/s][Astep: 76
extend+tolist() time: 0.001119852066040039

Evaluating:  34%|███▍      | 77/228 [00:21<00:39,  3.80it/s][Astep: 77
extend+tolist() time: 0.0018587112426757812

Evaluating:  34%|███▍      | 78/228 [00:22<00:42,  3.51it/s][Astep: 78
extend+tolist() time: 0.0008566379547119141

Evaluating:  35%|███▍      | 79/228 [00:22<00:48,  3.06it/s][Astep: 79
extend+tolist() time: 0.0013413429260253906

Evaluating:  35%|███▌      | 80/228 [00:22<00:44,  3.29it/s][Astep: 80
extend+tolist() time: 0.001607656478881836

Evaluating:  36%|███▌      | 81/228 [00:22<00:42,  3.48it/s][Astep: 81
extend+tolist() time: 0.0008478164672851562

Evaluating:  36%|███▌      | 82/228 [00:23<00:39,  3.65it/s][Astep: 82
extend+tolist() time: 0.0013127326965332031

Evaluating:  36%|███▋      | 83/228 [00:23<00:38,  3.78it/s][Astep: 83
extend+tolist() time: 0.0006844997406005859

Evaluating:  37%|███▋      | 84/228 [00:23<00:36,  3.96it/s][Astep: 84
extend+tolist() time: 0.0014617443084716797

Evaluating:  37%|███▋      | 85/228 [00:23<00:36,  3.87it/s][Astep: 85
extend+tolist() time: 0.00092315673828125

Evaluating:  38%|███▊      | 86/228 [00:24<00:36,  3.90it/s][Astep: 86
extend+tolist() time: 0.0013632774353027344

Evaluating:  38%|███▊      | 87/228 [00:24<00:35,  3.94it/s][Astep: 87
extend+tolist() time: 0.0009016990661621094

Evaluating:  39%|███▊      | 88/228 [00:24<00:35,  3.96it/s][Astep: 88
extend+tolist() time: 0.0007722377777099609

Evaluating:  39%|███▉      | 89/228 [00:24<00:34,  4.08it/s][Astep: 89
extend+tolist() time: 0.0007643699645996094

Evaluating:  39%|███▉      | 90/228 [00:25<00:33,  4.15it/s][Astep: 90
extend+tolist() time: 0.0014615058898925781

Evaluating:  40%|███▉      | 91/228 [00:25<00:33,  4.12it/s][Astep: 91
extend+tolist() time: 0.0011303424835205078

Evaluating:  40%|████      | 92/228 [00:25<00:32,  4.18it/s][Astep: 92
extend+tolist() time: 0.0007801055908203125

Evaluating:  41%|████      | 93/228 [00:25<00:31,  4.23it/s][Astep: 93
extend+tolist() time: 0.0014486312866210938

Evaluating:  41%|████      | 94/228 [00:26<00:32,  4.06it/s][Astep: 94
extend+tolist() time: 0.0007550716400146484

Evaluating:  42%|████▏     | 95/228 [00:26<00:31,  4.17it/s][Astep: 95
extend+tolist() time: 0.001667022705078125

Evaluating:  42%|████▏     | 96/228 [00:26<00:33,  3.88it/s][Astep: 96
extend+tolist() time: 0.000980377197265625

Evaluating:  43%|████▎     | 97/228 [00:26<00:33,  3.91it/s][Astep: 97
extend+tolist() time: 0.0013179779052734375

Evaluating:  43%|████▎     | 98/228 [00:27<00:32,  4.00it/s][Astep: 98
extend+tolist() time: 0.0013189315795898438

Evaluating:  43%|████▎     | 99/228 [00:27<00:32,  4.01it/s][Astep: 99
extend+tolist() time: 0.00090789794921875

Evaluating:  44%|████▍     | 100/228 [00:27<00:31,  4.02it/s][Astep: 100
extend+tolist() time: 0.0011856555938720703

Evaluating:  44%|████▍     | 101/228 [00:27<00:30,  4.13it/s][Astep: 101
extend+tolist() time: 0.0008347034454345703

Evaluating:  45%|████▍     | 102/228 [00:28<00:30,  4.14it/s][Astep: 102
extend+tolist() time: 0.0011661052703857422

Evaluating:  45%|████▌     | 103/228 [00:28<00:29,  4.21it/s][Astep: 103
extend+tolist() time: 0.0008089542388916016

Evaluating:  46%|████▌     | 104/228 [00:28<00:29,  4.27it/s][Astep: 104
extend+tolist() time: 0.0007212162017822266

Evaluating:  46%|████▌     | 105/228 [00:28<00:28,  4.33it/s][Astep: 105
extend+tolist() time: 0.0008409023284912109

Evaluating:  46%|████▋     | 106/228 [00:29<00:28,  4.27it/s][Astep: 106
extend+tolist() time: 0.0018355846405029297

Evaluating:  47%|████▋     | 107/228 [00:29<00:31,  3.85it/s][Astep: 107
extend+tolist() time: 0.0007812976837158203

Evaluating:  47%|████▋     | 108/228 [00:29<00:30,  3.98it/s][Astep: 108
extend+tolist() time: 0.0012745857238769531

Evaluating:  48%|████▊     | 109/228 [00:29<00:29,  4.07it/s][Astep: 109
extend+tolist() time: 0.0008332729339599609

Evaluating:  48%|████▊     | 110/228 [00:30<00:28,  4.08it/s][Astep: 110
extend+tolist() time: 0.0010731220245361328

Evaluating:  49%|████▊     | 111/228 [00:30<00:28,  4.17it/s][Astep: 111
extend+tolist() time: 0.0018506050109863281

Evaluating:  49%|████▉     | 112/228 [00:30<00:30,  3.79it/s][Astep: 112
extend+tolist() time: 0.00039315223693847656

Evaluating:  50%|████▉     | 113/228 [00:30<00:35,  3.27it/s][Astep: 113
extend+tolist() time: 0.0006902217864990234

Evaluating:  50%|█████     | 114/228 [00:31<00:32,  3.55it/s][Astep: 114
extend+tolist() time: 0.0015833377838134766

Evaluating:  50%|█████     | 115/228 [00:31<00:37,  3.01it/s][Astep: 115
extend+tolist() time: 0.0010190010070800781

Evaluating:  51%|█████     | 116/228 [00:31<00:33,  3.32it/s][Astep: 116
extend+tolist() time: 0.0008134841918945312

Evaluating:  51%|█████▏    | 117/228 [00:32<00:31,  3.56it/s][Astep: 117
extend+tolist() time: 0.0013072490692138672

Evaluating:  52%|█████▏    | 118/228 [00:32<00:29,  3.71it/s][Astep: 118
extend+tolist() time: 0.0005805492401123047

Evaluating:  52%|█████▏    | 119/228 [00:32<00:27,  3.91it/s][Astep: 119
extend+tolist() time: 0.000698089599609375

Evaluating:  53%|█████▎    | 120/228 [00:32<00:26,  4.04it/s][Astep: 120
extend+tolist() time: 0.26278090476989746

Evaluating:  53%|█████▎    | 121/228 [00:33<00:34,  3.13it/s][Astep: 121
extend+tolist() time: 0.0006697177886962891

Evaluating:  54%|█████▎    | 122/228 [00:33<00:30,  3.42it/s][Astep: 122
extend+tolist() time: 0.0011334419250488281

Evaluating:  54%|█████▍    | 123/228 [00:33<00:28,  3.66it/s][Astep: 123
extend+tolist() time: 0.0009181499481201172

Evaluating:  54%|█████▍    | 124/228 [00:33<00:26,  3.88it/s][Astep: 124
extend+tolist() time: 0.0012345314025878906

Evaluating:  55%|█████▍    | 125/228 [00:34<00:25,  3.99it/s][Astep: 125
extend+tolist() time: 0.0004134178161621094

Evaluating:  55%|█████▌    | 126/228 [00:34<00:24,  4.15it/s][Astep: 126
extend+tolist() time: 0.0016641616821289062

Evaluating:  56%|█████▌    | 127/228 [00:34<00:26,  3.78it/s][Astep: 127
extend+tolist() time: 0.0017137527465820312

Evaluating:  56%|█████▌    | 128/228 [00:35<00:28,  3.54it/s][Astep: 128
extend+tolist() time: 0.0007588863372802734

Evaluating:  57%|█████▋    | 129/228 [00:35<00:26,  3.77it/s][Astep: 129
extend+tolist() time: 0.001222372055053711

Evaluating:  57%|█████▋    | 130/228 [00:35<00:24,  3.92it/s][Astep: 130
extend+tolist() time: 0.0008895397186279297

Evaluating:  57%|█████▋    | 131/228 [00:35<00:30,  3.23it/s][Astep: 131
extend+tolist() time: 0.0008785724639892578

Evaluating:  58%|█████▊    | 132/228 [00:36<00:27,  3.53it/s][Astep: 132
extend+tolist() time: 0.0010688304901123047

Evaluating:  58%|█████▊    | 133/228 [00:36<00:26,  3.55it/s][Astep: 133
extend+tolist() time: 0.0009219646453857422

Evaluating:  59%|█████▉    | 134/228 [00:36<00:24,  3.79it/s][Astep: 134
extend+tolist() time: 0.0009756088256835938

Evaluating:  59%|█████▉    | 135/228 [00:36<00:24,  3.77it/s][Astep: 135
extend+tolist() time: 0.00042819976806640625

Evaluating:  60%|█████▉    | 136/228 [00:37<00:23,  3.97it/s][Astep: 136
extend+tolist() time: 0.0013072490692138672

Evaluating:  60%|██████    | 137/228 [00:37<00:22,  4.07it/s][Astep: 137
extend+tolist() time: 0.0003921985626220703

Evaluating:  61%|██████    | 138/228 [00:37<00:21,  4.20it/s][Astep: 138
extend+tolist() time: 0.0011391639709472656

Evaluating:  61%|██████    | 139/228 [00:37<00:20,  4.26it/s][Astep: 139
extend+tolist() time: 0.0004673004150390625

Evaluating:  61%|██████▏   | 140/228 [00:38<00:20,  4.32it/s][Astep: 140
extend+tolist() time: 0.0007431507110595703

Evaluating:  62%|██████▏   | 141/228 [00:38<00:20,  4.34it/s][Astep: 141
extend+tolist() time: 0.0012960433959960938

Evaluating:  62%|██████▏   | 142/228 [00:38<00:19,  4.34it/s][Astep: 142
extend+tolist() time: 0.0005896091461181641

Evaluating:  63%|██████▎   | 143/228 [00:38<00:19,  4.38it/s][Astep: 143
extend+tolist() time: 0.0005497932434082031

Evaluating:  63%|██████▎   | 144/228 [00:39<00:18,  4.43it/s][Astep: 144
extend+tolist() time: 0.0011799335479736328

Evaluating:  64%|██████▎   | 145/228 [00:39<00:18,  4.42it/s][Astep: 145
extend+tolist() time: 0.0005095005035400391

Evaluating:  64%|██████▍   | 146/228 [00:39<00:18,  4.45it/s][Astep: 146
extend+tolist() time: 0.00037360191345214844

Evaluating:  64%|██████▍   | 147/228 [00:39<00:18,  4.49it/s][Astep: 147
extend+tolist() time: 0.0011756420135498047

Evaluating:  65%|██████▍   | 148/228 [00:39<00:17,  4.47it/s][Astep: 148
extend+tolist() time: 0.0006418228149414062

Evaluating:  65%|██████▌   | 149/228 [00:40<00:17,  4.48it/s][Astep: 149
extend+tolist() time: 0.0003628730773925781

Evaluating:  66%|██████▌   | 150/228 [00:40<00:17,  4.51it/s][Astep: 150
extend+tolist() time: 0.0012454986572265625

Evaluating:  66%|██████▌   | 151/228 [00:40<00:17,  4.45it/s][Astep: 151
extend+tolist() time: 0.0005543231964111328

Evaluating:  67%|██████▋   | 152/228 [00:40<00:16,  4.48it/s][Astep: 152
extend+tolist() time: 0.003968715667724609

Evaluating:  67%|██████▋   | 153/228 [00:41<00:16,  4.42it/s][Astep: 153
extend+tolist() time: 0.0009000301361083984

Evaluating:  68%|██████▊   | 154/228 [00:41<00:17,  4.32it/s][Astep: 154
extend+tolist() time: 0.001911163330078125

Evaluating:  68%|██████▊   | 155/228 [00:41<00:18,  3.84it/s][Astep: 155
extend+tolist() time: 0.0009434223175048828

Evaluating:  68%|██████▊   | 156/228 [00:41<00:17,  4.02it/s][Astep: 156
extend+tolist() time: 0.0004961490631103516

Evaluating:  69%|██████▉   | 157/228 [00:42<00:17,  4.18it/s][Astep: 157
extend+tolist() time: 0.0006291866302490234

Evaluating:  69%|██████▉   | 158/228 [00:42<00:16,  4.26it/s][Astep: 158
extend+tolist() time: 0.0004963874816894531

Evaluating:  70%|██████▉   | 159/228 [00:42<00:15,  4.33it/s][Astep: 159
extend+tolist() time: 0.0011174678802490234

Evaluating:  70%|███████   | 160/228 [00:42<00:15,  4.37it/s][Astep: 160
extend+tolist() time: 0.00034809112548828125

Evaluating:  71%|███████   | 161/228 [00:42<00:15,  4.42it/s][Astep: 161
extend+tolist() time: 0.0011920928955078125

Evaluating:  71%|███████   | 162/228 [00:43<00:15,  4.22it/s][Astep: 162
extend+tolist() time: 0.0009131431579589844

Evaluating:  71%|███████▏  | 163/228 [00:43<00:15,  4.13it/s][Astep: 163
extend+tolist() time: 0.000377655029296875

Evaluating:  72%|███████▏  | 164/228 [00:43<00:15,  4.25it/s][Astep: 164
extend+tolist() time: 0.0005245208740234375

Evaluating:  72%|███████▏  | 165/228 [00:43<00:14,  4.34it/s][Astep: 165
extend+tolist() time: 0.0004093647003173828

Evaluating:  73%|███████▎  | 166/228 [00:44<00:14,  4.40it/s][Astep: 166
extend+tolist() time: 0.0008018016815185547

Evaluating:  73%|███████▎  | 167/228 [00:44<00:13,  4.46it/s][Astep: 167
extend+tolist() time: 0.0005006790161132812

Evaluating:  74%|███████▎  | 168/228 [00:44<00:13,  4.49it/s][Astep: 168
extend+tolist() time: 0.0014693737030029297

Evaluating:  74%|███████▍  | 169/228 [00:44<00:14,  4.21it/s][Astep: 169
extend+tolist() time: 0.00035071372985839844

Evaluating:  75%|███████▍  | 170/228 [00:45<00:13,  4.30it/s][Astep: 170
extend+tolist() time: 0.0007984638214111328

Evaluating:  75%|███████▌  | 171/228 [00:45<00:13,  4.29it/s][Astep: 171
extend+tolist() time: 0.00026917457580566406

Evaluating:  75%|███████▌  | 172/228 [00:45<00:12,  4.37it/s][Astep: 172
extend+tolist() time: 0.0012366771697998047

Evaluating:  76%|███████▌  | 173/228 [00:45<00:12,  4.38it/s][Astep: 173
extend+tolist() time: 0.0009918212890625

Evaluating:  76%|███████▋  | 174/228 [00:45<00:13,  4.15it/s][Astep: 174
extend+tolist() time: 0.0014753341674804688

Evaluating:  77%|███████▋  | 175/228 [00:46<00:13,  3.85it/s][Astep: 175
extend+tolist() time: 0.001331329345703125

Evaluating:  77%|███████▋  | 176/228 [00:46<00:13,  4.00it/s][Astep: 176
extend+tolist() time: 0.0007145404815673828

Evaluating:  78%|███████▊  | 177/228 [00:46<00:12,  4.14it/s][Astep: 177
extend+tolist() time: 0.0005903244018554688

Evaluating:  78%|███████▊  | 178/228 [00:46<00:11,  4.24it/s][Astep: 178
extend+tolist() time: 0.0017092227935791016

Evaluating:  79%|███████▊  | 179/228 [00:47<00:12,  4.03it/s][Astep: 179
extend+tolist() time: 0.0004024505615234375

Evaluating:  79%|███████▉  | 180/228 [00:47<00:11,  4.17it/s][Astep: 180
extend+tolist() time: 0.00042939186096191406

Evaluating:  79%|███████▉  | 181/228 [00:47<00:10,  4.28it/s][Astep: 181
extend+tolist() time: 0.0006258487701416016

Evaluating:  80%|███████▉  | 182/228 [00:47<00:10,  4.34it/s][Astep: 182
extend+tolist() time: 0.0007793903350830078

Evaluating:  80%|████████  | 183/228 [00:48<00:10,  4.39it/s][Astep: 183
extend+tolist() time: 0.0011262893676757812

Evaluating:  81%|████████  | 184/228 [00:48<00:09,  4.43it/s][Astep: 184
extend+tolist() time: 0.00043582916259765625

Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.46it/s][Astep: 185
extend+tolist() time: 0.0011506080627441406

Evaluating:  82%|████████▏ | 186/228 [00:48<00:09,  4.21it/s][Astep: 186
extend+tolist() time: 0.001667022705078125

Evaluating:  82%|████████▏ | 187/228 [00:49<00:09,  4.17it/s][Astep: 187
extend+tolist() time: 0.0004372596740722656

Evaluating:  82%|████████▏ | 188/228 [00:49<00:11,  3.38it/s][Astep: 188
extend+tolist() time: 0.00110626220703125

Evaluating:  83%|████████▎ | 189/228 [00:49<00:10,  3.66it/s][Astep: 189
extend+tolist() time: 0.00037288665771484375

Evaluating:  83%|████████▎ | 190/228 [00:49<00:09,  3.90it/s][Astep: 190
extend+tolist() time: 0.0012099742889404297

Evaluating:  84%|████████▍ | 191/228 [00:50<00:09,  3.83it/s][Astep: 191
extend+tolist() time: 0.0011675357818603516

Evaluating:  84%|████████▍ | 192/228 [00:50<00:09,  3.99it/s][Astep: 192
extend+tolist() time: 0.0004696846008300781

Evaluating:  85%|████████▍ | 193/228 [00:50<00:08,  4.14it/s][Astep: 193
extend+tolist() time: 0.0014882087707519531

Evaluating:  85%|████████▌ | 194/228 [00:50<00:08,  4.11it/s][Astep: 194
extend+tolist() time: 0.0006482601165771484

Evaluating:  86%|████████▌ | 195/228 [00:51<00:09,  3.37it/s][Astep: 195
extend+tolist() time: 0.0005536079406738281

Evaluating:  86%|████████▌ | 196/228 [00:51<00:08,  3.65it/s][Astep: 196
extend+tolist() time: 0.0010714530944824219

Evaluating:  86%|████████▋ | 197/228 [00:51<00:08,  3.86it/s][Astep: 197
extend+tolist() time: 0.0007338523864746094

Evaluating:  87%|████████▋ | 198/228 [00:51<00:07,  4.03it/s][Astep: 198
extend+tolist() time: 0.0006444454193115234

Evaluating:  87%|████████▋ | 199/228 [00:52<00:07,  4.13it/s][Astep: 199
extend+tolist() time: 0.002018451690673828

Evaluating:  88%|████████▊ | 200/228 [00:52<00:07,  3.84it/s][Astep: 200
extend+tolist() time: 0.0007617473602294922

Evaluating:  88%|████████▊ | 201/228 [00:52<00:06,  4.00it/s][Astep: 201
extend+tolist() time: 0.0015294551849365234

Evaluating:  89%|████████▊ | 202/228 [00:52<00:06,  4.11it/s][Astep: 202
extend+tolist() time: 0.0004086494445800781

Evaluating:  89%|████████▉ | 203/228 [00:53<00:05,  4.20it/s][Astep: 203
extend+tolist() time: 0.0005245208740234375

Evaluating:  89%|████████▉ | 204/228 [00:53<00:05,  4.30it/s][Astep: 204
extend+tolist() time: 0.0003790855407714844

Evaluating:  90%|████████▉ | 205/228 [00:53<00:05,  4.38it/s][Astep: 205
extend+tolist() time: 0.0007233619689941406

Evaluating:  90%|█████████ | 206/228 [00:53<00:04,  4.45it/s][Astep: 206
extend+tolist() time: 0.24274349212646484

Evaluating:  91%|█████████ | 207/228 [00:54<00:06,  3.37it/s][Astep: 207
extend+tolist() time: 0.0006659030914306641

Evaluating:  91%|█████████ | 208/228 [00:54<00:05,  3.64it/s][Astep: 208
extend+tolist() time: 0.0011751651763916016

Evaluating:  92%|█████████▏| 209/228 [00:54<00:04,  3.86it/s][Astep: 209
extend+tolist() time: 0.0006554126739501953

Evaluating:  92%|█████████▏| 210/228 [00:54<00:04,  4.02it/s][Astep: 210
extend+tolist() time: 0.0006544589996337891

Evaluating:  93%|█████████▎| 211/228 [00:55<00:04,  4.14it/s][Astep: 211
extend+tolist() time: 0.0011625289916992188

Evaluating:  93%|█████████▎| 212/228 [00:55<00:03,  4.00it/s][Astep: 212
extend+tolist() time: 0.0014445781707763672

Evaluating:  93%|█████████▎| 213/228 [00:55<00:03,  4.09it/s][Astep: 213
extend+tolist() time: 0.0008227825164794922

Evaluating:  94%|█████████▍| 214/228 [00:55<00:03,  4.17it/s][Astep: 214
extend+tolist() time: 0.0013020038604736328

Evaluating:  94%|█████████▍| 215/228 [00:56<00:03,  4.23it/s][Astep: 215
extend+tolist() time: 0.0006921291351318359

Evaluating:  95%|█████████▍| 216/228 [00:56<00:02,  4.29it/s][Astep: 216
extend+tolist() time: 0.0006155967712402344

Evaluating:  95%|█████████▌| 217/228 [00:56<00:02,  4.36it/s][Astep: 217
extend+tolist() time: 0.0010051727294921875

Evaluating:  96%|█████████▌| 218/228 [00:56<00:02,  4.41it/s][Astep: 218
extend+tolist() time: 0.001100778579711914

Evaluating:  96%|█████████▌| 219/228 [00:57<00:02,  4.31it/s][Astep: 219
extend+tolist() time: 0.0005016326904296875

Evaluating:  96%|█████████▋| 220/228 [00:57<00:01,  4.37it/s][Astep: 220
extend+tolist() time: 0.00087738037109375

Evaluating:  97%|█████████▋| 221/228 [00:57<00:01,  4.42it/s][Astep: 221
extend+tolist() time: 0.00070953369140625

Evaluating:  97%|█████████▋| 222/228 [00:57<00:01,  3.45it/s][Astep: 222
extend+tolist() time: 0.0004782676696777344

Evaluating:  98%|█████████▊| 223/228 [00:58<00:01,  3.71it/s][Astep: 223
extend+tolist() time: 0.00039649009704589844

Evaluating:  98%|█████████▊| 224/228 [00:58<00:01,  3.94it/s][Astep: 224
extend+tolist() time: 0.0008742809295654297

Evaluating:  99%|█████████▊| 225/228 [00:58<00:00,  4.11it/s][Astep: 225
extend+tolist() time: 0.0004885196685791016

Evaluating:  99%|█████████▉| 226/228 [00:58<00:00,  4.24it/s][Astep: 226
extend+tolist() time: 0.0005786418914794922

Evaluating: 100%|█████████▉| 227/228 [00:59<00:00,  4.32it/s][Astep: 227
extend+tolist() time: 0.0005075931549072266

Evaluating: 100%|██████████| 228/228 [00:59<00:00,  3.81it/s][A09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:49:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 17:49:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 17:49:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 17:49:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 17:49:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:01<00:00,  3.70it/s]
09/05/2023 17:49:51 - INFO - __main__ -   Step: 4900, Validation Metrics: {'pred_1_num': 9727, 'pred_-1_num': 767, 'pred_0_num': 307, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7872419220442551, 'f1_micro': 0.7872419220442551, 'f1_macro': 0.4516570335117455, 'f1_weighted': 0.7558410505784547, 'f1_-1': 0.3351907415345049, 'f1_0': 0.14164904862579283, 'f1_1': 0.8781313103749386, 'precision_micro': 0.7872419220442551, 'precision_macro': 0.5183662211619522, 'precision_weighted': 0.7450552991402727, 'precision_-1': 0.5097783572359843, 'precision_0': 0.2182410423452769, 'precision_1': 0.8270792639045954, 'recall_micro': 0.7872419220442551, 'recall_macro': 0.4301441547336137, 'recall_weighted': 0.7872419220442551, 'recall_-1': 0.24968071519795657, 'recall_0': 0.10485133020344288, 'recall_1': 0.9359004187994416, 'roc_auc_micro': 0.9040459851052447, 'roc_auc_macro': 0.7162486619195354, 'roc_auc_weighted': 0.6926207915772734, 'roc_auc_-1': 0.797368104433616, 'roc_auc_0': 0.6766527019714121, 'roc_auc_1': 0.6747251793535779}
[2023-09-05 17:49:58,769] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4901/66600 [3:46:10<562:16:03, 32.81s/it]09/05/2023 17:50:06 - INFO - __main__ -   Step: 4901, LR: 1.9100696447305767e-05, Loss: 0.18034085631370544
[2023-09-05 17:50:12,490] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4902/66600 [3:46:23<464:37:20, 27.11s/it]09/05/2023 17:50:20 - INFO - __main__ -   Step: 4902, LR: 1.910038674311199e-05, Loss: 0.0818345695734024
[2023-09-05 17:50:27,275] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4903/66600 [3:46:38<398:13:34, 23.24s/it]09/05/2023 17:50:34 - INFO - __main__ -   Step: 4903, LR: 1.9100077038918207e-05, Loss: 0.1442928910255432
[2023-09-05 17:50:40,458] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4904/66600 [3:46:51<349:04:42, 20.37s/it]09/05/2023 17:50:48 - INFO - __main__ -   Step: 4904, LR: 1.9099767334724425e-05, Loss: 0.10448561608791351
[2023-09-05 17:50:54,142] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4905/66600 [3:47:04<311:59:34, 18.21s/it]09/05/2023 17:51:01 - INFO - __main__ -   Step: 4905, LR: 1.909945763053064e-05, Loss: 0.13671934604644775
[2023-09-05 17:51:07,647] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4906/66600 [3:47:18<287:45:28, 16.79s/it]09/05/2023 17:51:14 - INFO - __main__ -   Step: 4906, LR: 1.9099147926336858e-05, Loss: 0.1138458102941513
[2023-09-05 17:51:21,076] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4907/66600 [3:47:32<274:20:08, 16.01s/it]09/05/2023 17:51:29 - INFO - __main__ -   Step: 4907, LR: 1.9098838222143076e-05, Loss: 0.12000369280576706
[2023-09-05 17:51:35,871] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4908/66600 [3:47:46<264:58:04, 15.46s/it]09/05/2023 17:51:43 - INFO - __main__ -   Step: 4908, LR: 1.9098528517949294e-05, Loss: 0.15622204542160034
[2023-09-05 17:51:49,268] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4909/66600 [3:48:00<257:45:09, 15.04s/it]09/05/2023 17:51:57 - INFO - __main__ -   Step: 4909, LR: 1.9098218813755515e-05, Loss: 0.1081523597240448
[2023-09-05 17:52:03,335] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4910/66600 [3:48:14<250:45:03, 14.63s/it]09/05/2023 17:52:10 - INFO - __main__ -   Step: 4910, LR: 1.9097909109561733e-05, Loss: 0.09586699306964874
[2023-09-05 17:52:16,930] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4911/66600 [3:48:28<249:42:54, 14.57s/it]09/05/2023 17:52:25 - INFO - __main__ -   Step: 4911, LR: 1.909759940536795e-05, Loss: 0.1936054825782776
[2023-09-05 17:52:31,794] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4912/66600 [3:48:42<245:47:12, 14.34s/it]09/05/2023 17:52:39 - INFO - __main__ -   Step: 4912, LR: 1.909728970117417e-05, Loss: 0.11542221903800964
[2023-09-05 17:52:46,537] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4913/66600 [3:48:57<250:10:50, 14.60s/it]09/05/2023 17:52:54 - INFO - __main__ -   Step: 4913, LR: 1.9096979996980384e-05, Loss: 0.10960531234741211
[2023-09-05 17:53:00,775] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4914/66600 [3:49:11<247:08:44, 14.42s/it]09/05/2023 17:53:08 - INFO - __main__ -   Step: 4914, LR: 1.9096670292786602e-05, Loss: 0.16204802691936493
[2023-09-05 17:53:15,058] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4915/66600 [3:49:26<248:09:58, 14.48s/it]09/05/2023 17:53:23 - INFO - __main__ -   Step: 4915, LR: 1.909636058859282e-05, Loss: 0.19977779686450958
[2023-09-05 17:53:29,436] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4916/66600 [3:49:40<245:44:19, 14.34s/it]09/05/2023 17:53:37 - INFO - __main__ -   Step: 4916, LR: 1.909605088439904e-05, Loss: 0.12166732549667358
[2023-09-05 17:53:43,289] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4917/66600 [3:49:54<241:08:02, 14.07s/it]09/05/2023 17:53:50 - INFO - __main__ -   Step: 4917, LR: 1.909574118020526e-05, Loss: 0.09560812264680862
[2023-09-05 17:53:56,401] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4918/66600 [3:50:06<235:24:49, 13.74s/it]09/05/2023 17:54:03 - INFO - __main__ -   Step: 4918, LR: 1.9095431476011478e-05, Loss: 0.14857837557792664
[2023-09-05 17:54:09,037] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4919/66600 [3:50:20<235:48:22, 13.76s/it]09/05/2023 17:54:17 - INFO - __main__ -   Step: 4919, LR: 1.9095121771817696e-05, Loss: 0.15990093350410461
[2023-09-05 17:54:23,566] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4920/66600 [3:50:34<235:51:19, 13.77s/it]09/05/2023 17:54:31 - INFO - __main__ -   Step: 4920, LR: 1.909481206762391e-05, Loss: 0.16264411807060242
[2023-09-05 17:54:37,739] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4921/66600 [3:50:48<236:48:18, 13.82s/it]09/05/2023 17:54:44 - INFO - __main__ -   Step: 4921, LR: 1.909450236343013e-05, Loss: 0.12210383266210556
[2023-09-05 17:54:50,711] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4922/66600 [3:51:01<233:05:53, 13.61s/it]09/05/2023 17:54:58 - INFO - __main__ -   Step: 4922, LR: 1.9094192659236347e-05, Loss: 0.1503911167383194
[2023-09-05 17:55:04,575] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4923/66600 [3:51:15<236:17:54, 13.79s/it]09/05/2023 17:55:12 - INFO - __main__ -   Step: 4923, LR: 1.9093882955042568e-05, Loss: 0.12075440585613251
[2023-09-05 17:55:19,026] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4924/66600 [3:51:30<239:05:44, 13.96s/it]09/05/2023 17:55:26 - INFO - __main__ -   Step: 4924, LR: 1.9093573250848786e-05, Loss: 0.132203608751297
[2023-09-05 17:55:33,363] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4925/66600 [3:51:44<242:25:16, 14.15s/it]09/05/2023 17:55:41 - INFO - __main__ -   Step: 4925, LR: 1.9093263546655004e-05, Loss: 0.08582831919193268
[2023-09-05 17:55:46,919] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4926/66600 [3:51:57<236:52:55, 13.83s/it]09/05/2023 17:55:54 - INFO - __main__ -   Step: 4926, LR: 1.9092953842461222e-05, Loss: 0.1662815511226654
[2023-09-05 17:56:00,816] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4927/66600 [3:52:12<240:43:13, 14.05s/it]09/05/2023 17:56:08 - INFO - __main__ -   Step: 4927, LR: 1.909264413826744e-05, Loss: 0.1103678047657013
[2023-09-05 17:56:15,019] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4928/66600 [3:52:26<239:46:01, 14.00s/it]09/05/2023 17:56:22 - INFO - __main__ -   Step: 4928, LR: 1.9092334434073655e-05, Loss: 0.11651317030191422
[2023-09-05 17:56:28,893] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4929/66600 [3:52:40<242:13:52, 14.14s/it]09/05/2023 17:56:37 - INFO - __main__ -   Step: 4929, LR: 1.9092024729879873e-05, Loss: 0.11733745783567429
[2023-09-05 17:56:44,351] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4930/66600 [3:52:55<246:31:34, 14.39s/it]09/05/2023 17:56:52 - INFO - __main__ -   Step: 4930, LR: 1.9091715025686094e-05, Loss: 0.09982837736606598
[2023-09-05 17:56:59,491] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4931/66600 [3:53:11<252:22:28, 14.73s/it]09/05/2023 17:57:07 - INFO - __main__ -   Step: 4931, LR: 1.9091405321492312e-05, Loss: 0.14447113871574402
[2023-09-05 17:57:13,985] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4932/66600 [3:53:25<247:51:38, 14.47s/it]09/05/2023 17:57:21 - INFO - __main__ -   Step: 4932, LR: 1.909109561729853e-05, Loss: 0.09969015419483185
[2023-09-05 17:57:27,849] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4933/66600 [3:53:38<241:32:13, 14.10s/it]09/05/2023 17:57:34 - INFO - __main__ -   Step: 4933, LR: 1.909078591310475e-05, Loss: 0.11626380681991577
[2023-09-05 17:57:41,683] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4934/66600 [3:53:52<241:56:07, 14.12s/it]09/05/2023 17:57:49 - INFO - __main__ -   Step: 4934, LR: 1.9090476208910967e-05, Loss: 0.14393603801727295
[2023-09-05 17:57:55,441] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4935/66600 [3:54:07<243:58:55, 14.24s/it]09/05/2023 17:58:03 - INFO - __main__ -   Step: 4935, LR: 1.909016650471718e-05, Loss: 0.14650693535804749
[2023-09-05 17:58:10,190] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4936/66600 [3:54:21<246:23:31, 14.38s/it]09/05/2023 17:58:18 - INFO - __main__ -   Step: 4936, LR: 1.90898568005234e-05, Loss: 0.12777119874954224
[2023-09-05 17:58:24,269] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4937/66600 [3:54:34<238:48:52, 13.94s/it]09/05/2023 17:58:31 - INFO - __main__ -   Step: 4937, LR: 1.908954709632962e-05, Loss: 0.09301397949457169
[2023-09-05 17:58:37,228] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4938/66600 [3:54:48<239:12:17, 13.97s/it]09/05/2023 17:58:45 - INFO - __main__ -   Step: 4938, LR: 1.908923739213584e-05, Loss: 0.11008741706609726
[2023-09-05 17:58:51,437] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4939/66600 [3:55:02<238:33:48, 13.93s/it]09/05/2023 17:58:59 - INFO - __main__ -   Step: 4939, LR: 1.9088927687942057e-05, Loss: 0.08663126826286316
[2023-09-05 17:59:05,278] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4940/66600 [3:55:18<246:27:53, 14.39s/it]09/05/2023 17:59:14 - INFO - __main__ -   Step: 4940, LR: 1.9088617983748275e-05, Loss: 0.13378968834877014
[2023-09-05 17:59:20,973] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4941/66600 [3:55:32<248:49:55, 14.53s/it]09/05/2023 17:59:29 - INFO - __main__ -   Step: 4941, LR: 1.9088308279554493e-05, Loss: 0.13096904754638672
[2023-09-05 17:59:35,802] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4942/66600 [3:55:46<246:18:11, 14.38s/it]09/05/2023 17:59:43 - INFO - __main__ -   Step: 4942, LR: 1.908799857536071e-05, Loss: 0.14879921078681946
[2023-09-05 17:59:49,491] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4943/66600 [3:56:00<243:40:04, 14.23s/it]09/05/2023 17:59:57 - INFO - __main__ -   Step: 4943, LR: 1.9087688871166926e-05, Loss: 0.1198236271739006
[2023-09-05 18:00:02,846] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4944/66600 [3:56:14<239:27:30, 13.98s/it]09/05/2023 18:00:10 - INFO - __main__ -   Step: 4944, LR: 1.9087379166973147e-05, Loss: 0.1767958104610443
[2023-09-05 18:00:16,708] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4945/66600 [3:56:28<238:39:41, 13.94s/it]09/05/2023 18:00:24 - INFO - __main__ -   Step: 4945, LR: 1.9087069462779365e-05, Loss: 0.11073944717645645
[2023-09-05 18:00:30,496] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4946/66600 [3:56:41<236:11:55, 13.79s/it]09/05/2023 18:00:37 - INFO - __main__ -   Step: 4946, LR: 1.9086759758585583e-05, Loss: 0.13868364691734314
[2023-09-05 18:00:45,159] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4947/66600 [3:56:56<243:43:34, 14.23s/it]09/05/2023 18:00:53 - INFO - __main__ -   Step: 4947, LR: 1.90864500543918e-05, Loss: 0.12419015914201736
[2023-09-05 18:00:58,865] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4948/66600 [3:57:10<239:42:29, 14.00s/it]09/05/2023 18:01:06 - INFO - __main__ -   Step: 4948, LR: 1.908614035019802e-05, Loss: 0.13130009174346924
[2023-09-05 18:01:13,525] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4949/66600 [3:57:24<241:37:40, 14.11s/it]09/05/2023 18:01:21 - INFO - __main__ -   Step: 4949, LR: 1.9085830646004238e-05, Loss: 0.09701137244701385
[2023-09-05 18:01:27,804] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4950/66600 [3:57:39<243:47:37, 14.24s/it]09/05/2023 18:01:35 - INFO - __main__ -   Step: 4950, LR: 1.9085520941810456e-05, Loss: 0.1475197970867157
[2023-09-05 18:01:42,193] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4951/66600 [3:57:54<247:52:01, 14.47s/it]09/05/2023 18:01:50 - INFO - __main__ -   Step: 4951, LR: 1.9085211237616674e-05, Loss: 0.09046405553817749
[2023-09-05 18:01:57,454] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4952/66600 [3:58:08<248:09:05, 14.49s/it]09/05/2023 18:02:05 - INFO - __main__ -   Step: 4952, LR: 1.9084901533422892e-05, Loss: 0.12989738583564758
[2023-09-05 18:02:11,034] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4953/66600 [3:58:22<243:05:41, 14.20s/it]09/05/2023 18:02:18 - INFO - __main__ -   Step: 4953, LR: 1.908459182922911e-05, Loss: 0.11833733320236206
[2023-09-05 18:02:24,713] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4954/66600 [3:58:36<242:06:04, 14.14s/it]09/05/2023 18:02:32 - INFO - __main__ -   Step: 4954, LR: 1.9084282125035328e-05, Loss: 0.1127835363149643
[2023-09-05 18:02:38,741] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4955/66600 [3:58:49<239:06:44, 13.96s/it]09/05/2023 18:02:46 - INFO - __main__ -   Step: 4955, LR: 1.9083972420841546e-05, Loss: 0.09806927293539047
[2023-09-05 18:02:52,187] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4956/66600 [3:59:03<239:42:23, 14.00s/it]09/05/2023 18:03:00 - INFO - __main__ -   Step: 4956, LR: 1.9083662716647764e-05, Loss: 0.09281787276268005
[2023-09-05 18:03:06,549] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4957/66600 [3:59:17<239:25:17, 13.98s/it]09/05/2023 18:03:14 - INFO - __main__ -   Step: 4957, LR: 1.9083353012453982e-05, Loss: 0.12641718983650208
[2023-09-05 18:03:20,526] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4958/66600 [3:59:32<243:29:16, 14.22s/it]09/05/2023 18:03:28 - INFO - __main__ -   Step: 4958, LR: 1.90830433082602e-05, Loss: 0.11494544893503189
[2023-09-05 18:03:35,616] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4959/66600 [3:59:46<243:46:16, 14.24s/it]09/05/2023 18:03:43 - INFO - __main__ -   Step: 4959, LR: 1.9082733604066418e-05, Loss: 0.1404186189174652
[2023-09-05 18:03:49,592] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4960/66600 [4:00:01<247:43:29, 14.47s/it]09/05/2023 18:03:58 - INFO - __main__ -   Step: 4960, LR: 1.9082423899872636e-05, Loss: 0.11547563225030899
[2023-09-05 18:04:04,853] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4961/66600 [4:00:15<245:11:18, 14.32s/it]09/05/2023 18:04:12 - INFO - __main__ -   Step: 4961, LR: 1.9082114195678854e-05, Loss: 0.16571563482284546
[2023-09-05 18:04:18,324] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4962/66600 [4:00:29<241:11:29, 14.09s/it]09/05/2023 18:04:25 - INFO - __main__ -   Step: 4962, LR: 1.9081804491485072e-05, Loss: 0.14529235661029816
[2023-09-05 18:04:31,651] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4963/66600 [4:00:43<239:39:58, 14.00s/it]09/05/2023 18:04:39 - INFO - __main__ -   Step: 4963, LR: 1.908149478729129e-05, Loss: 0.12444588541984558
[2023-09-05 18:04:45,807] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4964/66600 [4:00:57<241:26:49, 14.10s/it]09/05/2023 18:04:53 - INFO - __main__ -   Step: 4964, LR: 1.908118508309751e-05, Loss: 0.184204563498497
[2023-09-05 18:05:01,130] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4965/66600 [4:01:12<245:54:01, 14.36s/it]09/05/2023 18:05:08 - INFO - __main__ -   Step: 4965, LR: 1.9080875378903727e-05, Loss: 0.14895707368850708
[2023-09-05 18:05:15,377] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4966/66600 [4:01:26<243:37:28, 14.23s/it]09/05/2023 18:05:22 - INFO - __main__ -   Step: 4966, LR: 1.9080565674709945e-05, Loss: 0.1373414546251297
[2023-09-05 18:05:29,207] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4967/66600 [4:01:40<244:36:46, 14.29s/it]09/05/2023 18:05:37 - INFO - __main__ -   Step: 4967, LR: 1.9080255970516163e-05, Loss: 0.132627934217453
[2023-09-05 18:05:43,741] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4968/66600 [4:01:54<241:55:35, 14.13s/it]09/05/2023 18:05:50 - INFO - __main__ -   Step: 4968, LR: 1.907994626632238e-05, Loss: 0.11937563121318817
[2023-09-05 18:05:57,486] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4969/66600 [4:02:09<244:21:26, 14.27s/it]09/05/2023 18:06:05 - INFO - __main__ -   Step: 4969, LR: 1.90796365621286e-05, Loss: 0.1025746613740921
[2023-09-05 18:06:11,605] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4970/66600 [4:02:23<243:38:47, 14.23s/it]09/05/2023 18:06:19 - INFO - __main__ -   Step: 4970, LR: 1.9079326857934817e-05, Loss: 0.13437053561210632
[2023-09-05 18:06:27,105] [WARNING] [stage3.py:1898:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4971/66600 [4:02:39<254:55:38, 14.89s/it]09/05/2023 18:06:36 - INFO - __main__ -   Step: 4971, LR: 1.9079017153741035e-05, Loss: 0.14774540066719055
[2023-09-05 18:06:43,491] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4972/66600 [4:02:54<253:08:45, 14.79s/it]09/05/2023 18:06:50 - INFO - __main__ -   Step: 4972, LR: 1.9078707449547253e-05, Loss: 0.16020119190216064
[2023-09-05 18:06:56,708] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4973/66600 [4:03:07<244:58:29, 14.31s/it]09/05/2023 18:07:03 - INFO - __main__ -   Step: 4973, LR: 1.907839774535347e-05, Loss: 0.13485044240951538
[2023-09-05 18:07:10,683] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4974/66600 [4:03:21<242:45:41, 14.18s/it]09/05/2023 18:07:17 - INFO - __main__ -   Step: 4974, LR: 1.907808804115969e-05, Loss: 0.1417941302061081
[2023-09-05 18:07:23,671] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4975/66600 [4:03:34<238:43:03, 13.95s/it]09/05/2023 18:07:31 - INFO - __main__ -   Step: 4975, LR: 1.9077778336965907e-05, Loss: 0.15193235874176025
[2023-09-05 18:07:37,649] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4976/66600 [4:03:48<236:16:38, 13.80s/it]09/05/2023 18:07:44 - INFO - __main__ -   Step: 4976, LR: 1.9077468632772125e-05, Loss: 0.15492914617061615
[2023-09-05 18:07:50,632] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4977/66600 [4:04:01<234:53:42, 13.72s/it]09/05/2023 18:07:58 - INFO - __main__ -   Step: 4977, LR: 1.9077158928578343e-05, Loss: 0.09894377738237381
[2023-09-05 18:08:04,889] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4978/66600 [4:04:16<238:41:35, 13.94s/it]09/05/2023 18:08:12 - INFO - __main__ -   Step: 4978, LR: 1.907684922438456e-05, Loss: 0.14072836935520172
[2023-09-05 18:08:18,943] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4979/66600 [4:04:30<239:25:39, 13.99s/it]09/05/2023 18:08:26 - INFO - __main__ -   Step: 4979, LR: 1.907653952019078e-05, Loss: 0.15364190936088562
[2023-09-05 18:08:32,758] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4980/66600 [4:04:44<238:41:39, 13.95s/it]09/05/2023 18:08:40 - INFO - __main__ -   Step: 4980, LR: 1.9076229815996997e-05, Loss: 0.1359473466873169
[2023-09-05 18:08:46,597] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4981/66600 [4:04:57<237:22:58, 13.87s/it]09/05/2023 18:08:54 - INFO - __main__ -   Step: 4981, LR: 1.9075920111803215e-05, Loss: 0.11476899683475494
[2023-09-05 18:09:00,387] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4982/66600 [4:05:11<237:22:46, 13.87s/it]09/05/2023 18:09:08 - INFO - __main__ -   Step: 4982, LR: 1.9075610407609434e-05, Loss: 0.11508317291736603
[2023-09-05 18:09:14,737] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4983/66600 [4:05:25<239:16:01, 13.98s/it]09/05/2023 18:09:22 - INFO - __main__ -   Step: 4983, LR: 1.907530070341565e-05, Loss: 0.15056917071342468
[2023-09-05 18:09:28,468] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4984/66600 [4:05:39<238:39:11, 13.94s/it]09/05/2023 18:09:36 - INFO - __main__ -   Step: 4984, LR: 1.907499099922187e-05, Loss: 0.1155536025762558
[2023-09-05 18:09:42,036] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4985/66600 [4:05:54<240:33:50, 14.06s/it]09/05/2023 18:09:50 - INFO - __main__ -   Step: 4985, LR: 1.9074681295028088e-05, Loss: 0.1944849193096161
[2023-09-05 18:09:57,474] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4986/66600 [4:06:08<243:28:55, 14.23s/it]09/05/2023 18:10:05 - INFO - __main__ -   Step: 4986, LR: 1.9074371590834306e-05, Loss: 0.10806191712617874
[2023-09-05 18:10:11,714] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4987/66600 [4:06:23<244:06:26, 14.26s/it]09/05/2023 18:10:19 - INFO - __main__ -   Step: 4987, LR: 1.9074061886640524e-05, Loss: 0.13099423050880432
[2023-09-05 18:10:25,509] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4988/66600 [4:06:36<241:10:30, 14.09s/it]09/05/2023 18:10:33 - INFO - __main__ -   Step: 4988, LR: 1.9073752182446742e-05, Loss: 0.15668055415153503
[2023-09-05 18:10:39,261] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4989/66600 [4:06:49<235:33:14, 13.76s/it]09/05/2023 18:10:46 - INFO - __main__ -   Step: 4989, LR: 1.907344247825296e-05, Loss: 0.19118191301822662
[2023-09-05 18:10:52,095] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4990/66600 [4:07:03<233:28:19, 13.64s/it]09/05/2023 18:10:59 - INFO - __main__ -   Step: 4990, LR: 1.9073132774059178e-05, Loss: 0.1485484391450882
[2023-09-05 18:11:05,144] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4991/66600 [4:07:15<226:38:23, 13.24s/it]09/05/2023 18:11:11 - INFO - __main__ -   Step: 4991, LR: 1.9072823069865396e-05, Loss: 0.1491430401802063
[2023-09-05 18:11:17,625] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4992/66600 [4:07:29<228:37:56, 13.36s/it]09/05/2023 18:11:25 - INFO - __main__ -   Step: 4992, LR: 1.9072513365671614e-05, Loss: 0.10577619075775146
[2023-09-05 18:11:31,424] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4993/66600 [4:07:42<229:03:00, 13.38s/it]09/05/2023 18:11:38 - INFO - __main__ -   Step: 4993, LR: 1.9072203661477832e-05, Loss: 0.10779023915529251
[2023-09-05 18:11:45,530] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  7%|▋         | 4994/66600 [4:07:57<236:47:07, 13.84s/it]09/05/2023 18:11:53 - INFO - __main__ -   Step: 4994, LR: 1.907189395728405e-05, Loss: 0.1043192595243454
[2023-09-05 18:12:00,199] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 4995/66600 [4:08:11<238:58:39, 13.97s/it]09/05/2023 18:12:08 - INFO - __main__ -   Step: 4995, LR: 1.907158425309027e-05, Loss: 0.131269171833992
[2023-09-05 18:12:14,391] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 4996/66600 [4:08:25<237:18:48, 13.87s/it]09/05/2023 18:12:21 - INFO - __main__ -   Step: 4996, LR: 1.9071274548896486e-05, Loss: 0.11547303199768066
[2023-09-05 18:12:28,861] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 4997/66600 [4:08:40<242:14:13, 14.16s/it]09/05/2023 18:12:36 - INFO - __main__ -   Step: 4997, LR: 1.9070964844702704e-05, Loss: 0.1283893883228302
[2023-09-05 18:12:43,331] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 4998/66600 [4:08:55<247:08:14, 14.44s/it]09/05/2023 18:12:51 - INFO - __main__ -   Step: 4998, LR: 1.9070655140508922e-05, Loss: 0.10483552515506744
[2023-09-05 18:12:58,529] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 4999/66600 [4:09:10<249:26:22, 14.58s/it]09/05/2023 18:13:06 - INFO - __main__ -   Step: 4999, LR: 1.907034543631514e-05, Loss: 0.1335878074169159
[2023-09-05 18:13:12,885] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5000/66600 [4:09:23<242:19:02, 14.16s/it]09/05/2023 18:13:19 - INFO - __main__ -   Step: 5000, LR: 1.907003573212136e-05, Loss: 0.1135089099407196
09/05/2023 18:13:19 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.0017137527465820312

Evaluating:   0%|          | 1/228 [00:00<01:15,  2.99it/s][Astep: 1
extend+tolist() time: 0.0019063949584960938

Evaluating:   1%|          | 2/228 [00:00<01:04,  3.53it/s][Astep: 2
extend+tolist() time: 0.0021216869354248047

Evaluating:   1%|▏         | 3/228 [00:00<01:09,  3.25it/s][Astep: 3
extend+tolist() time: 0.001825094223022461

Evaluating:   2%|▏         | 4/228 [00:01<01:08,  3.26it/s][Astep: 4
extend+tolist() time: 0.0009512901306152344

Evaluating:   2%|▏         | 5/228 [00:01<01:03,  3.49it/s][Astep: 5
extend+tolist() time: 0.1299891471862793

Evaluating:   3%|▎         | 6/228 [00:01<01:16,  2.90it/s][Astep: 6
extend+tolist() time: 0.0022873878479003906

Evaluating:   3%|▎         | 7/228 [00:02<01:15,  2.92it/s][Astep: 7
extend+tolist() time: 0.0009255409240722656

Evaluating:   4%|▎         | 8/228 [00:02<01:08,  3.20it/s][Astep: 8
extend+tolist() time: 0.0011169910430908203

Evaluating:   4%|▍         | 9/228 [00:02<01:02,  3.49it/s][Astep: 9
extend+tolist() time: 0.0007688999176025391

Evaluating:   4%|▍         | 10/228 [00:02<00:58,  3.72it/s][Astep: 10
extend+tolist() time: 0.0012712478637695312

Evaluating:   5%|▍         | 11/228 [00:03<00:56,  3.82it/s][Astep: 11
extend+tolist() time: 0.0005803108215332031

Evaluating:   5%|▌         | 12/228 [00:03<00:53,  4.01it/s][Astep: 12
extend+tolist() time: 0.0010118484497070312

Evaluating:   6%|▌         | 13/228 [00:03<00:51,  4.16it/s][Astep: 13
extend+tolist() time: 0.0005800724029541016

Evaluating:   6%|▌         | 14/228 [00:03<00:50,  4.26it/s][Astep: 14
extend+tolist() time: 0.000553131103515625

Evaluating:   7%|▋         | 15/228 [00:04<00:49,  4.33it/s][Astep: 15
extend+tolist() time: 0.0010480880737304688

Evaluating:   7%|▋         | 16/228 [00:04<00:48,  4.36it/s][Astep: 16
extend+tolist() time: 0.0006074905395507812

Evaluating:   7%|▋         | 17/228 [00:04<00:47,  4.40it/s][Astep: 17
extend+tolist() time: 0.0012233257293701172

Evaluating:   8%|▊         | 18/228 [00:04<00:48,  4.33it/s][Astep: 18
extend+tolist() time: 0.0011212825775146484

Evaluating:   8%|▊         | 19/228 [00:05<00:51,  4.08it/s][Astep: 19
extend+tolist() time: 0.0013763904571533203

Evaluating:   9%|▉         | 20/228 [00:05<01:03,  3.28it/s][Astep: 20
extend+tolist() time: 0.0007994174957275391

Evaluating:   9%|▉         | 21/228 [00:05<00:58,  3.56it/s][Astep: 21
extend+tolist() time: 0.0010895729064941406

Evaluating:  10%|▉         | 22/228 [00:05<00:54,  3.79it/s][Astep: 22
extend+tolist() time: 0.0008025169372558594

Evaluating:  10%|█         | 23/228 [00:06<00:51,  3.98it/s][Astep: 23
extend+tolist() time: 0.0010864734649658203

Evaluating:  11%|█         | 24/228 [00:06<00:50,  4.07it/s][Astep: 24
extend+tolist() time: 0.0011069774627685547

Evaluating:  11%|█         | 25/228 [00:06<00:51,  3.91it/s][Astep: 25
extend+tolist() time: 0.001947164535522461

Evaluating:  11%|█▏        | 26/228 [00:07<00:56,  3.57it/s][Astep: 26
extend+tolist() time: 0.0011162757873535156

Evaluating:  12%|█▏        | 27/228 [00:07<01:01,  3.26it/s][Astep: 27
extend+tolist() time: 0.0016520023345947266

Evaluating:  12%|█▏        | 28/228 [00:07<01:09,  2.87it/s][Astep: 28
extend+tolist() time: 0.0003299713134765625

Evaluating:  13%|█▎        | 29/228 [00:08<01:01,  3.25it/s][Astep: 29
extend+tolist() time: 0.0007491111755371094

Evaluating:  13%|█▎        | 30/228 [00:08<00:56,  3.52it/s][Astep: 30
extend+tolist() time: 0.0015685558319091797

Evaluating:  14%|█▎        | 31/228 [00:08<00:55,  3.55it/s][Astep: 31
extend+tolist() time: 0.0005567073822021484

Evaluating:  14%|█▍        | 32/228 [00:08<00:51,  3.80it/s][Astep: 32
extend+tolist() time: 0.0013506412506103516

Evaluating:  14%|█▍        | 33/228 [00:09<00:51,  3.76it/s][Astep: 33
extend+tolist() time: 0.001613616943359375

Evaluating:  15%|█▍        | 34/228 [00:09<00:53,  3.61it/s][Astep: 34
extend+tolist() time: 0.0008461475372314453

Evaluating:  15%|█▌        | 35/228 [00:09<00:51,  3.74it/s][Astep: 35
extend+tolist() time: 0.15442848205566406

Evaluating:  16%|█▌        | 36/228 [00:10<00:58,  3.28it/s][Astep: 36
extend+tolist() time: 0.0008296966552734375

Evaluating:  16%|█▌        | 37/228 [00:10<00:56,  3.38it/s][Astep: 37
extend+tolist() time: 0.0013637542724609375

Evaluating:  17%|█▋        | 38/228 [00:10<00:57,  3.30it/s][Astep: 38
extend+tolist() time: 0.0009465217590332031

Evaluating:  17%|█▋        | 39/228 [00:10<00:56,  3.34it/s][Astep: 39
extend+tolist() time: 0.0008337497711181641

Evaluating:  18%|█▊        | 40/228 [00:11<00:55,  3.38it/s][Astep: 40
extend+tolist() time: 0.0005941390991210938

Evaluating:  18%|█▊        | 41/228 [00:11<00:53,  3.48it/s][Astep: 41
extend+tolist() time: 0.0008554458618164062

Evaluating:  18%|█▊        | 42/228 [00:11<00:51,  3.62it/s][Astep: 42
extend+tolist() time: 0.0011858940124511719

Evaluating:  19%|█▉        | 43/228 [00:11<00:52,  3.52it/s][Astep: 43
extend+tolist() time: 0.002017498016357422

Evaluating:  19%|█▉        | 44/228 [00:12<00:55,  3.33it/s][Astep: 44
extend+tolist() time: 0.001104593276977539

Evaluating:  20%|█▉        | 45/228 [00:12<00:51,  3.59it/s][Astep: 45
extend+tolist() time: 0.001638174057006836

Evaluating:  20%|██        | 46/228 [00:12<00:52,  3.50it/s][Astep: 46
extend+tolist() time: 0.001190185546875

Evaluating:  21%|██        | 47/228 [00:13<00:52,  3.43it/s][Astep: 47
extend+tolist() time: 0.001577615737915039

Evaluating:  21%|██        | 48/228 [00:13<00:51,  3.46it/s][Astep: 48
extend+tolist() time: 0.0016181468963623047

Evaluating:  21%|██▏       | 49/228 [00:13<00:52,  3.42it/s][Astep: 49
extend+tolist() time: 0.0013165473937988281

Evaluating:  22%|██▏       | 50/228 [00:14<00:49,  3.57it/s][Astep: 50
extend+tolist() time: 0.0012669563293457031

Evaluating:  22%|██▏       | 51/228 [00:14<01:01,  2.88it/s][Astep: 51
extend+tolist() time: 0.0016531944274902344

Evaluating:  23%|██▎       | 52/228 [00:14<00:58,  3.00it/s][Astep: 52
extend+tolist() time: 0.0013651847839355469

Evaluating:  23%|██▎       | 53/228 [00:15<00:55,  3.17it/s][Astep: 53
extend+tolist() time: 0.0016314983367919922

Evaluating:  24%|██▎       | 54/228 [00:15<00:54,  3.20it/s][Astep: 54
extend+tolist() time: 0.0008184909820556641

Evaluating:  24%|██▍       | 55/228 [00:15<00:50,  3.44it/s][Astep: 55
extend+tolist() time: 0.0012369155883789062

Evaluating:  25%|██▍       | 56/228 [00:15<00:47,  3.65it/s][Astep: 56
extend+tolist() time: 0.001238107681274414

Evaluating:  25%|██▌       | 57/228 [00:16<00:48,  3.55it/s][Astep: 57
extend+tolist() time: 0.0011851787567138672

Evaluating:  25%|██▌       | 58/228 [00:16<00:58,  2.89it/s][Astep: 58
extend+tolist() time: 0.0010001659393310547

Evaluating:  26%|██▌       | 59/228 [00:17<01:06,  2.54it/s][Astep: 59
extend+tolist() time: 0.0014424324035644531

Evaluating:  26%|██▋       | 60/228 [00:17<01:01,  2.74it/s][Astep: 60
extend+tolist() time: 0.0012791156768798828

Evaluating:  27%|██▋       | 61/228 [00:17<00:58,  2.87it/s][Astep: 61
extend+tolist() time: 0.001474142074584961

Evaluating:  27%|██▋       | 62/228 [00:18<00:56,  2.95it/s][Astep: 62
extend+tolist() time: 0.0007641315460205078

Evaluating:  28%|██▊       | 63/228 [00:18<00:52,  3.15it/s][Astep: 63
extend+tolist() time: 0.0012760162353515625

Evaluating:  28%|██▊       | 64/228 [00:18<00:47,  3.42it/s][Astep: 64
extend+tolist() time: 0.0008008480072021484

Evaluating:  29%|██▊       | 65/228 [00:18<00:44,  3.63it/s][Astep: 65
extend+tolist() time: 0.0012402534484863281

Evaluating:  29%|██▉       | 66/228 [00:19<00:42,  3.79it/s][Astep: 66
extend+tolist() time: 0.0007364749908447266

Evaluating:  29%|██▉       | 67/228 [00:19<00:40,  3.94it/s][Astep: 67
extend+tolist() time: 0.0013384819030761719

Evaluating:  30%|██▉       | 68/228 [00:19<00:40,  3.96it/s][Astep: 68
extend+tolist() time: 0.0007526874542236328

Evaluating:  30%|███       | 69/228 [00:19<00:38,  4.09it/s][Astep: 69
extend+tolist() time: 0.18365025520324707

Evaluating:  31%|███       | 70/228 [00:20<00:48,  3.23it/s][Astep: 70
extend+tolist() time: 0.0015172958374023438

Evaluating:  31%|███       | 71/228 [00:20<00:47,  3.33it/s][Astep: 71
extend+tolist() time: 0.0012819766998291016

Evaluating:  32%|███▏      | 72/228 [00:20<00:45,  3.43it/s][Astep: 72
extend+tolist() time: 0.0007772445678710938

Evaluating:  32%|███▏      | 73/228 [00:21<00:42,  3.66it/s][Astep: 73
extend+tolist() time: 0.0012333393096923828

Evaluating:  32%|███▏      | 74/228 [00:21<00:39,  3.85it/s][Astep: 74
extend+tolist() time: 0.0007219314575195312

Evaluating:  33%|███▎      | 75/228 [00:21<00:38,  4.00it/s][Astep: 75
extend+tolist() time: 0.0016396045684814453

Evaluating:  33%|███▎      | 76/228 [00:21<00:40,  3.75it/s][Astep: 76
extend+tolist() time: 0.0006401538848876953

Evaluating:  34%|███▍      | 77/228 [00:21<00:38,  3.93it/s][Astep: 77
extend+tolist() time: 0.0019609928131103516

Evaluating:  34%|███▍      | 78/228 [00:22<00:41,  3.59it/s][Astep: 78
extend+tolist() time: 0.001222372055053711

Evaluating:  35%|███▍      | 79/228 [00:22<00:40,  3.70it/s][Astep: 79
extend+tolist() time: 0.0012133121490478516

Evaluating:  35%|███▌      | 80/228 [00:22<00:39,  3.78it/s][Astep: 80
extend+tolist() time: 0.0008938312530517578

Evaluating:  36%|███▌      | 81/228 [00:23<00:38,  3.86it/s][Astep: 81
extend+tolist() time: 0.0014014244079589844

Evaluating:  36%|███▌      | 82/228 [00:23<00:38,  3.84it/s][Astep: 82
extend+tolist() time: 0.0011928081512451172

Evaluating:  36%|███▋      | 83/228 [00:23<00:38,  3.72it/s][Astep: 83
extend+tolist() time: 0.0013310909271240234

Evaluating:  37%|███▋      | 84/228 [00:23<00:38,  3.70it/s][Astep: 84
extend+tolist() time: 0.0011632442474365234

Evaluating:  37%|███▋      | 85/228 [00:24<00:39,  3.65it/s][Astep: 85
extend+tolist() time: 0.002487659454345703

Evaluating:  38%|███▊      | 86/228 [00:24<00:39,  3.59it/s][Astep: 86
extend+tolist() time: 0.001298666000366211

Evaluating:  38%|███▊      | 87/228 [00:24<00:39,  3.53it/s][Astep: 87
extend+tolist() time: 0.0009260177612304688

Evaluating:  39%|███▊      | 88/228 [00:25<00:38,  3.62it/s][Astep: 88
extend+tolist() time: 0.0013039112091064453

Evaluating:  39%|███▉      | 89/228 [00:25<00:36,  3.78it/s][Astep: 89
extend+tolist() time: 0.0007407665252685547

Evaluating:  39%|███▉      | 90/228 [00:25<00:35,  3.93it/s][Astep: 90
extend+tolist() time: 0.0013623237609863281

Evaluating:  40%|███▉      | 91/228 [00:25<00:42,  3.24it/s][Astep: 91
extend+tolist() time: 0.0009953975677490234

Evaluating:  40%|████      | 92/228 [00:26<00:38,  3.50it/s][Astep: 92
extend+tolist() time: 0.0012254714965820312

Evaluating:  41%|████      | 93/228 [00:26<00:38,  3.52it/s][Astep: 93
extend+tolist() time: 0.000949859619140625

Evaluating:  41%|████      | 94/228 [00:26<00:37,  3.58it/s][Astep: 94
extend+tolist() time: 0.0011496543884277344

Evaluating:  42%|████▏     | 95/228 [00:26<00:35,  3.79it/s][Astep: 95
extend+tolist() time: 0.0014386177062988281

Evaluating:  42%|████▏     | 96/228 [00:27<00:36,  3.65it/s][Astep: 96
extend+tolist() time: 0.0014286041259765625

Evaluating:  43%|████▎     | 97/228 [00:27<00:34,  3.75it/s][Astep: 97
extend+tolist() time: 0.0012140274047851562

Evaluating:  43%|████▎     | 98/228 [00:27<00:33,  3.89it/s][Astep: 98
extend+tolist() time: 0.0008728504180908203

Evaluating:  43%|████▎     | 99/228 [00:27<00:32,  3.92it/s][Astep: 99
extend+tolist() time: 0.0013298988342285156

Evaluating:  44%|████▍     | 100/228 [00:28<00:32,  3.95it/s][Astep: 100
extend+tolist() time: 0.0007393360137939453

Evaluating:  44%|████▍     | 101/228 [00:28<00:37,  3.34it/s][Astep: 101
extend+tolist() time: 0.0012936592102050781

Evaluating:  45%|████▍     | 102/228 [00:28<00:35,  3.55it/s][Astep: 102
extend+tolist() time: 0.0007734298706054688

Evaluating:  45%|████▌     | 103/228 [00:29<00:33,  3.76it/s][Astep: 103
extend+tolist() time: 0.001313924789428711

Evaluating:  46%|████▌     | 104/228 [00:29<00:39,  3.17it/s][Astep: 104
extend+tolist() time: 0.0007166862487792969

Evaluating:  46%|████▌     | 105/228 [00:29<00:35,  3.45it/s][Astep: 105
extend+tolist() time: 0.0013036727905273438

Evaluating:  46%|████▋     | 106/228 [00:30<00:33,  3.62it/s][Astep: 106
extend+tolist() time: 0.0014507770538330078

Evaluating:  47%|████▋     | 107/228 [00:30<00:35,  3.46it/s][Astep: 107
extend+tolist() time: 0.0007638931274414062

Evaluating:  47%|████▋     | 108/228 [00:30<00:32,  3.67it/s][Astep: 108
extend+tolist() time: 0.0012772083282470703

Evaluating:  48%|████▊     | 109/228 [00:30<00:31,  3.83it/s][Astep: 109
extend+tolist() time: 0.0009007453918457031

Evaluating:  48%|████▊     | 110/228 [00:31<00:30,  3.91it/s][Astep: 110
extend+tolist() time: 0.0006382465362548828

Evaluating:  49%|████▊     | 111/228 [00:31<00:29,  4.03it/s][Astep: 111
extend+tolist() time: 0.0019102096557617188

Evaluating:  49%|████▉     | 112/228 [00:31<00:31,  3.70it/s][Astep: 112
extend+tolist() time: 0.0007526874542236328

Evaluating:  50%|████▉     | 113/228 [00:31<00:29,  3.91it/s][Astep: 113
extend+tolist() time: 0.0007212162017822266

Evaluating:  50%|█████     | 114/228 [00:32<00:28,  4.04it/s][Astep: 114
extend+tolist() time: 0.001734018325805664

Evaluating:  50%|█████     | 115/228 [00:32<00:29,  3.88it/s][Astep: 115
extend+tolist() time: 0.0006771087646484375

Evaluating:  51%|█████     | 116/228 [00:32<00:27,  4.01it/s][Astep: 116
extend+tolist() time: 0.3142058849334717

Evaluating:  51%|█████▏    | 117/228 [00:33<00:37,  2.95it/s][Astep: 117
extend+tolist() time: 0.0008873939514160156

Evaluating:  52%|█████▏    | 118/228 [00:33<00:34,  3.21it/s][Astep: 118
extend+tolist() time: 0.0009734630584716797

Evaluating:  52%|█████▏    | 119/228 [00:33<00:31,  3.50it/s][Astep: 119
extend+tolist() time: 0.0007154941558837891

Evaluating:  53%|█████▎    | 120/228 [00:33<00:29,  3.72it/s][Astep: 120
extend+tolist() time: 0.0010483264923095703

Evaluating:  53%|█████▎    | 121/228 [00:34<00:27,  3.91it/s][Astep: 121
extend+tolist() time: 0.0006921291351318359

Evaluating:  54%|█████▎    | 122/228 [00:34<00:26,  4.02it/s][Astep: 122
extend+tolist() time: 0.0011315345764160156

Evaluating:  54%|█████▍    | 123/228 [00:34<00:25,  4.10it/s][Astep: 123
extend+tolist() time: 0.0006239414215087891

Evaluating:  54%|█████▍    | 124/228 [00:34<00:24,  4.20it/s][Astep: 124
extend+tolist() time: 0.0008599758148193359

Evaluating:  55%|█████▍    | 125/228 [00:34<00:24,  4.22it/s][Astep: 125
extend+tolist() time: 0.0004227161407470703

Evaluating:  55%|█████▌    | 126/228 [00:35<00:24,  4.19it/s][Astep: 126
extend+tolist() time: 0.0017464160919189453

Evaluating:  56%|█████▌    | 127/228 [00:35<00:26,  3.79it/s][Astep: 127
extend+tolist() time: 0.0013399124145507812

Evaluating:  56%|█████▌    | 128/228 [00:35<00:28,  3.52it/s][Astep: 128
extend+tolist() time: 0.0012371540069580078

Evaluating:  57%|█████▋    | 129/228 [00:36<00:26,  3.75it/s][Astep: 129
extend+tolist() time: 0.0008003711700439453

Evaluating:  57%|█████▋    | 130/228 [00:36<00:25,  3.91it/s][Astep: 130
extend+tolist() time: 0.001352548599243164

Evaluating:  57%|█████▋    | 131/228 [00:36<00:24,  3.93it/s][Astep: 131
extend+tolist() time: 0.0004551410675048828

Evaluating:  58%|█████▊    | 132/228 [00:36<00:23,  4.09it/s][Astep: 132
extend+tolist() time: 0.0015857219696044922

Evaluating:  58%|█████▊    | 133/228 [00:37<00:24,  3.93it/s][Astep: 133
extend+tolist() time: 0.0004551410675048828

Evaluating:  59%|█████▉    | 134/228 [00:37<00:23,  3.96it/s][Astep: 134
extend+tolist() time: 0.001474618911743164

Evaluating:  59%|█████▉    | 135/228 [00:37<00:23,  3.89it/s][Astep: 135
extend+tolist() time: 0.0005419254302978516

Evaluating:  60%|█████▉    | 136/228 [00:37<00:23,  3.93it/s][Astep: 136
extend+tolist() time: 0.0012035369873046875

Evaluating:  60%|██████    | 137/228 [00:38<00:22,  4.01it/s][Astep: 137
extend+tolist() time: 0.0003769397735595703

Evaluating:  61%|██████    | 138/228 [00:38<00:21,  4.16it/s][Astep: 138
extend+tolist() time: 0.0007145404815673828

Evaluating:  61%|██████    | 139/228 [00:38<00:21,  4.22it/s][Astep: 139
extend+tolist() time: 0.0004572868347167969

Evaluating:  61%|██████▏   | 140/228 [00:38<00:20,  4.28it/s][Astep: 140
extend+tolist() time: 0.0007607936859130859

Evaluating:  62%|██████▏   | 141/228 [00:38<00:20,  4.30it/s][Astep: 141
extend+tolist() time: 0.0007994174957275391

Evaluating:  62%|██████▏   | 142/228 [00:39<00:19,  4.31it/s][Astep: 142
extend+tolist() time: 0.001064300537109375

Evaluating:  63%|██████▎   | 143/228 [00:39<00:19,  4.35it/s][Astep: 143
extend+tolist() time: 0.0003383159637451172

Evaluating:  63%|██████▎   | 144/228 [00:39<00:18,  4.44it/s][Astep: 144
extend+tolist() time: 0.0006914138793945312

Evaluating:  64%|██████▎   | 145/228 [00:39<00:18,  4.43it/s][Astep: 145
extend+tolist() time: 0.0009415149688720703

Evaluating:  64%|██████▍   | 146/228 [00:40<00:18,  4.48it/s][Astep: 146
extend+tolist() time: 0.0007114410400390625

Evaluating:  64%|██████▍   | 147/228 [00:40<00:18,  4.49it/s][Astep: 147
extend+tolist() time: 0.0007581710815429688

Evaluating:  65%|██████▍   | 148/228 [00:40<00:17,  4.50it/s][Astep: 148
extend+tolist() time: 0.001085042953491211

Evaluating:  65%|██████▌   | 149/228 [00:40<00:17,  4.50it/s][Astep: 149
extend+tolist() time: 0.00034689903259277344

Evaluating:  66%|██████▌   | 150/228 [00:40<00:17,  4.52it/s][Astep: 150
extend+tolist() time: 0.0007898807525634766

Evaluating:  66%|██████▌   | 151/228 [00:41<00:17,  4.46it/s][Astep: 151
extend+tolist() time: 0.0010485649108886719

Evaluating:  67%|██████▋   | 152/228 [00:41<00:22,  3.32it/s][Astep: 152
extend+tolist() time: 0.0007879734039306641

Evaluating:  67%|██████▋   | 153/228 [00:41<00:20,  3.59it/s][Astep: 153
extend+tolist() time: 0.0013332366943359375

Evaluating:  68%|██████▊   | 154/228 [00:42<00:19,  3.74it/s][Astep: 154
extend+tolist() time: 0.0018055438995361328

Evaluating:  68%|██████▊   | 155/228 [00:42<00:20,  3.50it/s][Astep: 155
extend+tolist() time: 0.0006303787231445312

Evaluating:  68%|██████▊   | 156/228 [00:42<00:19,  3.74it/s][Astep: 156
extend+tolist() time: 0.0005064010620117188

Evaluating:  69%|██████▉   | 157/228 [00:42<00:18,  3.94it/s][Astep: 157
extend+tolist() time: 0.0010590553283691406

Evaluating:  69%|██████▉   | 158/228 [00:43<00:17,  4.11it/s][Astep: 158
extend+tolist() time: 0.0004990100860595703

Evaluating:  70%|██████▉   | 159/228 [00:43<00:16,  4.23it/s][Astep: 159
extend+tolist() time: 0.0006265640258789062

Evaluating:  70%|███████   | 160/228 [00:43<00:15,  4.28it/s][Astep: 160
extend+tolist() time: 0.0007772445678710938

Evaluating:  71%|███████   | 161/228 [00:43<00:15,  4.36it/s][Astep: 161
extend+tolist() time: 0.0006976127624511719

Evaluating:  71%|███████   | 162/228 [00:44<00:14,  4.41it/s][Astep: 162
extend+tolist() time: 0.00046634674072265625

Evaluating:  71%|███████▏  | 163/228 [00:44<00:14,  4.46it/s][Astep: 163
extend+tolist() time: 0.0003559589385986328

Evaluating:  72%|███████▏  | 164/228 [00:44<00:14,  4.47it/s][Astep: 164
extend+tolist() time: 0.0009479522705078125

Evaluating:  72%|███████▏  | 165/228 [00:44<00:14,  4.49it/s][Astep: 165
extend+tolist() time: 0.0004017353057861328

Evaluating:  73%|███████▎  | 166/228 [00:44<00:13,  4.52it/s][Astep: 166
extend+tolist() time: 0.0003349781036376953

Evaluating:  73%|███████▎  | 167/228 [00:45<00:13,  4.54it/s][Astep: 167
extend+tolist() time: 0.0005083084106445312

Evaluating:  74%|███████▎  | 168/228 [00:45<00:13,  4.53it/s][Astep: 168
extend+tolist() time: 0.0010411739349365234

Evaluating:  74%|███████▍  | 169/228 [00:45<00:13,  4.23it/s][Astep: 169
extend+tolist() time: 0.0003216266632080078

Evaluating:  75%|███████▍  | 170/228 [00:45<00:13,  4.33it/s][Astep: 170
extend+tolist() time: 0.0014176368713378906

Evaluating:  75%|███████▌  | 171/228 [00:46<00:13,  4.33it/s][Astep: 171
extend+tolist() time: 0.00026607513427734375

Evaluating:  75%|███████▌  | 172/228 [00:46<00:16,  3.48it/s][Astep: 172
extend+tolist() time: 0.0006906986236572266

Evaluating:  76%|███████▌  | 173/228 [00:46<00:14,  3.71it/s][Astep: 173
extend+tolist() time: 0.0014028549194335938

Evaluating:  76%|███████▋  | 174/228 [00:46<00:14,  3.71it/s][Astep: 174
extend+tolist() time: 0.0017812252044677734

Evaluating:  77%|███████▋  | 175/228 [00:47<00:17,  2.97it/s][Astep: 175
extend+tolist() time: 0.000823974609375

Evaluating:  77%|███████▋  | 176/228 [00:47<00:15,  3.30it/s][Astep: 176
extend+tolist() time: 0.0010609626770019531

Evaluating:  78%|███████▊  | 177/228 [00:47<00:14,  3.59it/s][Astep: 177
extend+tolist() time: 0.0005955696105957031

Evaluating:  78%|███████▊  | 178/228 [00:48<00:13,  3.84it/s][Astep: 178
extend+tolist() time: 0.001682281494140625

Evaluating:  79%|███████▊  | 179/228 [00:48<00:13,  3.76it/s][Astep: 179
extend+tolist() time: 0.00038933753967285156

Evaluating:  79%|███████▉  | 180/228 [00:48<00:12,  3.96it/s][Astep: 180
extend+tolist() time: 0.0003952980041503906

Evaluating:  79%|███████▉  | 181/228 [00:48<00:11,  4.15it/s][Astep: 181
extend+tolist() time: 0.0006210803985595703

Evaluating:  80%|███████▉  | 182/228 [00:49<00:10,  4.27it/s][Astep: 182
extend+tolist() time: 0.0012531280517578125

Evaluating:  80%|████████  | 183/228 [00:49<00:10,  4.32it/s][Astep: 183
extend+tolist() time: 0.0006680488586425781

Evaluating:  81%|████████  | 184/228 [00:49<00:10,  4.35it/s][Astep: 184
extend+tolist() time: 0.0004191398620605469

Evaluating:  81%|████████  | 185/228 [00:49<00:09,  4.41it/s][Astep: 185
extend+tolist() time: 0.00157928466796875

Evaluating:  82%|████████▏ | 186/228 [00:50<00:10,  4.18it/s][Astep: 186
extend+tolist() time: 0.001409769058227539

Evaluating:  82%|████████▏ | 187/228 [00:50<00:09,  4.16it/s][Astep: 187
extend+tolist() time: 0.0004451274871826172

Evaluating:  82%|████████▏ | 188/228 [00:50<00:09,  4.24it/s][Astep: 188
extend+tolist() time: 0.0007016658782958984

Evaluating:  83%|████████▎ | 189/228 [00:50<00:09,  4.33it/s][Astep: 189
extend+tolist() time: 0.0003726482391357422

Evaluating:  83%|████████▎ | 190/228 [00:50<00:08,  4.39it/s][Astep: 190
extend+tolist() time: 0.001596212387084961

Evaluating:  84%|████████▍ | 191/228 [00:51<00:08,  4.15it/s][Astep: 191
extend+tolist() time: 0.0008454322814941406

Evaluating:  84%|████████▍ | 192/228 [00:51<00:08,  4.24it/s][Astep: 192
extend+tolist() time: 0.0008077621459960938

Evaluating:  85%|████████▍ | 193/228 [00:51<00:08,  4.32it/s][Astep: 193
extend+tolist() time: 0.0010929107666015625

Evaluating:  85%|████████▌ | 194/228 [00:51<00:08,  4.24it/s][Astep: 194
extend+tolist() time: 0.0009741783142089844

Evaluating:  86%|████████▌ | 195/228 [00:52<00:07,  4.30it/s][Astep: 195
extend+tolist() time: 0.0005605220794677734

Evaluating:  86%|████████▌ | 196/228 [00:52<00:07,  4.38it/s][Astep: 196
extend+tolist() time: 0.0006759166717529297

Evaluating:  86%|████████▋ | 197/228 [00:52<00:07,  4.40it/s][Astep: 197
extend+tolist() time: 0.0007181167602539062

Evaluating:  87%|████████▋ | 198/228 [00:52<00:06,  4.44it/s][Astep: 198
extend+tolist() time: 0.2658965587615967

Evaluating:  87%|████████▋ | 199/228 [00:53<00:08,  3.30it/s][Astep: 199
extend+tolist() time: 0.0018608570098876953

Evaluating:  88%|████████▊ | 200/228 [00:53<00:08,  3.30it/s][Astep: 200
extend+tolist() time: 0.0007100105285644531

Evaluating:  88%|████████▊ | 201/228 [00:53<00:07,  3.59it/s][Astep: 201
extend+tolist() time: 0.0009129047393798828

Evaluating:  89%|████████▊ | 202/228 [00:53<00:06,  3.84it/s][Astep: 202
extend+tolist() time: 0.00047588348388671875

Evaluating:  89%|████████▉ | 203/228 [00:54<00:06,  4.04it/s][Astep: 203
extend+tolist() time: 0.0005192756652832031

Evaluating:  89%|████████▉ | 204/228 [00:54<00:05,  4.17it/s][Astep: 204
extend+tolist() time: 0.0003814697265625

Evaluating:  90%|████████▉ | 205/228 [00:54<00:05,  4.28it/s][Astep: 205
extend+tolist() time: 0.0006821155548095703

Evaluating:  90%|█████████ | 206/228 [00:54<00:05,  4.38it/s][Astep: 206
extend+tolist() time: 0.0006222724914550781

Evaluating:  91%|█████████ | 207/228 [00:55<00:04,  4.44it/s][Astep: 207
extend+tolist() time: 0.0006384849548339844

Evaluating:  91%|█████████ | 208/228 [00:55<00:04,  4.49it/s][Astep: 208
extend+tolist() time: 0.0011124610900878906

Evaluating:  92%|█████████▏| 209/228 [00:55<00:04,  4.46it/s][Astep: 209
extend+tolist() time: 0.0005996227264404297

Evaluating:  92%|█████████▏| 210/228 [00:55<00:04,  4.48it/s][Astep: 210
extend+tolist() time: 0.0005733966827392578

Evaluating:  93%|█████████▎| 211/228 [00:55<00:03,  4.50it/s][Astep: 211
extend+tolist() time: 0.0015511512756347656

Evaluating:  93%|█████████▎| 212/228 [00:56<00:03,  4.24it/s][Astep: 212
extend+tolist() time: 0.0009403228759765625

Evaluating:  93%|█████████▎| 213/228 [00:56<00:03,  4.25it/s][Astep: 213
extend+tolist() time: 0.0012056827545166016

Evaluating:  94%|█████████▍| 214/228 [00:56<00:03,  4.32it/s][Astep: 214
extend+tolist() time: 0.0008509159088134766

Evaluating:  94%|█████████▍| 215/228 [00:56<00:02,  4.37it/s][Astep: 215
extend+tolist() time: 0.0010738372802734375

Evaluating:  95%|█████████▍| 216/228 [00:57<00:02,  4.41it/s][Astep: 216
extend+tolist() time: 0.0009412765502929688

Evaluating:  95%|█████████▌| 217/228 [00:57<00:02,  4.47it/s][Astep: 217
extend+tolist() time: 0.000530242919921875

Evaluating:  96%|█████████▌| 218/228 [00:57<00:02,  4.47it/s][Astep: 218
extend+tolist() time: 0.0014934539794921875

Evaluating:  96%|█████████▌| 219/228 [00:57<00:02,  4.35it/s][Astep: 219
extend+tolist() time: 0.0004775524139404297

Evaluating:  96%|█████████▋| 220/228 [00:58<00:01,  4.42it/s][Astep: 220
extend+tolist() time: 0.00041174888610839844

Evaluating:  97%|█████████▋| 221/228 [00:58<00:01,  4.46it/s][Astep: 221
extend+tolist() time: 0.0011172294616699219

Evaluating:  97%|█████████▋| 222/228 [00:58<00:01,  4.47it/s][Astep: 222
extend+tolist() time: 0.00045490264892578125

Evaluating:  98%|█████████▊| 223/228 [00:58<00:01,  4.53it/s][Astep: 223
extend+tolist() time: 0.00038361549377441406

Evaluating:  98%|█████████▊| 224/228 [00:58<00:00,  4.54it/s][Astep: 224
extend+tolist() time: 0.00039577484130859375

Evaluating:  99%|█████████▊| 225/228 [00:59<00:00,  4.55it/s][Astep: 225
extend+tolist() time: 0.000446319580078125

Evaluating:  99%|█████████▉| 226/228 [00:59<00:00,  4.58it/s][Astep: 226
extend+tolist() time: 0.0010395050048828125

Evaluating: 100%|█████████▉| 227/228 [00:59<00:00,  4.57it/s][Astep: 227
extend+tolist() time: 0.00048351287841796875

Evaluating: 100%|██████████| 228/228 [00:59<00:00,  3.95it/s][A09/05/2023 18:14:19 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 18:14:20 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 18:14:21 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 18:14:21 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 18:14:21 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 18:14:21 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:01<00:00,  3.68it/s]
09/05/2023 18:14:21 - INFO - __main__ -   Step: 5000, Validation Metrics: {'pred_1_num': 9469, 'pred_-1_num': 944, 'pred_0_num': 388, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7805758726043884, 'f1_micro': 0.7805758726043884, 'f1_macro': 0.47083405158861186, 'f1_weighted': 0.7577401462377088, 'f1_-1': 0.3601593625498008, 'f1_0': 0.17916260954235635, 'f1_1': 0.8731801826736784, 'precision_micro': 0.7805758726043884, 'precision_macro': 0.5162851549739167, 'precision_weighted': 0.7463374608117099, 'precision_-1': 0.4788135593220339, 'precision_0': 0.23711340206185566, 'precision_1': 0.8329285035378604, 'recall_micro': 0.7805758726043884, 'recall_macro': 0.4500427328546404, 'recall_weighted': 0.7805758726043884, 'recall_-1': 0.2886334610472541, 'recall_0': 0.14397496087636932, 'recall_1': 0.9175197766402978, 'roc_auc_micro': 0.9008798426313385, 'roc_auc_macro': 0.7182360047714736, 'roc_auc_weighted': 0.6908821891074782, 'roc_auc_-1': 0.7874686851965944, 'roc_auc_0': 0.6941996618781991, 'roc_auc_1': 0.6730396672396273}
[2023-09-05 18:14:28,173] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5001/66600 [4:10:39<558:19:01, 32.63s/it]09/05/2023 18:14:35 - INFO - __main__ -   Step: 5001, LR: 1.9069726027927577e-05, Loss: 0.1487765908241272
[2023-09-05 18:14:42,274] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5002/66600 [4:10:54<467:55:34, 27.35s/it]09/05/2023 18:14:50 - INFO - __main__ -   Step: 5002, LR: 1.9069416323733795e-05, Loss: 0.11086215078830719
[2023-09-05 18:14:56,661] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5003/66600 [4:11:07<396:44:41, 23.19s/it]09/05/2023 18:15:04 - INFO - __main__ -   Step: 5003, LR: 1.9069106619540013e-05, Loss: 0.11354795843362808
[2023-09-05 18:15:10,408] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5004/66600 [4:11:21<351:20:36, 20.53s/it]09/05/2023 18:15:18 - INFO - __main__ -   Step: 5004, LR: 1.906879691534623e-05, Loss: 0.13357770442962646
[2023-09-05 18:15:24,728] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5005/66600 [4:11:36<320:00:37, 18.70s/it]09/05/2023 18:15:32 - INFO - __main__ -   Step: 5005, LR: 1.906848721115245e-05, Loss: 0.167092964053154
[2023-09-05 18:15:38,427] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5006/66600 [4:11:49<292:20:46, 17.09s/it]09/05/2023 18:15:46 - INFO - __main__ -   Step: 5006, LR: 1.9068177506958667e-05, Loss: 0.0702923908829689
[2023-09-05 18:15:52,156] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5007/66600 [4:12:04<278:34:33, 16.28s/it]09/05/2023 18:16:00 - INFO - __main__ -   Step: 5007, LR: 1.9067867802764885e-05, Loss: 0.10604580491781235
[2023-09-05 18:16:06,702] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5008/66600 [4:12:17<263:11:14, 15.38s/it]09/05/2023 18:16:13 - INFO - __main__ -   Step: 5008, LR: 1.9067558098571103e-05, Loss: 0.12129424512386322
[2023-09-05 18:16:19,362] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5009/66600 [4:12:31<257:05:35, 15.03s/it]09/05/2023 18:16:27 - INFO - __main__ -   Step: 5009, LR: 1.906724839437732e-05, Loss: 0.10267649590969086
[2023-09-05 18:16:35,045] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5010/66600 [4:12:46<257:20:38, 15.04s/it]09/05/2023 18:16:43 - INFO - __main__ -   Step: 5010, LR: 1.906693869018354e-05, Loss: 0.10189606249332428
[2023-09-05 18:16:51,040] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5011/66600 [4:13:02<262:04:55, 15.32s/it]09/05/2023 18:16:59 - INFO - __main__ -   Step: 5011, LR: 1.906662898598976e-05, Loss: 0.12393293529748917
[2023-09-05 18:17:05,759] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5012/66600 [4:13:16<255:12:53, 14.92s/it]09/05/2023 18:17:13 - INFO - __main__ -   Step: 5012, LR: 1.9066319281795975e-05, Loss: 0.1855054497718811
[2023-09-05 18:17:18,557] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5013/66600 [4:13:29<246:57:42, 14.44s/it]09/05/2023 18:17:26 - INFO - __main__ -   Step: 5013, LR: 1.9066009577602193e-05, Loss: 0.10567473620176315
[2023-09-05 18:17:32,770] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5014/66600 [4:13:43<244:36:25, 14.30s/it]09/05/2023 18:17:40 - INFO - __main__ -   Step: 5014, LR: 1.906569987340841e-05, Loss: 0.10922354459762573
[2023-09-05 18:17:46,423] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5015/66600 [4:13:57<241:15:17, 14.10s/it]09/05/2023 18:17:53 - INFO - __main__ -   Step: 5015, LR: 1.906539016921463e-05, Loss: 0.1376904398202896
[2023-09-05 18:18:00,506] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5016/66600 [4:14:12<244:26:34, 14.29s/it]09/05/2023 18:18:08 - INFO - __main__ -   Step: 5016, LR: 1.9065080465020848e-05, Loss: 0.11163167655467987
[2023-09-05 18:18:14,232] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5017/66600 [4:14:25<241:39:24, 14.13s/it]09/05/2023 18:18:22 - INFO - __main__ -   Step: 5017, LR: 1.9064770760827066e-05, Loss: 0.16100777685642242
[2023-09-05 18:18:28,559] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5018/66600 [4:14:40<241:52:46, 14.14s/it]09/05/2023 18:18:36 - INFO - __main__ -   Step: 5018, LR: 1.9064461056633287e-05, Loss: 0.16563302278518677
[2023-09-05 18:18:43,089] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5019/66600 [4:14:53<240:21:14, 14.05s/it]09/05/2023 18:18:50 - INFO - __main__ -   Step: 5019, LR: 1.9064151352439505e-05, Loss: 0.17037343978881836
[2023-09-05 18:18:56,571] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5020/66600 [4:15:08<241:01:18, 14.09s/it]09/05/2023 18:19:04 - INFO - __main__ -   Step: 5020, LR: 1.906384164824572e-05, Loss: 0.1311069279909134
[2023-09-05 18:19:11,247] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5021/66600 [4:15:22<244:14:26, 14.28s/it]09/05/2023 18:19:19 - INFO - __main__ -   Step: 5021, LR: 1.9063531944051938e-05, Loss: 0.11204982548952103
[2023-09-05 18:19:25,976] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5022/66600 [4:15:37<244:15:18, 14.28s/it]09/05/2023 18:19:33 - INFO - __main__ -   Step: 5022, LR: 1.9063222239858156e-05, Loss: 0.13375422358512878
[2023-09-05 18:19:40,345] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5023/66600 [4:15:51<245:13:47, 14.34s/it]09/05/2023 18:19:48 - INFO - __main__ -   Step: 5023, LR: 1.9062912535664374e-05, Loss: 0.16504335403442383
[2023-09-05 18:19:54,929] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5024/66600 [4:16:05<244:52:21, 14.32s/it]09/05/2023 18:20:02 - INFO - __main__ -   Step: 5024, LR: 1.9062602831470592e-05, Loss: 0.11673714965581894
[2023-09-05 18:20:09,477] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5025/66600 [4:16:20<248:52:42, 14.55s/it]09/05/2023 18:20:17 - INFO - __main__ -   Step: 5025, LR: 1.9062293127276814e-05, Loss: 0.17551788687705994
[2023-09-05 18:20:24,707] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5026/66600 [4:16:35<249:36:22, 14.59s/it]09/05/2023 18:20:32 - INFO - __main__ -   Step: 5026, LR: 1.906198342308303e-05, Loss: 0.18538472056388855
[2023-09-05 18:20:38,624] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5027/66600 [4:16:50<249:48:03, 14.61s/it]09/05/2023 18:20:46 - INFO - __main__ -   Step: 5027, LR: 1.9061673718889246e-05, Loss: 0.07679855823516846
[2023-09-05 18:20:53,333] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5028/66600 [4:17:04<248:07:48, 14.51s/it]09/05/2023 18:21:01 - INFO - __main__ -   Step: 5028, LR: 1.9061364014695464e-05, Loss: 0.11808951199054718
[2023-09-05 18:21:06,935] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5029/66600 [4:17:17<241:28:06, 14.12s/it]09/05/2023 18:21:14 - INFO - __main__ -   Step: 5029, LR: 1.9061054310501682e-05, Loss: 0.10995595157146454
[2023-09-05 18:21:19,962] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5030/66600 [4:17:31<237:29:26, 13.89s/it]09/05/2023 18:21:27 - INFO - __main__ -   Step: 5030, LR: 1.90607446063079e-05, Loss: 0.15320834517478943
[2023-09-05 18:21:33,163] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5031/66600 [4:17:44<235:27:44, 13.77s/it]09/05/2023 18:21:41 - INFO - __main__ -   Step: 5031, LR: 1.906043490211412e-05, Loss: 0.14141175150871277
[2023-09-05 18:21:47,130] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5032/66600 [4:17:58<235:01:03, 13.74s/it]09/05/2023 18:21:54 - INFO - __main__ -   Step: 5032, LR: 1.906012519792034e-05, Loss: 0.1559280902147293
[2023-09-05 18:22:01,433] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5033/66600 [4:18:12<238:51:21, 13.97s/it]09/05/2023 18:22:09 - INFO - __main__ -   Step: 5033, LR: 1.9059815493726558e-05, Loss: 0.13013914227485657
[2023-09-05 18:22:15,007] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5034/66600 [4:18:26<237:57:43, 13.91s/it]09/05/2023 18:22:23 - INFO - __main__ -   Step: 5034, LR: 1.9059505789532776e-05, Loss: 0.14371302723884583
[2023-09-05 18:22:29,668] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5035/66600 [4:18:40<239:33:35, 14.01s/it]09/05/2023 18:22:37 - INFO - __main__ -   Step: 5035, LR: 1.905919608533899e-05, Loss: 0.1441066563129425
[2023-09-05 18:22:43,871] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5036/66600 [4:18:55<243:06:13, 14.22s/it]09/05/2023 18:22:51 - INFO - __main__ -   Step: 5036, LR: 1.905888638114521e-05, Loss: 0.14171090722084045
[2023-09-05 18:22:58,845] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5037/66600 [4:19:09<244:00:59, 14.27s/it]09/05/2023 18:23:06 - INFO - __main__ -   Step: 5037, LR: 1.9058576676951427e-05, Loss: 0.14818178117275238
[2023-09-05 18:23:13,136] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5038/66600 [4:19:24<245:50:41, 14.38s/it]09/05/2023 18:23:21 - INFO - __main__ -   Step: 5038, LR: 1.9058266972757645e-05, Loss: 0.16439193487167358
[2023-09-05 18:23:27,416] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5039/66600 [4:19:38<244:00:42, 14.27s/it]09/05/2023 18:23:35 - INFO - __main__ -   Step: 5039, LR: 1.9057957268563866e-05, Loss: 0.1588294506072998
[2023-09-05 18:23:41,448] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5040/66600 [4:19:52<240:52:21, 14.09s/it]09/05/2023 18:23:48 - INFO - __main__ -   Step: 5040, LR: 1.9057647564370084e-05, Loss: 0.12911319732666016
[2023-09-05 18:23:55,379] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5041/66600 [4:20:06<241:14:58, 14.11s/it]09/05/2023 18:24:02 - INFO - __main__ -   Step: 5041, LR: 1.9057337860176302e-05, Loss: 0.14947232604026794
[2023-09-05 18:24:09,583] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5042/66600 [4:20:20<241:22:50, 14.12s/it]09/05/2023 18:24:16 - INFO - __main__ -   Step: 5042, LR: 1.9057028155982517e-05, Loss: 0.11825688928365707
[2023-09-05 18:24:23,732] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5043/66600 [4:20:34<241:07:30, 14.10s/it]09/05/2023 18:24:31 - INFO - __main__ -   Step: 5043, LR: 1.9056718451788735e-05, Loss: 0.14001411199569702
[2023-09-05 18:24:37,005] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5044/66600 [4:20:47<236:09:57, 13.81s/it]09/05/2023 18:24:44 - INFO - __main__ -   Step: 5044, LR: 1.9056408747594953e-05, Loss: 0.12661272287368774
[2023-09-05 18:24:50,802] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5045/66600 [4:21:02<239:59:36, 14.04s/it]09/05/2023 18:24:58 - INFO - __main__ -   Step: 5045, LR: 1.905609904340117e-05, Loss: 0.1278340071439743
[2023-09-05 18:25:05,014] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5046/66600 [4:21:16<238:57:57, 13.98s/it]09/05/2023 18:25:12 - INFO - __main__ -   Step: 5046, LR: 1.9055789339207393e-05, Loss: 0.13443808257579803
[2023-09-05 18:25:18,871] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5047/66600 [4:21:30<239:16:22, 13.99s/it]09/05/2023 18:25:26 - INFO - __main__ -   Step: 5047, LR: 1.905547963501361e-05, Loss: 0.10752291977405548
[2023-09-05 18:25:33,671] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5048/66600 [4:21:44<241:50:59, 14.15s/it]09/05/2023 18:25:41 - INFO - __main__ -   Step: 5048, LR: 1.905516993081983e-05, Loss: 0.13389146327972412
[2023-09-05 18:25:47,959] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5049/66600 [4:22:00<248:27:46, 14.53s/it]09/05/2023 18:25:56 - INFO - __main__ -   Step: 5049, LR: 1.9054860226626047e-05, Loss: 0.10928091406822205
[2023-09-05 18:26:03,104] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5050/66600 [4:22:14<248:56:56, 14.56s/it]09/05/2023 18:26:11 - INFO - __main__ -   Step: 5050, LR: 1.905455052243226e-05, Loss: 0.12727832794189453
[2023-09-05 18:26:17,444] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5051/66600 [4:22:28<247:22:59, 14.47s/it]09/05/2023 18:26:25 - INFO - __main__ -   Step: 5051, LR: 1.905424081823848e-05, Loss: 0.1300828903913498
[2023-09-05 18:26:31,044] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5052/66600 [4:22:41<239:52:33, 14.03s/it]09/05/2023 18:26:38 - INFO - __main__ -   Step: 5052, LR: 1.9053931114044698e-05, Loss: 0.1250477135181427
[2023-09-05 18:26:44,490] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5053/66600 [4:22:56<240:54:05, 14.09s/it]09/05/2023 18:26:52 - INFO - __main__ -   Step: 5053, LR: 1.905362140985092e-05, Loss: 0.13515058159828186
[2023-09-05 18:26:59,711] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5054/66600 [4:23:10<241:41:05, 14.14s/it]09/05/2023 18:27:06 - INFO - __main__ -   Step: 5054, LR: 1.9053311705657137e-05, Loss: 0.10174083709716797
[2023-09-05 18:27:13,944] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5055/66600 [4:23:25<246:42:06, 14.43s/it]09/05/2023 18:27:22 - INFO - __main__ -   Step: 5055, LR: 1.9053002001463355e-05, Loss: 0.18678203225135803
[2023-09-05 18:27:28,146] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5056/66600 [4:23:38<240:23:38, 14.06s/it]09/05/2023 18:27:35 - INFO - __main__ -   Step: 5056, LR: 1.9052692297269573e-05, Loss: 0.18261656165122986
[2023-09-05 18:27:41,144] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5057/66600 [4:23:52<240:49:21, 14.09s/it]09/05/2023 18:27:49 - INFO - __main__ -   Step: 5057, LR: 1.905238259307579e-05, Loss: 0.1591724455356598
[2023-09-05 18:27:56,467] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5058/66600 [4:24:08<247:29:13, 14.48s/it]09/05/2023 18:28:04 - INFO - __main__ -   Step: 5058, LR: 1.9052072888882006e-05, Loss: 0.1487209051847458
[2023-09-05 18:28:11,435] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5059/66600 [4:24:22<246:15:39, 14.41s/it]09/05/2023 18:28:19 - INFO - __main__ -   Step: 5059, LR: 1.9051763184688224e-05, Loss: 0.15828749537467957
[2023-09-05 18:28:25,302] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5060/66600 [4:24:36<244:33:24, 14.31s/it]09/05/2023 18:28:33 - INFO - __main__ -   Step: 5060, LR: 1.9051453480494446e-05, Loss: 0.10107860714197159
[2023-09-05 18:28:39,492] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5061/66600 [4:24:51<244:58:08, 14.33s/it]09/05/2023 18:28:47 - INFO - __main__ -   Step: 5061, LR: 1.9051143776300664e-05, Loss: 0.1419876664876938
[2023-09-05 18:28:53,300] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5062/66600 [4:25:04<241:36:05, 14.13s/it]09/05/2023 18:29:01 - INFO - __main__ -   Step: 5062, LR: 1.9050834072106882e-05, Loss: 0.11115991324186325
[2023-09-05 18:29:08,056] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5063/66600 [4:25:19<247:04:15, 14.45s/it]09/05/2023 18:29:16 - INFO - __main__ -   Step: 5063, LR: 1.90505243679131e-05, Loss: 0.1591160148382187
[2023-09-05 18:29:22,028] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5064/66600 [4:25:33<241:12:20, 14.11s/it]09/05/2023 18:29:29 - INFO - __main__ -   Step: 5064, LR: 1.9050214663719318e-05, Loss: 0.13346916437149048
[2023-09-05 18:29:36,231] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5065/66600 [4:25:47<241:11:07, 14.11s/it]09/05/2023 18:29:43 - INFO - __main__ -   Step: 5065, LR: 1.9049904959525533e-05, Loss: 0.09846262633800507
[2023-09-05 18:29:50,583] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5066/66600 [4:26:01<243:24:18, 14.24s/it]09/05/2023 18:29:58 - INFO - __main__ -   Step: 5066, LR: 1.904959525533175e-05, Loss: 0.10839235037565231
[2023-09-05 18:30:04,706] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5067/66600 [4:26:16<243:18:09, 14.23s/it]09/05/2023 18:30:12 - INFO - __main__ -   Step: 5067, LR: 1.9049285551137972e-05, Loss: 0.09593790024518967
[2023-09-05 18:30:18,584] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5068/66600 [4:26:29<239:49:05, 14.03s/it]09/05/2023 18:30:26 - INFO - __main__ -   Step: 5068, LR: 1.904897584694419e-05, Loss: 0.14383913576602936
[2023-09-05 18:30:32,709] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5069/66600 [4:26:43<240:40:21, 14.08s/it]09/05/2023 18:30:40 - INFO - __main__ -   Step: 5069, LR: 1.9048666142750408e-05, Loss: 0.13624869287014008
[2023-09-05 18:30:47,045] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5070/66600 [4:26:57<239:37:10, 14.02s/it]09/05/2023 18:30:54 - INFO - __main__ -   Step: 5070, LR: 1.9048356438556626e-05, Loss: 0.12255844473838806
[2023-09-05 18:31:01,013] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5071/66600 [4:27:11<240:04:49, 14.05s/it]09/05/2023 18:31:08 - INFO - __main__ -   Step: 5071, LR: 1.9048046734362844e-05, Loss: 0.14464694261550903
[2023-09-05 18:31:14,968] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5072/66600 [4:27:25<239:44:16, 14.03s/it]09/05/2023 18:31:22 - INFO - __main__ -   Step: 5072, LR: 1.9047737030169062e-05, Loss: 0.16938668489456177
[2023-09-05 18:31:29,119] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5073/66600 [4:27:40<242:06:26, 14.17s/it]09/05/2023 18:31:36 - INFO - __main__ -   Step: 5073, LR: 1.9047427325975277e-05, Loss: 0.10452251881361008
[2023-09-05 18:31:43,192] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5074/66600 [4:27:53<239:31:07, 14.01s/it]09/05/2023 18:31:50 - INFO - __main__ -   Step: 5074, LR: 1.90471176217815e-05, Loss: 0.1623898446559906
[2023-09-05 18:31:55,982] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5075/66600 [4:28:07<236:15:44, 13.82s/it]09/05/2023 18:32:03 - INFO - __main__ -   Step: 5075, LR: 1.9046807917587716e-05, Loss: 0.1708597093820572
[2023-09-05 18:32:10,441] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5076/66600 [4:28:22<241:06:13, 14.11s/it]09/05/2023 18:32:18 - INFO - __main__ -   Step: 5076, LR: 1.9046498213393935e-05, Loss: 0.16290244460105896
[2023-09-05 18:32:24,580] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5077/66600 [4:28:36<240:01:29, 14.04s/it]09/05/2023 18:32:32 - INFO - __main__ -   Step: 5077, LR: 1.9046188509200153e-05, Loss: 0.16442883014678955
[2023-09-05 18:32:38,429] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5078/66600 [4:28:49<238:38:39, 13.96s/it]09/05/2023 18:32:46 - INFO - __main__ -   Step: 5078, LR: 1.904587880500637e-05, Loss: 0.15677717328071594
[2023-09-05 18:32:52,565] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5079/66600 [4:29:03<238:49:02, 13.97s/it]09/05/2023 18:33:00 - INFO - __main__ -   Step: 5079, LR: 1.904556910081259e-05, Loss: 0.1674150824546814
[2023-09-05 18:33:07,117] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5080/66600 [4:29:18<242:08:49, 14.17s/it]09/05/2023 18:33:14 - INFO - __main__ -   Step: 5080, LR: 1.9045259396618807e-05, Loss: 0.17563587427139282
[2023-09-05 18:33:21,428] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5081/66600 [4:29:33<246:20:59, 14.42s/it]09/05/2023 18:33:29 - INFO - __main__ -   Step: 5081, LR: 1.9044949692425025e-05, Loss: 0.15496903657913208
[2023-09-05 18:33:36,327] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5082/66600 [4:29:46<241:37:57, 14.14s/it]09/05/2023 18:33:43 - INFO - __main__ -   Step: 5082, LR: 1.9044639988231243e-05, Loss: 0.14709189534187317
[2023-09-05 18:33:49,913] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5083/66600 [4:30:01<245:03:20, 14.34s/it]09/05/2023 18:33:58 - INFO - __main__ -   Step: 5083, LR: 1.904433028403746e-05, Loss: 0.11497195065021515
[2023-09-05 18:34:04,584] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5084/66600 [4:30:15<244:04:39, 14.28s/it]09/05/2023 18:34:12 - INFO - __main__ -   Step: 5084, LR: 1.904402057984368e-05, Loss: 0.13049198687076569
[2023-09-05 18:34:18,976] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5085/66600 [4:30:29<242:04:59, 14.17s/it]09/05/2023 18:34:26 - INFO - __main__ -   Step: 5085, LR: 1.9043710875649897e-05, Loss: 0.13749846816062927
[2023-09-05 18:34:32,639] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5086/66600 [4:30:44<244:17:14, 14.30s/it]09/05/2023 18:34:40 - INFO - __main__ -   Step: 5086, LR: 1.9043401171456115e-05, Loss: 0.12329152226448059
[2023-09-05 18:34:47,332] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5087/66600 [4:30:59<246:33:04, 14.43s/it]09/05/2023 18:34:55 - INFO - __main__ -   Step: 5087, LR: 1.9043091467262333e-05, Loss: 0.12971991300582886
[2023-09-05 18:35:01,764] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5088/66600 [4:31:12<242:04:40, 14.17s/it]09/05/2023 18:35:09 - INFO - __main__ -   Step: 5088, LR: 1.904278176306855e-05, Loss: 0.14462178945541382
[2023-09-05 18:35:15,749] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5089/66600 [4:31:27<245:34:05, 14.37s/it]09/05/2023 18:35:23 - INFO - __main__ -   Step: 5089, LR: 1.904247205887477e-05, Loss: 0.12323123216629028
[2023-09-05 18:35:30,249] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5090/66600 [4:31:40<240:03:25, 14.05s/it]09/05/2023 18:35:37 - INFO - __main__ -   Step: 5090, LR: 1.9042162354680987e-05, Loss: 0.14371538162231445
[2023-09-05 18:35:44,077] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5091/66600 [4:31:55<243:01:19, 14.22s/it]09/05/2023 18:35:51 - INFO - __main__ -   Step: 5091, LR: 1.9041852650487205e-05, Loss: 0.12695637345314026
[2023-09-05 18:35:58,422] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5092/66600 [4:32:10<246:07:20, 14.41s/it]09/05/2023 18:36:06 - INFO - __main__ -   Step: 5092, LR: 1.9041542946293424e-05, Loss: 0.09754675626754761
[2023-09-05 18:36:12,814] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5093/66600 [4:32:24<244:22:20, 14.30s/it]09/05/2023 18:36:20 - INFO - __main__ -   Step: 5093, LR: 1.904123324209964e-05, Loss: 0.11011935025453568
[2023-09-05 18:36:26,835] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5094/66600 [4:32:38<243:01:00, 14.22s/it]09/05/2023 18:36:34 - INFO - __main__ -   Step: 5094, LR: 1.904092353790586e-05, Loss: 0.12619070708751678
[2023-09-05 18:36:41,466] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5095/66600 [4:32:52<242:29:32, 14.19s/it]09/05/2023 18:36:48 - INFO - __main__ -   Step: 5095, LR: 1.9040613833712078e-05, Loss: 0.15467754006385803
[2023-09-05 18:36:55,467] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5096/66600 [4:33:06<242:22:15, 14.19s/it]09/05/2023 18:37:03 - INFO - __main__ -   Step: 5096, LR: 1.9040304129518296e-05, Loss: 0.12057226896286011
[2023-09-05 18:37:09,299] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5097/66600 [4:33:20<238:37:01, 13.97s/it]09/05/2023 18:37:16 - INFO - __main__ -   Step: 5097, LR: 1.9039994425324514e-05, Loss: 0.13728462159633636
[2023-09-05 18:37:22,090] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5098/66600 [4:33:33<233:13:25, 13.65s/it]09/05/2023 18:37:29 - INFO - __main__ -   Step: 5098, LR: 1.9039684721130732e-05, Loss: 0.1820305436849594
[2023-09-05 18:37:36,103] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5099/66600 [4:33:47<238:46:03, 13.98s/it]09/05/2023 18:37:44 - INFO - __main__ -   Step: 5099, LR: 1.903937501693695e-05, Loss: 0.12614363431930542
[2023-09-05 18:37:50,642] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5100/66600 [4:34:01<239:34:27, 14.02s/it]09/05/2023 18:37:58 - INFO - __main__ -   Step: 5100, LR: 1.9039065312743168e-05, Loss: 0.10938354581594467
09/05/2023 18:37:58 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.1254255771636963

Evaluating:   0%|          | 1/228 [00:00<01:43,  2.19it/s][Astep: 1
extend+tolist() time: 0.0010750293731689453

Evaluating:   1%|          | 2/228 [00:00<01:15,  3.00it/s][Astep: 2
extend+tolist() time: 0.0021185874938964844

Evaluating:   1%|▏         | 3/228 [00:01<01:15,  2.97it/s][Astep: 3
extend+tolist() time: 0.0019550323486328125

Evaluating:   2%|▏         | 4/228 [00:01<01:12,  3.08it/s][Astep: 4
extend+tolist() time: 0.0014450550079345703

Evaluating:   2%|▏         | 5/228 [00:01<01:17,  2.87it/s][Astep: 5
extend+tolist() time: 0.0019485950469970703

Evaluating:   3%|▎         | 6/228 [00:02<01:16,  2.92it/s][Astep: 6
extend+tolist() time: 0.0018558502197265625

Evaluating:   3%|▎         | 7/228 [00:02<01:15,  2.93it/s][Astep: 7
extend+tolist() time: 0.0009379386901855469

Evaluating:   4%|▎         | 8/228 [00:02<01:08,  3.20it/s][Astep: 8
extend+tolist() time: 0.0012426376342773438

Evaluating:   4%|▍         | 9/228 [00:03<01:11,  3.07it/s][Astep: 9
extend+tolist() time: 0.0008044242858886719

Evaluating:   4%|▍         | 10/228 [00:03<01:04,  3.37it/s][Astep: 10
extend+tolist() time: 0.0016493797302246094

Evaluating:   5%|▍         | 11/228 [00:03<01:00,  3.56it/s][Astep: 11
extend+tolist() time: 0.0005590915679931641

Evaluating:   5%|▌         | 12/228 [00:03<00:56,  3.82it/s][Astep: 12
extend+tolist() time: 0.0010728836059570312

Evaluating:   6%|▌         | 13/228 [00:03<00:53,  4.01it/s][Astep: 13
extend+tolist() time: 0.0005855560302734375

Evaluating:   6%|▌         | 14/228 [00:04<00:51,  4.15it/s][Astep: 14
extend+tolist() time: 0.0005338191986083984

Evaluating:   7%|▋         | 15/228 [00:04<00:50,  4.25it/s][Astep: 15
extend+tolist() time: 0.0010526180267333984

Evaluating:   7%|▋         | 16/228 [00:04<00:49,  4.30it/s][Astep: 16
extend+tolist() time: 0.0006167888641357422

Evaluating:   7%|▋         | 17/228 [00:04<00:48,  4.34it/s][Astep: 17
extend+tolist() time: 0.0012786388397216797

Evaluating:   8%|▊         | 18/228 [00:05<00:49,  4.28it/s][Astep: 18
extend+tolist() time: 0.001116037368774414

Evaluating:   8%|▊         | 19/228 [00:05<00:51,  4.05it/s][Astep: 19
extend+tolist() time: 0.0014834403991699219

Evaluating:   9%|▉         | 20/228 [00:05<00:52,  3.94it/s][Astep: 20
extend+tolist() time: 0.0007941722869873047

Evaluating:   9%|▉         | 21/228 [00:05<00:59,  3.49it/s][Astep: 21
extend+tolist() time: 0.0011081695556640625

Evaluating:  10%|▉         | 22/228 [00:06<00:55,  3.73it/s][Astep: 22
extend+tolist() time: 0.0007219314575195312

Evaluating:  10%|█         | 23/228 [00:06<00:52,  3.93it/s][Astep: 23
extend+tolist() time: 0.0010597705841064453

Evaluating:  11%|█         | 24/228 [00:06<00:50,  4.04it/s][Astep: 24
extend+tolist() time: 0.001108407974243164

Evaluating:  11%|█         | 25/228 [00:06<00:52,  3.90it/s][Astep: 25
extend+tolist() time: 0.0019698143005371094

Evaluating:  11%|█▏        | 26/228 [00:07<00:56,  3.56it/s][Astep: 26
extend+tolist() time: 0.14562201499938965

Evaluating:  12%|█▏        | 27/228 [00:07<01:01,  3.25it/s][Astep: 27
extend+tolist() time: 0.0017740726470947266

Evaluating:  12%|█▏        | 28/228 [00:07<01:01,  3.26it/s][Astep: 28
extend+tolist() time: 0.0003361701965332031

Evaluating:  13%|█▎        | 29/228 [00:08<00:55,  3.58it/s][Astep: 29
extend+tolist() time: 0.001043081283569336

Evaluating:  13%|█▎        | 30/228 [00:08<00:51,  3.81it/s][Astep: 30
extend+tolist() time: 0.0011031627655029297

Evaluating:  14%|█▎        | 31/228 [00:08<00:52,  3.73it/s][Astep: 31
extend+tolist() time: 0.0009775161743164062

Evaluating:  14%|█▍        | 32/228 [00:08<00:49,  3.94it/s][Astep: 32
extend+tolist() time: 0.0009922981262207031

Evaluating:  14%|█▍        | 33/228 [00:09<00:50,  3.85it/s][Astep: 33
extend+tolist() time: 0.0016705989837646484

Evaluating:  15%|█▍        | 34/228 [00:09<00:52,  3.67it/s][Astep: 34
extend+tolist() time: 0.0012726783752441406

Evaluating:  15%|█▌        | 35/228 [00:09<00:59,  3.25it/s][Astep: 35
extend+tolist() time: 0.0006537437438964844

Evaluating:  16%|█▌        | 36/228 [00:10<00:54,  3.54it/s][Astep: 36
extend+tolist() time: 0.0012166500091552734

Evaluating:  16%|█▌        | 37/228 [00:10<00:50,  3.75it/s][Astep: 37
extend+tolist() time: 0.0016024112701416016

Evaluating:  17%|█▋        | 38/228 [00:10<00:52,  3.59it/s][Astep: 38
extend+tolist() time: 0.0007929801940917969

Evaluating:  17%|█▋        | 39/228 [00:10<00:49,  3.80it/s][Astep: 39
extend+tolist() time: 0.0011510848999023438

Evaluating:  18%|█▊        | 40/228 [00:11<00:47,  3.97it/s][Astep: 40
extend+tolist() time: 0.0006206035614013672

Evaluating:  18%|█▊        | 41/228 [00:11<00:45,  4.11it/s][Astep: 41
extend+tolist() time: 0.0008571147918701172

Evaluating:  18%|█▊        | 42/228 [00:11<00:53,  3.47it/s][Astep: 42
extend+tolist() time: 0.0017075538635253906

Evaluating:  19%|█▉        | 43/228 [00:11<00:54,  3.42it/s][Astep: 43
extend+tolist() time: 0.0019369125366210938

Evaluating:  19%|█▉        | 44/228 [00:12<00:56,  3.28it/s][Astep: 44
extend+tolist() time: 0.0010776519775390625

Evaluating:  20%|█▉        | 45/228 [00:12<00:51,  3.56it/s][Astep: 45
extend+tolist() time: 0.00165557861328125

Evaluating:  20%|██        | 46/228 [00:12<00:52,  3.48it/s][Astep: 46
extend+tolist() time: 0.0012192726135253906

Evaluating:  21%|██        | 47/228 [00:13<00:53,  3.41it/s][Astep: 47
extend+tolist() time: 0.0015954971313476562

Evaluating:  21%|██        | 48/228 [00:13<00:52,  3.46it/s][Astep: 48
extend+tolist() time: 0.0016078948974609375

Evaluating:  21%|██▏       | 49/228 [00:13<00:52,  3.41it/s][Astep: 49
extend+tolist() time: 0.0009710788726806641

Evaluating:  22%|██▏       | 50/228 [00:13<00:50,  3.56it/s][Astep: 50
extend+tolist() time: 0.0017337799072265625

Evaluating:  22%|██▏       | 51/228 [00:14<00:59,  2.98it/s][Astep: 51
extend+tolist() time: 0.0015358924865722656

Evaluating:  23%|██▎       | 52/228 [00:14<00:57,  3.08it/s][Astep: 52
extend+tolist() time: 0.0013871192932128906

Evaluating:  23%|██▎       | 53/228 [00:15<00:54,  3.23it/s][Astep: 53
extend+tolist() time: 0.0016384124755859375

Evaluating:  24%|██▎       | 54/228 [00:15<00:53,  3.25it/s][Astep: 54
extend+tolist() time: 0.0008358955383300781

Evaluating:  24%|██▍       | 55/228 [00:15<00:49,  3.47it/s][Astep: 55
extend+tolist() time: 0.0012326240539550781

Evaluating:  25%|██▍       | 56/228 [00:15<00:46,  3.69it/s][Astep: 56
extend+tolist() time: 0.0012233257293701172

Evaluating:  25%|██▌       | 57/228 [00:16<00:47,  3.58it/s][Astep: 57
extend+tolist() time: 0.15609049797058105

Evaluating:  25%|██▌       | 58/228 [00:16<00:52,  3.24it/s][Astep: 58
extend+tolist() time: 0.0008463859558105469

Evaluating:  26%|██▌       | 59/228 [00:16<00:49,  3.44it/s][Astep: 59
extend+tolist() time: 0.0008881092071533203

Evaluating:  26%|██▋       | 60/228 [00:16<00:46,  3.59it/s][Astep: 60
extend+tolist() time: 0.001127481460571289

Evaluating:  27%|██▋       | 61/228 [00:17<00:43,  3.80it/s][Astep: 61
extend+tolist() time: 0.0008304119110107422

Evaluating:  27%|██▋       | 62/228 [00:17<00:42,  3.88it/s][Astep: 62
extend+tolist() time: 0.001155853271484375

Evaluating:  28%|██▊       | 63/228 [00:17<00:41,  4.00it/s][Astep: 63
extend+tolist() time: 0.0007956027984619141

Evaluating:  28%|██▊       | 64/228 [00:17<00:40,  4.09it/s][Astep: 64
extend+tolist() time: 0.0012395381927490234

Evaluating:  29%|██▊       | 65/228 [00:18<00:39,  4.13it/s][Astep: 65
extend+tolist() time: 0.0008008480072021484

Evaluating:  29%|██▉       | 66/228 [00:18<00:39,  4.15it/s][Astep: 66
extend+tolist() time: 0.0011610984802246094

Evaluating:  29%|██▉       | 67/228 [00:18<00:38,  4.20it/s][Astep: 67
extend+tolist() time: 0.0008792877197265625

Evaluating:  30%|██▉       | 68/228 [00:18<00:38,  4.18it/s][Astep: 68
extend+tolist() time: 0.0012054443359375

Evaluating:  30%|███       | 69/228 [00:19<00:45,  3.52it/s][Astep: 69
extend+tolist() time: 0.0017774105072021484

Evaluating:  31%|███       | 70/228 [00:19<00:44,  3.54it/s][Astep: 70
extend+tolist() time: 0.0010592937469482422

Evaluating:  31%|███       | 71/228 [00:19<00:44,  3.56it/s][Astep: 71
extend+tolist() time: 0.0015025138854980469

Evaluating:  32%|███▏      | 72/228 [00:20<00:43,  3.59it/s][Astep: 72
extend+tolist() time: 0.0007781982421875

Evaluating:  32%|███▏      | 73/228 [00:20<00:40,  3.79it/s][Astep: 73
extend+tolist() time: 0.0012671947479248047

Evaluating:  32%|███▏      | 74/228 [00:20<00:38,  3.96it/s][Astep: 74
extend+tolist() time: 0.0007398128509521484

Evaluating:  33%|███▎      | 75/228 [00:20<00:37,  4.07it/s][Astep: 75
extend+tolist() time: 0.0017037391662597656

Evaluating:  33%|███▎      | 76/228 [00:21<00:40,  3.80it/s][Astep: 76
extend+tolist() time: 0.00061798095703125

Evaluating:  34%|███▍      | 77/228 [00:21<00:45,  3.34it/s][Astep: 77
extend+tolist() time: 0.0018813610076904297

Evaluating:  34%|███▍      | 78/228 [00:21<00:46,  3.22it/s][Astep: 78
extend+tolist() time: 0.0012426376342773438

Evaluating:  35%|███▍      | 79/228 [00:22<00:43,  3.42it/s][Astep: 79
extend+tolist() time: 0.0012760162353515625

Evaluating:  35%|███▌      | 80/228 [00:22<00:41,  3.57it/s][Astep: 80
extend+tolist() time: 0.0014460086822509766

Evaluating:  36%|███▌      | 81/228 [00:22<00:39,  3.69it/s][Astep: 81
extend+tolist() time: 0.0012311935424804688

Evaluating:  36%|███▌      | 82/228 [00:22<00:38,  3.80it/s][Astep: 82
extend+tolist() time: 0.0018246173858642578

Evaluating:  36%|███▋      | 83/228 [00:23<00:37,  3.89it/s][Astep: 83
extend+tolist() time: 0.0011186599731445312

Evaluating:  37%|███▋      | 84/228 [00:23<00:35,  4.05it/s][Astep: 84
extend+tolist() time: 0.0010249614715576172

Evaluating:  37%|███▋      | 85/228 [00:23<00:36,  3.94it/s][Astep: 85
extend+tolist() time: 0.0013425350189208984

Evaluating:  38%|███▊      | 86/228 [00:23<00:35,  3.95it/s][Astep: 86
extend+tolist() time: 0.0008797645568847656

Evaluating:  38%|███▊      | 87/228 [00:24<00:35,  3.99it/s][Astep: 87
extend+tolist() time: 0.0013766288757324219

Evaluating:  39%|███▊      | 88/228 [00:24<00:35,  4.00it/s][Astep: 88
extend+tolist() time: 0.0011556148529052734

Evaluating:  39%|███▉      | 89/228 [00:24<00:33,  4.12it/s][Astep: 89
extend+tolist() time: 0.0007114410400390625

Evaluating:  39%|███▉      | 90/228 [00:24<00:32,  4.19it/s][Astep: 90
extend+tolist() time: 0.0013141632080078125

Evaluating:  40%|███▉      | 91/228 [00:24<00:33,  4.15it/s][Astep: 91
extend+tolist() time: 0.000762939453125

Evaluating:  40%|████      | 92/228 [00:25<00:39,  3.45it/s][Astep: 92
extend+tolist() time: 0.001194000244140625

Evaluating:  41%|████      | 93/228 [00:25<00:36,  3.67it/s][Astep: 93
extend+tolist() time: 0.0009393692016601562

Evaluating:  41%|████      | 94/228 [00:25<00:36,  3.69it/s][Astep: 94
extend+tolist() time: 0.0011441707611083984

Evaluating:  42%|████▏     | 95/228 [00:26<00:34,  3.88it/s][Astep: 95
extend+tolist() time: 0.0012042522430419922

Evaluating:  42%|████▏     | 96/228 [00:26<00:35,  3.72it/s][Astep: 96
extend+tolist() time: 0.0014383792877197266

Evaluating:  43%|████▎     | 97/228 [00:26<00:34,  3.78it/s][Astep: 97
extend+tolist() time: 0.0011835098266601562

Evaluating:  43%|████▎     | 98/228 [00:26<00:33,  3.93it/s][Astep: 98
extend+tolist() time: 0.0008749961853027344

Evaluating:  43%|████▎     | 99/228 [00:27<00:32,  3.96it/s][Astep: 99
extend+tolist() time: 0.0013399124145507812

Evaluating:  44%|████▍     | 100/228 [00:27<00:32,  3.99it/s][Astep: 100
extend+tolist() time: 0.0007152557373046875

Evaluating:  44%|████▍     | 101/228 [00:27<00:30,  4.11it/s][Astep: 101
extend+tolist() time: 0.18812823295593262

Evaluating:  45%|████▍     | 102/228 [00:28<00:37,  3.35it/s][Astep: 102
extend+tolist() time: 0.0007827281951904297

Evaluating:  45%|████▌     | 103/228 [00:28<00:34,  3.62it/s][Astep: 103
extend+tolist() time: 0.0011796951293945312

Evaluating:  46%|████▌     | 104/228 [00:28<00:32,  3.82it/s][Astep: 104
extend+tolist() time: 0.0006794929504394531

Evaluating:  46%|████▌     | 105/228 [00:28<00:30,  3.98it/s][Astep: 105
extend+tolist() time: 0.0012772083282470703

Evaluating:  46%|████▋     | 106/228 [00:28<00:30,  4.04it/s][Astep: 106
extend+tolist() time: 0.0017313957214355469

Evaluating:  47%|████▋     | 107/228 [00:29<00:32,  3.72it/s][Astep: 107
extend+tolist() time: 0.0007505416870117188

Evaluating:  47%|████▋     | 108/228 [00:29<00:30,  3.90it/s][Astep: 108
extend+tolist() time: 0.0012285709381103516

Evaluating:  48%|████▊     | 109/228 [00:29<00:29,  4.01it/s][Astep: 109
extend+tolist() time: 0.0008971691131591797

Evaluating:  48%|████▊     | 110/228 [00:29<00:29,  4.06it/s][Astep: 110
extend+tolist() time: 0.001064300537109375

Evaluating:  49%|████▊     | 111/228 [00:30<00:28,  4.17it/s][Astep: 111
extend+tolist() time: 0.0017609596252441406

Evaluating:  49%|████▉     | 112/228 [00:30<00:30,  3.78it/s][Astep: 112
extend+tolist() time: 0.00040340423583984375

Evaluating:  50%|████▉     | 113/228 [00:30<00:28,  3.97it/s][Astep: 113
extend+tolist() time: 0.0006999969482421875

Evaluating:  50%|█████     | 114/228 [00:30<00:27,  4.10it/s][Astep: 114
extend+tolist() time: 0.001598358154296875

Evaluating:  50%|█████     | 115/228 [00:31<00:35,  3.21it/s][Astep: 115
extend+tolist() time: 0.000972747802734375

Evaluating:  51%|█████     | 116/228 [00:31<00:32,  3.49it/s][Astep: 116
extend+tolist() time: 0.0008082389831542969

Evaluating:  51%|█████▏    | 117/228 [00:31<00:30,  3.69it/s][Astep: 117
extend+tolist() time: 0.0013015270233154297

Evaluating:  52%|█████▏    | 118/228 [00:32<00:28,  3.81it/s][Astep: 118
extend+tolist() time: 0.0006096363067626953

Evaluating:  52%|█████▏    | 119/228 [00:32<00:27,  3.99it/s][Astep: 119
extend+tolist() time: 0.0010356903076171875

Evaluating:  53%|█████▎    | 120/228 [00:32<00:26,  4.11it/s][Astep: 120
extend+tolist() time: 0.001079559326171875

Evaluating:  53%|█████▎    | 121/228 [00:32<00:25,  4.22it/s][Astep: 121
extend+tolist() time: 0.0006635189056396484

Evaluating:  54%|█████▎    | 122/228 [00:33<00:24,  4.30it/s][Astep: 122
extend+tolist() time: 0.0011458396911621094

Evaluating:  54%|█████▍    | 123/228 [00:33<00:24,  4.35it/s][Astep: 123
extend+tolist() time: 0.0006456375122070312

Evaluating:  54%|█████▍    | 124/228 [00:33<00:23,  4.40it/s][Astep: 124
extend+tolist() time: 0.0008645057678222656

Evaluating:  55%|█████▍    | 125/228 [00:33<00:23,  4.35it/s][Astep: 125
extend+tolist() time: 0.0008776187896728516

Evaluating:  55%|█████▌    | 126/228 [00:33<00:23,  4.41it/s][Astep: 126
extend+tolist() time: 0.0016772747039794922

Evaluating:  56%|█████▌    | 127/228 [00:34<00:25,  3.92it/s][Astep: 127
extend+tolist() time: 0.0013606548309326172

Evaluating:  56%|█████▌    | 128/228 [00:34<00:32,  3.05it/s][Astep: 128
extend+tolist() time: 0.0011343955993652344

Evaluating:  57%|█████▋    | 129/228 [00:34<00:29,  3.37it/s][Astep: 129
extend+tolist() time: 0.0007665157318115234

Evaluating:  57%|█████▋    | 130/228 [00:35<00:27,  3.62it/s][Astep: 130
extend+tolist() time: 0.0013539791107177734

Evaluating:  57%|█████▋    | 131/228 [00:35<00:25,  3.73it/s][Astep: 131
extend+tolist() time: 0.00045561790466308594

Evaluating:  58%|█████▊    | 132/228 [00:35<00:24,  3.96it/s][Astep: 132
extend+tolist() time: 0.0014679431915283203

Evaluating:  58%|█████▊    | 133/228 [00:35<00:24,  3.85it/s][Astep: 133
extend+tolist() time: 0.00044155120849609375

Evaluating:  59%|█████▉    | 134/228 [00:36<00:23,  4.04it/s][Astep: 134
extend+tolist() time: 0.00144195556640625

Evaluating:  59%|█████▉    | 135/228 [00:36<00:23,  3.95it/s][Astep: 135
extend+tolist() time: 0.0004277229309082031

Evaluating:  60%|█████▉    | 136/228 [00:36<00:22,  4.11it/s][Astep: 136
extend+tolist() time: 0.0011973381042480469

Evaluating:  60%|██████    | 137/228 [00:36<00:21,  4.17it/s][Astep: 137
extend+tolist() time: 0.00040078163146972656

Evaluating:  61%|██████    | 138/228 [00:37<00:20,  4.29it/s][Astep: 138
extend+tolist() time: 0.0007543563842773438

Evaluating:  61%|██████    | 139/228 [00:37<00:20,  4.33it/s][Astep: 139
extend+tolist() time: 0.00045990943908691406

Evaluating:  61%|██████▏   | 140/228 [00:37<00:19,  4.41it/s][Astep: 140
extend+tolist() time: 0.0012197494506835938

Evaluating:  62%|██████▏   | 141/228 [00:37<00:19,  4.39it/s][Astep: 141
extend+tolist() time: 0.0007767677307128906

Evaluating:  62%|██████▏   | 142/228 [00:37<00:19,  4.40it/s][Astep: 142
extend+tolist() time: 0.0010404586791992188

Evaluating:  63%|██████▎   | 143/228 [00:38<00:19,  4.44it/s][Astep: 143
extend+tolist() time: 0.00033926963806152344

Evaluating:  63%|██████▎   | 144/228 [00:38<00:18,  4.51it/s][Astep: 144
extend+tolist() time: 0.0007135868072509766

Evaluating:  64%|██████▎   | 145/228 [00:38<00:18,  4.47it/s][Astep: 145
extend+tolist() time: 0.0009379386901855469

Evaluating:  64%|██████▍   | 146/228 [00:38<00:18,  4.53it/s][Astep: 146
extend+tolist() time: 0.00038361549377441406

Evaluating:  64%|██████▍   | 147/228 [00:39<00:17,  4.54it/s][Astep: 147
extend+tolist() time: 0.0007355213165283203

Evaluating:  65%|██████▍   | 148/228 [00:39<00:17,  4.52it/s][Astep: 148
extend+tolist() time: 0.0011146068572998047

Evaluating:  65%|██████▌   | 149/228 [00:39<00:17,  4.53it/s][Astep: 149
extend+tolist() time: 0.0003521442413330078

Evaluating:  66%|██████▌   | 150/228 [00:39<00:17,  4.54it/s][Astep: 150
extend+tolist() time: 0.0008037090301513672

Evaluating:  66%|██████▌   | 151/228 [00:39<00:17,  4.48it/s][Astep: 151
extend+tolist() time: 0.001046895980834961

Evaluating:  67%|██████▋   | 152/228 [00:40<00:16,  4.50it/s][Astep: 152
extend+tolist() time: 0.0008249282836914062

Evaluating:  67%|██████▋   | 153/228 [00:40<00:16,  4.48it/s][Astep: 153
extend+tolist() time: 0.0013031959533691406

Evaluating:  68%|██████▊   | 154/228 [00:40<00:17,  4.35it/s][Astep: 154
extend+tolist() time: 0.001809835433959961

Evaluating:  68%|██████▊   | 155/228 [00:41<00:23,  3.14it/s][Astep: 155
extend+tolist() time: 0.0005795955657958984

Evaluating:  68%|██████▊   | 156/228 [00:41<00:20,  3.45it/s][Astep: 156
extend+tolist() time: 0.0004975795745849609

Evaluating:  69%|██████▉   | 157/228 [00:41<00:19,  3.71it/s][Astep: 157
extend+tolist() time: 0.001027822494506836

Evaluating:  69%|██████▉   | 158/228 [00:41<00:17,  3.92it/s][Astep: 158
extend+tolist() time: 0.0004980564117431641

Evaluating:  70%|██████▉   | 159/228 [00:42<00:16,  4.09it/s][Astep: 159
extend+tolist() time: 0.0006465911865234375

Evaluating:  70%|███████   | 160/228 [00:42<00:16,  4.19it/s][Astep: 160
extend+tolist() time: 0.0007660388946533203

Evaluating:  71%|███████   | 161/228 [00:42<00:15,  4.30it/s][Astep: 161
extend+tolist() time: 0.0007183551788330078

Evaluating:  71%|███████   | 162/228 [00:42<00:15,  4.36it/s][Astep: 162
extend+tolist() time: 0.0004849433898925781

Evaluating:  71%|███████▏  | 163/228 [00:42<00:14,  4.43it/s][Astep: 163
extend+tolist() time: 0.00039315223693847656

Evaluating:  72%|███████▏  | 164/228 [00:43<00:14,  4.50it/s][Astep: 164
extend+tolist() time: 0.0009751319885253906

Evaluating:  72%|███████▏  | 165/228 [00:43<00:13,  4.51it/s][Astep: 165
extend+tolist() time: 0.00039958953857421875

Evaluating:  73%|███████▎  | 166/228 [00:43<00:13,  4.55it/s][Astep: 166
extend+tolist() time: 0.00035381317138671875

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.55it/s][Astep: 167
extend+tolist() time: 0.0005457401275634766

Evaluating:  74%|███████▎  | 168/228 [00:44<00:13,  4.55it/s][Astep: 168
extend+tolist() time: 0.0015387535095214844

Evaluating:  74%|███████▍  | 169/228 [00:44<00:13,  4.24it/s][Astep: 169
extend+tolist() time: 0.0003299713134765625

Evaluating:  75%|███████▍  | 170/228 [00:44<00:13,  4.34it/s][Astep: 170
extend+tolist() time: 0.001195669174194336

Evaluating:  75%|███████▌  | 171/228 [00:44<00:13,  4.34it/s][Astep: 171
extend+tolist() time: 0.0002942085266113281

Evaluating:  75%|███████▌  | 172/228 [00:44<00:12,  4.43it/s][Astep: 172
extend+tolist() time: 0.0007090568542480469

Evaluating:  76%|███████▌  | 173/228 [00:45<00:12,  4.43it/s][Astep: 173
extend+tolist() time: 0.20158767700195312

Evaluating:  76%|███████▋  | 174/228 [00:45<00:16,  3.34it/s][Astep: 174
extend+tolist() time: 0.0017545223236083984

Evaluating:  77%|███████▋  | 175/228 [00:45<00:15,  3.33it/s][Astep: 175
extend+tolist() time: 0.0011208057403564453

Evaluating:  77%|███████▋  | 176/228 [00:46<00:14,  3.60it/s][Astep: 176
extend+tolist() time: 0.0006818771362304688

Evaluating:  78%|███████▊  | 177/228 [00:46<00:13,  3.85it/s][Astep: 177
extend+tolist() time: 0.0006115436553955078

Evaluating:  78%|███████▊  | 178/228 [00:46<00:12,  4.03it/s][Astep: 178
extend+tolist() time: 0.0017156600952148438

Evaluating:  79%|███████▊  | 179/228 [00:46<00:12,  3.89it/s][Astep: 179
extend+tolist() time: 0.0004017353057861328

Evaluating:  79%|███████▉  | 180/228 [00:47<00:11,  4.09it/s][Astep: 180
extend+tolist() time: 0.0004146099090576172

Evaluating:  79%|███████▉  | 181/228 [00:47<00:11,  4.25it/s][Astep: 181
extend+tolist() time: 0.0010235309600830078

Evaluating:  80%|███████▉  | 182/228 [00:47<00:10,  4.33it/s][Astep: 182
extend+tolist() time: 0.0007901191711425781

Evaluating:  80%|████████  | 183/228 [00:47<00:10,  4.37it/s][Astep: 183
extend+tolist() time: 0.0010955333709716797

Evaluating:  81%|████████  | 184/228 [00:48<00:09,  4.40it/s][Astep: 184
extend+tolist() time: 0.0004532337188720703

Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.45it/s][Astep: 185
extend+tolist() time: 0.0011823177337646484

Evaluating:  82%|████████▏ | 186/228 [00:48<00:09,  4.20it/s][Astep: 186
extend+tolist() time: 0.0015616416931152344

Evaluating:  82%|████████▏ | 187/228 [00:48<00:09,  4.16it/s][Astep: 187
extend+tolist() time: 0.00046825408935546875

Evaluating:  82%|████████▏ | 188/228 [00:48<00:09,  4.28it/s][Astep: 188
extend+tolist() time: 0.0011036396026611328

Evaluating:  83%|████████▎ | 189/228 [00:49<00:08,  4.37it/s][Astep: 189
extend+tolist() time: 0.00036072731018066406

Evaluating:  83%|████████▎ | 190/228 [00:49<00:08,  4.43it/s][Astep: 190
extend+tolist() time: 0.0012187957763671875

Evaluating:  84%|████████▍ | 191/228 [00:49<00:08,  4.17it/s][Astep: 191
extend+tolist() time: 0.0011911392211914062

Evaluating:  84%|████████▍ | 192/228 [00:49<00:08,  4.28it/s][Astep: 192
extend+tolist() time: 0.0004477500915527344

Evaluating:  85%|████████▍ | 193/228 [00:50<00:08,  4.37it/s][Astep: 193
extend+tolist() time: 0.0015077590942382812

Evaluating:  85%|████████▌ | 194/228 [00:50<00:07,  4.27it/s][Astep: 194
extend+tolist() time: 0.0006306171417236328

Evaluating:  86%|████████▌ | 195/228 [00:50<00:09,  3.40it/s][Astep: 195
extend+tolist() time: 0.0005552768707275391

Evaluating:  86%|████████▌ | 196/228 [00:51<00:08,  3.69it/s][Astep: 196
extend+tolist() time: 0.0006322860717773438

Evaluating:  86%|████████▋ | 197/228 [00:51<00:07,  3.90it/s][Astep: 197
extend+tolist() time: 0.0011386871337890625

Evaluating:  87%|████████▋ | 198/228 [00:51<00:07,  4.07it/s][Astep: 198
extend+tolist() time: 0.0006160736083984375

Evaluating:  87%|████████▋ | 199/228 [00:51<00:06,  4.20it/s][Astep: 199
extend+tolist() time: 0.0019125938415527344

Evaluating:  88%|████████▊ | 200/228 [00:51<00:07,  3.87it/s][Astep: 200
extend+tolist() time: 0.0007076263427734375

Evaluating:  88%|████████▊ | 201/228 [00:52<00:06,  4.03it/s][Astep: 201
extend+tolist() time: 0.0009734630584716797

Evaluating:  89%|████████▊ | 202/228 [00:52<00:06,  4.18it/s][Astep: 202
extend+tolist() time: 0.0004382133483886719

Evaluating:  89%|████████▉ | 203/228 [00:52<00:05,  4.31it/s][Astep: 203
extend+tolist() time: 0.0005216598510742188

Evaluating:  89%|████████▉ | 204/228 [00:52<00:05,  4.39it/s][Astep: 204
extend+tolist() time: 0.0004260540008544922

Evaluating:  90%|████████▉ | 205/228 [00:53<00:05,  4.45it/s][Astep: 205
extend+tolist() time: 0.0002846717834472656

Evaluating:  90%|█████████ | 206/228 [00:53<00:04,  4.51it/s][Astep: 206
extend+tolist() time: 0.0011029243469238281

Evaluating:  91%|█████████ | 207/228 [00:53<00:04,  4.52it/s][Astep: 207
extend+tolist() time: 0.0006527900695800781

Evaluating:  91%|█████████ | 208/228 [00:53<00:04,  4.52it/s][Astep: 208
extend+tolist() time: 0.0006947517395019531

Evaluating:  92%|█████████▏| 209/228 [00:53<00:04,  4.51it/s][Astep: 209
extend+tolist() time: 0.0010755062103271484

Evaluating:  92%|█████████▏| 210/228 [00:54<00:03,  4.51it/s][Astep: 210
extend+tolist() time: 0.0005867481231689453

Evaluating:  93%|█████████▎| 211/228 [00:54<00:03,  4.52it/s][Astep: 211
extend+tolist() time: 0.001833200454711914

Evaluating:  93%|█████████▎| 212/228 [00:54<00:03,  4.25it/s][Astep: 212
extend+tolist() time: 0.0008864402770996094

Evaluating:  93%|█████████▎| 213/228 [00:54<00:03,  4.25it/s][Astep: 213
extend+tolist() time: 0.0012354850769042969

Evaluating:  94%|█████████▍| 214/228 [00:55<00:04,  3.42it/s][Astep: 214
extend+tolist() time: 0.0008795261383056641

Evaluating:  94%|█████████▍| 215/228 [00:55<00:03,  3.67it/s][Astep: 215
extend+tolist() time: 0.00063323974609375

Evaluating:  95%|█████████▍| 216/228 [00:55<00:03,  3.88it/s][Astep: 216
extend+tolist() time: 0.0005719661712646484

Evaluating:  95%|█████████▌| 217/228 [00:56<00:02,  4.07it/s][Astep: 217
extend+tolist() time: 0.0005757808685302734

Evaluating:  96%|█████████▌| 218/228 [00:56<00:02,  4.20it/s][Astep: 218
extend+tolist() time: 0.0015118122100830078

Evaluating:  96%|█████████▌| 219/228 [00:56<00:02,  4.17it/s][Astep: 219
extend+tolist() time: 0.0008194446563720703

Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.28it/s][Astep: 220
extend+tolist() time: 0.00041103363037109375

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.37it/s][Astep: 221
extend+tolist() time: 0.0006706714630126953

Evaluating:  97%|█████████▋| 222/228 [00:57<00:01,  4.43it/s][Astep: 222
extend+tolist() time: 0.00044727325439453125

Evaluating:  98%|█████████▊| 223/228 [00:57<00:01,  4.47it/s][Astep: 223
extend+tolist() time: 0.00037860870361328125

Evaluating:  98%|█████████▊| 224/228 [00:57<00:00,  4.51it/s][Astep: 224
extend+tolist() time: 0.0003800392150878906

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.52it/s][Astep: 225
extend+tolist() time: 0.0004177093505859375

Evaluating:  99%|█████████▉| 226/228 [00:57<00:00,  4.56it/s][Astep: 226
extend+tolist() time: 0.0010790824890136719

Evaluating: 100%|█████████▉| 227/228 [00:58<00:00,  4.56it/s][Astep: 227
extend+tolist() time: 0.0004851818084716797

Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.94it/s][A09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 18:38:57 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 18:38:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 18:38:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 18:38:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 18:38:58 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:00<00:00,  3.77it/s]
09/05/2023 18:38:58 - INFO - __main__ -   Step: 5100, Validation Metrics: {'pred_1_num': 9580, 'pred_-1_num': 834, 'pred_0_num': 387, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7775205999444496, 'f1_micro': 0.7775205999444496, 'f1_macro': 0.4532714190438276, 'f1_weighted': 0.7509147907135598, 'f1_-1': 0.33249999999999996, 'f1_0': 0.15594541910331383, 'f1_1': 0.8713688380281691, 'precision_micro': 0.7775205999444496, 'precision_macro': 0.5039178555037539, 'precision_weighted': 0.739459531089611, 'precision_-1': 0.4784172661870504, 'precision_0': 0.20671834625322996, 'precision_1': 0.8266179540709813, 'recall_micro': 0.7775205999444496, 'recall_macro': 0.43374244284247715, 'recall_weighted': 0.7775205999444496, 'recall_-1': 0.2547892720306513, 'recall_0': 0.12519561815336464, 'recall_1': 0.9212424383434156, 'roc_auc_micro': 0.8970087166899072, 'roc_auc_macro': 0.6942078675599195, 'roc_auc_weighted': 0.6715615904913749, 'roc_auc_-1': 0.7490378584996138, 'roc_auc_0': 0.6765061712310646, 'roc_auc_1': 0.6570795729490804}
[2023-09-05 18:39:05,347] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5101/66600 [4:35:16<548:12:19, 32.09s/it]09/05/2023 18:39:12 - INFO - __main__ -   Step: 5101, LR: 1.9038755608549386e-05, Loss: 0.20141014456748962
[2023-09-05 18:39:18,354] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5102/66600 [4:35:29<451:04:53, 26.41s/it]09/05/2023 18:39:25 - INFO - __main__ -   Step: 5102, LR: 1.9038445904355604e-05, Loss: 0.13266025483608246
[2023-09-05 18:39:32,386] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5103/66600 [4:35:44<393:01:50, 23.01s/it]09/05/2023 18:39:40 - INFO - __main__ -   Step: 5103, LR: 1.9038136200161822e-05, Loss: 0.11887809634208679
[2023-09-05 18:39:46,905] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5104/66600 [4:35:58<346:45:12, 20.30s/it]09/05/2023 18:39:54 - INFO - __main__ -   Step: 5104, LR: 1.903782649596804e-05, Loss: 0.11003196239471436
[2023-09-05 18:40:00,751] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5105/66600 [4:36:11<310:28:25, 18.18s/it]09/05/2023 18:40:07 - INFO - __main__ -   Step: 5105, LR: 1.9037516791774258e-05, Loss: 0.16201770305633545
[2023-09-05 18:40:14,023] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5106/66600 [4:36:25<287:47:45, 16.85s/it]09/05/2023 18:40:21 - INFO - __main__ -   Step: 5106, LR: 1.9037207087580476e-05, Loss: 0.12591367959976196
[2023-09-05 18:40:28,111] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5107/66600 [4:36:39<274:28:17, 16.07s/it]09/05/2023 18:40:35 - INFO - __main__ -   Step: 5107, LR: 1.9036897383386694e-05, Loss: 0.10495895147323608
[2023-09-05 18:40:42,391] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5108/66600 [4:36:53<265:20:38, 15.53s/it]09/05/2023 18:40:50 - INFO - __main__ -   Step: 5108, LR: 1.9036587679192912e-05, Loss: 0.10752218961715698
[2023-09-05 18:40:56,786] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5109/66600 [4:37:07<257:49:27, 15.09s/it]09/05/2023 18:41:04 - INFO - __main__ -   Step: 5109, LR: 1.903627797499913e-05, Loss: 0.11262499541044235
[2023-09-05 18:41:10,860] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5110/66600 [4:37:22<255:03:51, 14.93s/it]09/05/2023 18:41:18 - INFO - __main__ -   Step: 5110, LR: 1.903596827080535e-05, Loss: 0.08610732108354568
[2023-09-05 18:41:25,684] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5111/66600 [4:37:38<258:14:09, 15.12s/it]09/05/2023 18:41:34 - INFO - __main__ -   Step: 5111, LR: 1.9035658566611567e-05, Loss: 0.10962354391813278
[2023-09-05 18:41:41,056] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5112/66600 [4:37:51<250:59:18, 14.69s/it]09/05/2023 18:41:48 - INFO - __main__ -   Step: 5112, LR: 1.9035348862417785e-05, Loss: 0.18997466564178467
[2023-09-05 18:41:54,694] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5113/66600 [4:38:06<250:09:45, 14.65s/it]09/05/2023 18:42:02 - INFO - __main__ -   Step: 5113, LR: 1.9035039158224003e-05, Loss: 0.1901320368051529
[2023-09-05 18:42:09,435] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5114/66600 [4:38:20<245:52:02, 14.40s/it]09/05/2023 18:42:16 - INFO - __main__ -   Step: 5114, LR: 1.903472945403022e-05, Loss: 0.12162881344556808
[2023-09-05 18:42:22,923] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5115/66600 [4:38:34<245:56:22, 14.40s/it]09/05/2023 18:42:30 - INFO - __main__ -   Step: 5115, LR: 1.903441974983644e-05, Loss: 0.14595836400985718
[2023-09-05 18:42:37,665] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5116/66600 [4:38:49<247:22:04, 14.48s/it]09/05/2023 18:42:45 - INFO - __main__ -   Step: 5116, LR: 1.9034110045642657e-05, Loss: 0.08667796105146408
[2023-09-05 18:42:51,868] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5117/66600 [4:39:03<246:16:17, 14.42s/it]09/05/2023 18:42:59 - INFO - __main__ -   Step: 5117, LR: 1.9033800341448875e-05, Loss: 0.09901869297027588
[2023-09-05 18:43:06,083] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5118/66600 [4:39:17<245:23:25, 14.37s/it]09/05/2023 18:43:14 - INFO - __main__ -   Step: 5118, LR: 1.9033490637255093e-05, Loss: 0.13862024247646332
[2023-09-05 18:43:21,002] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5119/66600 [4:39:32<247:21:01, 14.48s/it]09/05/2023 18:43:28 - INFO - __main__ -   Step: 5119, LR: 1.903318093306131e-05, Loss: 0.13334952294826508
[2023-09-05 18:43:35,442] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5120/66600 [4:39:46<246:02:13, 14.41s/it]09/05/2023 18:43:43 - INFO - __main__ -   Step: 5120, LR: 1.903287122886753e-05, Loss: 0.10409185290336609
[2023-09-05 18:43:49,443] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5121/66600 [4:40:01<246:04:00, 14.41s/it]09/05/2023 18:43:57 - INFO - __main__ -   Step: 5121, LR: 1.9032561524673747e-05, Loss: 0.22121451795101166
[2023-09-05 18:44:04,126] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5122/66600 [4:40:15<245:11:21, 14.36s/it]09/05/2023 18:44:11 - INFO - __main__ -   Step: 5122, LR: 1.9032251820479965e-05, Loss: 0.17152759432792664
[2023-09-05 18:44:18,104] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5123/66600 [4:40:28<241:33:46, 14.15s/it]09/05/2023 18:44:25 - INFO - __main__ -   Step: 5123, LR: 1.9031942116286183e-05, Loss: 0.15827879309654236
[2023-09-05 18:44:30,830] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5124/66600 [4:40:41<233:49:59, 13.69s/it]09/05/2023 18:44:38 - INFO - __main__ -   Step: 5124, LR: 1.90316324120924e-05, Loss: 0.09726215153932571
[2023-09-05 18:44:44,005] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5125/66600 [4:40:55<237:28:15, 13.91s/it]09/05/2023 18:44:52 - INFO - __main__ -   Step: 5125, LR: 1.903132270789862e-05, Loss: 0.1625918745994568
[2023-09-05 18:44:59,698] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5126/66600 [4:41:11<247:13:34, 14.48s/it]09/05/2023 18:45:08 - INFO - __main__ -   Step: 5126, LR: 1.9031013003704838e-05, Loss: 0.1300497055053711
[2023-09-05 18:45:14,540] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5127/66600 [4:41:25<245:00:14, 14.35s/it]09/05/2023 18:45:22 - INFO - __main__ -   Step: 5127, LR: 1.9030703299511056e-05, Loss: 0.11651980876922607
[2023-09-05 18:45:28,403] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5128/66600 [4:41:39<241:12:54, 14.13s/it]09/05/2023 18:45:35 - INFO - __main__ -   Step: 5128, LR: 1.9030393595317274e-05, Loss: 0.09607256948947906
[2023-09-05 18:45:42,445] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5129/66600 [4:41:53<242:44:07, 14.22s/it]09/05/2023 18:45:50 - INFO - __main__ -   Step: 5129, LR: 1.9030083891123492e-05, Loss: 0.15257108211517334
[2023-09-05 18:45:58,485] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5130/66600 [4:42:11<259:28:04, 15.20s/it]09/05/2023 18:46:07 - INFO - __main__ -   Step: 5130, LR: 1.902977418692971e-05, Loss: 0.11943666636943817
[2023-09-05 18:46:15,695] [WARNING] [stage3.py:1898:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5131/66600 [4:42:27<265:42:57, 15.56s/it]09/05/2023 18:46:24 - INFO - __main__ -   Step: 5131, LR: 1.9029464482735928e-05, Loss: 0.13789427280426025
[2023-09-05 18:46:31,527] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5132/66600 [4:42:43<264:17:40, 15.48s/it]09/05/2023 18:46:39 - INFO - __main__ -   Step: 5132, LR: 1.9029154778542146e-05, Loss: 0.13014739751815796
[2023-09-05 18:46:45,583] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5133/66600 [4:42:57<257:23:12, 15.07s/it]09/05/2023 18:46:53 - INFO - __main__ -   Step: 5133, LR: 1.9028845074348364e-05, Loss: 0.1189592182636261
[2023-09-05 18:46:59,862] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5134/66600 [4:43:10<249:43:11, 14.63s/it]09/05/2023 18:47:07 - INFO - __main__ -   Step: 5134, LR: 1.9028535370154582e-05, Loss: 0.12233980745077133
[2023-09-05 18:47:14,086] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5135/66600 [4:43:25<250:05:35, 14.65s/it]09/05/2023 18:47:21 - INFO - __main__ -   Step: 5135, LR: 1.90282256659608e-05, Loss: 0.13525447249412537
[2023-09-05 18:47:27,826] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5136/66600 [4:43:38<244:04:15, 14.30s/it]09/05/2023 18:47:35 - INFO - __main__ -   Step: 5136, LR: 1.9027915961767018e-05, Loss: 0.1447586715221405
[2023-09-05 18:47:41,104] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5137/66600 [4:43:51<235:20:42, 13.78s/it]09/05/2023 18:47:47 - INFO - __main__ -   Step: 5137, LR: 1.9027606257573236e-05, Loss: 0.13371124863624573
[2023-09-05 18:47:54,542] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5138/66600 [4:44:06<242:49:52, 14.22s/it]09/05/2023 18:48:03 - INFO - __main__ -   Step: 5138, LR: 1.9027296553379454e-05, Loss: 0.21892458200454712
[2023-09-05 18:48:09,817] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5139/66600 [4:44:21<244:34:01, 14.33s/it]09/05/2023 18:48:17 - INFO - __main__ -   Step: 5139, LR: 1.9026986849185672e-05, Loss: 0.13502559065818787
[2023-09-05 18:48:23,548] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5140/66600 [4:44:35<242:01:09, 14.18s/it]09/05/2023 18:48:31 - INFO - __main__ -   Step: 5140, LR: 1.902667714499189e-05, Loss: 0.10950218886137009
[2023-09-05 18:48:38,313] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5141/66600 [4:44:49<243:04:00, 14.24s/it]09/05/2023 18:48:46 - INFO - __main__ -   Step: 5141, LR: 1.9026367440798112e-05, Loss: 0.13153858482837677
[2023-09-05 18:48:52,457] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5142/66600 [4:45:03<241:10:58, 14.13s/it]09/05/2023 18:48:59 - INFO - __main__ -   Step: 5142, LR: 1.9026057736604326e-05, Loss: 0.1304534673690796
[2023-09-05 18:49:06,663] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5143/66600 [4:45:18<243:48:56, 14.28s/it]09/05/2023 18:49:14 - INFO - __main__ -   Step: 5143, LR: 1.9025748032410545e-05, Loss: 0.15868762135505676
[2023-09-05 18:49:20,774] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5144/66600 [4:45:31<240:39:15, 14.10s/it]09/05/2023 18:49:28 - INFO - __main__ -   Step: 5144, LR: 1.9025438328216763e-05, Loss: 0.12935101985931396
[2023-09-05 18:49:34,789] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5145/66600 [4:45:45<238:36:49, 13.98s/it]09/05/2023 18:49:41 - INFO - __main__ -   Step: 5145, LR: 1.902512862402298e-05, Loss: 0.1693267673254013
[2023-09-05 18:49:47,894] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5146/66600 [4:45:59<239:51:51, 14.05s/it]09/05/2023 18:49:56 - INFO - __main__ -   Step: 5146, LR: 1.90248189198292e-05, Loss: 0.17077821493148804
[2023-09-05 18:50:02,637] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5147/66600 [4:46:14<246:25:23, 14.44s/it]09/05/2023 18:50:11 - INFO - __main__ -   Step: 5147, LR: 1.9024509215635417e-05, Loss: 0.10136693716049194
[2023-09-05 18:50:17,681] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5148/66600 [4:46:29<245:57:15, 14.41s/it]09/05/2023 18:50:25 - INFO - __main__ -   Step: 5148, LR: 1.9024199511441638e-05, Loss: 0.20819836854934692
[2023-09-05 18:50:31,408] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5149/66600 [4:46:42<238:21:12, 13.96s/it]09/05/2023 18:50:38 - INFO - __main__ -   Step: 5149, LR: 1.9023889807247856e-05, Loss: 0.18340417742729187
[2023-09-05 18:50:45,006] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5150/66600 [4:46:56<240:05:50, 14.07s/it]09/05/2023 18:50:53 - INFO - __main__ -   Step: 5150, LR: 1.902358010305407e-05, Loss: 0.16813504695892334
[2023-09-05 18:50:59,073] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5151/66600 [4:47:10<238:30:00, 13.97s/it]09/05/2023 18:51:06 - INFO - __main__ -   Step: 5151, LR: 1.902327039886029e-05, Loss: 0.17011301219463348
[2023-09-05 18:51:12,586] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5152/66600 [4:47:23<235:07:25, 13.77s/it]09/05/2023 18:51:20 - INFO - __main__ -   Step: 5152, LR: 1.9022960694666507e-05, Loss: 0.1786307692527771
[2023-09-05 18:51:26,263] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5153/66600 [4:47:38<238:11:22, 13.95s/it]09/05/2023 18:51:34 - INFO - __main__ -   Step: 5153, LR: 1.9022650990472725e-05, Loss: 0.14963299036026
[2023-09-05 18:51:40,265] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5154/66600 [4:47:51<234:13:08, 13.72s/it]09/05/2023 18:51:47 - INFO - __main__ -   Step: 5154, LR: 1.9022341286278943e-05, Loss: 0.1579284965991974
[2023-09-05 18:51:54,781] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5155/66600 [4:48:05<238:15:26, 13.96s/it]09/05/2023 18:52:02 - INFO - __main__ -   Step: 5155, LR: 1.9022031582085165e-05, Loss: 0.1635531485080719
[2023-09-05 18:52:09,010] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5156/66600 [4:48:20<242:05:50, 14.18s/it]09/05/2023 18:52:16 - INFO - __main__ -   Step: 5156, LR: 1.9021721877891383e-05, Loss: 0.1619974821805954
[2023-09-05 18:52:24,095] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5157/66600 [4:48:34<243:58:49, 14.30s/it]09/05/2023 18:52:31 - INFO - __main__ -   Step: 5157, LR: 1.9021412173697597e-05, Loss: 0.09195499122142792
[2023-09-05 18:52:39,199] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5158/66600 [4:48:49<247:20:13, 14.49s/it]09/05/2023 18:52:46 - INFO - __main__ -   Step: 5158, LR: 1.9021102469503815e-05, Loss: 0.10252439230680466
[2023-09-05 18:52:52,723] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5159/66600 [4:49:03<242:05:40, 14.18s/it]09/05/2023 18:52:59 - INFO - __main__ -   Step: 5159, LR: 1.9020792765310034e-05, Loss: 0.17029061913490295
[2023-09-05 18:53:06,146] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5160/66600 [4:49:17<242:55:06, 14.23s/it]09/05/2023 18:53:14 - INFO - __main__ -   Step: 5160, LR: 1.902048306111625e-05, Loss: 0.15789218246936798
[2023-09-05 18:53:21,361] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5161/66600 [4:49:32<247:58:51, 14.53s/it]09/05/2023 18:53:29 - INFO - __main__ -   Step: 5161, LR: 1.902017335692247e-05, Loss: 0.16561248898506165
[2023-09-05 18:53:35,980] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5162/66600 [4:49:47<246:05:31, 14.42s/it]09/05/2023 18:53:43 - INFO - __main__ -   Step: 5162, LR: 1.901986365272869e-05, Loss: 0.12128590047359467
[2023-09-05 18:53:50,524] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5163/66600 [4:50:02<253:18:36, 14.84s/it]09/05/2023 18:53:59 - INFO - __main__ -   Step: 5163, LR: 1.901955394853491e-05, Loss: 0.10405983030796051
[2023-09-05 18:54:06,395] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5164/66600 [4:50:18<257:55:02, 15.11s/it]09/05/2023 18:54:15 - INFO - __main__ -   Step: 5164, LR: 1.9019244244341127e-05, Loss: 0.15413698554039001
[2023-09-05 18:54:21,385] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5165/66600 [4:50:32<251:58:56, 14.77s/it]09/05/2023 18:54:29 - INFO - __main__ -   Step: 5165, LR: 1.9018934540147342e-05, Loss: 0.1350875198841095
[2023-09-05 18:54:35,044] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5166/66600 [4:50:46<248:17:48, 14.55s/it]09/05/2023 18:54:43 - INFO - __main__ -   Step: 5166, LR: 1.901862483595356e-05, Loss: 0.1172761470079422
[2023-09-05 18:54:49,578] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5167/66600 [4:51:00<244:26:44, 14.32s/it]09/05/2023 18:54:56 - INFO - __main__ -   Step: 5167, LR: 1.9018315131759778e-05, Loss: 0.10421891510486603
[2023-09-05 18:55:03,515] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5168/66600 [4:51:14<243:45:32, 14.28s/it]09/05/2023 18:55:11 - INFO - __main__ -   Step: 5168, LR: 1.9018005427565996e-05, Loss: 0.19254547357559204
[2023-09-05 18:55:17,947] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5169/66600 [4:51:29<244:07:26, 14.31s/it]09/05/2023 18:55:25 - INFO - __main__ -   Step: 5169, LR: 1.9017695723372218e-05, Loss: 0.1179523766040802
[2023-09-05 18:55:31,862] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5170/66600 [4:51:42<242:05:58, 14.19s/it]09/05/2023 18:55:39 - INFO - __main__ -   Step: 5170, LR: 1.9017386019178436e-05, Loss: 0.15421456098556519
[2023-09-05 18:55:46,552] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5171/66600 [4:51:58<251:16:37, 14.73s/it]09/05/2023 18:55:55 - INFO - __main__ -   Step: 5171, LR: 1.9017076314984654e-05, Loss: 0.17888882756233215
[2023-09-05 18:56:02,109] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5172/66600 [4:52:13<248:00:02, 14.53s/it]09/05/2023 18:56:09 - INFO - __main__ -   Step: 5172, LR: 1.9016766610790868e-05, Loss: 0.17134371399879456
[2023-09-05 18:56:16,240] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5173/66600 [4:52:27<245:48:12, 14.41s/it]09/05/2023 18:56:23 - INFO - __main__ -   Step: 5173, LR: 1.9016456906597086e-05, Loss: 0.12524738907814026
[2023-09-05 18:56:30,713] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5174/66600 [4:52:42<249:39:41, 14.63s/it]09/05/2023 18:56:38 - INFO - __main__ -   Step: 5174, LR: 1.9016147202403304e-05, Loss: 0.14151906967163086
[2023-09-05 18:56:45,779] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5175/66600 [4:52:56<248:43:47, 14.58s/it]09/05/2023 18:56:53 - INFO - __main__ -   Step: 5175, LR: 1.9015837498209522e-05, Loss: 0.16244980692863464
[2023-09-05 18:56:59,928] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5176/66600 [4:53:11<249:02:59, 14.60s/it]09/05/2023 18:57:07 - INFO - __main__ -   Step: 5176, LR: 1.9015527794015744e-05, Loss: 0.13164083659648895
[2023-09-05 18:57:14,559] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5177/66600 [4:53:26<250:16:05, 14.67s/it]09/05/2023 18:57:22 - INFO - __main__ -   Step: 5177, LR: 1.9015218089821962e-05, Loss: 0.1532575637102127
[2023-09-05 18:57:28,580] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5178/66600 [4:53:40<245:52:06, 14.41s/it]09/05/2023 18:57:36 - INFO - __main__ -   Step: 5178, LR: 1.901490838562818e-05, Loss: 0.1187448501586914
[2023-09-05 18:57:43,283] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5179/66600 [4:53:54<246:04:16, 14.42s/it]09/05/2023 18:57:50 - INFO - __main__ -   Step: 5179, LR: 1.9014598681434398e-05, Loss: 0.13557493686676025
[2023-09-05 18:57:57,190] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5180/66600 [4:54:08<243:35:41, 14.28s/it]09/05/2023 18:58:04 - INFO - __main__ -   Step: 5180, LR: 1.9014288977240613e-05, Loss: 0.17128077149391174
[2023-09-05 18:58:11,498] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5181/66600 [4:54:23<245:30:20, 14.39s/it]09/05/2023 18:58:19 - INFO - __main__ -   Step: 5181, LR: 1.901397927304683e-05, Loss: 0.134949192404747
[2023-09-05 18:58:25,704] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5182/66600 [4:54:36<242:55:18, 14.24s/it]09/05/2023 18:58:33 - INFO - __main__ -   Step: 5182, LR: 1.901366956885305e-05, Loss: 0.18244758248329163
[2023-09-05 18:58:40,331] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5183/66600 [4:54:51<244:51:12, 14.35s/it]09/05/2023 18:58:48 - INFO - __main__ -   Step: 5183, LR: 1.901335986465927e-05, Loss: 0.1090589165687561
[2023-09-05 18:58:54,191] [WARNING] [stage3.py:1898:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5184/66600 [4:55:05<242:52:48, 14.24s/it]09/05/2023 18:59:01 - INFO - __main__ -   Step: 5184, LR: 1.901305016046549e-05, Loss: 0.1405663788318634
[2023-09-05 18:59:08,196] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5185/66600 [4:55:20<245:16:13, 14.38s/it]09/05/2023 18:59:16 - INFO - __main__ -   Step: 5185, LR: 1.9012740456271706e-05, Loss: 0.10302790254354477
[2023-09-05 18:59:23,572] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5186/66600 [4:55:34<245:34:49, 14.40s/it]09/05/2023 18:59:31 - INFO - __main__ -   Step: 5186, LR: 1.9012430752077925e-05, Loss: 0.15953058004379272
[2023-09-05 18:59:37,899] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5187/66600 [4:55:49<246:15:21, 14.44s/it]09/05/2023 18:59:45 - INFO - __main__ -   Step: 5187, LR: 1.9012121047884143e-05, Loss: 0.1733582317829132
[2023-09-05 18:59:52,668] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5188/66600 [4:56:03<244:55:04, 14.36s/it]09/05/2023 18:59:59 - INFO - __main__ -   Step: 5188, LR: 1.9011811343690357e-05, Loss: 0.1684059202671051
[2023-09-05 19:00:06,415] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5189/66600 [4:56:17<243:37:56, 14.28s/it]09/05/2023 19:00:13 - INFO - __main__ -   Step: 5189, LR: 1.9011501639496575e-05, Loss: 0.14030593633651733
[2023-09-05 19:00:19,771] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5190/66600 [4:56:30<236:03:21, 13.84s/it]09/05/2023 19:00:26 - INFO - __main__ -   Step: 5190, LR: 1.9011191935302797e-05, Loss: 0.10895494371652603
[2023-09-05 19:00:33,555] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5191/66600 [4:56:44<239:02:25, 14.01s/it]09/05/2023 19:00:41 - INFO - __main__ -   Step: 5191, LR: 1.9010882231109015e-05, Loss: 0.1567598283290863
[2023-09-05 19:00:47,960] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5192/66600 [4:57:00<249:46:19, 14.64s/it]09/05/2023 19:00:57 - INFO - __main__ -   Step: 5192, LR: 1.9010572526915233e-05, Loss: 0.1400144100189209
[2023-09-05 19:01:03,860] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5193/66600 [4:57:15<251:01:32, 14.72s/it]09/05/2023 19:01:12 - INFO - __main__ -   Step: 5193, LR: 1.901026282272145e-05, Loss: 0.1292913556098938
[2023-09-05 19:01:18,563] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5194/66600 [4:57:30<250:38:28, 14.69s/it]09/05/2023 19:01:26 - INFO - __main__ -   Step: 5194, LR: 1.900995311852767e-05, Loss: 0.15001170337200165
[2023-09-05 19:01:33,250] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5195/66600 [4:57:44<247:31:14, 14.51s/it]09/05/2023 19:01:40 - INFO - __main__ -   Step: 5195, LR: 1.9009643414333884e-05, Loss: 0.15693910419940948
[2023-09-05 19:01:46,984] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5196/66600 [4:57:58<245:04:18, 14.37s/it]09/05/2023 19:01:54 - INFO - __main__ -   Step: 5196, LR: 1.9009333710140102e-05, Loss: 0.11532324552536011
[2023-09-05 19:02:01,157] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5197/66600 [4:58:12<244:07:46, 14.31s/it]09/05/2023 19:02:09 - INFO - __main__ -   Step: 5197, LR: 1.9009024005946323e-05, Loss: 0.20550721883773804
[2023-09-05 19:02:16,299] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5198/66600 [4:58:26<243:52:38, 14.30s/it]09/05/2023 19:02:23 - INFO - __main__ -   Step: 5198, LR: 1.900871430175254e-05, Loss: 0.11943288147449493
[2023-09-05 19:02:29,950] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5199/66600 [4:58:40<241:51:37, 14.18s/it]09/05/2023 19:02:37 - INFO - __main__ -   Step: 5199, LR: 1.900840459755876e-05, Loss: 0.09979094564914703
[2023-09-05 19:02:43,533] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5200/66600 [4:58:54<240:52:28, 14.12s/it]09/05/2023 19:02:51 - INFO - __main__ -   Step: 5200, LR: 1.9008094893364977e-05, Loss: 0.13211499154567719
09/05/2023 19:02:51 - INFO - __main__ - ***** Running Validation *****

Evaluating:   0%|          | 0/228 [00:00<?, ?it/s][Astep: 0
extend+tolist() time: 0.001775979995727539

Evaluating:   0%|          | 1/228 [00:00<01:15,  3.00it/s][Astep: 1
extend+tolist() time: 0.0019648075103759766

Evaluating:   1%|          | 2/228 [00:00<01:03,  3.53it/s][Astep: 2
extend+tolist() time: 0.002108335494995117

Evaluating:   1%|▏         | 3/228 [00:00<01:09,  3.24it/s][Astep: 3
extend+tolist() time: 0.0019099712371826172

Evaluating:   2%|▏         | 4/228 [00:01<01:08,  3.25it/s][Astep: 4
extend+tolist() time: 0.0009558200836181641

Evaluating:   2%|▏         | 5/228 [00:01<01:03,  3.49it/s][Astep: 5
extend+tolist() time: 0.0027053356170654297

Evaluating:   3%|▎         | 6/228 [00:01<01:07,  3.30it/s][Astep: 6
extend+tolist() time: 0.14798784255981445

Evaluating:   3%|▎         | 7/228 [00:02<01:20,  2.76it/s][Astep: 7
extend+tolist() time: 0.0014994144439697266

Evaluating:   4%|▎         | 8/228 [00:02<01:11,  3.06it/s][Astep: 8
extend+tolist() time: 0.0010914802551269531

Evaluating:   4%|▍         | 9/228 [00:02<01:04,  3.38it/s][Astep: 9
extend+tolist() time: 0.0007801055908203125

Evaluating:   4%|▍         | 10/228 [00:03<01:00,  3.61it/s][Astep: 10
extend+tolist() time: 0.0012712478637695312

Evaluating:   5%|▍         | 11/228 [00:03<01:06,  3.27it/s][Astep: 11
extend+tolist() time: 0.0005693435668945312

Evaluating:   5%|▌         | 12/228 [00:03<01:00,  3.56it/s][Astep: 12
extend+tolist() time: 0.0010581016540527344

Evaluating:   6%|▌         | 13/228 [00:03<01:05,  3.29it/s][Astep: 13
extend+tolist() time: 0.00060272216796875

Evaluating:   6%|▌         | 14/228 [00:04<00:59,  3.58it/s][Astep: 14
extend+tolist() time: 0.0005669593811035156

Evaluating:   7%|▋         | 15/228 [00:04<00:55,  3.83it/s][Astep: 15
extend+tolist() time: 0.0010573863983154297

Evaluating:   7%|▋         | 16/228 [00:04<00:52,  4.00it/s][Astep: 16
extend+tolist() time: 0.00060272216796875

Evaluating:   7%|▋         | 17/228 [00:04<00:51,  4.13it/s][Astep: 17
extend+tolist() time: 0.0013129711151123047

Evaluating:   8%|▊         | 18/228 [00:05<00:50,  4.14it/s][Astep: 18
extend+tolist() time: 0.0011417865753173828

Evaluating:   8%|▊         | 19/228 [00:05<00:52,  3.96it/s][Astep: 19
extend+tolist() time: 0.0013988018035888672

Evaluating:   9%|▉         | 20/228 [00:05<00:53,  3.88it/s][Astep: 20
extend+tolist() time: 0.0007827281951904297

Evaluating:   9%|▉         | 21/228 [00:05<00:51,  4.01it/s][Astep: 21
extend+tolist() time: 0.00112152099609375

Evaluating:  10%|▉         | 22/228 [00:06<00:51,  4.00it/s][Astep: 22
extend+tolist() time: 0.0007166862487792969

Evaluating:  10%|█         | 23/228 [00:06<00:49,  4.12it/s][Astep: 23
extend+tolist() time: 0.0011136531829833984

Evaluating:  11%|█         | 24/228 [00:06<00:48,  4.21it/s][Astep: 24
extend+tolist() time: 0.0013921260833740234

Evaluating:  11%|█         | 25/228 [00:06<00:50,  4.00it/s][Astep: 25
extend+tolist() time: 0.0019192695617675781

Evaluating:  11%|█▏        | 26/228 [00:07<00:55,  3.62it/s][Astep: 26
extend+tolist() time: 0.0010657310485839844

Evaluating:  12%|█▏        | 27/228 [00:07<00:52,  3.83it/s][Astep: 27
extend+tolist() time: 0.0019183158874511719

Evaluating:  12%|█▏        | 28/228 [00:07<01:03,  3.16it/s][Astep: 28
extend+tolist() time: 0.000335693359375

Evaluating:  13%|█▎        | 29/228 [00:08<00:57,  3.49it/s][Astep: 29
extend+tolist() time: 0.0007693767547607422

Evaluating:  13%|█▎        | 30/228 [00:08<00:53,  3.73it/s][Astep: 30
extend+tolist() time: 0.0015943050384521484

Evaluating:  14%|█▎        | 31/228 [00:08<00:53,  3.67it/s][Astep: 31
extend+tolist() time: 0.0005459785461425781

Evaluating:  14%|█▍        | 32/228 [00:08<00:50,  3.89it/s][Astep: 32
extend+tolist() time: 0.0014069080352783203

Evaluating:  14%|█▍        | 33/228 [00:09<00:51,  3.82it/s][Astep: 33
extend+tolist() time: 0.0016279220581054688

Evaluating:  15%|█▍        | 34/228 [00:09<00:53,  3.65it/s][Astep: 34
extend+tolist() time: 0.0008490085601806641

Evaluating:  15%|█▌        | 35/228 [00:09<00:51,  3.76it/s][Astep: 35
extend+tolist() time: 0.0010921955108642578

Evaluating:  16%|█▌        | 36/228 [00:09<00:48,  3.94it/s][Astep: 36
extend+tolist() time: 0.0007617473602294922

Evaluating:  16%|█▌        | 37/228 [00:10<00:47,  4.04it/s][Astep: 37
extend+tolist() time: 0.001750946044921875

Evaluating:  17%|█▋        | 38/228 [00:10<00:50,  3.78it/s][Astep: 38
extend+tolist() time: 0.172529935836792

Evaluating:  17%|█▋        | 39/228 [00:10<00:58,  3.25it/s][Astep: 39
extend+tolist() time: 0.0007061958312988281

Evaluating:  18%|█▊        | 40/228 [00:11<00:53,  3.52it/s][Astep: 40
extend+tolist() time: 0.0010828971862792969

Evaluating:  18%|█▊        | 41/228 [00:11<00:49,  3.76it/s][Astep: 41
extend+tolist() time: 0.0008759498596191406

Evaluating:  18%|█▊        | 42/228 [00:11<00:48,  3.86it/s][Astep: 42
extend+tolist() time: 0.0016624927520751953

Evaluating:  19%|█▉        | 43/228 [00:11<00:50,  3.67it/s][Astep: 43
extend+tolist() time: 0.0018832683563232422

Evaluating:  19%|█▉        | 44/228 [00:12<01:01,  2.99it/s][Astep: 44
extend+tolist() time: 0.0011029243469238281

Evaluating:  20%|█▉        | 45/228 [00:12<01:03,  2.89it/s][Astep: 45
extend+tolist() time: 0.0016188621520996094

Evaluating:  20%|██        | 46/228 [00:12<01:00,  3.00it/s][Astep: 46
extend+tolist() time: 0.0012359619140625

Evaluating:  21%|██        | 47/228 [00:13<00:58,  3.08it/s][Astep: 47
extend+tolist() time: 0.0016274452209472656

Evaluating:  21%|██        | 48/228 [00:13<00:56,  3.20it/s][Astep: 48
extend+tolist() time: 0.0016362667083740234

Evaluating:  21%|██▏       | 49/228 [00:13<00:55,  3.24it/s][Astep: 49
extend+tolist() time: 0.0009639263153076172

Evaluating:  22%|██▏       | 50/228 [00:14<00:51,  3.43it/s][Astep: 50
extend+tolist() time: 0.001672983169555664

Evaluating:  22%|██▏       | 51/228 [00:14<00:52,  3.39it/s][Astep: 51
extend+tolist() time: 0.0015094280242919922

Evaluating:  23%|██▎       | 52/228 [00:14<00:52,  3.38it/s][Astep: 52
extend+tolist() time: 0.0013208389282226562

Evaluating:  23%|██▎       | 53/228 [00:14<00:50,  3.46it/s][Astep: 53
extend+tolist() time: 0.0016222000122070312

Evaluating:  24%|██▎       | 54/228 [00:15<00:51,  3.41it/s][Astep: 54
extend+tolist() time: 0.0008254051208496094

Evaluating:  24%|██▍       | 55/228 [00:15<00:47,  3.61it/s][Astep: 55
extend+tolist() time: 0.001180887222290039

Evaluating:  25%|██▍       | 56/228 [00:15<00:45,  3.80it/s][Astep: 56
extend+tolist() time: 0.0012199878692626953

Evaluating:  25%|██▌       | 57/228 [00:16<00:46,  3.65it/s][Astep: 57
extend+tolist() time: 0.0009775161743164062

Evaluating:  25%|██▌       | 58/228 [00:16<00:43,  3.87it/s][Astep: 58
extend+tolist() time: 0.0008494853973388672

Evaluating:  26%|██▌       | 59/228 [00:16<00:51,  3.29it/s][Astep: 59
extend+tolist() time: 0.0013141632080078125

Evaluating:  26%|██▋       | 60/228 [00:16<00:48,  3.48it/s][Astep: 60
extend+tolist() time: 0.0007002353668212891

Evaluating:  27%|██▋       | 61/228 [00:17<00:44,  3.71it/s][Astep: 61
extend+tolist() time: 0.0013036727905273438

Evaluating:  27%|██▋       | 62/228 [00:17<00:43,  3.82it/s][Astep: 62
extend+tolist() time: 0.0007843971252441406

Evaluating:  28%|██▊       | 63/228 [00:17<00:41,  3.97it/s][Astep: 63
extend+tolist() time: 0.0011911392211914062

Evaluating:  28%|██▊       | 64/228 [00:17<00:40,  4.06it/s][Astep: 64
extend+tolist() time: 0.0008208751678466797

Evaluating:  29%|██▊       | 65/228 [00:18<00:39,  4.11it/s][Astep: 65
extend+tolist() time: 0.0011775493621826172

Evaluating:  29%|██▉       | 66/228 [00:18<00:39,  4.13it/s][Astep: 66
extend+tolist() time: 0.0007469654083251953

Evaluating:  29%|██▉       | 67/228 [00:18<00:38,  4.20it/s][Astep: 67
extend+tolist() time: 0.001310110092163086

Evaluating:  30%|██▉       | 68/228 [00:18<00:38,  4.15it/s][Astep: 68
extend+tolist() time: 0.0007390975952148438

Evaluating:  30%|███       | 69/228 [00:19<00:37,  4.22it/s][Astep: 69
extend+tolist() time: 0.0015041828155517578

Evaluating:  31%|███       | 70/228 [00:19<00:39,  4.01it/s][Astep: 70
extend+tolist() time: 0.0014050006866455078

Evaluating:  31%|███       | 71/228 [00:19<00:40,  3.88it/s][Astep: 71
extend+tolist() time: 0.0009429454803466797

Evaluating:  32%|███▏      | 72/228 [00:19<00:40,  3.82it/s][Astep: 72
extend+tolist() time: 0.0011870861053466797

Evaluating:  32%|███▏      | 73/228 [00:20<00:39,  3.95it/s][Astep: 73
extend+tolist() time: 0.0005483627319335938

Evaluating:  32%|███▏      | 74/228 [00:20<00:37,  4.10it/s][Astep: 74
extend+tolist() time: 0.16391348838806152

Evaluating:  33%|███▎      | 75/228 [00:20<00:43,  3.48it/s][Astep: 75
extend+tolist() time: 0.001661062240600586

Evaluating:  33%|███▎      | 76/228 [00:21<00:44,  3.41it/s][Astep: 76
extend+tolist() time: 0.0006246566772460938

Evaluating:  34%|███▍      | 77/228 [00:21<00:41,  3.67it/s][Astep: 77
extend+tolist() time: 0.0019197463989257812

Evaluating:  34%|███▍      | 78/228 [00:21<00:43,  3.43it/s][Astep: 78
extend+tolist() time: 0.0011701583862304688

Evaluating:  35%|███▍      | 79/228 [00:21<00:48,  3.06it/s][Astep: 79
extend+tolist() time: 0.0008955001831054688

Evaluating:  35%|███▌      | 80/228 [00:22<00:44,  3.29it/s][Astep: 80
extend+tolist() time: 0.0009045600891113281

Evaluating:  36%|███▌      | 81/228 [00:22<00:42,  3.49it/s][Astep: 81
extend+tolist() time: 0.0033729076385498047

Evaluating:  36%|███▌      | 82/228 [00:22<00:48,  3.04it/s][Astep: 82
extend+tolist() time: 0.000858306884765625

Evaluating:  36%|███▋      | 83/228 [00:23<00:43,  3.31it/s][Astep: 83
extend+tolist() time: 0.0010998249053955078

Evaluating:  37%|███▋      | 84/228 [00:23<00:40,  3.58it/s][Astep: 84
extend+tolist() time: 0.0010471343994140625

Evaluating:  37%|███▋      | 85/228 [00:23<00:39,  3.61it/s][Astep: 85
extend+tolist() time: 0.0013072490692138672

Evaluating:  38%|███▊      | 86/228 [00:23<00:38,  3.72it/s][Astep: 86
extend+tolist() time: 0.0008819103240966797

Evaluating:  38%|███▊      | 87/228 [00:24<00:37,  3.81it/s][Astep: 87
extend+tolist() time: 0.0013027191162109375

Evaluating:  39%|███▊      | 88/228 [00:24<00:36,  3.86it/s][Astep: 88
extend+tolist() time: 0.0011463165283203125

Evaluating:  39%|███▉      | 89/228 [00:24<00:34,  4.02it/s][Astep: 89
extend+tolist() time: 0.0007317066192626953

Evaluating:  39%|███▉      | 90/228 [00:24<00:33,  4.14it/s][Astep: 90
extend+tolist() time: 0.0013625621795654297

Evaluating:  40%|███▉      | 91/228 [00:25<00:33,  4.10it/s][Astep: 91
extend+tolist() time: 0.0007526874542236328

Evaluating:  40%|████      | 92/228 [00:25<00:32,  4.18it/s][Astep: 92
extend+tolist() time: 0.0011491775512695312

Evaluating:  41%|████      | 93/228 [00:25<00:31,  4.22it/s][Astep: 93
extend+tolist() time: 0.0009529590606689453

Evaluating:  41%|████      | 94/228 [00:25<00:33,  4.06it/s][Astep: 94
extend+tolist() time: 0.0011115074157714844

Evaluating:  42%|████▏     | 95/228 [00:26<00:32,  4.15it/s][Astep: 95
extend+tolist() time: 0.0011875629425048828

Evaluating:  42%|████▏     | 96/228 [00:26<00:33,  3.89it/s][Astep: 96
extend+tolist() time: 0.0014295578002929688

Evaluating:  43%|████▎     | 97/228 [00:26<00:33,  3.92it/s][Astep: 97
extend+tolist() time: 0.001209259033203125

Evaluating:  43%|████▎     | 98/228 [00:26<00:32,  4.02it/s][Astep: 98
extend+tolist() time: 0.0008895397186279297

Evaluating:  43%|████▎     | 99/228 [00:27<00:32,  4.01it/s][Astep: 99
extend+tolist() time: 0.0013115406036376953

Evaluating:  44%|████▍     | 100/228 [00:27<00:31,  4.02it/s][Astep: 100
extend+tolist() time: 0.0007436275482177734

Evaluating:  44%|████▍     | 101/228 [00:27<00:30,  4.14it/s][Astep: 101
extend+tolist() time: 0.0012392997741699219

Evaluating:  45%|████▍     | 102/228 [00:27<00:30,  4.15it/s][Astep: 102
extend+tolist() time: 0.0007612705230712891

Evaluating:  45%|████▌     | 103/228 [00:28<00:29,  4.23it/s][Astep: 103
extend+tolist() time: 0.0011527538299560547

Evaluating:  46%|████▌     | 104/228 [00:28<00:35,  3.48it/s][Astep: 104
extend+tolist() time: 0.0006799697875976562

Evaluating:  46%|████▌     | 105/228 [00:28<00:33,  3.72it/s][Astep: 105
extend+tolist() time: 0.0013184547424316406

Evaluating:  46%|████▋     | 106/228 [00:28<00:31,  3.85it/s][Astep: 106
extend+tolist() time: 0.00135040283203125

Evaluating:  47%|████▋     | 107/228 [00:29<00:33,  3.60it/s][Astep: 107
extend+tolist() time: 0.001203775405883789

Evaluating:  47%|████▋     | 108/228 [00:29<00:31,  3.80it/s][Astep: 108
extend+tolist() time: 0.0008261203765869141

Evaluating:  48%|████▊     | 109/228 [00:29<00:30,  3.94it/s][Astep: 109
extend+tolist() time: 0.001329660415649414

Evaluating:  48%|████▊     | 110/228 [00:29<00:29,  4.01it/s][Astep: 110
extend+tolist() time: 0.0006124973297119141

Evaluating:  49%|████▊     | 111/228 [00:30<00:28,  4.12it/s][Astep: 111
extend+tolist() time: 0.0018110275268554688

Evaluating:  49%|████▉     | 112/228 [00:30<00:30,  3.75it/s][Astep: 112
extend+tolist() time: 0.0007309913635253906

Evaluating:  50%|████▉     | 113/228 [00:30<00:29,  3.96it/s][Astep: 113
extend+tolist() time: 0.000698089599609375

Evaluating:  50%|█████     | 114/228 [00:30<00:27,  4.10it/s][Astep: 114
extend+tolist() time: 0.0015130043029785156

Evaluating:  50%|█████     | 115/228 [00:31<00:28,  3.92it/s][Astep: 115
extend+tolist() time: 0.0006365776062011719

Evaluating:  51%|█████     | 116/228 [00:31<00:27,  4.06it/s][Astep: 116
extend+tolist() time: 0.0012209415435791016

Evaluating:  51%|█████▏    | 117/228 [00:31<00:26,  4.13it/s][Astep: 117
extend+tolist() time: 0.0008461475372314453

Evaluating:  52%|█████▏    | 118/228 [00:31<00:26,  4.12it/s][Astep: 118
extend+tolist() time: 0.0005946159362792969

Evaluating:  52%|█████▏    | 119/228 [00:32<00:25,  4.22it/s][Astep: 119
extend+tolist() time: 0.0011410713195800781

Evaluating:  53%|█████▎    | 120/228 [00:32<00:25,  4.28it/s][Astep: 120
extend+tolist() time: 0.0006020069122314453

Evaluating:  53%|█████▎    | 121/228 [00:32<00:24,  4.35it/s][Astep: 121
extend+tolist() time: 0.0010526180267333984

Evaluating:  54%|█████▎    | 122/228 [00:32<00:24,  4.40it/s][Astep: 122
extend+tolist() time: 0.0007178783416748047

Evaluating:  54%|█████▍    | 123/228 [00:32<00:23,  4.41it/s][Astep: 123
extend+tolist() time: 0.0009784698486328125

Evaluating:  54%|█████▍    | 124/228 [00:33<00:23,  4.44it/s][Astep: 124
extend+tolist() time: 0.18525981903076172

Evaluating:  55%|█████▍    | 125/228 [00:33<00:29,  3.53it/s][Astep: 125
extend+tolist() time: 0.00041365623474121094

Evaluating:  55%|█████▌    | 126/228 [00:33<00:26,  3.80it/s][Astep: 126
extend+tolist() time: 0.0012831687927246094

Evaluating:  56%|█████▌    | 127/228 [00:34<00:28,  3.57it/s][Astep: 127
extend+tolist() time: 0.001758575439453125

Evaluating:  56%|█████▌    | 128/228 [00:34<00:29,  3.42it/s][Astep: 128
extend+tolist() time: 0.001055002212524414

Evaluating:  57%|█████▋    | 129/228 [00:34<00:26,  3.67it/s][Astep: 129
extend+tolist() time: 0.0008444786071777344

Evaluating:  57%|█████▋    | 130/228 [00:34<00:25,  3.86it/s][Astep: 130
extend+tolist() time: 0.001346588134765625

Evaluating:  57%|█████▋    | 131/228 [00:35<00:30,  3.23it/s][Astep: 131
extend+tolist() time: 0.00044035911560058594

Evaluating:  58%|█████▊    | 132/228 [00:35<00:27,  3.54it/s][Astep: 132
extend+tolist() time: 0.0015017986297607422

Evaluating:  58%|█████▊    | 133/228 [00:35<00:26,  3.55it/s][Astep: 133
extend+tolist() time: 0.0004138946533203125

Evaluating:  59%|█████▉    | 134/228 [00:36<00:24,  3.81it/s][Astep: 134
extend+tolist() time: 0.001430511474609375

Evaluating:  59%|█████▉    | 135/228 [00:36<00:30,  3.07it/s][Astep: 135
extend+tolist() time: 0.0004379749298095703

Evaluating:  60%|█████▉    | 136/228 [00:36<00:27,  3.41it/s][Astep: 136
extend+tolist() time: 0.0011990070343017578

Evaluating:  60%|██████    | 137/228 [00:37<00:25,  3.63it/s][Astep: 137
extend+tolist() time: 0.00039076805114746094

Evaluating:  61%|██████    | 138/228 [00:37<00:23,  3.87it/s][Astep: 138
extend+tolist() time: 0.0006852149963378906

Evaluating:  61%|██████    | 139/228 [00:37<00:22,  4.02it/s][Astep: 139
extend+tolist() time: 0.0004401206970214844

Evaluating:  61%|██████▏   | 140/228 [00:37<00:21,  4.17it/s][Astep: 140
extend+tolist() time: 0.0012161731719970703

Evaluating:  62%|██████▏   | 141/228 [00:37<00:20,  4.23it/s][Astep: 141
extend+tolist() time: 0.0007967948913574219

Evaluating:  62%|██████▏   | 142/228 [00:38<00:20,  4.26it/s][Astep: 142
extend+tolist() time: 0.0009479522705078125

Evaluating:  63%|██████▎   | 143/228 [00:38<00:19,  4.35it/s][Astep: 143
extend+tolist() time: 0.00032258033752441406

Evaluating:  63%|██████▎   | 144/228 [00:38<00:18,  4.43it/s][Astep: 144
extend+tolist() time: 0.0006911754608154297

Evaluating:  64%|██████▎   | 145/228 [00:38<00:18,  4.43it/s][Astep: 145
extend+tolist() time: 0.0009102821350097656

Evaluating:  64%|██████▍   | 146/228 [00:39<00:18,  4.47it/s][Astep: 146
extend+tolist() time: 0.00038695335388183594

Evaluating:  64%|██████▍   | 147/228 [00:39<00:17,  4.50it/s][Astep: 147
extend+tolist() time: 0.0007543563842773438

Evaluating:  65%|██████▍   | 148/228 [00:39<00:17,  4.48it/s][Astep: 148
extend+tolist() time: 0.0011074542999267578

Evaluating:  65%|██████▌   | 149/228 [00:39<00:17,  4.51it/s][Astep: 149
extend+tolist() time: 0.00038242340087890625

Evaluating:  66%|██████▌   | 150/228 [00:39<00:17,  4.53it/s][Astep: 150
extend+tolist() time: 0.0008063316345214844

Evaluating:  66%|██████▌   | 151/228 [00:40<00:17,  4.45it/s][Astep: 151
extend+tolist() time: 0.0010464191436767578

Evaluating:  67%|██████▋   | 152/228 [00:40<00:16,  4.47it/s][Astep: 152
extend+tolist() time: 0.0008218288421630859

Evaluating:  67%|██████▋   | 153/228 [00:40<00:16,  4.43it/s][Astep: 153
extend+tolist() time: 0.0013117790222167969

Evaluating:  68%|██████▊   | 154/228 [00:40<00:17,  4.33it/s][Astep: 154
extend+tolist() time: 0.0017583370208740234

Evaluating:  68%|██████▊   | 155/228 [00:41<00:19,  3.84it/s][Astep: 155
extend+tolist() time: 0.0005946159362792969

Evaluating:  68%|██████▊   | 156/228 [00:41<00:17,  4.01it/s][Astep: 156
extend+tolist() time: 0.0004887580871582031

Evaluating:  69%|██████▉   | 157/228 [00:41<00:17,  4.17it/s][Astep: 157
extend+tolist() time: 0.0010142326354980469

Evaluating:  69%|██████▉   | 158/228 [00:41<00:16,  4.26it/s][Astep: 158
extend+tolist() time: 0.0004980564117431641

Evaluating:  70%|██████▉   | 159/228 [00:42<00:15,  4.33it/s][Astep: 159
extend+tolist() time: 0.0006220340728759766

Evaluating:  70%|███████   | 160/228 [00:42<00:15,  4.37it/s][Astep: 160
extend+tolist() time: 0.0007753372192382812

Evaluating:  71%|███████   | 161/228 [00:42<00:15,  4.44it/s][Astep: 161
extend+tolist() time: 0.0007064342498779297

Evaluating:  71%|███████   | 162/228 [00:42<00:14,  4.47it/s][Astep: 162
extend+tolist() time: 0.0004525184631347656

Evaluating:  71%|███████▏  | 163/228 [00:42<00:14,  4.51it/s][Astep: 163
extend+tolist() time: 0.0003514289855957031

Evaluating:  72%|███████▏  | 164/228 [00:43<00:14,  4.53it/s][Astep: 164
extend+tolist() time: 0.0009350776672363281

Evaluating:  72%|███████▏  | 165/228 [00:43<00:13,  4.55it/s][Astep: 165
extend+tolist() time: 0.0003933906555175781

Evaluating:  73%|███████▎  | 166/228 [00:43<00:13,  4.57it/s][Astep: 166
extend+tolist() time: 0.00034928321838378906

Evaluating:  73%|███████▎  | 167/228 [00:43<00:13,  4.58it/s][Astep: 167
extend+tolist() time: 0.0004990100860595703

Evaluating:  74%|███████▎  | 168/228 [00:44<00:13,  4.57it/s][Astep: 168
extend+tolist() time: 0.001458883285522461

Evaluating:  74%|███████▍  | 169/228 [00:44<00:13,  4.24it/s][Astep: 169
extend+tolist() time: 0.00034499168395996094

Evaluating:  75%|███████▍  | 170/228 [00:44<00:13,  4.34it/s][Astep: 170
extend+tolist() time: 0.0011878013610839844

Evaluating:  75%|███████▌  | 171/228 [00:44<00:13,  4.35it/s][Astep: 171
extend+tolist() time: 0.0002701282501220703

Evaluating:  75%|███████▌  | 172/228 [00:44<00:12,  4.43it/s][Astep: 172
extend+tolist() time: 0.0007040500640869141

Evaluating:  76%|███████▌  | 173/228 [00:45<00:12,  4.42it/s][Astep: 173
extend+tolist() time: 0.0014667510986328125

Evaluating:  76%|███████▋  | 174/228 [00:45<00:12,  4.18it/s][Astep: 174
extend+tolist() time: 0.0016093254089355469

Evaluating:  77%|███████▋  | 175/228 [00:45<00:16,  3.14it/s][Astep: 175
extend+tolist() time: 0.0007407665252685547

Evaluating:  77%|███████▋  | 176/228 [00:46<00:15,  3.44it/s][Astep: 176
extend+tolist() time: 0.0009915828704833984

Evaluating:  78%|███████▊  | 177/228 [00:46<00:13,  3.70it/s][Astep: 177
extend+tolist() time: 0.0005295276641845703

Evaluating:  78%|███████▊  | 178/228 [00:46<00:12,  3.93it/s][Astep: 178
extend+tolist() time: 0.0014584064483642578

Evaluating:  79%|███████▊  | 179/228 [00:46<00:12,  3.84it/s][Astep: 179
extend+tolist() time: 0.0003972053527832031

Evaluating:  79%|███████▉  | 180/228 [00:47<00:11,  4.03it/s][Astep: 180
extend+tolist() time: 0.0003809928894042969

Evaluating:  79%|███████▉  | 181/228 [00:47<00:11,  4.19it/s][Astep: 181
extend+tolist() time: 0.0006663799285888672

Evaluating:  80%|███████▉  | 182/228 [00:47<00:10,  4.29it/s][Astep: 182
extend+tolist() time: 0.0011920928955078125

Evaluating:  80%|████████  | 183/228 [00:47<00:10,  4.36it/s][Astep: 183
extend+tolist() time: 0.0006344318389892578

Evaluating:  81%|████████  | 184/228 [00:47<00:09,  4.40it/s][Astep: 184
extend+tolist() time: 0.00044083595275878906

Evaluating:  81%|████████  | 185/228 [00:48<00:09,  4.45it/s][Astep: 185
extend+tolist() time: 0.0016014575958251953

Evaluating:  82%|████████▏ | 186/228 [00:48<00:10,  4.20it/s][Astep: 186
extend+tolist() time: 0.0014102458953857422

Evaluating:  82%|████████▏ | 187/228 [00:48<00:09,  4.17it/s][Astep: 187
extend+tolist() time: 0.00043892860412597656

Evaluating:  82%|████████▏ | 188/228 [00:48<00:09,  4.28it/s][Astep: 188
extend+tolist() time: 0.0007262229919433594

Evaluating:  83%|████████▎ | 189/228 [00:49<00:08,  4.35it/s][Astep: 189
extend+tolist() time: 0.00033402442932128906

Evaluating:  83%|████████▎ | 190/228 [00:49<00:08,  4.42it/s][Astep: 190
extend+tolist() time: 0.0016274452209472656

Evaluating:  84%|████████▍ | 191/228 [00:49<00:08,  4.17it/s][Astep: 191
extend+tolist() time: 0.0007698535919189453

Evaluating:  84%|████████▍ | 192/228 [00:49<00:08,  4.24it/s][Astep: 192
extend+tolist() time: 0.0008823871612548828

Evaluating:  85%|████████▍ | 193/228 [00:50<00:08,  4.31it/s][Astep: 193
extend+tolist() time: 0.0010714530944824219

Evaluating:  85%|████████▌ | 194/228 [00:50<00:08,  4.22it/s][Astep: 194
extend+tolist() time: 0.0005977153778076172

Evaluating:  86%|████████▌ | 195/228 [00:50<00:07,  4.30it/s][Astep: 195
extend+tolist() time: 0.0005249977111816406

Evaluating:  86%|████████▌ | 196/228 [00:50<00:07,  4.36it/s][Astep: 196
extend+tolist() time: 0.0007767677307128906

Evaluating:  86%|████████▋ | 197/228 [00:51<00:07,  4.39it/s][Astep: 197
extend+tolist() time: 0.0007023811340332031

Evaluating:  87%|████████▋ | 198/228 [00:51<00:06,  4.42it/s][Astep: 198
extend+tolist() time: 0.0011093616485595703

Evaluating:  87%|████████▋ | 199/228 [00:51<00:06,  4.45it/s][Astep: 199
extend+tolist() time: 0.0018811225891113281

Evaluating:  88%|████████▊ | 200/228 [00:51<00:06,  4.01it/s][Astep: 200
extend+tolist() time: 0.000989675521850586

Evaluating:  88%|████████▊ | 201/228 [00:51<00:06,  4.13it/s][Astep: 201
extend+tolist() time: 0.0006403923034667969

Evaluating:  89%|████████▊ | 202/228 [00:52<00:06,  3.98it/s][Astep: 202
extend+tolist() time: 0.0004773139953613281

Evaluating:  89%|████████▉ | 203/228 [00:52<00:06,  4.13it/s][Astep: 203
extend+tolist() time: 0.001001119613647461

Evaluating:  89%|████████▉ | 204/228 [00:52<00:05,  4.25it/s][Astep: 204
extend+tolist() time: 0.00043487548828125

Evaluating:  90%|████████▉ | 205/228 [00:52<00:05,  4.35it/s][Astep: 205
extend+tolist() time: 0.0003027915954589844

Evaluating:  90%|█████████ | 206/228 [00:53<00:04,  4.41it/s][Astep: 206
extend+tolist() time: 0.0006759166717529297

Evaluating:  91%|█████████ | 207/228 [00:53<00:04,  4.29it/s][Astep: 207
extend+tolist() time: 0.0011501312255859375

Evaluating:  91%|█████████ | 208/228 [00:53<00:04,  4.36it/s][Astep: 208
extend+tolist() time: 0.000705718994140625

Evaluating:  92%|█████████▏| 209/228 [00:53<00:04,  4.39it/s][Astep: 209
extend+tolist() time: 0.0006186962127685547

Evaluating:  92%|█████████▏| 210/228 [00:54<00:04,  4.42it/s][Astep: 210
extend+tolist() time: 0.21270298957824707

Evaluating:  93%|█████████▎| 211/228 [00:54<00:04,  3.46it/s][Astep: 211
extend+tolist() time: 0.001150369644165039

Evaluating:  93%|█████████▎| 212/228 [00:54<00:04,  3.54it/s][Astep: 212
extend+tolist() time: 0.0009589195251464844

Evaluating:  93%|█████████▎| 213/228 [00:54<00:04,  3.73it/s][Astep: 213
extend+tolist() time: 0.001203775405883789

Evaluating:  94%|█████████▍| 214/228 [00:55<00:03,  3.92it/s][Astep: 214
extend+tolist() time: 0.0008642673492431641

Evaluating:  94%|█████████▍| 215/228 [00:55<00:03,  4.05it/s][Astep: 215
extend+tolist() time: 0.0006694793701171875

Evaluating:  95%|█████████▍| 216/228 [00:55<00:02,  4.18it/s][Astep: 216
extend+tolist() time: 0.0009996891021728516

Evaluating:  95%|█████████▌| 217/228 [00:55<00:02,  4.30it/s][Astep: 217
extend+tolist() time: 0.0005662441253662109

Evaluating:  96%|█████████▌| 218/228 [00:56<00:02,  4.36it/s][Astep: 218
extend+tolist() time: 0.0014369487762451172

Evaluating:  96%|█████████▌| 219/228 [00:56<00:02,  4.27it/s][Astep: 219
extend+tolist() time: 0.0004930496215820312

Evaluating:  96%|█████████▋| 220/228 [00:56<00:01,  4.34it/s][Astep: 220
extend+tolist() time: 0.0004127025604248047

Evaluating:  97%|█████████▋| 221/228 [00:56<00:01,  4.41it/s][Astep: 221
extend+tolist() time: 0.0010738372802734375

Evaluating:  97%|█████████▋| 222/228 [00:57<00:01,  3.48it/s][Astep: 222
extend+tolist() time: 0.0004525184631347656

Evaluating:  98%|█████████▊| 223/228 [00:57<00:01,  3.73it/s][Astep: 223
extend+tolist() time: 0.0003859996795654297

Evaluating:  98%|█████████▊| 224/228 [00:57<00:01,  3.95it/s][Astep: 224
extend+tolist() time: 0.0003802776336669922

Evaluating:  99%|█████████▊| 225/228 [00:57<00:00,  4.13it/s][Astep: 225
extend+tolist() time: 0.00044655799865722656

Evaluating:  99%|█████████▉| 226/228 [00:58<00:00,  4.25it/s][Astep: 226
extend+tolist() time: 0.0009732246398925781

Evaluating: 100%|█████████▉| 227/228 [00:58<00:00,  4.34it/s][Astep: 227
extend+tolist() time: 0.0004918575286865234

Evaluating: 100%|██████████| 228/228 [00:58<00:00,  3.83it/s][A09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 19:03:50 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow
09/05/2023 19:03:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 19:03:51 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 19:03:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
09/05/2023 19:03:52 - INFO - evaluate.module - Removing /data/users/zhangjunlei/tyx/.cache/huggingface/metrics/rocauc/multiclass/default_experiment-1-0.arrow
Evaluating: 100%|██████████| 228/228 [01:00<00:00,  3.74it/s]
09/05/2023 19:03:52 - INFO - __main__ -   Step: 5200, Validation Metrics: {'pred_1_num': 9476, 'pred_-1_num': 917, 'pred_0_num': 408, 'ref_1_num': 8596, 'ref_0_num': 639, 'ref_-1_num': 1566, 'accuracy': 0.7747430793445051, 'f1_micro': 0.7747430793445051, 'f1_macro': 0.4559534815312287, 'f1_weighted': 0.7515153881157484, 'f1_-1': 0.3536045106725735, 'f1_0': 0.1451766953199618, 'f1_1': 0.8690792386011509, 'precision_micro': 0.7747430793445051, 'precision_macro': 0.49791157192100904, 'precision_weighted': 0.7399731741373634, 'precision_-1': 0.4787350054525627, 'precision_0': 0.18627450980392157, 'precision_1': 0.8287252005065429, 'recall_micro': 0.7747430793445051, 'recall_macro': 0.43761078067351894, 'recall_weighted': 0.7747430793445051, 'recall_-1': 0.28033205619412516, 'recall_0': 0.1189358372456964, 'recall_1': 0.9135644485807353, 'roc_auc_micro': 0.8843518592720153, 'roc_auc_macro': 0.6700048236042347, 'roc_auc_weighted': 0.6500151058482024, 'roc_auc_-1': 0.6929474187889512, 'roc_auc_0': 0.6768695335871865, 'roc_auc_1': 0.6401975184365665}
[2023-09-05 19:03:57,924] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5201/66600 [5:00:09<549:06:35, 32.20s/it]09/05/2023 19:04:05 - INFO - __main__ -   Step: 5201, LR: 1.9007785189171195e-05, Loss: 0.13346511125564575
[2023-09-05 19:04:11,929] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5202/66600 [5:00:23<458:59:10, 26.91s/it]09/05/2023 19:04:20 - INFO - __main__ -   Step: 5202, LR: 1.9007475484977413e-05, Loss: 0.16001245379447937
[2023-09-05 19:04:27,361] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5203/66600 [5:00:38<398:41:00, 23.38s/it]09/05/2023 19:04:35 - INFO - __main__ -   Step: 5203, LR: 1.9007165780783628e-05, Loss: 0.10078296810388565
[2023-09-05 19:04:42,173] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5204/66600 [5:00:53<355:03:06, 20.82s/it]09/05/2023 19:04:50 - INFO - __main__ -   Step: 5204, LR: 1.900685607658985e-05, Loss: 0.1428241729736328
[2023-09-05 19:04:56,135] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5205/66600 [5:01:07<318:07:18, 18.65s/it]09/05/2023 19:05:03 - INFO - __main__ -   Step: 5205, LR: 1.9006546372396068e-05, Loss: 0.104559987783432
[2023-09-05 19:05:09,583] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5206/66600 [5:01:21<293:40:06, 17.22s/it]09/05/2023 19:05:17 - INFO - __main__ -   Step: 5206, LR: 1.9006236668202286e-05, Loss: 0.13977238535881042
[2023-09-05 19:05:24,016] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5207/66600 [5:01:35<279:05:14, 16.37s/it]09/05/2023 19:05:32 - INFO - __main__ -   Step: 5207, LR: 1.9005926964008504e-05, Loss: 0.15936161577701569
[2023-09-05 19:05:39,190] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5208/66600 [5:01:50<271:27:05, 15.92s/it]09/05/2023 19:05:46 - INFO - __main__ -   Step: 5208, LR: 1.9005617259814722e-05, Loss: 0.15321144461631775
[2023-09-05 19:05:53,369] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5209/66600 [5:02:04<261:49:21, 15.35s/it]09/05/2023 19:06:00 - INFO - __main__ -   Step: 5209, LR: 1.900530755562094e-05, Loss: 0.13992391526699066
[2023-09-05 19:06:07,332] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5210/66600 [5:02:18<254:14:57, 14.91s/it]09/05/2023 19:06:14 - INFO - __main__ -   Step: 5210, LR: 1.9004997851427158e-05, Loss: 0.1156887635588646
[2023-09-05 19:06:20,649] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5211/66600 [5:02:32<250:16:01, 14.68s/it]09/05/2023 19:06:28 - INFO - __main__ -   Step: 5211, LR: 1.9004688147233376e-05, Loss: 0.1429663449525833
[2023-09-05 19:06:35,861] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5212/66600 [5:02:46<247:24:49, 14.51s/it]09/05/2023 19:06:43 - INFO - __main__ -   Step: 5212, LR: 1.9004378443039594e-05, Loss: 0.15674173831939697
[2023-09-05 19:06:50,115] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5213/66600 [5:03:00<245:26:26, 14.39s/it]09/05/2023 19:06:57 - INFO - __main__ -   Step: 5213, LR: 1.9004068738845812e-05, Loss: 0.15439336001873016
[2023-09-05 19:07:04,554] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5214/66600 [5:03:15<247:46:43, 14.53s/it]09/05/2023 19:07:12 - INFO - __main__ -   Step: 5214, LR: 1.900375903465203e-05, Loss: 0.1602984070777893
[2023-09-05 19:07:18,169] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5215/66600 [5:03:29<243:50:05, 14.30s/it]09/05/2023 19:07:25 - INFO - __main__ -   Step: 5215, LR: 1.9003449330458248e-05, Loss: 0.13145916163921356
[2023-09-05 19:07:32,426] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5216/66600 [5:03:43<243:52:53, 14.30s/it]09/05/2023 19:07:40 - INFO - __main__ -   Step: 5216, LR: 1.9003139626264466e-05, Loss: 0.15861879289150238
[2023-09-05 19:07:46,547] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5217/66600 [5:03:57<241:41:11, 14.17s/it]09/05/2023 19:07:53 - INFO - __main__ -   Step: 5217, LR: 1.9002829922070684e-05, Loss: 0.21121281385421753
[2023-09-05 19:08:00,256] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5218/66600 [5:04:12<247:43:32, 14.53s/it]09/05/2023 19:08:09 - INFO - __main__ -   Step: 5218, LR: 1.9002520217876902e-05, Loss: 0.12198157608509064
[2023-09-05 19:08:15,439] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5219/66600 [5:04:26<245:16:30, 14.39s/it]09/05/2023 19:08:23 - INFO - __main__ -   Step: 5219, LR: 1.900221051368312e-05, Loss: 0.12011299282312393
[2023-09-05 19:08:29,288] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5220/66600 [5:04:40<238:33:10, 13.99s/it]09/05/2023 19:08:36 - INFO - __main__ -   Step: 5220, LR: 1.900190080948934e-05, Loss: 0.18068556487560272
[2023-09-05 19:08:42,903] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5221/66600 [5:04:54<238:36:14, 13.99s/it]09/05/2023 19:08:50 - INFO - __main__ -   Step: 5221, LR: 1.9001591105295557e-05, Loss: 0.16171526908874512
[2023-09-05 19:08:57,583] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5222/66600 [5:05:10<249:57:39, 14.66s/it]09/05/2023 19:09:06 - INFO - __main__ -   Step: 5222, LR: 1.9001281401101775e-05, Loss: 0.16226711869239807
[2023-09-05 19:09:13,654] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5223/66600 [5:05:24<248:53:42, 14.60s/it]09/05/2023 19:09:21 - INFO - __main__ -   Step: 5223, LR: 1.9000971696907993e-05, Loss: 0.13495934009552002
[2023-09-05 19:09:27,441] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5224/66600 [5:05:39<249:02:44, 14.61s/it]09/05/2023 19:09:35 - INFO - __main__ -   Step: 5224, LR: 1.900066199271421e-05, Loss: 0.12668399512767792
[2023-09-05 19:09:41,823] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5225/66600 [5:05:52<244:05:33, 14.32s/it]09/05/2023 19:09:49 - INFO - __main__ -   Step: 5225, LR: 1.900035228852043e-05, Loss: 0.19771459698677063
[2023-09-05 19:09:56,348] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  8%|▊         | 5226/66600 [5:06:08<248:05:07, 14.55s/it]09/05/2023 19:10:04 - INFO - __main__ -   Step: 5226, LR: 1.9000042584326647e-05, Loss: 0.15263459086418152
[2023-09-05 19:10:10,563] [WARNING] [stage3.py:1898:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3960798 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3960799 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3960800 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 3 (pid: 3960801) of binary: /data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/python
Traceback (most recent call last):
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 45, in main
    args.func(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 964, in launch_command
    deepspeed_launcher(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 687, in deepspeed_launcher
    distrib_run.run(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/data/users/zhangjunlei/tyx/reward-by-prm800k/src/finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-05_19:10:15
  host      : a100
  rank      : 3 (local_rank: 3)
  exitcode  : -9 (pid: 3960801)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 3960801
============================================================
