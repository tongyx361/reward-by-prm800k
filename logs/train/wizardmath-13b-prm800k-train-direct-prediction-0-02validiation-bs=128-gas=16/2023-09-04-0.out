nohup: ignoring input
[2023-09-04 20:39:11,512] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-09-04 20:39:16,171] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-04 20:39:16,231] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-04 20:39:16,239] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-04 20:39:16,264] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using xformers
Initializing accelerator...
[2023-09-04 20:39:17,410] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-04 20:39:17,410] [INFO] [comm.py:616:init_distributed] cdb=None
Using xformers
Initializing accelerator...
[2023-09-04 20:39:17,478] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-04 20:39:17,478] [INFO] [comm.py:616:init_distributed] cdb=None
Using xformers
Initializing accelerator...
[2023-09-04 20:39:17,590] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-04 20:39:17,591] [INFO] [comm.py:616:init_distributed] cdb=None
Using xformers
Initializing accelerator...
[2023-09-04 20:39:17,620] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-04 20:39:17,620] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-04 20:39:17,620] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
accelerator.state = Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

09/04/2023 20:39:17 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

09/04/2023 20:39:17 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

09/04/2023 20:39:17 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

09/04/2023 20:39:17 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

loading configuration file config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-13B-V1.0/snapshots/7ef412d2c680ef0fbdcd88d0df31b396d8d3049c/config.json
Model config LlamaConfig {
  "_name_or_path": "WizardLM/WizardMath-13B-V1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": false,
  "vocab_size": 32001
}

loading file tokenizer.model
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
loading weights file pytorch_model.bin from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-13B-V1.0/snapshots/7ef412d2c680ef0fbdcd88d0df31b396d8d3049c/pytorch_model.bin.index.json
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.31.0",
  "use_cache": false
}

[2023-09-04 20:39:36,879] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 13.02B parameters
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:05<00:28,  5.67s/it]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:05<00:28,  5.68s/it]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:05<00:28,  5.79s/it]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:09<00:46,  9.24s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:10<00:21,  5.42s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:10<00:21,  5.44s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:10<00:21,  5.45s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:16<00:16,  5.52s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:16<00:16,  5.53s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:16<00:16,  5.55s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:18<00:35,  8.97s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:22<00:11,  5.58s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:22<00:11,  5.58s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:22<00:11,  5.58s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:26<00:26,  8.83s/it]Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:27<00:05,  5.55s/it]Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:27<00:05,  5.55s/it]Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:27<00:05,  5.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:28<00:00,  4.00s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:28<00:00,  4.79s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:28<00:00,  4.02s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:28<00:00,  4.80s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:29<00:00,  4.24s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:29<00:00,  4.91s/it]
Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/162 [00:00<?, ?B/s]Downloading (‚Ä¶)neration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162/162 [00:00<00:00, 1.28MB/s]
09/04/2023 20:40:09 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
09/04/2023 20:40:09 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
09/04/2023 20:40:09 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:34<00:17,  8.60s/it]09/04/2023 20:40:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:2 (world_size=4, worker_count=3, timeout=0:30:00)
09/04/2023 20:40:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:2 (world_size=4, worker_count=3, timeout=0:30:00)
09/04/2023 20:40:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:2 (world_size=4, worker_count=3, timeout=0:30:00)
Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:43<00:08,  8.46s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:45<00:00,  6.34s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:45<00:00,  7.56s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at WizardLM/WizardMath-13B-V1.0.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-13B-V1.0/snapshots/7ef412d2c680ef0fbdcd88d0df31b396d8d3049c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.9,
  "top_p": 0.6,
  "transformers_version": "4.31.0"
}

Assigning <s> to the bos_token key of the tokenizer
Assigning </s> to the eos_token key of the tokenizer
Assigning <unk> to the unk_token key of the tokenizer
Assigning <pad> to the pad_token key of the tokenizer
Adding <pad> to the vocabulary
09/04/2023 20:40:22 - INFO - __main__ - Sample 25479 of the training set: {'input_ids': tensor([    1,   450,   610,   370, 16118,  3342,   491,   278, 10693,   395,
        29891, 29922, 29916, 29985, 29906, 29974, 29946, 29916, 29974, 29953,
        29938,   322,   395, 29891,  2013,  1154, 29912, 29896,  1157, 29906,
        29913, 29916, 29985, 29906, 29974, 29916, 29974, 29953, 29938, 25869,
          472,  3291,  2427, 29874, 29892, 29890,  1262,   322,  2427, 29883,
        29892, 29881,  5767,   988,   395, 29883, 29905,   479,   263,  1504,
         1724,   338,   395, 29883, 29899, 29874, 15485,    13,  1762,  1284,
          278,  3291,   310, 17686,   310,   278,  1023,   610,   370, 16118,
        29892,   306,   817,   304,  4505,   278,  1788,   310, 10693,   395,
        29891, 29922, 29916, 29985, 29906, 29974, 29946, 29916, 29974, 29953,
        29938,   322,   395, 29891,  2013,  1154, 29912, 29896,  1157, 29906,
        29913, 29916, 29985, 29906, 29974, 29916, 29974, 29953,  1504,    13,
        29902,   508,   437,   445,   491,  4444,   278, 12241,   363,   395,
        29891, 29938,  5186,   304,  1269,   916,   322,  5466,  9215, 29901,
          395, 29916, 29985, 29906, 29974, 29946, 29916, 29974, 29953,  2013,
         1154, 29912, 29896,  1157, 29906, 29913, 29916, 29985, 29906, 29974,
        29916, 29974, 29953,  1504,    13,  6857,   666,  5890,  1716, 11192,
          491, 29871, 29906, 29892,   306,   679,   395, 29906, 29916, 29985,
        29906, 29974, 29947, 29916, 29974, 29896, 29906, 29922, 29916, 29985,
        29906, 29974, 29906, 29916, 29974, 29896, 29906,  1504,    13,  4035,
        29873,  1461,   292,   395, 29916, 29985, 29906, 29974, 29906, 29916,
        29974, 29896, 29906, 29938,   515,  1716, 11192, 29892,   306,   679,
          395, 29916, 29985, 29906, 29974, 29953, 29916, 29922, 29900,  1504,
           13, 29943,  7168,   292,   714,   395, 29916,  1628,   306,   679,
          395, 29916, 29898, 29916, 29974, 29953,  3892, 29900,  1504,    13,
         6295,   395, 29916, 29922, 29900, 29938,   470,   395, 29916, 10457,
        29953,  1504,    13,  1349,   968,   526,   278,   921, 29899,  1111,
        24266,   310,   278,  3291,   310, 17686, 29889,    13,  1762,  1284,
          278,   343, 29899,  1111, 24266, 29892,   306,   508, 18665,  1438,
         1819,   310,   395, 29916, 29938,   964,  2845,   310,   278,  2441,
        10693,   363,   395, 29891,  1504,    13,  2831,   395, 29916, 29922,
        29900,  1628,   306,   679,   395, 29891, 29922, 29900, 29985, 29906,
        29974, 29946, 29898, 29900,  7240, 29953, 29922, 29953,  1504,    13,
         2831,   395, 29916, 10457, 29953,  1628,   306,   679,   395, 29891,
        29922,  6278, 29953,  4887, 29906, 29974, 29946,  6278, 29953,  7240,
        29953, 29922, 29953,  1504,    13]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  6374,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  6374,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100, 21104,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 21104,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  8178])}.
09/04/2023 20:40:22 - INFO - __main__ - Sample 82476 of the training set: {'input_ids': tensor([    1,  2180, 16146,   293, 10355, 29892,   304,  1209,   385,  9623,
         1243,   366,  1818,  8158,   472,  3203,   395, 29947, 29900,  8958,
         1504,   960,   727,   526, 29871, 29941, 29945,  4828,   373,   278,
         1243, 29892,   825,   338,   278, 14176,  1353,   366,   508,  3052,
          322,  1603,  1209, 29973,    13,  6295,   591,  1073,   393,   591,
          817,   472,  3203,   385, 29871, 29947, 29900, 29995,   304,  1209,
        29889,  2193,  2794,   393,   591,   508,  3052,   701,   304, 29871,
        29906, 29900, 29995,   310,   278,  4828,   322,  1603,  1209, 29889,
           13,  7341, 29892,   541,   591,   864,   304,  1073,   825,   278,
        14176,  1353,   310,  4828,   591,   508,  3052,   338, 29889,    13,
         7341, 29889,  1105,  1235, 29915, 29879,   731,   701,   385, 14585,
        29889,   395, 29876,   320,  3797, 29871, 29941, 29945, 29930, 29900,
        29889, 29906, 29900, 29938,    13, 11760,   302,   338,  3109,  1135,
          470,  5186,   304, 29871, 29955, 29889,    13,  7058,  2794,   591,
          508,  3052,   701,   304, 29871, 29955,  4828, 29889,    13,  6295,
          278, 14176,  1353,   310,  4828,   591,   508,  3052,   322,  1603,
         1209,   338, 29871, 29953, 29889,    13]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, 8178])}.
09/04/2023 20:40:22 - INFO - __main__ - Sample 79560 of the training set: {'input_ids': tensor([    1,   382,  4387,   403,   278,  2183, 10160, 19470, 29905, 15618,
          320,   524,   426,   320, 29883,  1154,   426, 29871, 29896, 24333,
        29871, 29941, 29974, 29906, 29905,  5223,   426,   921,   500, 17501,
         3944,   426,   921,   500, 30081,   500, 30081,   500, 15414,  3997,
           13,  4013, 10160,  3430, 28722, 29892,   541,   306,  8369,   393,
          278, 14267,  1061,   338,   263,  5608, 10296,   310,   269,   457,
          322,  6776,   457, 29892,   607,  3732,   592,  1348,   310,   263,
        16222,   265, 14066, 10110, 29889,    13,  3644,   306, 17386,  5149,
        29892,   727,   338,   385, 10110,   393,  4083,  6118,  3944,   426,
          320,  1563, 29898,   921,  3124,  1154,   426,   320,  1631, 29871,
        24333, 29871, 29946,   500, 29871,   320,  1266, 29897, 29871,   500,
        17313,  1154,   426, 29871, 29896, 24333,   320,  3676,   426, 29871,
        29906,   500, 29871,   500,   320,  1563, 29898,   320,  3944,   426,
          921,   500, 17501,  5223,   426,   921,   500, 29871,   320,  1266,
        29897,  2046,    13]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8178])}.
09/04/2023 20:40:22 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (2).
09/04/2023 20:40:22 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.
[2023-09-04 20:40:22,945] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
09/04/2023 20:40:22 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
09/04/2023 20:40:22 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-09-04 20:40:22,968] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-04 20:40:22,969] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-04 20:40:22,969] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
09/04/2023 20:40:22 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
09/04/2023 20:40:22 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
09/04/2023 20:40:22 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-09-04 20:40:22,984] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-04 20:40:22,984] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-04 20:40:22,984] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2023-09-04 20:40:22,984] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2023-09-04 20:40:23,093] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
[2023-09-04 20:40:23,093] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 7.07 GB         CA 8.68 GB         Max_CA 9 GB 
[2023-09-04 20:40:23,094] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 122.49 GB, percent = 12.2%
[2023-09-04 20:40:23,095] [INFO] [stage3.py:117:__init__] Reduce bucket size 26214400
[2023-09-04 20:40:23,095] [INFO] [stage3.py:118:__init__] Prefetch bucket size 23592960
[2023-09-04 20:40:23,199] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-09-04 20:40:23,199] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 8.68 GB         Max_CA 9 GB 
[2023-09-04 20:40:23,199] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 122.49 GB, percent = 12.2%
Parameter Offload: Total persistent parameters: 414720 in 81 params
[2023-09-04 20:40:23,321] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-09-04 20:40:23,321] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 8.68 GB         Max_CA 9 GB 
[2023-09-04 20:40:23,322] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 122.49 GB, percent = 12.2%
[2023-09-04 20:40:23,427] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
[2023-09-04 20:40:23,428] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 8.68 GB         Max_CA 9 GB 
[2023-09-04 20:40:23,428] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 122.49 GB, percent = 12.2%
[2023-09-04 20:40:27,881] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 4
[2023-09-04 20:40:27,882] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 6.14 GB         Max_CA 9 GB 
[2023-09-04 20:40:27,882] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 137.74 GB, percent = 13.7%
[2023-09-04 20:40:28,030] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
[2023-09-04 20:40:28,031] [INFO] [utils.py:786:see_memory_usage] MA 6.14 GB         Max_MA 6.14 GB         CA 6.14 GB         Max_CA 6 GB 
[2023-09-04 20:40:28,031] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 131.93 GB, percent = 13.1%
[2023-09-04 20:40:28,187] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
[2023-09-04 20:40:28,187] [INFO] [utils.py:786:see_memory_usage] MA 18.26 GB         Max_MA 19.27 GB         CA 21.16 GB         Max_CA 21 GB 
[2023-09-04 20:40:28,188] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 128.56 GB, percent = 12.8%
[2023-09-04 20:40:31,233] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-04 20:40:31,234] [INFO] [utils.py:786:see_memory_usage] MA 18.26 GB         Max_MA 18.26 GB         CA 21.16 GB         Max_CA 21 GB 
[2023-09-04 20:40:31,234] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 122.48 GB, percent = 12.2%
[2023-09-04 20:40:31,405] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-04 20:40:31,405] [INFO] [utils.py:786:see_memory_usage] MA 42.51 GB         Max_MA 52.08 GB         CA 62.48 GB         Max_CA 62 GB 
[2023-09-04 20:40:31,405] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 122.48 GB, percent = 12.2%
[2023-09-04 20:40:31,441] [INFO] [stage3.py:424:_setup_for_real_optimizer] optimizer state initialized
[2023-09-04 20:40:31,702] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-04 20:40:31,702] [INFO] [utils.py:786:see_memory_usage] MA 48.62 GB         Max_MA 49.23 GB         CA 69.69 GB         Max_CA 70 GB 
[2023-09-04 20:40:31,703] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 122.51 GB, percent = 12.2%
[2023-09-04 20:40:31,703] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-04 20:40:31,703] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-04 20:40:31,703] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-04 20:40:31,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-09-04 20:40:31,704] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5b5f4c9840>
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-04 20:40:31,704] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 16
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   train_batch_size ............. 128
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   world_size ................... 4
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=26214400 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=23592960 param_persistence_threshold=51200 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-04 20:40:31,705] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3
[2023-09-04 20:40:31,705] [INFO] [config.py:950:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 2.621440e+07, 
        "stage3_prefetch_bucket_size": 2.359296e+07, 
        "stage3_param_persistence_threshold": 5.120000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
wandb: Currently logged in as: kidrain61. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.15.9
wandb: Run data is saved locally in /data/users/zhangjunlei/tyx/wandb/wandb/run-20230904_204033-zwdzl70h
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16-2023-09-04-0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kidrain61/step-reward
wandb: üöÄ View run at https://wandb.ai/kidrain61/step-reward/runs/zwdzl70h
wandb: ERROR Attempted to change value of key "model_name_or_path" from /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/db6b8eb1feabb38985fdf785a89895959e944936 to WizardLM/WizardMath-13B-V1.0
wandb: ERROR If you really want to do this, pass allow_val_change=True to config.update()
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/src/finetune.py", line 1388, in <module>
    train(config, args)
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/src/finetune.py", line 1076, in train
    accelerator.init_trackers(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 602, in _inner
    return PartialState().on_main_process(function)(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 2205, in init_trackers
    tracker.store_init_configuration(config)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/tracking.py", line 86, in execute_on_main_process
    return function(self, *args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/tracking.py", line 316, in store_init_configuration
    wandb.config.update(values)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_config.py", line 184, in update
    sanitized = self._update(d, allow_val_change)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_config.py", line 177, in _update
    sanitized = self._sanitize_dict(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_config.py", line 237, in _sanitize_dict
    k, v = self._sanitize(k, v, allow_val_change)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_config.py", line 258, in _sanitize
    raise config_util.ConfigError(
wandb.sdk.lib.config_util.ConfigError: Attempted to change value of key "model_name_or_path" from /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/db6b8eb1feabb38985fdf785a89895959e944936 to WizardLM/WizardMath-13B-V1.0
If you really want to do this, pass allow_val_change=True to config.update()
Traceback (most recent call last):
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/src/finetune.py", line 1388, in <module>
    train(config, args)
  File "/data/users/zhangjunlei/tyx/reward-by-prm800k/src/finetune.py", line 1076, in train
    accelerator.init_trackers(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 602, in _inner
    return PartialState().on_main_process(function)(*args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py", line 2205, in init_trackers
    tracker.store_init_configuration(config)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/tracking.py", line 86, in execute_on_main_process
    return function(self, *args, **kwargs)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/tracking.py", line 316, in store_init_configuration
    wandb.config.update(values)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_config.py", line 184, in update
    sanitized = self._update(d, allow_val_change)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_config.py", line 177, in _update
    sanitized = self._sanitize_dict(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_config.py", line 237, in _sanitize_dict
    k, v = self._sanitize(k, v, allow_val_change)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/wandb/sdk/wandb_config.py", line 258, in _sanitize
    raise config_util.ConfigError(
wandb.sdk.lib.config_util.ConfigError: Attempted to change value of key "model_name_or_path" from /data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/db6b8eb1feabb38985fdf785a89895959e944936 to WizardLM/WizardMath-13B-V1.0
If you really want to do this, pass allow_val_change=True to config.update()
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: | 0.000 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.000 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run summary:
wandb:           accuracy 0.79261
wandb:              f1_-1 0.11086
wandb:               f1_0 0.05255
wandb:               f1_1 0.88348
wandb:           f1_macro 0.34897
wandb:           f1_micro 0.79261
wandb:        f1_weighted 0.7223
wandb:      learning_rate 0.0
wandb:       precision_-1 0.42017
wandb:        precision_0 0.3913
wandb:        precision_1 0.8028
wandb:    precision_macro 0.53809
wandb:    precision_micro 0.79261
wandb: precision_weighted 0.72298
wandb:        pred_-1_num 238
wandb:         pred_0_num 46
wandb:         pred_1_num 10517
wandb:          recall_-1 0.06386
wandb:           recall_0 0.02817
wandb:           recall_1 0.9822
wandb:       recall_macro 0.35808
wandb:       recall_micro 0.79261
wandb:    recall_weighted 0.79261
wandb:         ref_-1_num 1566
wandb:          ref_0_num 639
wandb:          ref_1_num 8596
wandb:         roc_auc_-1 0.63122
wandb:          roc_auc_0 0.66141
wandb:          roc_auc_1 0.62919
wandb:      roc_auc_macro 0.64061
wandb:      roc_auc_micro 0.88934
wandb:   roc_auc_weighted 0.63139
wandb:         train_loss 0.66291
wandb: 
wandb: üöÄ View run wizardmath-13b-prm800k-train-direct-prediction-0-02validiation-bs=128-gas=16-2023-09-04-0 at: https://wandb.ai/kidrain61/step-reward/runs/zwdzl70h
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/wandb/run-20230904_204033-zwdzl70h/logs
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2875317 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2875318 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2875319 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Unable to shutdown process 2875317 via 15, forcefully exiting via 9
WARNING:torch.distributed.elastic.multiprocessing.api:Unable to shutdown process 2875318 via 15, forcefully exiting via 9
WARNING:torch.distributed.elastic.multiprocessing.api:Unable to shutdown process 2875319 via 15, forcefully exiting via 9
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2875316) of binary: /data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/python
Traceback (most recent call last):
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 45, in main
    args.func(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 964, in launch_command
    deepspeed_launcher(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py", line 687, in deepspeed_launcher
    distrib_run.run(args)
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/users/zhangjunlei/anaconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/data/users/zhangjunlei/tyx/reward-by-prm800k/src/finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-04_20:40:53
  host      : a100
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2875316)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
