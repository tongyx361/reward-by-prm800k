{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\"\n",
    "tokenizer_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-16 21:08:20 llm_engine.py:70] Initializing an LLM engine with config: model='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9', tokenizer='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.float16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n",
      "INFO 08-16 21:08:20 tokenizer.py:29] For some LLaMA-based models, initializing the fast tokenizer may take a long time. To eliminate the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n",
      "INFO 08-16 21:08:28 llm_engine.py:196] # GPU blocks: 7439, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=model_name_or_path,\n",
    "    tokenizer=tokenizer_name_or_path,\n",
    "    tokenizer_mode=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"auto\",\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vllm.entrypoints.llm.LLM object at 0x7fbdd8280a00>\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00, 18.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'Hello, my name is'\n",
      "Generated text: \"ez Hon Catalog distributedaret Pennsylvan coefficientcalcul Get Global commonly IE $('#cket roman truth\"\n",
      "\n",
      "\n",
      "Prompt: 'The president of the United States is'\n",
      "Generated text: 'dataframewaltPath allowingрийhttp jejмерикан vain Fir Imagetaisms Stat updateFAULT'\n",
      "\n",
      "\n",
      "Prompt: 'The capital of France is'\n",
      "Generated text: 'ienst scalproducts Kre $-zza XV albumant Patrick sem\\x08 VII Amer Head own'\n",
      "\n",
      "\n",
      "Prompt: 'The future of AI is'\n",
      "Generated text: 'ца Teil goals \\\\, secret beschitzuterWillinitial ottomorphчка structures coachprodu'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    top_k=3,\n",
    "    max_tokens=1,\n",
    "    logprobs=3,\n",
    ")\n",
    "\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"\"\"\n",
    "Prompt: {prompt!r}\n",
    "Generated text: {generated_text!r}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-instruct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
