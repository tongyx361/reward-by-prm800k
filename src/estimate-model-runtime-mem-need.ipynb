{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-21 11:52:57,827] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "import os\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(utils.data_root, \".cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\")\n",
    "# model_path = utils.default_13b_model_path\n",
    "# model_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-70B-V1.0/snapshots/e089c3f9d2ad9d1acb62425aec3f4126f498f4c5\"\n",
    "model_config_path = os.path.join(model_path, \"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data/users/zhangjunlei/tyx/reward-by-prm800k/src/estimate-model-runtime-mem-need.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/estimate-model-runtime-mem-need.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(model_config)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/estimate-model-runtime-mem-need.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m init_empty_weights():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/estimate-model-runtime-mem-need.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_config(model_config, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/estimate-model-runtime-mem-need.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m total_params \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mdict\u001b[39m((p\u001b[39m.\u001b[39mdata_ptr(), p\u001b[39m.\u001b[39mnumel()) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters())\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/estimate-model-runtime-mem-need.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtotal_params = \u001b[39m\u001b[39m{\u001b[39;00mtotal_params\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:430\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    429\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 430\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49m_from_config(config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    432\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    433\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/modeling_utils.py:1142\u001b[0m, in \u001b[0;36mPreTrainedModel._from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1142\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1144\u001b[0m \u001b[39m# restore default dtype if it was modified\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.__init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_config_path)\n",
    "print(model_config)\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(model_config)\n",
    "\n",
    "total_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n",
    "print(f\"total_params = {total_params}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "\n",
    "device_map = infer_auto_device_map(model)\n",
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "model LlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n",
      "model.embed_tokens Embedding(32000, 4096, padding_idx=0)\n",
      "model.layers ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "      (act_fn): SiLUActivation()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm()\n",
      "    (post_attention_layernorm): LlamaRMSNorm()\n",
      "  )\n",
      ")\n",
      "model.layers.0 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.0.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.0.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.0.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.0.mlp.act_fn SiLUActivation()\n",
      "model.layers.0.input_layernorm LlamaRMSNorm()\n",
      "model.layers.0.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.1 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.1.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.1.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.1.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.1.mlp.act_fn SiLUActivation()\n",
      "model.layers.1.input_layernorm LlamaRMSNorm()\n",
      "model.layers.1.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.2 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.2.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.2.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.2.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.2.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.2.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.2.mlp.act_fn SiLUActivation()\n",
      "model.layers.2.input_layernorm LlamaRMSNorm()\n",
      "model.layers.2.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.3 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.3.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.3.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.3.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.3.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.3.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.3.mlp.act_fn SiLUActivation()\n",
      "model.layers.3.input_layernorm LlamaRMSNorm()\n",
      "model.layers.3.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.4 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.4.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.4.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.4.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.4.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.4.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.4.mlp.act_fn SiLUActivation()\n",
      "model.layers.4.input_layernorm LlamaRMSNorm()\n",
      "model.layers.4.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.5 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.5.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.5.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.5.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.5.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.5.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.5.mlp.act_fn SiLUActivation()\n",
      "model.layers.5.input_layernorm LlamaRMSNorm()\n",
      "model.layers.5.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.6 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.6.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.6.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.6.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.6.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.6.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.6.mlp.act_fn SiLUActivation()\n",
      "model.layers.6.input_layernorm LlamaRMSNorm()\n",
      "model.layers.6.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.7 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.7.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.7.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.7.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.7.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.7.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.7.mlp.act_fn SiLUActivation()\n",
      "model.layers.7.input_layernorm LlamaRMSNorm()\n",
      "model.layers.7.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.8 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.8.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.8.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.8.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.8.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.8.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.8.mlp.act_fn SiLUActivation()\n",
      "model.layers.8.input_layernorm LlamaRMSNorm()\n",
      "model.layers.8.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.9 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.9.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.9.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.9.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.9.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.9.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.9.mlp.act_fn SiLUActivation()\n",
      "model.layers.9.input_layernorm LlamaRMSNorm()\n",
      "model.layers.9.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.10 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.10.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.10.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.10.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.10.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.10.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.10.mlp.act_fn SiLUActivation()\n",
      "model.layers.10.input_layernorm LlamaRMSNorm()\n",
      "model.layers.10.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.11 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.11.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.11.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.11.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.11.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.11.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.11.mlp.act_fn SiLUActivation()\n",
      "model.layers.11.input_layernorm LlamaRMSNorm()\n",
      "model.layers.11.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.12 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.12.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.12.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.12.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.12.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.12.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.12.mlp.act_fn SiLUActivation()\n",
      "model.layers.12.input_layernorm LlamaRMSNorm()\n",
      "model.layers.12.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.13 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.13.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.13.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.13.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.13.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.13.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.13.mlp.act_fn SiLUActivation()\n",
      "model.layers.13.input_layernorm LlamaRMSNorm()\n",
      "model.layers.13.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.14 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.14.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.14.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.14.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.14.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.14.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.14.mlp.act_fn SiLUActivation()\n",
      "model.layers.14.input_layernorm LlamaRMSNorm()\n",
      "model.layers.14.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.15 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.15.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.15.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.15.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.15.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.15.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.15.mlp.act_fn SiLUActivation()\n",
      "model.layers.15.input_layernorm LlamaRMSNorm()\n",
      "model.layers.15.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.16 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.16.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.16.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.16.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.16.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.16.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.16.mlp.act_fn SiLUActivation()\n",
      "model.layers.16.input_layernorm LlamaRMSNorm()\n",
      "model.layers.16.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.17 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.17.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.17.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.17.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.17.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.17.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.17.mlp.act_fn SiLUActivation()\n",
      "model.layers.17.input_layernorm LlamaRMSNorm()\n",
      "model.layers.17.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.18 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.18.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.18.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.18.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.18.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.18.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.18.mlp.act_fn SiLUActivation()\n",
      "model.layers.18.input_layernorm LlamaRMSNorm()\n",
      "model.layers.18.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.19 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.19.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.19.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.19.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.19.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.19.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.19.mlp.act_fn SiLUActivation()\n",
      "model.layers.19.input_layernorm LlamaRMSNorm()\n",
      "model.layers.19.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.20 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.20.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.20.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.20.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.20.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.20.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.20.mlp.act_fn SiLUActivation()\n",
      "model.layers.20.input_layernorm LlamaRMSNorm()\n",
      "model.layers.20.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.21 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.21.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.21.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.21.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.21.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.21.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.21.mlp.act_fn SiLUActivation()\n",
      "model.layers.21.input_layernorm LlamaRMSNorm()\n",
      "model.layers.21.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.22 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.22.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.22.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.22.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.22.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.22.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.22.mlp.act_fn SiLUActivation()\n",
      "model.layers.22.input_layernorm LlamaRMSNorm()\n",
      "model.layers.22.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.23 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.23.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.23.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.23.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.23.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.23.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.23.mlp.act_fn SiLUActivation()\n",
      "model.layers.23.input_layernorm LlamaRMSNorm()\n",
      "model.layers.23.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.24 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.24.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.24.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.24.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.24.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.24.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.24.mlp.act_fn SiLUActivation()\n",
      "model.layers.24.input_layernorm LlamaRMSNorm()\n",
      "model.layers.24.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.25 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.25.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.25.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.25.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.25.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.25.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.25.mlp.act_fn SiLUActivation()\n",
      "model.layers.25.input_layernorm LlamaRMSNorm()\n",
      "model.layers.25.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.26 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.26.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.26.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.26.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.26.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.26.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.26.mlp.act_fn SiLUActivation()\n",
      "model.layers.26.input_layernorm LlamaRMSNorm()\n",
      "model.layers.26.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.27 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.27.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.27.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.27.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.27.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.27.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.27.mlp.act_fn SiLUActivation()\n",
      "model.layers.27.input_layernorm LlamaRMSNorm()\n",
      "model.layers.27.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.28 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.28.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.28.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.28.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.28.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.28.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.28.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.28.mlp.act_fn SiLUActivation()\n",
      "model.layers.28.input_layernorm LlamaRMSNorm()\n",
      "model.layers.28.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.29 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.29.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.29.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.29.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.29.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.29.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.29.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.29.mlp.act_fn SiLUActivation()\n",
      "model.layers.29.input_layernorm LlamaRMSNorm()\n",
      "model.layers.29.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.30 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.30.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.30.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.30.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.30.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.30.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.30.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.30.mlp.act_fn SiLUActivation()\n",
      "model.layers.30.input_layernorm LlamaRMSNorm()\n",
      "model.layers.30.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.31 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.31.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.31.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.31.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.31.mlp.gate_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.31.mlp.up_proj Linear(in_features=4096, out_features=11008, bias=False)\n",
      "model.layers.31.mlp.down_proj Linear(in_features=11008, out_features=4096, bias=False)\n",
      "model.layers.31.mlp.act_fn SiLUActivation()\n",
      "model.layers.31.input_layernorm LlamaRMSNorm()\n",
      "model.layers.31.post_attention_layernorm LlamaRMSNorm()\n",
      "model.norm LlamaRMSNorm()\n",
      "lm_head Linear(in_features=4096, out_features=32000, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.named_modules():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config.num_attention_heads = 32\n",
      "model_config.num_key_value_heads = 32\n"
     ]
    }
   ],
   "source": [
    "h = hidden_size = model_config.hidden_size\n",
    "# intermediate_size = model_config.intermediate_size\n",
    "intermediate_size = 4 * h\n",
    "V = vocab_size = model_config.vocab_size\n",
    "l = num_hidden_layers = model_config.num_hidden_layers\n",
    "# assert (\n",
    "#     model_config.num_attention_heads == model_config.num_key_value_heads\n",
    "# ), f\"num_attention_heads ({model_config.num_attention_heads}) != num_key_value_heads ({model_config.num_key_value_heads})\"\n",
    "\n",
    "print(f\"model_config.num_attention_heads = {model_config.num_attention_heads}\")\n",
    "print(f\"model_config.num_key_value_heads = {model_config.num_key_value_heads}\")\n",
    "\n",
    "a = model_config.num_attention_heads\n",
    "\n",
    "num_gpus = 4\n",
    "\n",
    "b = batch_size_per_gpu = 2\n",
    "# s = max_seq_len = 4096\n",
    "s = max_seq_len = 1024\n",
    "\n",
    "# b = batch_size_per_gpu = 20\n",
    "# b = batch_size_per_gpu = 3\n",
    "# s = max_seq_len = 2048\n",
    "\n",
    "# num_gpus = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters per self-attention layer: 67125248\n",
      "Number of parameters per feed-forward network: 134238208\n",
      "Number of parameters per transformer layer: 201379840\n",
      "Number of parameters in the token embedding matrix: 131072000\n",
      "Number of parameters in the transformer model: 6575226880\n"
     ]
    }
   ],
   "source": [
    "num_params_per_self_attention = 4 * (h**2 + h)\n",
    "num_params_per_ffn = h * intermediate_size * 2 + intermediate_size + h\n",
    "num_params_per_layer_norm = 2 * h\n",
    "num_params_per_transformer_layer = (\n",
    "    num_params_per_self_attention + num_params_per_ffn + 2 * num_params_per_layer_norm\n",
    ")\n",
    "\n",
    "num_params_token_embedding_matrix = vocab_size * hidden_size\n",
    "\n",
    "num_params_transformer_model = (\n",
    "    num_params_per_transformer_layer * num_hidden_layers\n",
    "    + num_params_token_embedding_matrix\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Number of parameters per self-attention layer:\", num_params_per_self_attention)\n",
    "print(\"Number of parameters per feed-forward network:\", num_params_per_ffn)\n",
    "print(\"Number of parameters per transformer layer:\", num_params_per_transformer_layer)\n",
    "print(\n",
    "    \"Number of parameters in the token embedding matrix:\",\n",
    "    num_params_token_embedding_matrix,\n",
    ")\n",
    "print(\"Number of parameters in the transformer model:\", num_params_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16777216\n",
      "201326592\n",
      "6442450944\n"
     ]
    }
   ],
   "source": [
    "print(4096**2)\n",
    "print(12 * 4096**2)\n",
    "print(12 * 32 * 4096**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_mem_train_state = 122.47 GB\n"
     ]
    }
   ],
   "source": [
    "# mixed precision\n",
    "mem_params_mixed_precision = num_params_transformer_model * (2 + 4)\n",
    "mem_grads_mixed_precision = num_params_transformer_model * (2 + 4)\n",
    "mem_adam_states = num_params_transformer_model * (4 + 4)\n",
    "gb = 1024**3\n",
    "gb_mem_train_state = (\n",
    "    sum([mem_params_mixed_precision, mem_grads_mixed_precision, mem_adam_states]) / gb\n",
    ")\n",
    "print(f\"gb_mem_train_state = {gb_mem_train_state:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_zero3_train_per_gpu_gb = 30.62 GB\n"
     ]
    }
   ],
   "source": [
    "mem_zero3_train_per_gpu = (\n",
    "    sum([mem_params_mixed_precision, mem_grads_mixed_precision, mem_adam_states])\n",
    "    / num_gpus\n",
    ")\n",
    "\n",
    "mem_zero3_train_per_gpu_gb = mem_zero3_train_per_gpu / gb\n",
    "print(f\"mem_zero3_train_per_gpu_gb = {mem_zero3_train_per_gpu_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_mem_activation_transformer_model_per_gpu = 18.50 GB\n"
     ]
    }
   ],
   "source": [
    "mem_activation_per_transformer_layer_per_gpu = b * (34 * s * h + 5 * s**2 * a)\n",
    "mem_activation_transformer_model_per_gpu = (\n",
    "    num_hidden_layers * mem_activation_per_transformer_layer_per_gpu\n",
    ")\n",
    "gb_mem_activation_transformer_model_per_gpu = (\n",
    "    mem_activation_transformer_model_per_gpu / gb\n",
    ")\n",
    "print(\n",
    "    f\"gb_mem_activation_transformer_model_per_gpu = {gb_mem_activation_transformer_model_per_gpu:.2f} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_men_kv_cache = 1.00 GB\n"
     ]
    }
   ],
   "source": [
    "mem_kv_cache_per_token = b * h * l * 2 * 2\n",
    "mem_kv_cache = mem_kv_cache_per_token * s\n",
    "gb_men_kv_cache = mem_kv_cache / gb\n",
    "print(f\"gb_men_kv_cache = {gb_men_kv_cache:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
