{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 09:51:34,443] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "import os\n",
    "from accelerate import infer_auto_device_map, init_empty_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"/data/tongyx361/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\"\n",
    "model_config_path = os.path.join(model_name_or_path, \"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/tongyx361/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "total_params = 131072000\n",
      "model.embed_tokens.weight torch.Size([32000, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([32000, 4096])\n",
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 0, 'model.layers.25': 0, 'model.layers.26': 0, 'model.layers.27': 0, 'model.layers.28': 0, 'model.layers.29.self_attn.q_proj': 0, 'model.layers.29.self_attn.k_proj': 1, 'model.layers.29.self_attn.v_proj': 1, 'model.layers.29.self_attn.o_proj': 1, 'model.layers.29.self_attn.rotary_emb': 1, 'model.layers.29.mlp': 1, 'model.layers.29.input_layernorm': 1, 'model.layers.29.post_attention_layernorm': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_config_path)\n",
    "print(model_config)\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(model_config)\n",
    "\n",
    "total_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n",
    "print(f\"total_params = {total_params}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "\n",
    "device_map = infer_auto_device_map(model)\n",
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hidden_size = model_config.hidden_size\n",
    "# intermediate_size = model_config.intermediate_size\n",
    "intermediate_size = 4 * h\n",
    "V = vocab_size = model_config.vocab_size\n",
    "l = num_hidden_layers = model_config.num_hidden_layers\n",
    "assert model_config.num_attention_heads == model_config.num_key_value_heads\n",
    "a = model_config.num_attention_heads\n",
    "\n",
    "num_gpus = 4\n",
    "\n",
    "b = batch_size_per_gpu = 2\n",
    "# s = max_seq_len = 4096\n",
    "s = max_seq_len = 1024\n",
    "\n",
    "# b = batch_size_per_gpu = 20\n",
    "# b = batch_size_per_gpu = 3\n",
    "# s = max_seq_len = 2048\n",
    "\n",
    "# num_gpus = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters per self-attention layer: 67125248\n",
      "Number of parameters per feed-forward network: 134238208\n",
      "Number of parameters per transformer layer: 201379840\n",
      "Number of parameters in the token embedding matrix: 131072000\n",
      "Number of parameters in the transformer model: 6575226880\n"
     ]
    }
   ],
   "source": [
    "num_params_per_self_attention = 4 * (h**2 + h)\n",
    "num_params_per_ffn = h * intermediate_size * 2 + intermediate_size + h\n",
    "num_params_per_layer_norm = 2 * h\n",
    "num_params_per_transformer_layer = (\n",
    "    num_params_per_self_attention + num_params_per_ffn + 2 * num_params_per_layer_norm\n",
    ")\n",
    "\n",
    "num_params_token_embedding_matrix = vocab_size * hidden_size\n",
    "\n",
    "num_params_transformer_model = (\n",
    "    num_params_per_transformer_layer * num_hidden_layers\n",
    "    + num_params_token_embedding_matrix\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Number of parameters per self-attention layer:\", num_params_per_self_attention)\n",
    "print(\"Number of parameters per feed-forward network:\", num_params_per_ffn)\n",
    "print(\"Number of parameters per transformer layer:\", num_params_per_transformer_layer)\n",
    "print(\n",
    "    \"Number of parameters in the token embedding matrix:\",\n",
    "    num_params_token_embedding_matrix,\n",
    ")\n",
    "print(\"Number of parameters in the transformer model:\", num_params_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16777216\n",
      "201326592\n",
      "6442450944\n"
     ]
    }
   ],
   "source": [
    "print(4096**2)\n",
    "print(12 * 4096**2)\n",
    "print(12 * 32 * 4096**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122.47314453125\n"
     ]
    }
   ],
   "source": [
    "# mixed precision\n",
    "mem_params_mixed_precision = num_params_transformer_model * (2 + 4)\n",
    "mem_grads_mixed_precision = num_params_transformer_model * (2 + 4)\n",
    "mem_adam_states = num_params_transformer_model * (4 + 4)\n",
    "print(\n",
    "    sum([mem_params_mixed_precision, mem_grads_mixed_precision, mem_adam_states])\n",
    "    / (1024**3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_zero3_train_per_gpu_gb = 30.62 GB\n"
     ]
    }
   ],
   "source": [
    "mem_zero3_train_per_gpu = (\n",
    "    sum([mem_params_mixed_precision, mem_grads_mixed_precision, mem_adam_states])\n",
    "    / num_gpus\n",
    ")\n",
    "\n",
    "mem_zero3_train_per_gpu_gb = mem_zero3_train_per_gpu / (1024**3)\n",
    "print(f\"mem_zero3_train_per_gpu_gb = {mem_zero3_train_per_gpu_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_mem_activation_transformer_model_per_gpu = 18.50 GB\n"
     ]
    }
   ],
   "source": [
    "mem_activation_per_transformer_layer_per_gpu = 34 * b * s * h + 5 * b * s**2 * a\n",
    "mem_activation_transformer_model_per_gpu = (\n",
    "    num_hidden_layers * mem_activation_per_transformer_layer_per_gpu\n",
    ")\n",
    "gb_mem_activation_transformer_model_per_gpu = (\n",
    "    mem_activation_transformer_model_per_gpu / (1024**3)\n",
    ")\n",
    "print(\n",
    "    f\"gb_mem_activation_transformer_model_per_gpu = {gb_mem_activation_transformer_model_per_gpu:.2f} GB\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
