{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-05 22:21:26,970] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "import os\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "import utils\n",
    "\n",
    "utils.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = \"/data/tongyx361/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\"\n",
    "# model_path = utils.default_13b_model_path\n",
    "model_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-70B-V1.0/snapshots/e089c3f9d2ad9d1acb62425aec3f4126f498f4c5\"\n",
    "model_config_path = os.path.join(model_path, \"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-70B-V1.0/snapshots/e089c3f9d2ad9d1acb62425aec3f4126f498f4c5/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params = 262160384\n",
      "model.embed_tokens.weight torch.Size([32002, 8192])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.0.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.1.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.2.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.3.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.4.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.5.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.6.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.7.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.8.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.9.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.10.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.11.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.12.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.13.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.14.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.15.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.16.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.17.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.18.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.19.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.20.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.21.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.22.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.23.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.24.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.25.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.26.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.27.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.28.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.29.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.30.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.31.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.32.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.32.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.32.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.32.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.32.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.32.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.32.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.32.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.32.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.33.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.33.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.33.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.33.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.33.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.33.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.33.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.33.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.33.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.34.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.34.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.34.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.34.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.34.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.34.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.34.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.34.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.34.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.35.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.35.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.35.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.35.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.35.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.35.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.35.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.35.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.35.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.36.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.36.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.36.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.36.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.36.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.36.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.36.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.36.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.36.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.37.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.37.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.37.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.37.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.37.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.37.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.37.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.37.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.37.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.38.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.38.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.38.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.38.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.38.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.38.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.38.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.38.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.38.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.39.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.39.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.39.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.39.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.39.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.39.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.39.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.39.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.39.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.40.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.40.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.40.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.40.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.40.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.40.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.40.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.40.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.40.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.41.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.41.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.41.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.41.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.41.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.41.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.41.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.41.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.41.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.42.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.42.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.42.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.42.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.42.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.42.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.42.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.42.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.42.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.43.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.43.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.43.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.43.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.43.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.43.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.43.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.43.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.43.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.44.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.44.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.44.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.44.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.44.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.44.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.44.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.44.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.44.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.45.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.45.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.45.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.45.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.45.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.45.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.45.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.45.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.45.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.46.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.46.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.46.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.46.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.46.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.46.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.46.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.46.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.46.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.47.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.47.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.47.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.47.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.47.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.47.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.47.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.47.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.47.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.48.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.48.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.48.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.48.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.48.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.48.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.48.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.48.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.48.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.49.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.49.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.49.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.49.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.49.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.49.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.49.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.49.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.49.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.50.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.50.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.50.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.50.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.50.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.50.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.50.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.50.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.50.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.51.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.51.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.51.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.51.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.51.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.51.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.51.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.51.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.51.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.52.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.52.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.52.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.52.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.52.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.52.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.52.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.52.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.52.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.53.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.53.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.53.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.53.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.53.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.53.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.53.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.53.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.53.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.54.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.54.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.54.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.54.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.54.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.54.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.54.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.54.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.54.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.55.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.55.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.55.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.55.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.55.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.55.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.55.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.55.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.55.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.56.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.56.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.56.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.56.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.56.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.56.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.56.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.56.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.56.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.57.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.57.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.57.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.57.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.57.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.57.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.57.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.57.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.57.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.58.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.58.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.58.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.58.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.58.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.58.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.58.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.58.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.58.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.59.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.59.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.59.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.59.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.59.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.59.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.59.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.59.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.59.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.60.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.60.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.60.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.60.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.60.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.60.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.60.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.60.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.60.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.61.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.61.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.61.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.61.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.61.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.61.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.61.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.61.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.61.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.62.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.62.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.62.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.62.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.62.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.62.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.62.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.62.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.62.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.63.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.63.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.63.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.63.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.63.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.63.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.63.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.63.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.63.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.64.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.64.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.64.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.64.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.64.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.64.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.64.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.64.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.64.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.65.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.65.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.65.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.65.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.65.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.65.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.65.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.65.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.65.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.66.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.66.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.66.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.66.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.66.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.66.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.66.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.66.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.66.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.67.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.67.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.67.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.67.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.67.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.67.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.67.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.67.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.67.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.68.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.68.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.68.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.68.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.68.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.68.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.68.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.68.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.68.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.69.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.69.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.69.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.69.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.69.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.69.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.69.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.69.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.69.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.70.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.70.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.70.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.70.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.70.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.70.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.70.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.70.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.70.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.71.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.71.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.71.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.71.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.71.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.71.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.71.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.71.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.71.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.72.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.72.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.72.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.72.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.72.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.72.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.72.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.72.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.72.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.73.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.73.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.73.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.73.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.73.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.73.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.73.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.73.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.73.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.74.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.74.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.74.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.74.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.74.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.74.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.74.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.74.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.74.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.75.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.75.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.75.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.75.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.75.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.75.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.75.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.75.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.75.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.76.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.76.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.76.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.76.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.76.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.76.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.76.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.76.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.76.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.77.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.77.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.77.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.77.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.77.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.77.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.77.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.77.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.77.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.78.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.78.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.78.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.78.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.78.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.78.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.78.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.78.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.78.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.79.self_attn.q_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.79.self_attn.k_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.79.self_attn.v_proj.weight torch.Size([1024, 8192])\n",
      "model.layers.79.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.79.mlp.gate_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.79.mlp.up_proj.weight torch.Size([28672, 8192])\n",
      "model.layers.79.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.79.input_layernorm.weight torch.Size([8192])\n",
      "model.layers.79.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.norm.weight torch.Size([8192])\n",
      "lm_head.weight torch.Size([32002, 8192])\n",
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5.self_attn': 0, 'model.layers.5.mlp.gate_proj': 0, 'model.layers.5.mlp.up_proj': 0, 'model.layers.5.mlp.down_proj': 1, 'model.layers.5.mlp.act_fn': 1, 'model.layers.5.input_layernorm': 1, 'model.layers.5.post_attention_layernorm': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30.self_attn': 1, 'model.layers.30.input_layernorm': 2, 'model.layers.30.post_attention_layernorm': 2, 'model.layers.31': 2, 'model.layers.32': 2, 'model.layers.33': 2, 'model.layers.34': 2, 'model.layers.35': 2, 'model.layers.36': 2, 'model.layers.37': 2, 'model.layers.38': 2, 'model.layers.39': 2, 'model.layers.40': 2, 'model.layers.41.self_attn': 2, 'model.layers.41.mlp.gate_proj': 2, 'model.layers.41.mlp.up_proj': 2, 'model.layers.41.mlp.down_proj': 3, 'model.layers.41.mlp.act_fn': 3, 'model.layers.41.input_layernorm': 3, 'model.layers.41.post_attention_layernorm': 3, 'model.layers.42': 3, 'model.layers.43': 3, 'model.layers.44': 3, 'model.layers.45': 3, 'model.layers.46': 3, 'model.layers.47': 3, 'model.layers.48': 3, 'model.layers.49': 3, 'model.layers.50': 3, 'model.layers.51': 3, 'model.layers.52': 3, 'model.layers.53': 3, 'model.layers.54': 3, 'model.layers.55': 3, 'model.layers.56': 3, 'model.layers.57': 3, 'model.layers.58': 3, 'model.layers.59': 3, 'model.layers.60': 3, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65.self_attn': 3, 'model.layers.65.input_layernorm': 4, 'model.layers.65.post_attention_layernorm': 4, 'model.layers.66': 4, 'model.layers.67': 4, 'model.layers.68': 4, 'model.layers.69': 4, 'model.layers.70': 4, 'model.layers.71': 4, 'model.layers.72': 4, 'model.layers.73': 4, 'model.layers.74': 4, 'model.layers.75': 4, 'model.layers.76': 4, 'model.layers.77': 4, 'model.layers.78': 4, 'model.layers.79': 4, 'model.norm': 4, 'lm_head': 4, 'model.layers.30.mlp': 2, 'model.layers.65.mlp': 4}\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_config_path)\n",
    "print(model_config)\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(model_config)\n",
    "\n",
    "total_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n",
    "print(f\"total_params = {total_params}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "\n",
    "device_map = infer_auto_device_map(model)\n",
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32002, 8192, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-79): 80 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "          (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "          (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "          (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=32002, bias=False)\n",
      ")\n",
      "model LlamaModel(\n",
      "  (embed_tokens): Embedding(32002, 8192, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-79): 80 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "        (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "        (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "        (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n",
      "model.embed_tokens Embedding(32002, 8192, padding_idx=0)\n",
      "model.layers ModuleList(\n",
      "  (0-79): 80 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "      (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "      (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "      (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "      (act_fn): SiLUActivation()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm()\n",
      "    (post_attention_layernorm): LlamaRMSNorm()\n",
      "  )\n",
      ")\n",
      "model.layers.0 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.0.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.0.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.0.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.0.mlp.act_fn SiLUActivation()\n",
      "model.layers.0.input_layernorm LlamaRMSNorm()\n",
      "model.layers.0.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.1 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.1.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.1.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.1.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.1.mlp.act_fn SiLUActivation()\n",
      "model.layers.1.input_layernorm LlamaRMSNorm()\n",
      "model.layers.1.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.2 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.2.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.2.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.2.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.2.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.2.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.2.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.2.mlp.act_fn SiLUActivation()\n",
      "model.layers.2.input_layernorm LlamaRMSNorm()\n",
      "model.layers.2.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.3 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.3.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.3.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.3.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.3.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.3.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.3.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.3.mlp.act_fn SiLUActivation()\n",
      "model.layers.3.input_layernorm LlamaRMSNorm()\n",
      "model.layers.3.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.4 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.4.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.4.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.4.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.4.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.4.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.4.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.4.mlp.act_fn SiLUActivation()\n",
      "model.layers.4.input_layernorm LlamaRMSNorm()\n",
      "model.layers.4.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.5 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.5.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.5.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.5.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.5.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.5.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.5.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.5.mlp.act_fn SiLUActivation()\n",
      "model.layers.5.input_layernorm LlamaRMSNorm()\n",
      "model.layers.5.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.6 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.6.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.6.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.6.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.6.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.6.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.6.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.6.mlp.act_fn SiLUActivation()\n",
      "model.layers.6.input_layernorm LlamaRMSNorm()\n",
      "model.layers.6.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.7 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.7.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.7.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.7.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.7.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.7.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.7.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.7.mlp.act_fn SiLUActivation()\n",
      "model.layers.7.input_layernorm LlamaRMSNorm()\n",
      "model.layers.7.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.8 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.8.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.8.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.8.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.8.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.8.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.8.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.8.mlp.act_fn SiLUActivation()\n",
      "model.layers.8.input_layernorm LlamaRMSNorm()\n",
      "model.layers.8.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.9 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.9.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.9.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.9.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.9.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.9.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.9.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.9.mlp.act_fn SiLUActivation()\n",
      "model.layers.9.input_layernorm LlamaRMSNorm()\n",
      "model.layers.9.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.10 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.10.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.10.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.10.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.10.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.10.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.10.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.10.mlp.act_fn SiLUActivation()\n",
      "model.layers.10.input_layernorm LlamaRMSNorm()\n",
      "model.layers.10.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.11 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.11.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.11.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.11.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.11.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.11.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.11.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.11.mlp.act_fn SiLUActivation()\n",
      "model.layers.11.input_layernorm LlamaRMSNorm()\n",
      "model.layers.11.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.12 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.12.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.12.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.12.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.12.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.12.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.12.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.12.mlp.act_fn SiLUActivation()\n",
      "model.layers.12.input_layernorm LlamaRMSNorm()\n",
      "model.layers.12.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.13 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.13.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.13.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.13.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.13.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.13.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.13.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.13.mlp.act_fn SiLUActivation()\n",
      "model.layers.13.input_layernorm LlamaRMSNorm()\n",
      "model.layers.13.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.14 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.14.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.14.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.14.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.14.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.14.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.14.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.14.mlp.act_fn SiLUActivation()\n",
      "model.layers.14.input_layernorm LlamaRMSNorm()\n",
      "model.layers.14.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.15 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.15.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.15.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.15.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.15.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.15.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.15.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.15.mlp.act_fn SiLUActivation()\n",
      "model.layers.15.input_layernorm LlamaRMSNorm()\n",
      "model.layers.15.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.16 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.16.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.16.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.16.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.16.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.16.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.16.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.16.mlp.act_fn SiLUActivation()\n",
      "model.layers.16.input_layernorm LlamaRMSNorm()\n",
      "model.layers.16.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.17 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.17.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.17.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.17.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.17.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.17.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.17.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.17.mlp.act_fn SiLUActivation()\n",
      "model.layers.17.input_layernorm LlamaRMSNorm()\n",
      "model.layers.17.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.18 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.18.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.18.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.18.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.18.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.18.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.18.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.18.mlp.act_fn SiLUActivation()\n",
      "model.layers.18.input_layernorm LlamaRMSNorm()\n",
      "model.layers.18.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.19 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.19.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.19.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.19.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.19.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.19.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.19.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.19.mlp.act_fn SiLUActivation()\n",
      "model.layers.19.input_layernorm LlamaRMSNorm()\n",
      "model.layers.19.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.20 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.20.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.20.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.20.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.20.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.20.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.20.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.20.mlp.act_fn SiLUActivation()\n",
      "model.layers.20.input_layernorm LlamaRMSNorm()\n",
      "model.layers.20.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.21 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.21.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.21.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.21.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.21.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.21.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.21.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.21.mlp.act_fn SiLUActivation()\n",
      "model.layers.21.input_layernorm LlamaRMSNorm()\n",
      "model.layers.21.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.22 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.22.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.22.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.22.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.22.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.22.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.22.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.22.mlp.act_fn SiLUActivation()\n",
      "model.layers.22.input_layernorm LlamaRMSNorm()\n",
      "model.layers.22.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.23 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.23.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.23.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.23.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.23.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.23.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.23.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.23.mlp.act_fn SiLUActivation()\n",
      "model.layers.23.input_layernorm LlamaRMSNorm()\n",
      "model.layers.23.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.24 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.24.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.24.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.24.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.24.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.24.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.24.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.24.mlp.act_fn SiLUActivation()\n",
      "model.layers.24.input_layernorm LlamaRMSNorm()\n",
      "model.layers.24.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.25 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.25.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.25.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.25.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.25.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.25.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.25.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.25.mlp.act_fn SiLUActivation()\n",
      "model.layers.25.input_layernorm LlamaRMSNorm()\n",
      "model.layers.25.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.26 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.26.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.26.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.26.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.26.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.26.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.26.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.26.mlp.act_fn SiLUActivation()\n",
      "model.layers.26.input_layernorm LlamaRMSNorm()\n",
      "model.layers.26.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.27 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.27.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.27.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.27.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.27.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.27.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.27.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.27.mlp.act_fn SiLUActivation()\n",
      "model.layers.27.input_layernorm LlamaRMSNorm()\n",
      "model.layers.27.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.28 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.28.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.28.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.28.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.28.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.28.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.28.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.28.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.28.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.28.mlp.act_fn SiLUActivation()\n",
      "model.layers.28.input_layernorm LlamaRMSNorm()\n",
      "model.layers.28.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.29 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.29.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.29.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.29.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.29.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.29.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.29.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.29.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.29.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.29.mlp.act_fn SiLUActivation()\n",
      "model.layers.29.input_layernorm LlamaRMSNorm()\n",
      "model.layers.29.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.30 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.30.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.30.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.30.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.30.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.30.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.30.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.30.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.30.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.30.mlp.act_fn SiLUActivation()\n",
      "model.layers.30.input_layernorm LlamaRMSNorm()\n",
      "model.layers.30.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.31 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.31.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.31.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.31.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.31.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.31.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.31.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.31.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.31.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.31.mlp.act_fn SiLUActivation()\n",
      "model.layers.31.input_layernorm LlamaRMSNorm()\n",
      "model.layers.31.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.32 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.32.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.32.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.32.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.32.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.32.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.32.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.32.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.32.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.32.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.32.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.32.mlp.act_fn SiLUActivation()\n",
      "model.layers.32.input_layernorm LlamaRMSNorm()\n",
      "model.layers.32.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.33 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.33.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.33.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.33.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.33.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.33.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.33.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.33.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.33.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.33.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.33.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.33.mlp.act_fn SiLUActivation()\n",
      "model.layers.33.input_layernorm LlamaRMSNorm()\n",
      "model.layers.33.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.34 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.34.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.34.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.34.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.34.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.34.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.34.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.34.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.34.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.34.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.34.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.34.mlp.act_fn SiLUActivation()\n",
      "model.layers.34.input_layernorm LlamaRMSNorm()\n",
      "model.layers.34.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.35 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.35.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.35.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.35.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.35.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.35.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.35.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.35.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.35.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.35.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.35.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.35.mlp.act_fn SiLUActivation()\n",
      "model.layers.35.input_layernorm LlamaRMSNorm()\n",
      "model.layers.35.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.36 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.36.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.36.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.36.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.36.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.36.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.36.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.36.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.36.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.36.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.36.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.36.mlp.act_fn SiLUActivation()\n",
      "model.layers.36.input_layernorm LlamaRMSNorm()\n",
      "model.layers.36.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.37 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.37.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.37.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.37.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.37.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.37.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.37.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.37.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.37.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.37.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.37.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.37.mlp.act_fn SiLUActivation()\n",
      "model.layers.37.input_layernorm LlamaRMSNorm()\n",
      "model.layers.37.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.38 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.38.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.38.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.38.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.38.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.38.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.38.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.38.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.38.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.38.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.38.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.38.mlp.act_fn SiLUActivation()\n",
      "model.layers.38.input_layernorm LlamaRMSNorm()\n",
      "model.layers.38.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.39 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.39.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.39.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.39.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.39.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.39.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.39.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.39.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.39.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.39.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.39.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.39.mlp.act_fn SiLUActivation()\n",
      "model.layers.39.input_layernorm LlamaRMSNorm()\n",
      "model.layers.39.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.40 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.40.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.40.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.40.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.40.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.40.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.40.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.40.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.40.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.40.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.40.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.40.mlp.act_fn SiLUActivation()\n",
      "model.layers.40.input_layernorm LlamaRMSNorm()\n",
      "model.layers.40.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.41 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.41.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.41.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.41.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.41.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.41.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.41.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.41.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.41.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.41.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.41.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.41.mlp.act_fn SiLUActivation()\n",
      "model.layers.41.input_layernorm LlamaRMSNorm()\n",
      "model.layers.41.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.42 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.42.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.42.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.42.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.42.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.42.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.42.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.42.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.42.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.42.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.42.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.42.mlp.act_fn SiLUActivation()\n",
      "model.layers.42.input_layernorm LlamaRMSNorm()\n",
      "model.layers.42.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.43 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.43.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.43.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.43.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.43.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.43.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.43.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.43.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.43.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.43.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.43.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.43.mlp.act_fn SiLUActivation()\n",
      "model.layers.43.input_layernorm LlamaRMSNorm()\n",
      "model.layers.43.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.44 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.44.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.44.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.44.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.44.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.44.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.44.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.44.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.44.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.44.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.44.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.44.mlp.act_fn SiLUActivation()\n",
      "model.layers.44.input_layernorm LlamaRMSNorm()\n",
      "model.layers.44.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.45 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.45.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.45.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.45.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.45.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.45.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.45.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.45.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.45.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.45.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.45.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.45.mlp.act_fn SiLUActivation()\n",
      "model.layers.45.input_layernorm LlamaRMSNorm()\n",
      "model.layers.45.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.46 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.46.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.46.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.46.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.46.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.46.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.46.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.46.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.46.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.46.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.46.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.46.mlp.act_fn SiLUActivation()\n",
      "model.layers.46.input_layernorm LlamaRMSNorm()\n",
      "model.layers.46.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.47 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.47.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.47.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.47.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.47.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.47.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.47.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.47.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.47.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.47.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.47.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.47.mlp.act_fn SiLUActivation()\n",
      "model.layers.47.input_layernorm LlamaRMSNorm()\n",
      "model.layers.47.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.48 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.48.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.48.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.48.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.48.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.48.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.48.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.48.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.48.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.48.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.48.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.48.mlp.act_fn SiLUActivation()\n",
      "model.layers.48.input_layernorm LlamaRMSNorm()\n",
      "model.layers.48.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.49 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.49.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.49.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.49.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.49.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.49.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.49.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.49.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.49.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.49.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.49.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.49.mlp.act_fn SiLUActivation()\n",
      "model.layers.49.input_layernorm LlamaRMSNorm()\n",
      "model.layers.49.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.50 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.50.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.50.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.50.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.50.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.50.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.50.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.50.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.50.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.50.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.50.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.50.mlp.act_fn SiLUActivation()\n",
      "model.layers.50.input_layernorm LlamaRMSNorm()\n",
      "model.layers.50.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.51 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.51.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.51.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.51.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.51.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.51.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.51.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.51.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.51.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.51.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.51.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.51.mlp.act_fn SiLUActivation()\n",
      "model.layers.51.input_layernorm LlamaRMSNorm()\n",
      "model.layers.51.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.52 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.52.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.52.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.52.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.52.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.52.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.52.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.52.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.52.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.52.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.52.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.52.mlp.act_fn SiLUActivation()\n",
      "model.layers.52.input_layernorm LlamaRMSNorm()\n",
      "model.layers.52.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.53 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.53.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.53.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.53.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.53.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.53.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.53.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.53.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.53.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.53.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.53.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.53.mlp.act_fn SiLUActivation()\n",
      "model.layers.53.input_layernorm LlamaRMSNorm()\n",
      "model.layers.53.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.54 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.54.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.54.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.54.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.54.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.54.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.54.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.54.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.54.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.54.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.54.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.54.mlp.act_fn SiLUActivation()\n",
      "model.layers.54.input_layernorm LlamaRMSNorm()\n",
      "model.layers.54.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.55 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.55.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.55.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.55.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.55.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.55.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.55.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.55.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.55.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.55.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.55.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.55.mlp.act_fn SiLUActivation()\n",
      "model.layers.55.input_layernorm LlamaRMSNorm()\n",
      "model.layers.55.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.56 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.56.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.56.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.56.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.56.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.56.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.56.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.56.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.56.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.56.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.56.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.56.mlp.act_fn SiLUActivation()\n",
      "model.layers.56.input_layernorm LlamaRMSNorm()\n",
      "model.layers.56.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.57 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.57.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.57.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.57.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.57.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.57.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.57.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.57.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.57.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.57.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.57.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.57.mlp.act_fn SiLUActivation()\n",
      "model.layers.57.input_layernorm LlamaRMSNorm()\n",
      "model.layers.57.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.58 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.58.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.58.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.58.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.58.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.58.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.58.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.58.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.58.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.58.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.58.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.58.mlp.act_fn SiLUActivation()\n",
      "model.layers.58.input_layernorm LlamaRMSNorm()\n",
      "model.layers.58.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.59 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.59.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.59.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.59.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.59.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.59.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.59.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.59.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.59.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.59.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.59.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.59.mlp.act_fn SiLUActivation()\n",
      "model.layers.59.input_layernorm LlamaRMSNorm()\n",
      "model.layers.59.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.60 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.60.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.60.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.60.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.60.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.60.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.60.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.60.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.60.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.60.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.60.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.60.mlp.act_fn SiLUActivation()\n",
      "model.layers.60.input_layernorm LlamaRMSNorm()\n",
      "model.layers.60.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.61 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.61.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.61.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.61.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.61.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.61.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.61.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.61.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.61.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.61.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.61.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.61.mlp.act_fn SiLUActivation()\n",
      "model.layers.61.input_layernorm LlamaRMSNorm()\n",
      "model.layers.61.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.62 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.62.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.62.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.62.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.62.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.62.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.62.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.62.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.62.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.62.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.62.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.62.mlp.act_fn SiLUActivation()\n",
      "model.layers.62.input_layernorm LlamaRMSNorm()\n",
      "model.layers.62.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.63 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.63.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.63.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.63.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.63.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.63.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.63.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.63.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.63.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.63.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.63.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.63.mlp.act_fn SiLUActivation()\n",
      "model.layers.63.input_layernorm LlamaRMSNorm()\n",
      "model.layers.63.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.64 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.64.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.64.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.64.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.64.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.64.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.64.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.64.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.64.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.64.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.64.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.64.mlp.act_fn SiLUActivation()\n",
      "model.layers.64.input_layernorm LlamaRMSNorm()\n",
      "model.layers.64.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.65 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.65.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.65.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.65.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.65.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.65.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.65.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.65.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.65.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.65.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.65.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.65.mlp.act_fn SiLUActivation()\n",
      "model.layers.65.input_layernorm LlamaRMSNorm()\n",
      "model.layers.65.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.66 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.66.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.66.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.66.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.66.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.66.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.66.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.66.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.66.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.66.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.66.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.66.mlp.act_fn SiLUActivation()\n",
      "model.layers.66.input_layernorm LlamaRMSNorm()\n",
      "model.layers.66.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.67 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.67.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.67.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.67.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.67.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.67.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.67.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.67.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.67.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.67.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.67.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.67.mlp.act_fn SiLUActivation()\n",
      "model.layers.67.input_layernorm LlamaRMSNorm()\n",
      "model.layers.67.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.68 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.68.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.68.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.68.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.68.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.68.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.68.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.68.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.68.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.68.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.68.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.68.mlp.act_fn SiLUActivation()\n",
      "model.layers.68.input_layernorm LlamaRMSNorm()\n",
      "model.layers.68.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.69 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.69.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.69.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.69.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.69.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.69.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.69.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.69.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.69.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.69.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.69.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.69.mlp.act_fn SiLUActivation()\n",
      "model.layers.69.input_layernorm LlamaRMSNorm()\n",
      "model.layers.69.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.70 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.70.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.70.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.70.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.70.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.70.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.70.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.70.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.70.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.70.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.70.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.70.mlp.act_fn SiLUActivation()\n",
      "model.layers.70.input_layernorm LlamaRMSNorm()\n",
      "model.layers.70.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.71 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.71.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.71.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.71.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.71.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.71.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.71.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.71.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.71.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.71.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.71.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.71.mlp.act_fn SiLUActivation()\n",
      "model.layers.71.input_layernorm LlamaRMSNorm()\n",
      "model.layers.71.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.72 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.72.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.72.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.72.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.72.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.72.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.72.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.72.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.72.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.72.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.72.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.72.mlp.act_fn SiLUActivation()\n",
      "model.layers.72.input_layernorm LlamaRMSNorm()\n",
      "model.layers.72.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.73 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.73.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.73.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.73.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.73.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.73.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.73.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.73.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.73.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.73.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.73.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.73.mlp.act_fn SiLUActivation()\n",
      "model.layers.73.input_layernorm LlamaRMSNorm()\n",
      "model.layers.73.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.74 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.74.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.74.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.74.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.74.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.74.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.74.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.74.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.74.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.74.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.74.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.74.mlp.act_fn SiLUActivation()\n",
      "model.layers.74.input_layernorm LlamaRMSNorm()\n",
      "model.layers.74.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.75 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.75.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.75.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.75.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.75.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.75.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.75.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.75.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.75.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.75.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.75.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.75.mlp.act_fn SiLUActivation()\n",
      "model.layers.75.input_layernorm LlamaRMSNorm()\n",
      "model.layers.75.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.76 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.76.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.76.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.76.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.76.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.76.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.76.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.76.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.76.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.76.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.76.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.76.mlp.act_fn SiLUActivation()\n",
      "model.layers.76.input_layernorm LlamaRMSNorm()\n",
      "model.layers.76.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.77 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.77.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.77.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.77.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.77.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.77.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.77.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.77.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.77.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.77.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.77.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.77.mlp.act_fn SiLUActivation()\n",
      "model.layers.77.input_layernorm LlamaRMSNorm()\n",
      "model.layers.77.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.78 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.78.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.78.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.78.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.78.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.78.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.78.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.78.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.78.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.78.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.78.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.78.mlp.act_fn SiLUActivation()\n",
      "model.layers.78.input_layernorm LlamaRMSNorm()\n",
      "model.layers.78.post_attention_layernorm LlamaRMSNorm()\n",
      "model.layers.79 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "    (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "model.layers.79.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.79.self_attn.q_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.79.self_attn.k_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.79.self_attn.v_proj Linear(in_features=8192, out_features=1024, bias=False)\n",
      "model.layers.79.self_attn.o_proj Linear(in_features=8192, out_features=8192, bias=False)\n",
      "model.layers.79.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.79.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "  (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.79.mlp.gate_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.79.mlp.up_proj Linear(in_features=8192, out_features=28672, bias=False)\n",
      "model.layers.79.mlp.down_proj Linear(in_features=28672, out_features=8192, bias=False)\n",
      "model.layers.79.mlp.act_fn SiLUActivation()\n",
      "model.layers.79.input_layernorm LlamaRMSNorm()\n",
      "model.layers.79.post_attention_layernorm LlamaRMSNorm()\n",
      "model.norm LlamaRMSNorm()\n",
      "lm_head Linear(in_features=8192, out_features=32002, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.named_modules():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--WizardLM--WizardMath-70B-V1.0/snapshots/e089c3f9d2ad9d1acb62425aec3f4126f498f4c5/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config.num_attention_heads = 64\n",
      "model_config.num_key_value_heads = 8\n"
     ]
    }
   ],
   "source": [
    "h = hidden_size = model_config.hidden_size\n",
    "# intermediate_size = model_config.intermediate_size\n",
    "intermediate_size = 4 * h\n",
    "V = vocab_size = model_config.vocab_size\n",
    "l = num_hidden_layers = model_config.num_hidden_layers\n",
    "# assert (\n",
    "#     model_config.num_attention_heads == model_config.num_key_value_heads\n",
    "# ), f\"num_attention_heads ({model_config.num_attention_heads}) != num_key_value_heads ({model_config.num_key_value_heads})\"\n",
    "\n",
    "print(f\"model_config.num_attention_heads = {model_config.num_attention_heads}\")\n",
    "print(f\"model_config.num_key_value_heads = {model_config.num_key_value_heads}\")\n",
    "\n",
    "a = model_config.num_attention_heads\n",
    "\n",
    "num_gpus = 4\n",
    "\n",
    "b = batch_size_per_gpu = 2\n",
    "# s = max_seq_len = 4096\n",
    "s = max_seq_len = 1024\n",
    "\n",
    "# b = batch_size_per_gpu = 20\n",
    "# b = batch_size_per_gpu = 3\n",
    "# s = max_seq_len = 2048\n",
    "\n",
    "# num_gpus = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters per self-attention layer: 268468224\n",
      "Number of parameters per feed-forward network: 536911872\n",
      "Number of parameters per transformer layer: 805412864\n",
      "Number of parameters in the token embedding matrix: 262160384\n",
      "Number of parameters in the transformer model: 64695189504\n"
     ]
    }
   ],
   "source": [
    "num_params_per_self_attention = 4 * (h**2 + h)\n",
    "num_params_per_ffn = h * intermediate_size * 2 + intermediate_size + h\n",
    "num_params_per_layer_norm = 2 * h\n",
    "num_params_per_transformer_layer = (\n",
    "    num_params_per_self_attention + num_params_per_ffn + 2 * num_params_per_layer_norm\n",
    ")\n",
    "\n",
    "num_params_token_embedding_matrix = vocab_size * hidden_size\n",
    "\n",
    "num_params_transformer_model = (\n",
    "    num_params_per_transformer_layer * num_hidden_layers\n",
    "    + num_params_token_embedding_matrix\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Number of parameters per self-attention layer:\", num_params_per_self_attention)\n",
    "print(\"Number of parameters per feed-forward network:\", num_params_per_ffn)\n",
    "print(\"Number of parameters per transformer layer:\", num_params_per_transformer_layer)\n",
    "print(\n",
    "    \"Number of parameters in the token embedding matrix:\",\n",
    "    num_params_token_embedding_matrix,\n",
    ")\n",
    "print(\"Number of parameters in the transformer model:\", num_params_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16777216\n",
      "201326592\n",
      "6442450944\n"
     ]
    }
   ],
   "source": [
    "print(4096**2)\n",
    "print(12 * 4096**2)\n",
    "print(12 * 32 * 4096**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_mem_train_state = 1205.04 GB\n"
     ]
    }
   ],
   "source": [
    "# mixed precision\n",
    "mem_params_mixed_precision = num_params_transformer_model * (2 + 4)\n",
    "mem_grads_mixed_precision = num_params_transformer_model * (2 + 4)\n",
    "mem_adam_states = num_params_transformer_model * (4 + 4)\n",
    "gb = 1024**3\n",
    "gb_mem_train_state = (\n",
    "    sum([mem_params_mixed_precision, mem_grads_mixed_precision, mem_adam_states]) / gb\n",
    ")\n",
    "print(f\"gb_mem_train_state = {gb_mem_train_state:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_zero3_train_per_gpu_gb = 301.26 GB\n"
     ]
    }
   ],
   "source": [
    "mem_zero3_train_per_gpu = (\n",
    "    sum([mem_params_mixed_precision, mem_grads_mixed_precision, mem_adam_states])\n",
    "    / num_gpus\n",
    ")\n",
    "\n",
    "mem_zero3_train_per_gpu_gb = mem_zero3_train_per_gpu / gb\n",
    "print(f\"mem_zero3_train_per_gpu_gb = {mem_zero3_train_per_gpu_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_mem_activation_transformer_model_per_gpu = 92.50 GB\n"
     ]
    }
   ],
   "source": [
    "mem_activation_per_transformer_layer_per_gpu = 34 * b * s * h + 5 * b * s**2 * a\n",
    "mem_activation_transformer_model_per_gpu = (\n",
    "    num_hidden_layers * mem_activation_per_transformer_layer_per_gpu\n",
    ")\n",
    "gb_mem_activation_transformer_model_per_gpu = (\n",
    "    mem_activation_transformer_model_per_gpu / gb\n",
    ")\n",
    "print(\n",
    "    f\"gb_mem_activation_transformer_model_per_gpu = {gb_mem_activation_transformer_model_per_gpu:.2f} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_men_kv_cache = 5.00 GB\n"
     ]
    }
   ],
   "source": [
    "mem_kv_cache_per_token = b * h * l * 2 * 2\n",
    "mem_kv_cache = mem_kv_cache_per_token * s\n",
    "gb_men_kv_cache = mem_kv_cache / gb\n",
    "print(f\"gb_men_kv_cache = {gb_men_kv_cache:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
