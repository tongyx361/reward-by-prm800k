{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Union\n",
    "from prepare_dataset import DataCollatorForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockArgs:\n",
    "    debug = True\n",
    "    model_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\"\n",
    "    tokenizer_name = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\"\n",
    "    encoded_datasets_name_or_path = \"/data/users/zhangjunlei/tyx/reward-by-prm800k/datasets/encoded-datasets-direct-prediction\"\n",
    "    use_slow_tokenizer = True\n",
    "    max_seq_length = 4096\n",
    "    per_device_train_batch_size = 2\n",
    "\n",
    "args = MockArgs()\n",
    "\n",
    "def print_dict_of_tensors(d):\n",
    "    for k, v in d.items():\n",
    "        print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DebugDataCollatorForSeq2Seq:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        model ([`PreTrainedModel`]):\n",
    "            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n",
    "            prepare the *decoder_input_ids*\n",
    "\n",
    "            This is useful when using *label_smoothing* to avoid calculating loss twice.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
    "            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
    "    model: Optional[Any] = None\n",
    "    padding: Union[bool, str, transformers.utils.PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        if return_tensors is None:\n",
    "            return_tensors = self.return_tensors\n",
    "        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n",
    "        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n",
    "        # same length to return tensors.\n",
    "        if labels is not None:\n",
    "            max_label_length = max(len(l) for l in labels)\n",
    "            if self.pad_to_multiple_of is not None:\n",
    "                max_label_length = (\n",
    "                    (max_label_length + self.pad_to_multiple_of - 1)\n",
    "                    // self.pad_to_multiple_of\n",
    "                    * self.pad_to_multiple_of\n",
    "                )\n",
    "\n",
    "            padding_side = self.tokenizer.padding_side\n",
    "            for feature in features:\n",
    "                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n",
    "                if isinstance(feature[\"labels\"], list):\n",
    "                    feature[\"labels\"] = (\n",
    "                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n",
    "                    )\n",
    "                elif padding_side == \"right\":\n",
    "                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n",
    "                else:\n",
    "                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n",
    "        feature[\"labels\"] = torch.tensor(feature[\"labels\"])\n",
    "\n",
    "        print(\"features[0]:\")\n",
    "        print_dict_of_tensors(features[0])\n",
    "        \n",
    "\n",
    "        features = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=return_tensors,\n",
    "        )\n",
    "        \n",
    "        print(\"features:\")\n",
    "        print_dict_of_tensors(features)\n",
    "\n",
    "        # prepare decoder_input_ids\n",
    "        if (\n",
    "            labels is not None\n",
    "            and self.model is not None\n",
    "            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n",
    "        ):\n",
    "            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n",
    "            features[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = None\n",
    "\n",
    "# tokenizer\n",
    "\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        args.tokenizer_name, use_fast=not args.use_slow_tokenizer\n",
    "    )\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path, use_fast=not args.use_slow_tokenizer\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map\n",
    "if isinstance(tokenizer, transformers.LlamaTokenizer):\n",
    "    num_added_tokens = tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"bos_token\": \"<s>\",\n",
    "            \"eos_token\": \"</s>\",\n",
    "            \"unk_token\": \"<unk>\",\n",
    "            \"pad_token\": \"<pad>\",\n",
    "        }\n",
    "    )\n",
    "    assert num_added_tokens in [\n",
    "        0,\n",
    "        1,\n",
    "    ], \"LlamaTokenizer should only add one special token - the pad_token, or no tokens if pad token present.\"\n",
    "elif isinstance(tokenizer, transformers.GPTNeoXTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"pad_token\": \"<pad>\",\n",
    "        }\n",
    "    )\n",
    "    assert (\n",
    "        num_added_tokens == 1\n",
    "    ), \"GPTNeoXTokenizer should only add one special token - the pad_token.\"\n",
    "elif isinstance(tokenizer, transformers.GPT2Tokenizer) and isinstance(model, transformers.OPTForCausalLM):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\"unk_token\": \"<unk>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.model_input_names)\n",
    "tokenizer.model_input_names = ['input_ids', 'labels', 'attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.encoded_datasets_name_or_path:\n",
    "    lm_datasets = datasets.load_from_disk(args.encoded_datasets_name_or_path)\n",
    "    train_dataset = lm_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([138])\n",
      "labels torch.Size([138])\n",
      "attention_mask torch.Size([138])\n",
      "{'input_ids': tensor([    1,  1128,  1784,  6923,   526,   297, 29871, 29955, 29889, 29947,\n",
      "         6233, 29973,    13, 29955, 29889, 29947,  6233,   338,   278,  1021,\n",
      "          408, 29871, 29955,  6233,   322, 29871, 29900, 29889, 29947,  6233,\n",
      "        29889,    13,  7341, 29892,   322,  1951,   727,   526, 29871, 29953,\n",
      "        29900,  6923,   297,   263, 11015, 29892,   769,   727,   526, 29871,\n",
      "        29953, 29900,   334, 29871, 29955,   353, 29871, 29946, 29906, 29900,\n",
      "         6923,   297, 29871, 29955,  6233, 29889,    13,  2855,  1951,   727,\n",
      "          526, 29871, 29953, 29900,  6923,   297,   263, 11015, 29892,   769,\n",
      "          727,   526, 29871, 29953, 29900,   334, 29871, 29900, 29889, 29947,\n",
      "          353, 29871, 29946, 29947,  6923,   297, 29871, 29900, 29889, 29947,\n",
      "         6233, 29889,    13,  6295, 29892,   297,  3001, 29892,   727,   526,\n",
      "        29871, 29946, 29906, 29900,   718, 29871, 29946, 29947,   353, 29871,\n",
      "        29946, 29953, 29947,  6923,   297, 29871, 29955, 29889, 29947,  6233,\n",
      "        29889,    13,  1252, 23617, 29889,    13,    13, 29937,   673,    13,\n",
      "           13, 29946, 29953, 29947,    13]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        6374]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1])}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = lm_datasets[\"train\"]\n",
    "train_example = train_dataset[random.randint(0, len(train_dataset) - 1)]\n",
    "for feature_name, data in train_example.items():\n",
    "    print(feature_name, data.shape)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padding_strategy = transformers.utils.PaddingStrategy.MAX_LENGTH\n",
    "\n",
    "# padding_strategy = transformers.utils.PaddingStrategy.LONGEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 4096])\n",
      "labels: torch.Size([1, 4096])\n",
      "attention_mask: torch.Size([1, 4096])\n",
      "{'input_ids': tensor([[    1,  3118,   714,  ..., 32000, 32000, 32000]]), 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# data_collator = DebugDataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=padding_strategy, max_length=args.max_seq_length)\n",
    "# data_collator = transformers.DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=padding_strategy, max_length=args.max_seq_length)\n",
    "data_collator = DataCollatorForCausalLM(tokenizer=tokenizer, model=model, padding=padding_strategy, max_length=args.max_seq_length)\n",
    "collated_example = data_collator([train_example])\n",
    "\n",
    "print_dict_of_tensors(collated_example)\n",
    "print(collated_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch = train_dataloader[0]:\n",
      "input_ids: torch.Size([2, 4096])\n",
      "labels: torch.Size([2, 4096])\n",
      "attention_mask: torch.Size([2, 4096])\n"
     ]
    }
   ],
   "source": [
    "# print(train_dataloader)\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(f\"batch = train_dataloader[{step}]:\")\n",
    "    for k, v in batch.items():\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-instruct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
