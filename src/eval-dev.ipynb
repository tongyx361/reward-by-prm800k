{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "model_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\" # \"meta-llama/Llama-2-7b-hf\"\n",
    "gpu_id = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{gpu_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizer(name_or_path='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map\n",
    "\n",
    "num_added_tokens = tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    }\n",
    ")\n",
    "assert num_added_tokens in [\n",
    "    0,\n",
    "    1,\n",
    "], \"LlamaTokenizer should only add one special token - the pad_token, or no tokens if pad token present.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizer(name_or_path='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cda36d26f4d4841a9d5b2778bb26b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(model_name_or_path, low_cpu_mem_usage=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"A $90^\\\\circ$ rotation around the origin in the counter-clockwise direction is applied to $7 + 2i.$  What is the resulting complex number?\"\n",
    "steps = [\n",
    "    \"I know that a $90^\\\\circ$ rotation around the origin in the complex plane can be achieved by multiplying the complex number by $i$, since $i$ has a magnitude of 1 and an argument of $90^\\\\circ$.\",\n",
    "    \"So, I can write the rotation as $(7 + 2i)i = 7i + 2i^2.$\",\n",
    "    \"To simplify this expression, I recall that $i^2 = -1$, so I can substitute that and get $7i - 2.$\",\n",
    "    \"This is the resulting complex number after the rotation.\\n\\n# Answer\\n\\n7i - 2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6374], [8178], [21104]]\n"
     ]
    }
   ],
   "source": [
    "rating2word = {1: \"positive\", -1: \"negative\", 0: \"neutral\"}\n",
    "rating_words = list(rating2word.values())\n",
    "rating_token_ids = tokenizer(rating_words, add_special_tokens=False).input_ids\n",
    "print(rating_token_ids)\n",
    "\n",
    "\n",
    "\n",
    "# constraint = transformers.Constraint(type=\"must_include\", token=\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   319,   395, 29929, 29900,  3823,  6034, 29938, 13733,  2820,\n",
      "           278,  3978,   297,   278,  6795, 29899, 13058,  3538,  5305,   338,\n",
      "          7436,   304,   395, 29955,   718, 29871, 29906, 29875,  7449, 29871,\n",
      "          1724,   338,   278,  9819,  4280,  1353, 29973]], device='cuda:5'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:5')}\n",
      "BeamSearchDecoderOnlyOutput(sequences=tensor([[    1,   319,   395, 29929, 29900,  3823,  6034, 29938, 13733,  2820,\n",
      "           278,  3978,   297,   278,  6795, 29899, 13058,  3538,  5305,   338,\n",
      "          7436,   304,   395, 29955,   718, 29871, 29906, 29875,  7449, 29871,\n",
      "          1724,   338,   278,  9819,  4280,  1353, 29973,    13]],\n",
      "       device='cuda:5'), sequences_scores=tensor([-0.0062], device='cuda:5'), scores=(tensor([[-2.0333e+01, -2.0216e+01, -2.5555e+00,  ..., -2.0804e+01,\n",
      "         -1.9792e+01, -1.9454e+01],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09]], device='cuda:5'),), beam_indices=None, attentions=None, hidden_states=None)\n",
      "tensor([[-2.0333e+01, -2.0216e+01, -2.5555e+00,  ..., -2.0804e+01,\n",
      "         -1.9792e+01, -1.9454e+01],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09]], device='cuda:5')\n",
      "torch.Size([2, 32001])\n",
      "tensor([1.4780e-09, 1.6609e-09, 7.7650e-02,  ..., 9.2279e-10, 2.5368e-09,\n",
      "        3.5600e-09], device='cuda:5')\n",
      "tensor(0.9999, device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(problem, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n",
    "\n",
    "outputs = model.generate(**inputs, \n",
    "                         do_sample=False,\n",
    "                         max_new_tokens=1,\n",
    "                         force_words_ids=rating_token_ids,\n",
    "                         num_beams=2,\n",
    "                         remove_invalid_values=True,\n",
    "                         return_dict_in_generate=True,\n",
    "                         output_scores=True, \n",
    "                         )\n",
    "print(outputs)\n",
    "\n",
    "first_scores = outputs[\"scores\"][0]\n",
    "print(first_scores)\n",
    "print(first_scores.shape)\n",
    "first_probs = torch.nn.functional.softmax(first_scores, dim=-1)\n",
    "print(first_probs[0])\n",
    "print(sum(first_probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,    13],\n",
      "        [    0, 29871]], device='cuda:5')\n",
      "tensor([[1.4780e-09, 7.8869e-01],\n",
      "        [1.4780e-09, 1.0245e-01]], device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "# for idx, prob in enumerate(first_probs[0]):\n",
    "#     if prob > 1e-5:\n",
    "#         print(f\"token_id: {idx}, token: {tokenizer.decode([idx])}, prob: {prob}\")\n",
    "\n",
    "# fast\n",
    "# non_zero_idxs = torch.gt(first_probs.view(-1),1e-1)\n",
    "threshold = 0.1\n",
    "mask = first_probs > threshold\n",
    "non_zero_idxs = torch.nonzero(mask)\n",
    "print(non_zero_idxs)\n",
    "non_zero_vals = first_probs.view(-1)[non_zero_idxs]\n",
    "print(non_zero_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
