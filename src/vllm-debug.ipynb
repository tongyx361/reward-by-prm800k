{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "utils.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\"\n",
    "tokenizer_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-25 15:48:16 llm_engine.py:70] Initializing an LLM engine with config: model='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9', tokenizer='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.float16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n",
      "INFO 08-25 15:48:16 tokenizer.py:29] For some LLaMA-based models, initializing the fast tokenizer may take a long time. To eliminate the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n",
      "INFO 08-25 15:48:21 llm_engine.py:196] # GPU blocks: 7439, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=model_name_or_path,\n",
    "    tokenizer=tokenizer_name_or_path,\n",
    "    tokenizer_mode=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"auto\",\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"Hello, my name is\",\n",
    "#     \"The president of the United States is\",\n",
    "#     \"The capital of France is\",\n",
    "#     \"The future of AI is\",\n",
    "# ]\n",
    "problem_solution_hierarchical_samples = utils.load_pickle(\n",
    "    utils.gpt4_generated_problem_solution_hierarchical_samples_path_wo_basename + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815631\n",
      "988\n"
     ]
    }
   ],
   "source": [
    "def problem_solutions2solution_texts(problem_solution):\n",
    "    for problem_sample in problem_solution:\n",
    "        problem_text = problem_sample[\"problem\"]\n",
    "        for solution_sample in problem_sample[\"solutions\"]:\n",
    "            solution_text = problem_text\n",
    "            for step in solution_sample[\"steps\"]:\n",
    "                solution_text += \"\\n\" + step\n",
    "            yield solution_text\n",
    "\n",
    "\n",
    "solution_texts = list(\n",
    "    problem_solutions2solution_texts(problem_solution_hierarchical_samples)\n",
    ")\n",
    "\n",
    "print(len(solution_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m sampling_params \u001b[39m=\u001b[39m SamplingParams(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mutils\u001b[39m.\u001b[39mgeneration_config)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Generate texts from the prompts. The output is a list of RequestOutput objects\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# that contain the prompt, generated text, and other information.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mgenerate(solution_texts, sampling_params)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Print the outputs.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# for output in outputs:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#     prompt = output.prompt\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m#     )\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/open-instruct/lib/python3.10/site-packages/vllm/entrypoints/llm.py:129\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         token_ids \u001b[39m=\u001b[39m prompt_token_ids[i]\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_request(prompt, sampling_params, token_ids)\n\u001b[1;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_engine(use_tqdm)\n",
      "File \u001b[0;32m~/anaconda3/envs/open-instruct/lib/python3.10/site-packages/vllm/entrypoints/llm.py:139\u001b[0m, in \u001b[0;36mLLM._add_request\u001b[0;34m(self, prompt, sampling_params, prompt_token_ids)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_add_request\u001b[39m(\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    134\u001b[0m     prompt: Optional[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    135\u001b[0m     sampling_params: SamplingParams,\n\u001b[1;32m    136\u001b[0m     prompt_token_ids: Optional[List[\u001b[39mint\u001b[39m]],\n\u001b[1;32m    137\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     request_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_counter))\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_engine\u001b[39m.\u001b[39;49madd_request(request_id, prompt, sampling_params,\n\u001b[1;32m    140\u001b[0m                                 prompt_token_ids)\n",
      "File \u001b[0;32m~/anaconda3/envs/open-instruct/lib/python3.10/site-packages/vllm/engine/llm_engine.py:261\u001b[0m, in \u001b[0;36mLLMEngine.add_request\u001b[0;34m(self, request_id, prompt, sampling_params, prompt_token_ids, arrival_time)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sampling_params\u001b[39m.\u001b[39mbest_of):\n\u001b[1;32m    260\u001b[0m     seq_id \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_counter)\n\u001b[0;32m--> 261\u001b[0m     seq \u001b[39m=\u001b[39m Sequence(seq_id, prompt, prompt_token_ids, block_size)\n\u001b[1;32m    262\u001b[0m     seqs\u001b[39m.\u001b[39mappend(seq)\n\u001b[1;32m    264\u001b[0m \u001b[39m# Create the sequence group.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/open-instruct/lib/python3.10/site-packages/vllm/sequence.py:121\u001b[0m, in \u001b[0;36mSequence.__init__\u001b[0;34m(self, seq_id, prompt, prompt_token_ids, block_size)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogical_token_blocks: List[LogicalTokenBlock] \u001b[39m=\u001b[39m []\n\u001b[1;32m    120\u001b[0m \u001b[39m# Initialize the logical token blocks with the prompt token ids.\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_append_tokens_to_blocks(prompt_token_ids)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m SequenceStatus\u001b[39m.\u001b[39mWAITING\n",
      "File \u001b[0;32m~/anaconda3/envs/open-instruct/lib/python3.10/site-packages/vllm/sequence.py:143\u001b[0m, in \u001b[0;36mSequence._append_tokens_to_blocks\u001b[0;34m(self, token_ids)\u001b[0m\n\u001b[1;32m    140\u001b[0m     last_block \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogical_token_blocks[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    142\u001b[0m num_empty_slots \u001b[39m=\u001b[39m last_block\u001b[39m.\u001b[39mget_num_empty_slots()\n\u001b[0;32m--> 143\u001b[0m last_block\u001b[39m.\u001b[39;49mappend_tokens(token_ids[cursor:cursor \u001b[39m+\u001b[39;49m\n\u001b[1;32m    144\u001b[0m                                    num_empty_slots])\n\u001b[1;32m    145\u001b[0m cursor \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_empty_slots\n",
      "File \u001b[0;32m~/anaconda3/envs/open-instruct/lib/python3.10/site-packages/vllm/block.py:40\u001b[0m, in \u001b[0;36mLogicalTokenBlock.append_tokens\u001b[0;34m(self, token_ids)\u001b[0m\n\u001b[1;32m     38\u001b[0m curr_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_tokens\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_ids[curr_idx:curr_idx \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(token_ids)] \u001b[39m=\u001b[39m token_ids\n\u001b[0;32m---> 40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(token_ids)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(**utils.generation_config)\n",
    "\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.generate(solution_texts, sampling_params)\n",
    "# Print the outputs.\n",
    "# for output in outputs:\n",
    "#     prompt = output.prompt\n",
    "#     generated_text = output.outputs[0].text\n",
    "#     print(\n",
    "#         f\"\"\"\n",
    "# Prompt: {prompt!r}\n",
    "# Generated text: {generated_text!r}\n",
    "# \"\"\"\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-instruct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
