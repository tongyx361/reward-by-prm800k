{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from typing import Dict, List\n",
    "from functools import partial\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2Tokenizer,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    LlamaTokenizer,\n",
    "    OPTForCausalLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockArgs:\n",
    "    raw_dataset_json_path = [\n",
    "        \"/data/tongyx361/reward-by-prm800k/prm800k-main/prm800k/data/phase1_train.jsonl\",\n",
    "        \"/data/tongyx361/reward-by-prm800k/prm800k-main/prm800k/data/phase2_train.jsonl\",\n",
    "    ]\n",
    "    encoded_dataset_output_dirpath = (\n",
    "        \"/data/tongyx361/reward-by-prm800k/datasets/encoded-datasets-pure-prediction\"\n",
    "    )\n",
    "    preprocessing_num_workers = 16\n",
    "    overwrite_cache = False\n",
    "    model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "    tokenizer_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "    use_slow_tokenizer = True\n",
    "    max_seq_length = None\n",
    "\n",
    "\n",
    "args = MockArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_prm800k_sample(sample: dict) -> dict:\n",
    "    problem = sample[\"question\"][\"problem\"]\n",
    "    step_ratings = []\n",
    "\n",
    "    if sample[\"is_quality_control_question\"]:\n",
    "        raise RuntimeError(\"is_quality_control_question is True\")\n",
    "\n",
    "    label = sample[\"label\"]\n",
    "    finish_reason = label[\"finish_reason\"]\n",
    "    if finish_reason not in [\"found_error\", \"solution\"]:\n",
    "        raise RuntimeError(f\"finish_reason is {finish_reason}\")\n",
    "\n",
    "    steps = label[\"steps\"]\n",
    "    for step in steps:\n",
    "        chosen_completion = step[\"chosen_completion\"]\n",
    "        if step[\"human_completion\"] is not None:\n",
    "            completion = step[\"human_completion\"]\n",
    "            rating = 1\n",
    "        else:\n",
    "            completions = step[\"completions\"]\n",
    "            if chosen_completion is not None:\n",
    "                completion = completions[chosen_completion]\n",
    "            else:\n",
    "                for completion in completions:\n",
    "                    if completion[\"rating\"] == -1:\n",
    "                        break\n",
    "            rating = completion[\"rating\"]\n",
    "\n",
    "        step_text = completion[\"text\"]\n",
    "\n",
    "        if completion[\"flagged\"] not in [None, False]:\n",
    "            print(f\"{sample['timestamp']} flagged: \", completion[\"flagged\"])\n",
    "            print(sample)\n",
    "        step_ratings.append({\"step\": step_text, \"rating\": rating})\n",
    "\n",
    "    reformatted_sample = {\"problem\": problem, \"step_ratings\": step_ratings}\n",
    "\n",
    "    if finish_reason == \"found_error\":\n",
    "        last_rating = reformatted_sample[\"step_ratings\"][-1][\"rating\"]\n",
    "        assert last_rating == -1, f\"last step should be -1 but {last_rating}\"\n",
    "\n",
    "    return reformatted_sample\n",
    "\n",
    "\n",
    "def encode_with_problem_step_ratings_format(reformatted_sample, tokenizer, test=False):\n",
    "    \"\"\"\n",
    "    Here we assume each sample has a 'step_ratings' field. Each step_rating is a dict.\n",
    "    \"\"\"\n",
    "\n",
    "    step_ratings = reformatted_sample[\"step_ratings\"]\n",
    "    if len(step_ratings) == 0:\n",
    "        raise ValueError(\"step_ratings field is empty.\")\n",
    "\n",
    "    rating2word = {1: \"positive\", -1: \"negative\", 0: \"neutral\"}\n",
    "    rating2token_id = {\n",
    "        rating: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))[0]\n",
    "        for rating, word in rating2word.items()\n",
    "    }\n",
    "\n",
    "    problem = reformatted_sample[\"problem\"].strip()\n",
    "    problem_step_ratings_text = problem + \"\\n\"\n",
    "    sample_input_ids = tokenizer(\n",
    "        problem + \"\\n\",\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "    ignore_index = -100\n",
    "    sample_labels = torch.ones_like(sample_input_ids) * ignore_index\n",
    "\n",
    "    for step_rating in step_ratings:\n",
    "        step = step_rating[\"step\"].strip()\n",
    "\n",
    "        problem_step_ratings_text += step + \"\\n\"\n",
    "\n",
    "        step_input_ids = tokenizer(\n",
    "            \"\\n\" + step + \"\\n\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "            # add_special_tokens=True,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False,\n",
    "        )[\"input_ids\"]\n",
    "        step_input_ids = step_input_ids[:, 2:]  # remove \"\\n\"\n",
    "        sample_input_ids = torch.cat((sample_input_ids, step_input_ids), dim=1)\n",
    "\n",
    "        step_labels = torch.ones_like(step_input_ids) * ignore_index\n",
    "        step_labels[:, -2] = rating2token_id[\n",
    "            step_rating[\"rating\"]\n",
    "        ]  # set the label for the last token_id before \"\\n\"\n",
    "        sample_labels = torch.cat((sample_labels, step_labels), dim=1)\n",
    "\n",
    "    # remove the last \"\\n\"\n",
    "    sample_input_ids = sample_input_ids[:, :-1]\n",
    "    sample_labels = sample_labels[:, :-1]\n",
    "    problem_step_ratings_text = problem_step_ratings_text.strip()\n",
    "\n",
    "    if test:\n",
    "        sample_input_ids_from_simple_call = tokenizer(\n",
    "            problem_step_ratings_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=False,\n",
    "        )[\"input_ids\"]\n",
    "        assert torch.equal(sample_input_ids, sample_input_ids_from_simple_call), (\n",
    "            sample_input_ids != sample_input_ids_from_simple_call\n",
    "        )\n",
    "\n",
    "    attention_mask = torch.ones_like(sample_input_ids)\n",
    "    train_sample = {\n",
    "        \"input_ids\": sample_input_ids.flatten(),\n",
    "        \"labels\": sample_labels.flatten(),\n",
    "        \"attention_mask\": attention_mask.flatten(),\n",
    "    }\n",
    "    return train_sample\n",
    "\n",
    "\n",
    "def pick_prm800k_samples(x):\n",
    "    return (not x[\"is_quality_control_question\"]) and (\n",
    "        x[\"label\"][\"finish_reason\"] in [\"found_error\", \"solution\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /data/tongyx361/.cache/huggingface/datasets/json/default-2a52b90631375b50/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ab9bdba87a4920aaa39611437f3d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7331a2f52d3e4a62b86b6123d6335a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cbd391e3524c3a9707ecb84bc2266b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /data/tongyx361/.cache/huggingface/datasets/json/default-2a52b90631375b50/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd439c8289bf4ee08ff1aec07903ffc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c0d99c5aca4f4bbdaf0df3907418fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reformatting(extractting) SFT data (num_proc=16):   0%|          | 0/808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /data/tongyx361/.cache/huggingface/datasets/json/default-0f474ed7b76bb2f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054fa61784724ceab861e773b0c680bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ded4c2bd25e4c368bacd090274a6959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6abadec56464e73a3ca70b92c083a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /data/tongyx361/.cache/huggingface/datasets/json/default-0f474ed7b76bb2f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d09ca0ed4c443fb409ec7c9b6b74a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be832e1403a24c90b152be8d1902a1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reformatting(extractting) SFT data (num_proc=16):   0%|          | 0/86204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_args = {\n",
    "    # \"streaming\": True,\n",
    "}\n",
    "\n",
    "reformat_col_names = [\"problem\", \"step_ratings\"]\n",
    "train_data_files_list = args.raw_dataset_json_path\n",
    "reformatted_prm800k_phase_train_list = [None] * len(train_data_files_list)\n",
    "for idx, data_files_train in enumerate(train_data_files_list):\n",
    "    prm800k_phase_train = datasets.load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": data_files_train},\n",
    "        split=\"train\",\n",
    "    )\n",
    "    filtered_prm800k_phase_train = prm800k_phase_train.filter(pick_prm800k_samples)\n",
    "    reformatted_filtered_prm800k_phase_train = filtered_prm800k_phase_train.map(\n",
    "        reformat_prm800k_sample,\n",
    "        batched=False,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        remove_columns=[\n",
    "            name\n",
    "            for name in filtered_prm800k_phase_train.column_names\n",
    "            if name not in reformat_col_names\n",
    "        ],\n",
    "        desc=\"Reformatting(extractting) SFT data\",\n",
    "    )\n",
    "    reformatted_prm800k_phase_train_list[idx] = reformatted_filtered_prm800k_phase_train\n",
    "train_dataset = datasets.concatenate_datasets(reformatted_prm800k_phase_train_list)\n",
    "raw_datasets = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\": train_dataset,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "# tokenizer\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.tokenizer_name, use_fast=not args.use_slow_tokenizer\n",
    "    )\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path, use_fast=not args.use_slow_tokenizer\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map\n",
    "if isinstance(tokenizer, LlamaTokenizer):\n",
    "    num_added_tokens = tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"bos_token\": \"<s>\",\n",
    "            \"eos_token\": \"</s>\",\n",
    "            \"unk_token\": \"<unk>\",\n",
    "            \"pad_token\": \"<pad>\",\n",
    "        }\n",
    "    )\n",
    "    assert num_added_tokens in [\n",
    "        0,\n",
    "        1,\n",
    "    ], \"LlamaTokenizer should only add one special token - the pad_token, or no tokens if pad token present.\"\n",
    "elif isinstance(tokenizer, GPTNeoXTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"pad_token\": \"<pad>\",\n",
    "        }\n",
    "    )\n",
    "    assert (\n",
    "        num_added_tokens == 1\n",
    "    ), \"GPTNeoXTokenizer should only add one special token - the pad_token.\"\n",
    "elif isinstance(tokenizer, GPT2Tokenizer) and isinstance(model, OPTForCausalLM):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\"unk_token\": \"<unk>\"})\n",
    "encode_function = partial(\n",
    "    encode_with_problem_step_ratings_format,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5de9994105451f8f880c546e9f7628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting SFT data (num_proc=16):   0%|          | 0/87012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea2b9ae40884a06b1db484d26ba3e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/87012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = raw_datasets.map(\n",
    "    encode_function,\n",
    "    batched=False,\n",
    "    num_proc=args.preprocessing_num_workers,\n",
    "    load_from_cache_file=not args.overwrite_cache,\n",
    "    remove_columns=[\n",
    "        name\n",
    "        for name in raw_datasets[\"train\"].column_names\n",
    "        if name not in [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "    ],\n",
    "    desc=\"Tokenizing and reformatting SFT data\",\n",
    ")\n",
    "lm_datasets.set_format(type=\"pt\")\n",
    "lm_datasets = lm_datasets.filter(lambda example: (example[\"labels\"] != -100).any())\n",
    "\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e48a8524d5d452cb2d08ba3d8be6057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/87012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets.save_to_disk(args.encoded_dataset_output_dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_datasets[\"train\"][random.randint(0, len(raw_datasets[\"train\"]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87012\n",
      "{'input_ids': tensor([    1,   319,  4402,   756, 29871, 29896, 29900,  5144, 29892, 29871,\n",
      "        29945, 12544,   322, 29871, 29945, 14000, 29889, 29871,  7803,   310,\n",
      "          278,  5144,   526, 10434,   472,  4036, 29889, 29871,  1724,   338,\n",
      "          278,  6976,   393,   896,   526,  1716, 14000, 29973,    13, 29902,\n",
      "          864,   304,  1284,   278, 15958,   310,   714, 26807,   988,  1716,\n",
      "         5144, 10434,   526, 14000,   714,   310,   599,  1950,   714, 26807,\n",
      "          310, 23906,  1023,  5144, 29889,    13,  1762,   437,   393, 29892,\n",
      "          306,   817,   304,  2302,   920,  1784,  5837,   727,   526,   304,\n",
      "         6755,  1023, 14000,   322,   920,  1784,  5837,   727,   526,   304,\n",
      "         6755,   738,  1023,  5144, 29889,    13, 29902,   508,   671,   278,\n",
      "        10296,  7063,   304,  1284,  1438,  3694, 29889,    13,  1576, 10296,\n",
      "         7063,   338,   302,  6755,   364,   353,   302, 29991, 14571, 29878,\n",
      "        13311, 29876, 29899, 29878, 20198,   511,   988,   302,   338,   278,\n",
      "         3001,  1353,   310,  3161,   322,   364,   338,   278,  1353,   310,\n",
      "         3161, 10434, 29889,    13,  1762,  6755,  1023, 14000,   714,   310,\n",
      "        29871, 29945, 14000, 29892,   306,   505, 29871, 29945,  6755, 29871,\n",
      "        29906,   353, 29871, 29945, 29991, 14571, 29906, 13311, 29945, 29899,\n",
      "        29906, 29897, 14366,   353, 29871, 29896, 29900,  5837, 29889,    13,\n",
      "         1762,  6755,   738,  1023,  5144,   714,   310, 29871, 29896, 29900,\n",
      "         5144, 29892,   306,   505, 29871, 29896, 29900,  6755, 29871, 29906,\n",
      "          353, 29871, 29896, 29900, 29991, 14571, 29906, 13311, 29896, 29900,\n",
      "        29899, 29906, 29897, 14366,   353, 29871, 29946, 29945,  5837, 29889,\n",
      "           13,  8439,  1079, 29892,   278,  6976,   310, 23906,  1023, 14000,\n",
      "          338, 29871, 29896, 29900, 29914, 29946, 29945, 29892,   607,  5466,\n",
      "        11057,   304, 29871, 29906, 29914, 29929, 29889,    13, 29937,   673,\n",
      "           13,    13, 29906, 29914, 29929]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6374, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, 6374, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, 6374, -100, -100, -100, -100, -100,\n",
      "        -100, -100, 6374]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))  # 87012\n",
    "print(train_dataset[random.randint(0, len(train_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_dataset_sample = raw_dataset[random.randint(0, len(raw_dataset))]\n",
    "# print(raw_dataset_sample)\n",
    "raw_dataset_example = {\n",
    "    \"labeler\": \"68e6d5f3-747c-4420-97b4-a30a73f639f9\",\n",
    "    \"timestamp\": \"2022-11-01T17:20:23.427734\",\n",
    "    \"generation\": 4,\n",
    "    \"is_quality_control_question\": False,\n",
    "    \"is_initial_screening_question\": False,\n",
    "    \"question\": {\n",
    "        \"problem\": \"If the system of equations\\n\\n\\\\begin{align*}\\n6x-4y&=a,\\\\\\\\\\n6y-9x &=b.\\n\\\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\\\frac{a}{b},$ assuming $b$ is nonzero.\",\n",
    "        \"ground_truth_solution\": \"If we multiply the first equation by $-\\\\frac{3}{2}$, we obtain\\n\\n$$6y-9x=-\\\\frac{3}{2}a.$$Since we also know that $6y-9x=b$, we have\\n\\n$$-\\\\frac{3}{2}a=b\\\\Rightarrow\\\\frac{a}{b}=\\\\boxed{-\\\\frac{2}{3}}.$$\",\n",
    "        \"ground_truth_answer\": \"-\\\\frac{2}{3}\",\n",
    "        \"pre_generated_steps\": [\n",
    "            \"I notice that the coefficients of the system are multiples of 3, so I can divide both equations by 3 to simplify them.\",\n",
    "            \"\\\\begin{align*}\\n2x-\\\\frac{4}{3}y&=\\\\frac{a}{3},\\\\\\\\\\n2y-3x &=\\\\frac{b}{3}.\\n\\\\end{align*}\",\n",
    "            \"I also notice that the coefficients of $x$ and $y$ in the second equation are the negatives of the coefficients of $y$ and $x$ in the first equation, respectively.\",\n",
    "            \"This suggests that I can add the two equations to eliminate one of the variables.\",\n",
    "            \"\\\\begin{align*}\\n2x-\\\\frac{4}{3}y+\\\\left(2y-3x\\\\right)&=\\\\frac{a}{3}+\\\\frac{b}{3},\\\\\\\\\\n\\\\frac{2}{3}y- x &=\\\\frac{a+b}{3}.\\n\\\\end{align*}\",\n",
    "            \"Now I can solve for $y$ in terms of $x$ and the constants.\",\n",
    "            \"\\\\begin{align*}\\n\\\\frac{2}{3}y&= x + \\\\frac{a+b}{3},\\\\\\\\\\ny&= \\\\frac{3}{2}x + \\\\frac{a+b}{2}.\\n\\\\end{align*}\",\n",
    "            \"I can substitute this expression for $y$ into either of the original equations to get an equation involving only $x$ and the constants.\",\n",
    "            \"I choose the first equation for convenience.\",\n",
    "            \"\\\\begin{align*}\\n2x-\\\\frac{4}{3}\\\\left(\\\\frac{3}{2}x + \\\\frac{a+b}{2}\\\\right)&=\\\\frac{a}{3},\\\\\\\\\\n2x-2x - \\\\frac{2}{3}\\\\left(a+b\\\\right)&=\\\\frac{a}{3},\\\\\\\\\\n-\\\\frac{2}{3}\\\\left(a+b\\\\right)&=\\\\frac{a}{3},\\\\\\\\\\n-2\\\\left(a+b\\\\right)&=a,\\\\\\\\\\na&=-2b.\\n\\\\end{align*}\",\n",
    "            \"I have found a relationship between $a$ and $b$ that must hold for the system to have a nonzero solution.\",\n",
    "            \"I can use this to find the ratio $\\\\frac{a}{b}$ by dividing both sides by $b$.\",\n",
    "            \"\\\\begin{align*}\\n\\\\frac{a}{b}&=\\\\frac{-2b}{b},\\\\\\\\\\n\\\\frac{a}{b}&=-2.\\n\\\\end{align*}\",\n",
    "            \"This is the answer I was looking for.\",\n",
    "            \"# Answer\\n\\n-2\",\n",
    "        ],\n",
    "        \"pre_generated_answer\": \"-2\",\n",
    "        \"pre_generated_verifier_score\": 0.12514245681574954,\n",
    "    },\n",
    "    \"label\": {\n",
    "        \"steps\": [\n",
    "            {\n",
    "                \"completions\": [\n",
    "                    {\n",
    "                        \"text\": \"I notice that the coefficients of the system are multiples of 3, so I can divide both equations by 3 to simplify them.\",\n",
    "                        \"rating\": 0,\n",
    "                        \"flagged\": None,\n",
    "                    }\n",
    "                ],\n",
    "                \"human_completion\": None,\n",
    "                \"chosen_completion\": 0,\n",
    "            },\n",
    "            {\n",
    "                \"completions\": [\n",
    "                    {\n",
    "                        \"text\": \"\\\\begin{align*}\\n2x-\\\\frac{4}{3}y&=\\\\frac{a}{3},\\\\\\\\\\n2y-3x &=\\\\frac{b}{3}.\\n\\\\end{align*}\",\n",
    "                        \"rating\": 1,\n",
    "                        \"flagged\": None,\n",
    "                    }\n",
    "                ],\n",
    "                \"human_completion\": None,\n",
    "                \"chosen_completion\": 0,\n",
    "            },\n",
    "            {\n",
    "                \"completions\": [\n",
    "                    {\n",
    "                        \"text\": \"I also notice that the coefficients of $x$ and $y$ in the second equation are the negatives of the coefficients of $y$ and $x$ in the first equation, respectively.\",\n",
    "                        \"rating\": -1,\n",
    "                        \"flagged\": None,\n",
    "                    },\n",
    "                    {\n",
    "                        \"text\": \"To eliminate one of the variables, I can try to make the coefficients of $x$ or $y$ match in both equations. I see that multiplying the first equation by $\\\\frac{3}{4}$ and the second equation by $-\\\\frac{4}{3}$ will achieve this for $y$.\",\n",
    "                        \"rating\": -1,\n",
    "                        \"flagged\": False,\n",
    "                    },\n",
    "                    {\n",
    "                        \"text\": \"I want to eliminate one of the variables, so I multiply the first equation by $\\\\frac{3}{2}$ and the second equation by $x$ to get\",\n",
    "                        \"rating\": -1,\n",
    "                        \"flagged\": False,\n",
    "                    },\n",
    "                    {\n",
    "                        \"text\": \"I want to find a way to eliminate one of the variables and solve for the other, so I can use the substitution or elimination method.\",\n",
    "                        \"rating\": 1,\n",
    "                        \"flagged\": False,\n",
    "                    },\n",
    "                    {\n",
    "                        \"text\": \"I want to eliminate one of the variables, so I can multiply the first equation by $\\\\frac{3}{2}$ and add it to the second equation.\",\n",
    "                        \"rating\": -1,\n",
    "                        \"flagged\": False,\n",
    "                    },\n",
    "                ],\n",
    "                \"human_completion\": None,\n",
    "                \"chosen_completion\": None,\n",
    "            },\n",
    "        ],\n",
    "        \"total_time\": 418405,\n",
    "        \"finish_reason\": \"found_error\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'If the system of equations\\n\\n\\\\begin{align*}\\n6x-4y&=a,\\\\\\\\\\n6y-9x &=b.\\n\\\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\\\frac{a}{b},$ assuming $b$ is nonzero.', 'step_ratings': [{'step': 'I notice that the coefficients of the system are multiples of 3, so I can divide both equations by 3 to simplify them.', 'rating': 0}, {'step': '\\\\begin{align*}\\n2x-\\\\frac{4}{3}y&=\\\\frac{a}{3},\\\\\\\\\\n2y-3x &=\\\\frac{b}{3}.\\n\\\\end{align*}', 'rating': 1}, {'step': 'I also notice that the coefficients of $x$ and $y$ in the second equation are the negatives of the coefficients of $y$ and $x$ in the first equation, respectively.', 'rating': -1}]}\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "reformatted_raw_dataset_example = reformat_prm800k_sample(raw_dataset_example)\n",
    "print(reformatted_raw_dataset_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1, 263, 13, 29890]\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "newline_id = tokenizer.convert_tokens_to_ids(\"\\n\")\n",
    "print(newline_id)\n",
    "newline_in_sentence_ids = tokenizer.encode(\"a\\nb\")\n",
    "print(newline_in_sentence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True\n",
    "test_for = {\"token_prefix\": True, \"equal\": True, \"common\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reformatted_sample: \n",
      "{'problem': 'If the system of equations\\n\\n\\\\begin{align*}\\n6x-4y&=a,\\\\\\\\\\n6y-9x &=b.\\n\\\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\\\frac{a}{b},$ assuming $b$ is nonzero.', 'step_ratings': [{'step': 'I notice that the coefficients of the system are multiples of 3, so I can divide both equations by 3 to simplify them.', 'rating': 0}, {'step': '\\\\begin{align*}\\n2x-\\\\frac{4}{3}y&=\\\\frac{a}{3},\\\\\\\\\\n2y-3x &=\\\\frac{b}{3}.\\n\\\\end{align*}', 'rating': 1}, {'step': 'I also notice that the coefficients of $x$ and $y$ in the second equation are the negatives of the coefficients of $y$ and $x$ in the first equation, respectively.', 'rating': -1}]}\n",
      "\n",
      "sample_text: \n",
      "If the system of equations\n",
      "\n",
      "\\begin{align*}\n",
      "6x-4y&=a,\\\\\n",
      "6y-9x &=b.\n",
      "\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\frac{a}{b},$ assuming $b$ is nonzero.\n",
      "I notice that the coefficients of the system are multiples of 3, so I can divide both equations by 3 to simplify them.\n",
      "\\begin{align*}\n",
      "2x-\\frac{4}{3}y&=\\frac{a}{3},\\\\\n",
      "2y-3x &=\\frac{b}{3}.\n",
      "\\end{align*}\n",
      "I also notice that the coefficients of $x$ and $y$ in the second equation are the negatives of the coefficients of $y$ and $x$ in the first equation, respectively.\n",
      "\n",
      "tokenized_sample: \n",
      "{'input_ids': tensor([[    1,   960,   278,  1788,   310, 10693,    13,    13, 29905,   463,\n",
      "         29912,  2520,  4044,    13, 29953, 29916, 29899, 29946, 29891, 20644,\n",
      "         29874, 29892,  1966,    13, 29953, 29891, 29899, 29929, 29916,  7878,\n",
      "         29890, 29889,    13, 29905,   355, 29912,  2520,  4044,  5349,   263,\n",
      "          1650,  2427, 29916, 29892,   343,  1262,   988,   395, 29916, 29938,\n",
      "           322,   395, 29891, 29938,   526,  1716,  1661,  9171, 29892,  1284,\n",
      "           779,  1154, 29912, 29874,  1157, 29890,  1118, 29938, 10241,   395,\n",
      "         29890, 29938,   338,  1661,  9171, 29889,    13, 29902,  8369,   393,\n",
      "           278, 16127,   310,   278,  1788,   526,  2473,  2701,   310, 29871,\n",
      "         29941, 29892,   577,   306,   508, 16429,  1716, 10693,   491, 29871,\n",
      "         29941,   304, 21092,   963, 29889,    13, 29905,   463, 29912,  2520,\n",
      "          4044,    13, 29906, 29916,  2612,  1154, 29912, 29946,  1157, 29941,\n",
      "         29913, 29891, 29987,  2013,  1154, 29912, 29874,  1157, 29941,  1118,\n",
      "          1966,    13, 29906, 29891, 29899, 29941, 29916, 29072,  1154, 29912,\n",
      "         29890,  1157, 29941,  1836,    13, 29905,   355, 29912,  2520,  4044,\n",
      "            13, 29902,   884,  8369,   393,   278, 16127,   310,   395, 29916,\n",
      "         29938,   322,   395, 29891, 29938,   297,   278,  1473,  6306,   526,\n",
      "           278,  3480,  5056,   310,   278, 16127,   310,   395, 29891, 29938,\n",
      "           322,   395, 29916, 29938,   297,   278,   937,  6306, 29892,  8307,\n",
      "         29889]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaTokenizer' object has no attribute 'char_to_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m# mask the non-rating part for avoiding loss\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m idx, input_id \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(input_ids[\u001b[39m0\u001b[39m]):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mif\u001b[39;00m input_id \u001b[39m==\u001b[39m tokenizer\u001b[39m.\u001b[39;49mchar_to_token(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     43\u001b[0m         rating_idx \u001b[39m=\u001b[39m idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     44\u001b[0m         labels[\u001b[39m0\u001b[39m][rating_idx] \u001b[39m=\u001b[39m input_ids[\u001b[39m0\u001b[39m][rating_idx]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaTokenizer' object has no attribute 'char_to_token'"
     ]
    }
   ],
   "source": [
    "reformatted_sample = reformatted_raw_dataset_example\n",
    "\n",
    "# function\n",
    "if test_for == True:\n",
    "    print(\"reformatted_sample: \", reformatted_sample, sep=\"\\n\", end=\"\\n\\n\")\n",
    "step_ratings = reformatted_sample[\"step_ratings\"]\n",
    "if len(step_ratings) == 0:\n",
    "    raise ValueError(\"step_ratings field is empty.\")\n",
    "\n",
    "rating2word = {1: \"positive\", -1: \"negative\", 0: \"neutral\"}\n",
    "rating2token_id = {\n",
    "    rating: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))[0]\n",
    "    for rating, word in rating2word.items()\n",
    "}\n",
    "\n",
    "problem = reformatted_sample[\"problem\"].strip()\n",
    "problem_step_ratings_text = problem + \"\\n\"\n",
    "sample_input_ids = tokenizer(\n",
    "    problem + \"\\n\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=False,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=False,\n",
    ")[\"input_ids\"]\n",
    "ignore_index = -100\n",
    "sample_labels = torch.ones_like(sample_input_ids) * ignore_index\n",
    "\n",
    "for step_rating in step_ratings:\n",
    "    step = step_rating[\"step\"].strip()\n",
    "\n",
    "    problem_step_ratings_text += step + \"\\n\"\n",
    "\n",
    "    step_input_ids = tokenizer(\n",
    "        \"\\n\" + step + \"\\n\",\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        # add_special_tokens=True,\n",
    "        add_special_tokens=False,\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "    step_input_ids = step_input_ids[:, 2:]  # remove \"\\n\"\n",
    "    sample_input_ids = torch.cat((sample_input_ids, step_input_ids), dim=1)\n",
    "\n",
    "    step_labels = torch.ones_like(step_input_ids) * ignore_index\n",
    "    step_labels[:, -2] = rating2token_id[\n",
    "        step_rating[\"rating\"]\n",
    "    ]  # set the label for the last token_id before \"\\n\"\n",
    "    sample_labels = torch.cat((sample_labels, step_labels), dim=1)\n",
    "\n",
    "sample_input_ids = sample_input_ids[:, :-1]\n",
    "sample_labels = sample_labels[:, :-1]\n",
    "problem_step_ratings_text = problem_step_ratings_text.strip()\n",
    "\n",
    "if test:\n",
    "    sample_input_ids_from_simple_call = tokenizer(\n",
    "        problem_step_ratings_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "    assert torch.equal(sample_input_ids, sample_input_ids_from_simple_call), (\n",
    "        sample_input_ids != sample_input_ids_from_simple_call\n",
    "    )\n",
    "\n",
    "attention_mask = torch.ones_like(sample_input_ids)\n",
    "train_sample = {\n",
    "    \"input_ids\": sample_input_ids.flatten(),\n",
    "    \"labels\": sample_labels.flatten(),\n",
    "    \"attention_mask\": attention_mask.flatten(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_tokens:\n",
      "['I', '▁notice', '▁that', '▁the', '▁coefficients', '▁of', '▁the', '▁system', '▁are', '▁multi', 'ples', '▁of', '▁', '3', ',', '▁so', '▁I', '▁can', '▁divide', '▁both', '▁equations', '▁by', '▁', '3', '▁to', '▁simplify', '▁them', '.', '<0x0A>']\n",
      "\n",
      "decoded_tokens:\n",
      "['\\\\', 'begin', '{', 'align', '*}', '<0x0A>', '2', 'x', '-\\\\', 'frac', '{', '4', '}{', '3', '}', 'y', '&', '=\\\\', 'frac', '{', 'a', '}{', '3', '},', '\\\\\\\\', '<0x0A>', '2', 'y', '-', '3', 'x', '▁&=\\\\', 'frac', '{', 'b', '}{', '3', '}.', '<0x0A>', '\\\\', 'end', '{', 'align', '*}', '<0x0A>']\n",
      "\n",
      "decoded_tokens:\n",
      "['I', '▁also', '▁notice', '▁that', '▁the', '▁coefficients', '▁of', '▁$', 'x', '$', '▁and', '▁$', 'y', '$', '▁in', '▁the', '▁second', '▁equation', '▁are', '▁the', '▁neg', 'atives', '▁of', '▁the', '▁coefficients', '▁of', '▁$', 'y', '$', '▁and', '▁$', 'x', '$', '▁in', '▁the', '▁first', '▁equation', ',', '▁respectively', '.', '<0x0A>']\n",
      "\n",
      "decoded_tokens:\n",
      "['<s>', '▁If', '▁the', '▁system', '▁of', '▁equations', '<0x0A>', '<0x0A>', '\\\\', 'begin', '{', 'align', '*}', '<0x0A>', '6', 'x', '-', '4', 'y', '&=', 'a', ',', '\\\\\\\\', '<0x0A>', '6', 'y', '-', '9', 'x', '▁&=', 'b', '.', '<0x0A>', '\\\\', 'end', '{', 'align', '*}', 'has', '▁a', '▁solution', '▁$(', 'x', ',', '▁y', ')$', '▁where', '▁$', 'x', '$', '▁and', '▁$', 'y', '$', '▁are', '▁both', '▁non', 'zero', ',', '▁find', '▁$\\\\', 'frac', '{', 'a', '}{', 'b', '},', '$', '▁assuming', '▁$', 'b', '$', '▁is', '▁non', 'zero', '.', '<0x0A>', 'I', '▁notice', '▁that', '▁the', '▁coefficients', '▁of', '▁the', '▁system', '▁are', '▁multi', 'ples', '▁of', '▁', '3', ',', '▁so', '▁I', '▁can', '▁divide', '▁both', '▁equations', '▁by', '▁', '3', '▁to', '▁simplify', '▁them', '.', '<0x0A>', '\\\\', 'begin', '{', 'align', '*}', '<0x0A>', '2', 'x', '-\\\\', 'frac', '{', '4', '}{', '3', '}', 'y', '&', '=\\\\', 'frac', '{', 'a', '}{', '3', '},', '\\\\\\\\', '<0x0A>', '2', 'y', '-', '3', 'x', '▁&=\\\\', 'frac', '{', 'b', '}{', '3', '}.', '<0x0A>', '\\\\', 'end', '{', 'align', '*}', '<0x0A>', 'I', '▁also', '▁notice', '▁that', '▁the', '▁coefficients', '▁of', '▁$', 'x', '$', '▁and', '▁$', 'y', '$', '▁in', '▁the', '▁second', '▁equation', '▁are', '▁the', '▁neg', 'atives', '▁of', '▁the', '▁coefficients', '▁of', '▁$', 'y', '$', '▁and', '▁$', 'x', '$', '▁in', '▁the', '▁first', '▁equation', ',', '▁respectively', '.']\n",
      "\n",
      "sample_input_ids_from_simple_call:\n",
      "tensor([[    1,   960,   278,  1788,   310, 10693,    13,    13, 29905,   463,\n",
      "         29912,  2520,  4044,    13, 29953, 29916, 29899, 29946, 29891, 20644,\n",
      "         29874, 29892,  1966,    13, 29953, 29891, 29899, 29929, 29916,  7878,\n",
      "         29890, 29889,    13, 29905,   355, 29912,  2520,  4044,  5349,   263,\n",
      "          1650,  2427, 29916, 29892,   343,  1262,   988,   395, 29916, 29938,\n",
      "           322,   395, 29891, 29938,   526,  1716,  1661,  9171, 29892,  1284,\n",
      "           779,  1154, 29912, 29874,  1157, 29890,  1118, 29938, 10241,   395,\n",
      "         29890, 29938,   338,  1661,  9171, 29889,    13, 29902,  8369,   393,\n",
      "           278, 16127,   310,   278,  1788,   526,  2473,  2701,   310, 29871,\n",
      "         29941, 29892,   577,   306,   508, 16429,  1716, 10693,   491, 29871,\n",
      "         29941,   304, 21092,   963, 29889,    13, 29905,   463, 29912,  2520,\n",
      "          4044,    13, 29906, 29916,  2612,  1154, 29912, 29946,  1157, 29941,\n",
      "         29913, 29891, 29987,  2013,  1154, 29912, 29874,  1157, 29941,  1118,\n",
      "          1966,    13, 29906, 29891, 29899, 29941, 29916, 29072,  1154, 29912,\n",
      "         29890,  1157, 29941,  1836,    13, 29905,   355, 29912,  2520,  4044,\n",
      "            13, 29902,   884,  8369,   393,   278, 16127,   310,   395, 29916,\n",
      "         29938,   322,   395, 29891, 29938,   297,   278,  1473,  6306,   526,\n",
      "           278,  3480,  5056,   310,   278, 16127,   310,   395, 29891, 29938,\n",
      "           322,   395, 29916, 29938,   297,   278,   937,  6306, 29892,  8307,\n",
      "         29889]])\n",
      "\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "def simple_text2token_ids(tokenizer, text, **kwargs):\n",
    "    return tokenizer(\n",
    "        text, return_tensors=\"pt\", padding=False, truncation=False, **kwargs\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "\n",
    "def print_decoded_tokens(input_ids):\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    print(\n",
    "        \"decoded_tokens:\",\n",
    "        decoded_tokens,\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "\n",
    "def print_tokenization(tokenizer, text):\n",
    "    # print(\n",
    "    #     f'text:\\n\"\"\"{text}\"\"\"',\n",
    "    #     end=\"\\n\\n\",\n",
    "    # )\n",
    "    # text_prepared_for_tokenization = tokenizer.prepare_for_tokenization(text)\n",
    "    # print(\n",
    "    #     \"text_prepared_for_tokenizations:\",\n",
    "    #     text_prepared_for_tokenization,\n",
    "    #     sep=\"\\n\",\n",
    "    #     end=\"\\n\\n\",\n",
    "    # )\n",
    "    # tokens = tokenizer.tokenize(text, **text_prepared_for_tokenization[-1])\n",
    "    # print(\n",
    "    #     \"tokens:\",\n",
    "    #     tokens,\n",
    "    #     sep=\"\\n\",\n",
    "    #     end=\"\\n\\n\",\n",
    "    # )\n",
    "    # input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # print(\n",
    "    #     \"input_ids:\",\n",
    "    #     input_ids,\n",
    "    #     sep=\"\\n\",\n",
    "    #     end=\"\\n\\n\",\n",
    "    # )\n",
    "    # input_ids_from_call_wo_special_tokens = simple_text2token_ids(\n",
    "    #     tokenizer, text, add_special_tokens=False\n",
    "    # )\n",
    "    # print(\n",
    "    #     \"input_ids_from_call_wo_special_tokens:\",\n",
    "    #     input_ids_from_call_wo_special_tokens[0],\n",
    "    #     sep=\"\\n\",\n",
    "    #     end=\"\\n\\n\",\n",
    "    # )\n",
    "    input_ids_from_simple_call = simple_text2token_ids(tokenizer, text)\n",
    "    print(\n",
    "        \"input_ids_from_simple_call:\",\n",
    "        input_ids_from_simple_call[0],\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "    print_decoded_tokens(input_ids_from_simple_call)\n",
    "\n",
    "\n",
    "rating2word = {1: \"positive\", -1: \"negative\", 0: \"neutral\"}\n",
    "rating2token_id = {\n",
    "    rating: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))[0]\n",
    "    for rating, word in rating2word.items()\n",
    "}\n",
    "\n",
    "problem = reformatted_sample[\"problem\"].strip()\n",
    "problem_step_ratings_text = problem + \"\\n\"\n",
    "sample_input_ids = tokenizer(\n",
    "    problem + \"\\n\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=False,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=False,\n",
    ")[\"input_ids\"]\n",
    "ignore_index = -100\n",
    "sample_labels = torch.ones_like(sample_input_ids) * ignore_index\n",
    "\n",
    "if test:\n",
    "    if test_for[\"common\"]:\n",
    "        print_tokenization(tokenizer, problem_step_ratings_text)\n",
    "        print(\n",
    "            \"sample_labels:\",\n",
    "            sample_labels,\n",
    "            sep=\"\\n\",\n",
    "            end=\"\\n\\n\",\n",
    "        )\n",
    "\n",
    "\n",
    "for step_rating in step_ratings:\n",
    "    step = step_rating[\"step\"].strip()\n",
    "\n",
    "    problem_step_ratings_text += step + \"\\n\"\n",
    "\n",
    "    step_input_ids = tokenizer(\n",
    "        \"\\n\" + step + \"\\n\",\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        # add_special_tokens=True,\n",
    "        add_special_tokens=False,\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "    step_input_ids = step_input_ids[:, 2:]  # remove \"\\n\"\n",
    "    sample_input_ids = torch.cat((sample_input_ids, step_input_ids), dim=1)\n",
    "\n",
    "    step_labels = torch.ones_like(step_input_ids) * ignore_index\n",
    "    step_labels[:, -2] = rating2token_id[\n",
    "        step_rating[\"rating\"]\n",
    "    ]  # set the label for the last token_id before \"\\n\"\n",
    "    sample_labels = torch.cat((sample_labels, step_labels), dim=1)\n",
    "\n",
    "    if test:\n",
    "        if test_for[\"common\"]:\n",
    "            print_tokenization(tokenizer, problem_step_ratings_text)\n",
    "            print(\n",
    "                \"sample_labels:\",\n",
    "                sample_labels,\n",
    "                sep=\"\\n\",\n",
    "                end=\"\\n\\n\",\n",
    "            )\n",
    "        if test_for[\"token_prefix\"]:\n",
    "            print_decoded_tokens(step_input_ids)\n",
    "\n",
    "\n",
    "sample_input_ids = sample_input_ids[:, :-1]\n",
    "sample_labels = sample_labels[:, :-1]\n",
    "problem_step_ratings_text = problem_step_ratings_text.strip()\n",
    "\n",
    "sample_input_ids_from_simple_call = tokenizer(\n",
    "    problem_step_ratings_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=False,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=False,\n",
    ")[\"input_ids\"]\n",
    "\n",
    "\n",
    "if test:\n",
    "    if test_for[\"token_prefix\"]:\n",
    "        sample_input_ids_from_simple_call = tokenizer(\n",
    "            problem_step_ratings_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=False,\n",
    "        )[\"input_ids\"]\n",
    "        print_decoded_tokens(sample_input_ids_from_simple_call)\n",
    "    if test_for[\"common\"]:\n",
    "        print(\n",
    "            \"sample_input_ids:\",\n",
    "            sample_input_ids,\n",
    "            sep=\"\\n\",\n",
    "            end=\"\\n\\n\",\n",
    "        )\n",
    "        print(\n",
    "            \"sample_labels:\",\n",
    "            sample_labels,\n",
    "            sep=\"\\n\",\n",
    "            end=\"\\n\\n\",\n",
    "        )\n",
    "    if test_for[\"equal\"]:\n",
    "        print(\n",
    "            \"sample_input_ids_from_simple_call:\",\n",
    "            sample_input_ids_from_simple_call,\n",
    "            sep=\"\\n\",\n",
    "            end=\"\\n\\n\",\n",
    "        )\n",
    "\n",
    "        def print_difference(input_ids, different_mask):\n",
    "            difference = input_ids * different_mask\n",
    "            print(difference)\n",
    "            print(tokenizer.convert_ids_to_tokens(difference[0]))\n",
    "\n",
    "        def print_difference_between(input_ids_pair):\n",
    "            different_mask = input_ids_pair[0] != input_ids_pair[1]\n",
    "            for input_ids in input_ids_pair:\n",
    "                print_difference(input_ids, different_mask)\n",
    "\n",
    "        print_difference_between((sample_input_ids, sample_input_ids_from_simple_call))\n",
    "\n",
    "        assert torch.equal(sample_input_ids, sample_input_ids_from_simple_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁positive']\n",
      "0\n",
      "['▁negative']\n",
      "22198\n",
      "['▁neutral']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# debug: rating tokens\n",
    "for rating_token in rating2token.values():\n",
    "    print(tokenizer.tokenize(rating_token))\n",
    "    print(tokenizer.convert_tokens_to_ids(rating_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   960,   278,  1788,   310, 10693,    13,    13, 29905,   463,\n",
       "         29912,  2520,  4044,    13, 29953, 29916, 29899, 29946, 29891, 20644,\n",
       "         29874, 29892,  1966,    13, 29953, 29891, 29899, 29929, 29916,  7878,\n",
       "         29890, 29889,    13, 29905,   355, 29912,  2520,  4044,  5349,   263,\n",
       "          1650,  2427, 29916, 29892,   343,  1262,   988,   395, 29916, 29938,\n",
       "           322,   395, 29891, 29938,   526,  1716,  1661,  9171, 29892,  1284,\n",
       "           779,  1154, 29912, 29874,  1157, 29890,  1118, 29938, 10241,   395,\n",
       "         29890, 29938,   338,  1661,  9171, 29889,    13, 29902,  8369,   393,\n",
       "           278, 16127,   310,   278,  1788,   526,  2473,  2701,   310, 29871,\n",
       "         29941, 29892,   577,   306,   508, 16429,  1716, 10693,   491, 29871,\n",
       "         29941,   304, 21092,   963, 29889,    13, 29905,   463, 29912,  2520,\n",
       "          4044,    13, 29906, 29916,  2612,  1154, 29912, 29946,  1157, 29941,\n",
       "         29913, 29891, 29987,  2013,  1154, 29912, 29874,  1157, 29941,  1118,\n",
       "          1966,    13, 29906, 29891, 29899, 29941, 29916, 29072,  1154, 29912,\n",
       "         29890,  1157, 29941,  1836,    13, 29905,   355, 29912,  2520,  4044,\n",
       "            13, 29902,   884,  8369,   393,   278, 16127,   310,   395, 29916,\n",
       "         29938,   322,   395, 29891, 29938,   297,   278,  1473,  6306,   526,\n",
       "           278,  3480,  5056,   310,   278, 16127,   310,   395, 29891, 29938,\n",
       "           322,   395, 29916, 29938,   297,   278,   937,  6306, 29892,  8307,\n",
       "         29889,    13]])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug: return\n",
    "tokenizer(\n",
    "    problem_step_ratings_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=False,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁If', '▁the', '▁system', '▁of', '▁equations', '<0x0A>', '<0x0A>', '\\\\', 'begin', '{', 'align', '*}', '<0x0A>', '6', 'x', '-', '4', 'y', '&=', 'a', ',', '\\\\\\\\', '<0x0A>', '6', 'y', '-', '9', 'x', '▁&=', 'b', '.', '<0x0A>', '\\\\', 'end', '{', 'align', '*}', 'has', '▁a', '▁solution', '▁$(', 'x', ',', '▁y', ')$', '▁where', '▁$', 'x', '$', '▁and', '▁$', 'y', '$', '▁are', '▁both', '▁non', 'zero', ',', '▁find', '▁$\\\\', 'frac', '{', 'a', '}{', 'b', '},', '$', '▁assuming', '▁$', 'b', '$', '▁is', '▁non', 'zero', '.']\n",
      "['▁rating']\n",
      "['▁something', '<0x0A>']\n",
      "<0x0A>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mtokenize(\u001b[39m\"\u001b[39m\u001b[39msomething\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mtokenize(\u001b[39m\"\u001b[39m\u001b[39msomething\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])  \u001b[39m# ['▁something', '<0x0A>']\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mprint\u001b[39m(tokenizer(\u001b[39m\"\u001b[39;49m\u001b[39msomething\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])  \u001b[39m# ['▁something', '<0x0A>']\u001b[39;00m\n",
      "File \u001b[0;32m/data/tongyx361/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:247\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m {key: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[key][item] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid key. Only three types of key are available: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "\n",
    "print(\n",
    "    tokenizer.tokenize(\n",
    "        \"If the system of equations\\n\\n\\\\begin{align*}\\n6x-4y&=a,\\\\\\\\\\n6y-9x &=b.\\n\\\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\\\frac{a}{b},$ assuming $b$ is nonzero.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(tokenizer.tokenize(\"rating\"))\n",
    "print(tokenizer.tokenize(\"something\\n\"))\n",
    "print(tokenizer.tokenize(\"something\\n\")[-1])  # ['▁something', '<0x0A>']\n",
    "print(tokenizer(\"something\\n\")[] [-1])  # ['▁something', '<0x0A>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      "\"\"\"If the system of equations\n",
      "\n",
      "\\begin{align*}\n",
      "6x-4y&=a,\\\\\n",
      "6y-9x &=b.\n",
      "\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\frac{a}{b},$ assuming $b$ is nonzero.\n",
      "\"\"\"\n",
      "\n",
      "input_ids:\n",
      "tensor([    1,   960,   278,  1788,   310, 10693,    13,    13, 29905,   463,\n",
      "        29912,  2520,  4044,    13, 29953, 29916, 29899, 29946, 29891, 20644,\n",
      "        29874, 29892,  1966,    13, 29953, 29891, 29899, 29929, 29916,  7878,\n",
      "        29890, 29889,    13, 29905,   355, 29912,  2520,  4044,  5349,   263,\n",
      "         1650,  2427, 29916, 29892,   343,  1262,   988,   395, 29916, 29938,\n",
      "          322,   395, 29891, 29938,   526,  1716,  1661,  9171, 29892,  1284,\n",
      "          779,  1154, 29912, 29874,  1157, 29890,  1118, 29938, 10241,   395,\n",
      "        29890, 29938,   338,  1661,  9171, 29889,    13])\n",
      "\n",
      "tokens:\n",
      "['<s>', '▁If', '▁the', '▁system', '▁of', '▁equations', '<0x0A>', '<0x0A>', '\\\\', 'begin', '{', 'align', '*}', '<0x0A>', '6', 'x', '-', '4', 'y', '&=', 'a', ',', '\\\\\\\\', '<0x0A>', '6', 'y', '-', '9', 'x', '▁&=', 'b', '.', '<0x0A>', '\\\\', 'end', '{', 'align', '*}', 'has', '▁a', '▁solution', '▁$(', 'x', ',', '▁y', ')$', '▁where', '▁$', 'x', '$', '▁and', '▁$', 'y', '$', '▁are', '▁both', '▁non', 'zero', ',', '▁find', '▁$\\\\', 'frac', '{', 'a', '}{', 'b', '},', '$', '▁assuming', '▁$', 'b', '$', '▁is', '▁non', 'zero', '.', '<0x0A>']\n",
      "\n",
      "text:\n",
      "\"\"\"If the system of equations\n",
      "\n",
      "\\begin{align*}\n",
      "6x-4y&=a,\\\\\n",
      "6y-9x &=b.\n",
      "\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\frac{a}{b},$ assuming $b$ is nonzero.\n",
      "I notice that the coefficients of the system are multiples of 3, so I can divide both equations by 3 to simplify them.\n",
      "\\begin{align*}\n",
      "2x-\\frac{4}{3}y&=\\frac{a}{3},\\\\\n",
      "2y-3x &=\\frac{b}{3}.\n",
      "\\end{align*}\n",
      "I also notice that the coefficients of $x$ and $y$ in the second equation are the negatives of the coefficients of $y$ and $x$ in the first equation, respectively.\n",
      "\"\"\"\n",
      "\n",
      "input_ids:\n",
      "tensor([    1,   960,   278,  1788,   310, 10693,    13,    13, 29905,   463,\n",
      "        29912,  2520,  4044,    13, 29953, 29916, 29899, 29946, 29891, 20644,\n",
      "        29874, 29892,  1966,    13, 29953, 29891, 29899, 29929, 29916,  7878,\n",
      "        29890, 29889,    13, 29905,   355, 29912,  2520,  4044,  5349,   263,\n",
      "         1650,  2427, 29916, 29892,   343,  1262,   988,   395, 29916, 29938,\n",
      "          322,   395, 29891, 29938,   526,  1716,  1661,  9171, 29892,  1284,\n",
      "          779,  1154, 29912, 29874,  1157, 29890,  1118, 29938, 10241,   395,\n",
      "        29890, 29938,   338,  1661,  9171, 29889,    13, 29902,  8369,   393,\n",
      "          278, 16127,   310,   278,  1788,   526,  2473,  2701,   310, 29871,\n",
      "        29941, 29892,   577,   306,   508, 16429,  1716, 10693,   491, 29871,\n",
      "        29941,   304, 21092,   963, 29889,    13, 29905,   463, 29912,  2520,\n",
      "         4044,    13, 29906, 29916,  2612,  1154, 29912, 29946,  1157, 29941,\n",
      "        29913, 29891, 29987,  2013,  1154, 29912, 29874,  1157, 29941,  1118,\n",
      "         1966,    13, 29906, 29891, 29899, 29941, 29916, 29072,  1154, 29912,\n",
      "        29890,  1157, 29941,  1836,    13, 29905,   355, 29912,  2520,  4044,\n",
      "           13, 29902,   884,  8369,   393,   278, 16127,   310,   395, 29916,\n",
      "        29938,   322,   395, 29891, 29938,   297,   278,  1473,  6306,   526,\n",
      "          278,  3480,  5056,   310,   278, 16127,   310,   395, 29891, 29938,\n",
      "          322,   395, 29916, 29938,   297,   278,   937,  6306, 29892,  8307,\n",
      "        29889,    13])\n",
      "\n",
      "tokens:\n",
      "['<s>', '▁If', '▁the', '▁system', '▁of', '▁equations', '<0x0A>', '<0x0A>', '\\\\', 'begin', '{', 'align', '*}', '<0x0A>', '6', 'x', '-', '4', 'y', '&=', 'a', ',', '\\\\\\\\', '<0x0A>', '6', 'y', '-', '9', 'x', '▁&=', 'b', '.', '<0x0A>', '\\\\', 'end', '{', 'align', '*}', 'has', '▁a', '▁solution', '▁$(', 'x', ',', '▁y', ')$', '▁where', '▁$', 'x', '$', '▁and', '▁$', 'y', '$', '▁are', '▁both', '▁non', 'zero', ',', '▁find', '▁$\\\\', 'frac', '{', 'a', '}{', 'b', '},', '$', '▁assuming', '▁$', 'b', '$', '▁is', '▁non', 'zero', '.', '<0x0A>', 'I', '▁notice', '▁that', '▁the', '▁coefficients', '▁of', '▁the', '▁system', '▁are', '▁multi', 'ples', '▁of', '▁', '3', ',', '▁so', '▁I', '▁can', '▁divide', '▁both', '▁equations', '▁by', '▁', '3', '▁to', '▁simplify', '▁them', '.', '<0x0A>', '\\\\', 'begin', '{', 'align', '*}', '<0x0A>', '2', 'x', '-\\\\', 'frac', '{', '4', '}{', '3', '}', 'y', '&', '=\\\\', 'frac', '{', 'a', '}{', '3', '},', '\\\\\\\\', '<0x0A>', '2', 'y', '-', '3', 'x', '▁&=\\\\', 'frac', '{', 'b', '}{', '3', '}.', '<0x0A>', '\\\\', 'end', '{', 'align', '*}', '<0x0A>', 'I', '▁also', '▁notice', '▁that', '▁the', '▁coefficients', '▁of', '▁$', 'x', '$', '▁and', '▁$', 'y', '$', '▁in', '▁the', '▁second', '▁equation', '▁are', '▁the', '▁neg', 'atives', '▁of', '▁the', '▁coefficients', '▁of', '▁$', 'y', '$', '▁and', '▁$', 'x', '$', '▁in', '▁the', '▁first', '▁equation', ',', '▁respectively', '.', '<0x0A>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# debug: whether concatenated token_ids equals to the original token_ids\n",
    "\n",
    "\n",
    "problem = reformatted_sample[\"problem\"].strip()\n",
    "problem_step_ratings_text = problem + \"\\n\"\n",
    "sample_input_ids = simple_text2token_ids(tokenizer, problem_step_ratings_text)\n",
    "sample_rating_idx = []\n",
    "\n",
    "if test:\n",
    "    print_tokenization(tokenizer, problem_step_ratings_text)\n",
    "\n",
    "for step_rating in step_ratings:\n",
    "    step = step_rating[\"step\"].strip()\n",
    "    step\n",
    "\n",
    "    problem_step_ratings_text += step + \"\\n\"\n",
    "\n",
    "\n",
    "sample_input_ids = simple_text2token_ids(tokenizer, problem_step_ratings_text)\n",
    "if test:\n",
    "    print_tokenization(tokenizer, problem_step_ratings_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      "\"\"\"\n",
      "a\n",
      "a\n",
      "\n",
      "a\n",
      "\"\"\"\n",
      "\n",
      "input_ids:\n",
      "tensor([    1, 29871,    13, 29874,    13, 29874,    13,    13, 29874,    13])\n",
      "\n",
      "tokens:\n",
      "['<s>', '▁', '<0x0A>', 'a', '<0x0A>', 'a', '<0x0A>', '<0x0A>', 'a', '<0x0A>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dehug: whether newline at the beginning will be removed\n",
    "text = \"a\"\n",
    "text_in_newline = \"\\n\" + text + \"\\n\" + text + \"\\n\\n\" + text + \"\\n\"\n",
    "sample_input_ids = simple_text2token_ids(tokenizer, text_in_newline)\n",
    "sample_rating_idx = []\n",
    "\n",
    "if test:\n",
    "    print_tokenization(text_in_newline, sample_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_collator(raw_dataset_json_path):\n",
    "    # load\n",
    "    # if list\n",
    "    \n",
    "    \n",
    "    # filter\n",
    "    \n",
    "    # reformat\n",
    "    \n",
    "    # encode\n",
    "    \n",
    "    # sample check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
