{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-13 17:07:19,774] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/data/users/zhangjunlei/tyx/reward-by-prm800k/src/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, GenerationConfig\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 17:07:20.802 [INFO] test\n",
      "2023-09-13 17:07:20.802 [INFO] test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Logger __main__ (INFO)>\n"
     ]
    }
   ],
   "source": [
    "utils.init_logging()\n",
    "logger = utils.get_logger(__name__)\n",
    "logger.info(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.set_gpu_ids([6])\n",
    "gpu_id = \"7\"\n",
    "device = f\"cuda:{gpu_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_tokens = <unk><s></s>\u0000\u0001\n",
      "tokenizer.vocab_size = 32000\n",
      "len(tokenizer.vocab) = 32000\n",
      "tokenizer.special_tokens_map = {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "num_added_tokens = 1\n",
      "head_tokens = <unk><s></s>\u0000\u0001\n",
      "tokenizer.vocab_size = 32000\n",
      "len(tokenizer.vocab) = 32001\n",
      "tokenizer.special_tokens_map = {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "LlamaTokenizerFast(name_or_path='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "tokenizer = utils.get_complete_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cedf8d90b544188d9ad6838be5b8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32001, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
      ")\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    utils.default_7b_model_path, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(model)\n",
    "print(model.config)\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_print(*arg, **kwargs):\n",
    "    print(*arg, **kwargs, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "def generate(prompt_or_list, generation_config):\n",
    "    if not isinstance(generation_config, GenerationConfig):\n",
    "        if isinstance(generation_config, dict):\n",
    "            generation_config = GenerationConfig.from_dict(generation_config)\n",
    "\n",
    "    inputs = tokenizer(prompt_or_list, padding=True, return_tensors=\"pt\").to(device)\n",
    "    sparse_print(\"inputs:\")\n",
    "    sparse_print(inputs)\n",
    "\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    sparse_print(outputs)\n",
    "\n",
    "    if not generation_config.return_dict_in_generate:\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        sparse_print(decoded_outputs)\n",
    "\n",
    "        return outputs\n",
    "    else:  # generation_config.return_dict_in_generate == True\n",
    "        outputs_len = outputs[\"sequences\"].shape[0]\n",
    "\n",
    "        example_idx = random.randint(0, outputs_len - 1)\n",
    "        sparse_print(f\"example_idx = {example_idx}\")\n",
    "\n",
    "        if outputs_len > 5:\n",
    "            example_decoded_output = tokenizer.decode(\n",
    "                outputs[\"sequences\"][example_idx], skip_special_tokens=True\n",
    "            )\n",
    "            sparse_print(example_decoded_output)\n",
    "        else:  # outputs_len <= 5\n",
    "            decoded_outputs = tokenizer.batch_decode(\n",
    "                outputs[\"sequences\"], skip_special_tokens=True\n",
    "            )\n",
    "            sparse_print(decoded_outputs)\n",
    "\n",
    "        if generation_config.output_scores:\n",
    "            example_scores = outputs[\"scores\"][example_idx]\n",
    "            sparse_print(example_scores.shape)\n",
    "            sparse_print(example_scores)\n",
    "\n",
    "            example_probs = torch.nn.functional.softmax(example_scores, dim=-1)\n",
    "            sparse_print(example_probs[example_idx])\n",
    "            sparse_print(sum(example_probs[example_idx]))\n",
    "\n",
    "            # slow cpu\n",
    "            # for idx, prob in enumerate(first_probs[0]):\n",
    "            #     if prob > 1e-5:\n",
    "            #         print(f\"token_id: {idx}, token: {tokenizer.decode([idx])}, prob: {prob}\")\n",
    "\n",
    "            # fast gpu\n",
    "            # non_zero_idxs = torch.gt(first_probs.view(-1),1e-1)\n",
    "            threshold = 0.1\n",
    "            mask = example_probs > threshold\n",
    "            non_zero_idxs = torch.nonzero(mask)\n",
    "            sparse_print(non_zero_idxs)\n",
    "            non_zero_vals = example_probs.view(-1)[non_zero_idxs]\n",
    "            sparse_print(non_zero_vals)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "\n",
      "{'input_ids': tensor([[32000, 32000,     1, 15043, 29892,   590,  1024,   338],\n",
      "        [    1,   450,  6673,   310,   278,  3303,  3900,   338],\n",
      "        [32000, 32000,     1,   450,  7483,   310,  3444,   338],\n",
      "        [32000,     1,   450,  5434,   310,   319, 29902,   338]],\n",
      "       device='cuda:7'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1]], device='cuda:7')}\n",
      "\n",
      "SampleDecoderOnlyOutput(sequences=tensor([[32000, 32000,     1, 15043, 29892,   590,  1024,   338,   476,  2486,\n",
      "         29876, 29889,   306,   626,   263, 20183,   472,  8068],\n",
      "        [    1,   450,  6673,   310,   278,  3303,  3900,   338,   278,  1556,\n",
      "         13988,  2022,   297,   278,  3186, 29889,  2193, 29915],\n",
      "        [32000, 32000,     1,   450,  7483,   310,  3444,   338,   697,   310,\n",
      "           278,  1556,  9560, 14368,   297,   278,  3186, 29889],\n",
      "        [32000,     1,   450,  5434,   310,   319, 29902,   338, 11785, 29889,\n",
      "           450,  9999,   363,   319, 29902,   338, 15678,   472]],\n",
      "       device='cuda:7'), scores=None, attentions=None, hidden_states=None)\n",
      "\n",
      "example_idx = 2\n",
      "\n",
      "['Hello, my name is Katelyn. I am a junior at Central', \"The president of the United States is the most powerful person in the world. That'\", 'The capital of France is one of the most beautiful cities in the world.', 'The future of AI is bright. The market for AI is growing at']\n",
      "\n",
      "Prompt    : Hello, my name is\n",
      "Generation:  Katelyn. I am a junior at Central\n",
      "\n",
      "Prompt    : The president of the United States is\n",
      "Generation:  the most powerful person in the world. That'\n",
      "\n",
      "Prompt    : The capital of France is\n",
      "Generation:  one of the most beautiful cities in the world.\n",
      "\n",
      "Prompt    : The future of AI is\n",
      "Generation:  bright. The market for AI is growing at\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "generation_config = {\n",
    "    \"do_sample\": True,\n",
    "    # \"temperature\": 0.8,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"return_dict_in_generate\": True,\n",
    "}\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "outputs = generate(prompt_or_list=prompts, generation_config=generation_config)\n",
    "\n",
    "decoded_outputs = tokenizer.batch_decode(**outputs, skip_special_tokens=True)\n",
    "\n",
    "for prompt, decoded_output in zip(prompts, decoded_outputs):\n",
    "    print(f\"Prompt    : {prompt}\")\n",
    "    print(f\"Generation: {decoded_output[len(prompt):]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# print(generation_config.return_dict_in_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6374], [8178], [21104]]\n"
     ]
    }
   ],
   "source": [
    "problem = \"A $90^\\\\circ$ rotation around the origin in the counter-clockwise direction is applied to $7 + 2i.$  What is the resulting complex number?\"\n",
    "steps = [\n",
    "    \"I know that a $90^\\\\circ$ rotation around the origin in the complex plane can be achieved by multiplying the complex number by $i$, since $i$ has a magnitude of 1 and an argument of $90^\\\\circ$.\",\n",
    "    \"So, I can write the rotation as $(7 + 2i)i = 7i + 2i^2.$\",\n",
    "    \"To simplify this expression, I recall that $i^2 = -1$, so I can substitute that and get $7i - 2.$\",\n",
    "    \"This is the resulting complex number after the rotation.\\n\\n# Answer\\n\\n7i - 2\",\n",
    "]\n",
    "\n",
    "rating2word = {1: \"positive\", -1: \"negative\", 0: \"neutral\"}\n",
    "rating_words = list(rating2word.values())\n",
    "rating_token_ids = tokenizer(rating_words, add_special_tokens=False).input_ids\n",
    "print(rating_token_ids)\n",
    "\n",
    "# constraint = transformers.Constraint(type=\"must_include\", token=\"apple\")\n",
    "\n",
    "# config\n",
    "generation_config = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"force_words_ids\": rating_token_ids,\n",
    "    \"num_beams\": 2,\n",
    "    \"remove_invalid_values\": True,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_solution_hierchical_samples = utils.load_pickle(\"/data/users/zhangjunlei/tyx/reward-by-prm800k/datasets/problem-solution-hierarchical-samples.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = {\n",
    "    # \"sample\": True,\n",
    "    # \"prompt\": True,\n",
    "    # \"problem\": True,\n",
    "    # \"step\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem2steps_prompts = defaultdict(list)\n",
    "\n",
    "for problem_solution_samples in problem_solution_hierchical_samples:\n",
    "    problem = problem_solution_samples[\"problem\"]\n",
    "    for solution_sample in problem_solution_samples[\"solutions\"]:\n",
    "        if debug.get(\"sample\"):\n",
    "            print(solution_sample)\n",
    "            raise Exception\n",
    "        # reformatted_sample = utils.reformat_prm800k_sample(solution_sample) # different format\n",
    "        # prompt = problem\n",
    "        step_prompt = \"\\n\".join(solution_sample[\"steps\"])\n",
    "        problem2steps_prompts[problem].append(step_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2457624929.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"problem_ids\": None\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "problem_step_ids_list = []\n",
    "for problem, step_prompts in tqdm(problem2steps_prompts.items()):\n",
    "    problem_step_ids = {\n",
    "        \"problem_ids\": None,\n",
    "        \"step_prompt_ids\": []\n",
    "    }\n",
    "    problem_ids = tokenizer(problem + \"\\n\", add_special_tokens=True).input_ids\n",
    "    problem_step_ids[\"problem_ids\"] = problem_ids\n",
    "    if debug.get(\"problem\"):\n",
    "        print(problem)\n",
    "        print(problem_ids)\n",
    "        raise Exception\n",
    "    for step_prompt in step_prompts:\n",
    "        step_prompt_ids = tokenizer(step_prompt, add_special_tokens=False).input_ids\n",
    "        problem_step_ids[\"step_prompt_ids\"].append(step_prompt_ids)\n",
    "        if debug.get(\"step\"):\n",
    "            print(step_prompt)\n",
    "            print(step_prompt_ids)\n",
    "            raise Exception\n",
    "        \n",
    "print(len(problem_step_ids_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "# validation_set_path = os.path.join(utils.project_root, \"datasets\", \"prm800k-002validation-seed42.jsonl\")\n",
    "# validation_set = utils.load_jsonl(validation_set_path)\n",
    "# print(len(validation_set))\n",
    "# print(validation_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "\n",
      "{'input_ids': tensor([[    1,  2803,   395, 29888, 29898, 29916,  3892, 29989, 29916, 29899,\n",
      "         29886, 29989, 29974, 29989, 29916, 29899, 29896, 29945, 29989, 29974,\n",
      "         29989, 29916, 29899, 29886, 29899, 29896, 29945, 29989,  8209,   988,\n",
      "           395, 29900,   529,   282,   529, 29871, 29896, 29945,  7449,  5953,\n",
      "           837,   457,   278,  9212,   995,  4586,   491,   395, 29888, 29898,\n",
      "         29916,  1262,   363,   395, 29916, 29938,   297,   278,  7292,   395,\n",
      "         29886,   320,  3797,   921, 29905,  3797, 29896, 29945,  7449, 29902,\n",
      "          1073,   393,   263,   395, 29929, 29900,  3823,  6034, 29938, 13733,\n",
      "          2820,   278,  3978,   297,   278,  4280, 10694,   508,   367, 14363,\n",
      "           491,  6674,  5890,   278,  4280,  1353,   491,   395, 29875,  1628,\n",
      "          1951,   395, 29875, 29938,   756,   263, 18497,   310, 29871, 29896,\n",
      "           322,   385,  2980,   310,   395, 29929, 29900,  3823,  6034,  1504]],\n",
      "       device='cuda:7'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:7')}\n",
      "\n",
      "BeamSearchDecoderOnlyOutput(sequences=tensor([[    1,  2803,   395, 29888, 29898, 29916,  3892, 29989, 29916, 29899,\n",
      "         29886, 29989, 29974, 29989, 29916, 29899, 29896, 29945, 29989, 29974,\n",
      "         29989, 29916, 29899, 29886, 29899, 29896, 29945, 29989,  8209,   988,\n",
      "           395, 29900,   529,   282,   529, 29871, 29896, 29945,  7449,  5953,\n",
      "           837,   457,   278,  9212,   995,  4586,   491,   395, 29888, 29898,\n",
      "         29916,  1262,   363,   395, 29916, 29938,   297,   278,  7292,   395,\n",
      "         29886,   320,  3797,   921, 29905,  3797, 29896, 29945,  7449, 29902,\n",
      "          1073,   393,   263,   395, 29929, 29900,  3823,  6034, 29938, 13733,\n",
      "          2820,   278,  3978,   297,   278,  4280, 10694,   508,   367, 14363,\n",
      "           491,  6674,  5890,   278,  4280,  1353,   491,   395, 29875,  1628,\n",
      "          1951,   395, 29875, 29938,   756,   263, 18497,   310, 29871, 29896,\n",
      "           322,   385,  2980,   310,   395, 29929, 29900,  3823,  6034,  1504,\n",
      "           306]], device='cuda:7'), sequences_scores=tensor([-0.0179], device='cuda:7'), scores=(tensor([[-2.3552e+01, -2.6371e+01, -6.0167e+00,  ..., -1.7168e+01,\n",
      "         -1.7575e+01, -1.5908e+01],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09]], device='cuda:7'),), beam_indices=None, attentions=None, hidden_states=None)\n",
      "\n",
      "example_idx = 0\n",
      "\n",
      "['Let $f(x)=|x-p|+|x-15|+|x-p-15|,$ where $0 < p < 15.$ Determine the minimum value taken by $f(x)$ for $x$ in the interval $p \\\\leq x\\\\leq15.$I know that a $90^\\\\circ$ rotation around the origin in the complex plane can be achieved by multiplying the complex number by $i$, since $i$ has a magnitude of 1 and an argument of $90^\\\\circ$. I']\n",
      "\n",
      "torch.Size([2, 32001])\n",
      "\n",
      "tensor([[-2.3552e+01, -2.6371e+01, -6.0167e+00,  ..., -1.7168e+01,\n",
      "         -1.7575e+01, -1.5908e+01],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09]], device='cuda:7')\n",
      "\n",
      "tensor([5.9083e-11, 3.5271e-12, 2.4377e-03,  ..., 3.5014e-08, 2.3299e-08,\n",
      "        1.2340e-07], device='cuda:7')\n",
      "\n",
      "tensor(0.9999, device='cuda:7')\n",
      "\n",
      "tensor([[   0,  306],\n",
      "        [   0, 1105]], device='cuda:7')\n",
      "\n",
      "tensor([[5.9083e-11, 1.1496e-01],\n",
      "        [5.9083e-11, 1.0577e-01]], device='cuda:7')\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BeamSearchDecoderOnlyOutput(sequences=tensor([[    1,  2803,   395, 29888, 29898, 29916,  3892, 29989, 29916, 29899,\n",
       "         29886, 29989, 29974, 29989, 29916, 29899, 29896, 29945, 29989, 29974,\n",
       "         29989, 29916, 29899, 29886, 29899, 29896, 29945, 29989,  8209,   988,\n",
       "           395, 29900,   529,   282,   529, 29871, 29896, 29945,  7449,  5953,\n",
       "           837,   457,   278,  9212,   995,  4586,   491,   395, 29888, 29898,\n",
       "         29916,  1262,   363,   395, 29916, 29938,   297,   278,  7292,   395,\n",
       "         29886,   320,  3797,   921, 29905,  3797, 29896, 29945,  7449, 29902,\n",
       "          1073,   393,   263,   395, 29929, 29900,  3823,  6034, 29938, 13733,\n",
       "          2820,   278,  3978,   297,   278,  4280, 10694,   508,   367, 14363,\n",
       "           491,  6674,  5890,   278,  4280,  1353,   491,   395, 29875,  1628,\n",
       "          1951,   395, 29875, 29938,   756,   263, 18497,   310, 29871, 29896,\n",
       "           322,   385,  2980,   310,   395, 29929, 29900,  3823,  6034,  1504,\n",
       "           306]], device='cuda:7'), sequences_scores=tensor([-0.0179], device='cuda:7'), scores=(tensor([[-2.3552e+01, -2.6371e+01, -6.0167e+00,  ..., -1.7168e+01,\n",
       "         -1.7575e+01, -1.5908e+01],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09]], device='cuda:7'),), beam_indices=None, attentions=None, hidden_states=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(problem + steps[0], generation_config=generation_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
