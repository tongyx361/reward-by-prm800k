{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/data/tongyx361/reward-by-prm800k/src/utils.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, GenerationConfig\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 07:49:59.035 [INFO] test\n",
      "2023-09-13 07:49:59.035 [INFO] test\n",
      "2023-09-13 07:49:59.035 [INFO] test\n",
      "2023-09-13 07:49:59.035 [INFO] test\n",
      "2023-09-13 07:49:59.035 [INFO] test\n",
      "2023-09-13 07:49:59.035 [INFO] test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Logger __main__ (INFO)>\n"
     ]
    }
   ],
   "source": [
    "utils.init_logging()\n",
    "logger = utils.get_logger(__name__)\n",
    "logger.info(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ[\"CUDA_VISIBLE_DEVICES\"] = 6\n"
     ]
    }
   ],
   "source": [
    "# utils.set_gpu_ids([6])\n",
    "gpu_id = \"6\"\n",
    "device = f\"cuda:{gpu_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_tokens = <unk><s></s>\u0000\u0001\n",
      "tokenizer.vocab_size = 32000\n",
      "len(tokenizer.vocab) = 32000\n",
      "tokenizer.special_tokens_map = {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "num_added_tokens = 1\n",
      "head_tokens = <unk><s></s>\u0000\u0001\n",
      "tokenizer.vocab_size = 32000\n",
      "len(tokenizer.vocab) = 32001\n",
      "tokenizer.special_tokens_map = {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "LlamaTokenizerFast(name_or_path='/data/tongyx361/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "tokenizer = utils.get_complete_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ceea0b756c5484895fd72721e75a52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 23.06 GiB already allocated; 3.69 MiB free; 23.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/data/tongyx361/reward-by-prm800k/src/hf-generate-debug.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B120.92.108.162/data/tongyx361/reward-by-prm800k/src/hf-generate-debug.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m LlamaForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B120.92.108.162/data/tongyx361/reward-by-prm800k/src/hf-generate-debug.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     utils\u001b[39m.\u001b[39;49mdefault_7b_model_path, low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B120.92.108.162/data/tongyx361/reward-by-prm800k/src/hf-generate-debug.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m )\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B120.92.108.162/data/tongyx361/reward-by-prm800k/src/hf-generate-debug.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m embedding_size \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_input_embeddings()\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B120.92.108.162/data/tongyx361/reward-by-prm800k/src/hf-generate-debug.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tokenizer) \u001b[39m>\u001b[39m embedding_size:\n",
      "File \u001b[0;32m/data/tongyx361/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:2053\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2048\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2049\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2050\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2051\u001b[0m     )\n\u001b[1;32m   2052\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2053\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/tongyx361/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/data/tongyx361/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/tongyx361/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/data/tongyx361/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/tongyx361/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/data/tongyx361/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 23.06 GiB already allocated; 3.69 MiB free; 23.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    utils.default_7b_model_path, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(model)\n",
    "print(model.config)\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_print(*arg, **kwargs):\n",
    "    print(*arg, **kwargs, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "def generate(prompt_or_list, generation_config):\n",
    "    if not isinstance(generation_config, GenerationConfig):\n",
    "        if isinstance(generation_config, dict):\n",
    "            generation_config = GenerationConfig.from_dict(generation_config)\n",
    "\n",
    "    inputs = tokenizer(prompt_or_list, padding=True, return_tensors=\"pt\").to(device)\n",
    "    sparse_print(\"inputs:\")\n",
    "    sparse_print(inputs)\n",
    "\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    sparse_print(outputs)\n",
    "\n",
    "    if not generation_config.return_dict_in_generate:\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        sparse_print(decoded_outputs)\n",
    "\n",
    "        return outputs\n",
    "    else:  # generation_config.return_dict_in_generate == True\n",
    "        outputs_len = outputs[\"sequences\"].shape[0]\n",
    "\n",
    "        example_idx = random.randint(0, outputs_len - 1)\n",
    "        sparse_print(f\"example_idx = {example_idx}\")\n",
    "\n",
    "        if outputs_len > 5:\n",
    "            example_decoded_output = tokenizer.decode(\n",
    "                outputs[\"sequences\"][example_idx], skip_special_tokens=True\n",
    "            )\n",
    "            sparse_print(example_decoded_output)\n",
    "        else:  # outputs_len <= 5\n",
    "            decoded_outputs = tokenizer.batch_decode(\n",
    "                outputs[\"sequences\"], skip_special_tokens=True\n",
    "            )\n",
    "            sparse_print(decoded_outputs)\n",
    "\n",
    "        if generation_config.output_scores:\n",
    "            example_scores = outputs[\"scores\"][example_idx]\n",
    "            sparse_print(example_scores.shape)\n",
    "            sparse_print(example_scores)\n",
    "\n",
    "            example_probs = torch.nn.functional.softmax(example_scores, dim=-1)\n",
    "            sparse_print(example_probs[example_idx])\n",
    "            sparse_print(sum(example_probs[example_idx]))\n",
    "\n",
    "            # slow cpu\n",
    "            # for idx, prob in enumerate(first_probs[0]):\n",
    "            #     if prob > 1e-5:\n",
    "            #         print(f\"token_id: {idx}, token: {tokenizer.decode([idx])}, prob: {prob}\")\n",
    "\n",
    "            # fast gpu\n",
    "            # non_zero_idxs = torch.gt(first_probs.view(-1),1e-1)\n",
    "            threshold = 0.1\n",
    "            mask = example_probs > threshold\n",
    "            non_zero_idxs = torch.nonzero(mask)\n",
    "            sparse_print(non_zero_idxs)\n",
    "            non_zero_vals = example_probs.view(-1)[non_zero_idxs]\n",
    "            sparse_print(non_zero_vals)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "\n",
      "{'input_ids': tensor([[    1, 15043, 29892,   590,  1024,   338, 32000, 32000],\n",
      "        [    1,   450,  6673,   310,   278,  3303,  3900,   338],\n",
      "        [    1,   450,  7483,   310,  3444,   338, 32000, 32000],\n",
      "        [    1,   450,  5434,   310,   319, 29902,   338, 32000]],\n",
      "       device='cuda:3'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:3')}\n",
      "\n",
      "SampleDecoderOnlyOutput(sequences=tensor([[    1, 15043, 29892,   590,  1024,   338, 32000, 32000, 30488, 30488,\n",
      "         30488, 30488, 30488, 30488, 30488, 30488, 30488, 30488],\n",
      "        [    1,   450,  6673,   310,   278,  3303,  3900,   338,   278,  2343,\n",
      "           310,  2106,   322,  2343,   310,  5874,   310,   278],\n",
      "        [    1,   450,  7483,   310,  3444,   338, 32000, 32000, 30488,  3681,\n",
      "         29889,   450,  4272,   338,  5982,   297,   278,  6641],\n",
      "        [    1,   450,  5434,   310,   319, 29902,   338, 32000, 30488, 30093,\n",
      "           297,   278,  6567,   310,   278,  2305,    13,  1576]],\n",
      "       device='cuda:3'), scores=None, attentions=None, hidden_states=None)\n",
      "\n",
      "example_idx = 0\n",
      "\n",
      "['Hello, my name isЉЉЉЉЉЉЉЉЉЉ', 'The president of the United States is the head of state and head of government of the', 'The capital of France isЉ Paris. The city is located in the north', 'The future of AI isЉћ in the hands of the people\\nThe']\n",
      "\n",
      "Prompt    : Hello, my name is\n",
      "Generation: ЉЉЉЉЉЉЉЉЉЉ\n",
      "\n",
      "Prompt    : The president of the United States is\n",
      "Generation:  the head of state and head of government of the\n",
      "\n",
      "Prompt    : The capital of France is\n",
      "Generation: Љ Paris. The city is located in the north\n",
      "\n",
      "Prompt    : The future of AI is\n",
      "Generation: Љћ in the hands of the people\n",
      "The\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "generation_config = {\n",
    "    \"do_sample\": True,\n",
    "    # \"temperature\": 0.8,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"return_dict_in_generate\": True,\n",
    "}\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "outputs = generate(prompt_or_list=prompts, generation_config=generation_config)\n",
    "\n",
    "decoded_outputs = tokenizer.batch_decode(**outputs, skip_special_tokens=True)\n",
    "\n",
    "for prompt, decoded_output in zip(prompts, decoded_outputs):\n",
    "    print(f\"Prompt    : {prompt}\")\n",
    "    print(f\"Generation: {decoded_output[len(prompt):]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "print(generation_config.return_dict_in_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"A $90^\\\\circ$ rotation around the origin in the counter-clockwise direction is applied to $7 + 2i.$  What is the resulting complex number?\"\n",
    "steps = [\n",
    "    \"I know that a $90^\\\\circ$ rotation around the origin in the complex plane can be achieved by multiplying the complex number by $i$, since $i$ has a magnitude of 1 and an argument of $90^\\\\circ$.\",\n",
    "    \"So, I can write the rotation as $(7 + 2i)i = 7i + 2i^2.$\",\n",
    "    \"To simplify this expression, I recall that $i^2 = -1$, so I can substitute that and get $7i - 2.$\",\n",
    "    \"This is the resulting complex number after the rotation.\\n\\n# Answer\\n\\n7i - 2\",\n",
    "]\n",
    "\n",
    "rating2word = {1: \"positive\", -1: \"negative\", 0: \"neutral\"}\n",
    "rating_words = list(rating2word.values())\n",
    "rating_token_ids = tokenizer(rating_words, add_special_tokens=False).input_ids\n",
    "print(rating_token_ids)\n",
    "\n",
    "# constraint = transformers.Constraint(type=\"must_include\", token=\"apple\")\n",
    "\n",
    "# config\n",
    "generation_config = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"force_words_ids\": rating_token_ids,\n",
    "    \"num_beams\": 2,\n",
    "    \"remove_invalid_values\": True,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(problem + steps[0], generation_config=generation_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
