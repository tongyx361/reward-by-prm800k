{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/data/users/zhangjunlei/tyx/reward-by-prm800k/src/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, GenerationConfig\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 20:29:16.507 [INFO] test\n",
      "2023-09-13 20:29:16.507 [INFO] test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Logger __main__ (INFO)>\n"
     ]
    }
   ],
   "source": [
    "utils.init_logging()\n",
    "logger = utils.get_logger(__name__)\n",
    "logger.info(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = {\n",
    "    # \"sample\": True,\n",
    "    # \"prompt\": True,\n",
    "    # \"problem\": True,\n",
    "    # \"step\": True\n",
    "}\n",
    "\n",
    "# utils.set_gpu_ids([6])\n",
    "gpu_id = \"0\"\n",
    "device = f\"cuda:{gpu_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_tokens = <unk><s></s>\u0000\u0001\n",
      "tokenizer.vocab_size = 32000\n",
      "len(tokenizer.vocab) = 32000\n",
      "tokenizer.special_tokens_map = {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "num_added_tokens = 1\n",
      "head_tokens = <unk><s></s>\u0000\u0001\n",
      "tokenizer.vocab_size = 32000\n",
      "len(tokenizer.vocab) = 32001\n",
      "tokenizer.special_tokens_map = {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "LlamaTokenizerFast(name_or_path='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "tokenizer = utils.get_complete_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_print(*arg, **kwargs):\n",
    "    print(*arg, **kwargs, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "def generate(prompt_or_list, generation_config):\n",
    "    if not isinstance(generation_config, GenerationConfig):\n",
    "        if isinstance(generation_config, dict):\n",
    "            generation_config = GenerationConfig.from_dict(generation_config)\n",
    "\n",
    "    inputs = tokenizer(prompt_or_list, padding=True, return_tensors=\"pt\").to(device)\n",
    "    sparse_print(\"inputs:\")\n",
    "    sparse_print(inputs)\n",
    "\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    sparse_print(outputs)\n",
    "\n",
    "    if not generation_config.return_dict_in_generate:\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        sparse_print(decoded_outputs)\n",
    "\n",
    "        return outputs\n",
    "    else:  # generation_config.return_dict_in_generate == True\n",
    "        outputs_len = outputs[\"sequences\"].shape[0]\n",
    "\n",
    "        example_idx = random.randint(0, outputs_len - 1)\n",
    "        sparse_print(f\"example_idx = {example_idx}\")\n",
    "\n",
    "        if outputs_len > 5:\n",
    "            example_decoded_output = tokenizer.decode(\n",
    "                outputs[\"sequences\"][example_idx], skip_special_tokens=True\n",
    "            )\n",
    "            sparse_print(example_decoded_output)\n",
    "        else:  # outputs_len <= 5\n",
    "            decoded_outputs = tokenizer.batch_decode(\n",
    "                outputs[\"sequences\"], skip_special_tokens=True\n",
    "            )\n",
    "            sparse_print(decoded_outputs)\n",
    "\n",
    "        if generation_config.output_scores:\n",
    "            example_scores = outputs[\"scores\"][example_idx]\n",
    "            sparse_print(example_scores.shape)\n",
    "            sparse_print(example_scores)\n",
    "\n",
    "            example_probs = torch.nn.functional.softmax(example_scores, dim=-1)\n",
    "            sparse_print(example_probs[example_idx])\n",
    "            sparse_print(sum(example_probs[example_idx]))\n",
    "\n",
    "            # slow cpu\n",
    "            # for idx, prob in enumerate(first_probs[0]):\n",
    "            #     if prob > 1e-5:\n",
    "            #         print(f\"token_id: {idx}, token: {tokenizer.decode([idx])}, prob: {prob}\")\n",
    "\n",
    "            # fast gpu\n",
    "            # non_zero_idxs = torch.gt(first_probs.view(-1),1e-1)\n",
    "            threshold = 0.1\n",
    "            mask = example_probs > threshold\n",
    "            non_zero_idxs = torch.nonzero(mask)\n",
    "            sparse_print(non_zero_idxs)\n",
    "            non_zero_vals = example_probs.view(-1)[non_zero_idxs]\n",
    "            sparse_print(non_zero_vals)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_solution_hierchical_samples = utils.load_pickle(\"/data/users/zhangjunlei/tyx/reward-by-prm800k/datasets/problem-solution-hierarchical-samples.pkl\")\n",
    "# validation set\n",
    "# validation_set_path = os.path.join(utils.project_root, \"datasets\", \"prm800k-002validation-seed42.jsonl\")\n",
    "# validation_set = utils.load_jsonl(validation_set_path)\n",
    "# print(len(validation_set))\n",
    "# print(validation_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:55<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "problem2steps_prompts = defaultdict(list)\n",
    "\n",
    "for problem_solution_samples in problem_solution_hierchical_samples:\n",
    "    problem = problem_solution_samples[\"problem\"]\n",
    "    for solution_sample in problem_solution_samples[\"solutions\"]:\n",
    "        if debug.get(\"sample\"):\n",
    "            print(solution_sample)\n",
    "            raise Exception\n",
    "        # reformatted_sample = utils.reformat_prm800k_sample(solution_sample) # different format\n",
    "        # prompt = problem\n",
    "        step_prompt = \"\\n\".join(solution_sample[\"steps\"])\n",
    "        problem2steps_prompts[problem].append(step_prompt)\n",
    "        \n",
    "        \n",
    "problem_step_ids_list_path = os.path.join(utils.project_root, \"datasets\", \"problem-step-ids-list.pkl\")\n",
    "if not os.path.exists(problem_step_ids_list_path):\n",
    "    problem_step_ids_list = []\n",
    "    for problem, step_prompts in tqdm(problem2steps_prompts.items()):\n",
    "        problem_step_ids = {\n",
    "            \"problem_ids\": None,\n",
    "            \"step_prompt_ids\": []\n",
    "        }\n",
    "        problem_ids = tokenizer(problem + \"\\n\", add_special_tokens=True).input_ids\n",
    "        problem_step_ids[\"problem_ids\"] = problem_ids\n",
    "        if debug.get(\"problem\"):\n",
    "            print(problem)\n",
    "            print(problem_ids)\n",
    "            raise Exception\n",
    "        for step_prompt in step_prompts:\n",
    "            step_prompt_ids = tokenizer(step_prompt, add_special_tokens=False).input_ids\n",
    "            problem_step_ids[\"step_prompt_ids\"].append(step_prompt_ids)\n",
    "            if debug.get(\"step\"):\n",
    "                print(step_prompt)\n",
    "                print(step_prompt_ids)\n",
    "                raise Exception\n",
    "        problem_step_ids_list.append(problem_step_ids)\n",
    "        \n",
    "    utils.save_pickle(problem_step_ids_list, problem_step_ids_list_path)\n",
    "else:\n",
    "    problem_step_ids_list = utils.load_pickle(problem_step_ids_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164025cdd95540e7a6aa9dab6d34acd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32001, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
      ")\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    utils.default_7b_model_path, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(model)\n",
    "print(model.config)\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "\n",
      "{'input_ids': tensor([[32000, 32000,     1, 15043, 29892,   590,  1024,   338],\n",
      "        [    1,   450,  6673,   310,   278,  3303,  3900,   338],\n",
      "        [32000, 32000,     1,   450,  7483,   310,  3444,   338],\n",
      "        [32000,     1,   450,  5434,   310,   319, 29902,   338]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "SampleDecoderOnlyOutput(sequences=tensor([[32000, 32000,     1, 15043, 29892,   590,  1024,   338,   435,   453,\n",
      "         29889,    13, 29902, 30010, 29885,   263, 17739,   261],\n",
      "        [    1,   450,  6673,   310,   278,  3303,  3900,   338,   278,  1556,\n",
      "         13988,  2022,   373, 11563, 29889,    13,  3868, 11761],\n",
      "        [32000, 32000,     1,   450,  7483,   310,  3444,   338,   278,  4272,\n",
      "           310,  3681, 29889,   739,   338,  5982,   373,   278],\n",
      "        [32000,     1,   450,  5434,   310,   319, 29902,   338,  1048,  5199,\n",
      "         29899, 23523, 24771,    13,  1576,  5434,   310,   319]],\n",
      "       device='cuda:0'), scores=None, attentions=None, hidden_states=None)\n",
      "\n",
      "example_idx = 1\n",
      "\n",
      "['Hello, my name is Jill.\\nI’m a photographer', 'The president of the United States is the most powerful person on Earth.\\nHe controls', 'The capital of France is the city of Paris. It is located on the', 'The future of AI is about human-machine collaboration\\nThe future of A']\n",
      "\n",
      "Prompt    : Hello, my name is\n",
      "Generation:  Jill.\n",
      "I’m a photographer\n",
      "\n",
      "Prompt    : The president of the United States is\n",
      "Generation:  the most powerful person on Earth.\n",
      "He controls\n",
      "\n",
      "Prompt    : The capital of France is\n",
      "Generation:  the city of Paris. It is located on the\n",
      "\n",
      "Prompt    : The future of AI is\n",
      "Generation:  about human-machine collaboration\n",
      "The future of A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "generation_config = {\n",
    "    \"do_sample\": True,\n",
    "    # \"temperature\": 0.8,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"return_dict_in_generate\": True,\n",
    "}\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "outputs = generate(prompt_or_list=prompts, generation_config=generation_config)\n",
    "\n",
    "decoded_outputs = tokenizer.batch_decode(**outputs, skip_special_tokens=True)\n",
    "\n",
    "for prompt, decoded_output in zip(prompts, decoded_outputs):\n",
    "    print(f\"Prompt    : {prompt}\")\n",
    "    print(f\"Generation: {decoded_output[len(prompt):]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# print(generation_config.return_dict_in_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6374], [8178], [21104]]\n"
     ]
    }
   ],
   "source": [
    "# problem = \"A $90^\\\\circ$ rotation around the origin in the counter-clockwise direction is applied to $7 + 2i.$  What is the resulting complex number?\"\n",
    "# steps = [\n",
    "#     \"I know that a $90^\\\\circ$ rotation around the origin in the complex plane can be achieved by multiplying the complex number by $i$, since $i$ has a magnitude of 1 and an argument of $90^\\\\circ$.\",\n",
    "#     \"So, I can write the rotation as $(7 + 2i)i = 7i + 2i^2.$\",\n",
    "#     \"To simplify this expression, I recall that $i^2 = -1$, so I can substitute that and get $7i - 2.$\",\n",
    "#     \"This is the resulting complex number after the rotation.\\n\\n# Answer\\n\\n7i - 2\",\n",
    "# ]\n",
    "\n",
    "rating2word = {1: \"positive\", -1: \"negative\", 0: \"neutral\"}\n",
    "rating_words = list(rating2word.values())\n",
    "rating_token_ids = tokenizer(rating_words, add_special_tokens=False).input_ids\n",
    "print(rating_token_ids)\n",
    "\n",
    "# constraint = transformers.Constraint(type=\"must_include\", token=\"apple\")\n",
    "\n",
    "# config\n",
    "generation_config = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"force_words_ids\": rating_token_ids,\n",
    "    \"num_beams\": 2,\n",
    "    \"remove_invalid_values\": True,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 38])\n",
      "tensor([[    1,   319,   395, 29929, 29900,  3823,  6034, 29938, 13733,  2820,\n",
      "           278,  3978,   297,   278,  6795, 29899, 13058,  3538,  5305,   338,\n",
      "          7436,   304,   395, 29955,   718, 29871, 29906, 29875,  7449, 29871,\n",
      "          1724,   338,   278,  9819,  4280,  1353, 29973,    13]],\n",
      "       device='cuda:0')\n",
      "logits\n",
      "past_key_values\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data/users/zhangjunlei/tyx/reward-by-prm800k/src/hf-generate-debug.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/hf-generate-debug.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/hf-generate-debug.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m# print(k, getattr(v, \"shape\", v))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/hf-generate-debug.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39mprint\u001b[39m(k)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/hf-generate-debug.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.75.141/data/users/zhangjunlei/tyx/reward-by-prm800k/src/hf-generate-debug.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m step_prompt_ids_list \u001b[39m=\u001b[39m problem_step_ids[\u001b[39m\"\u001b[39m\u001b[39mstep_prompt_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "debug = {\n",
    "    # \"forward\": True\n",
    "}\n",
    "\n",
    "for problem_step_ids in problem_step_ids_list:\n",
    "    problem_ids = problem_step_ids[\"problem_ids\"]\n",
    "    problem_ids = torch.tensor(problem_ids).unsqueeze(0).to(device)\n",
    "    output = model.forward(input_ids=problem_ids, use_cache=True, return_dict=True)\n",
    "    \n",
    "    if debug.get(\"forward\"):\n",
    "        print(problem_ids.shape)\n",
    "        print(problem_ids)\n",
    "        for k, v in output.items():\n",
    "            # print(k, getattr(v, \"shape\", v))\n",
    "            print(k)\n",
    "        raise Exception\n",
    "    \n",
    "    step_prompt_ids_list = problem_step_ids[\"step_prompt_ids\"]\n",
    "    for step_prompt_ids in step_prompt_ids_list:\n",
    "        logits = model.forward(input_ids=step_prompt_ids, use_cache=False, return_dict=False).logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
