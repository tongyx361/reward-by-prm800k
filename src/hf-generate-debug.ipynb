{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-16 20:02:55,544] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global configs\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "model_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\" # \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470\"\n",
    "gpu_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{gpu_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizer(name_or_path='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map\n",
    "\n",
    "num_added_tokens = tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    }\n",
    ")\n",
    "assert num_added_tokens in [\n",
    "    0,\n",
    "    1,\n",
    "], \"LlamaTokenizer should only add one special token - the pad_token, or no tokens if pad token present.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizer(name_or_path='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12a2e19408d43fabdbd0175b1527330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(model_name_or_path, low_cpu_mem_usage=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_print(*arg, **kwargs):\n",
    "    print(*arg, **kwargs, sep=\"\\n\", end=\"\\n\\n\")\n",
    "    \n",
    "def generate(prompt_or_list, generation_config):\n",
    "    if not isinstance(generation_config, transformers.GenerationConfig):\n",
    "        if isinstance(generation_config, dict):\n",
    "            generation_config = transformers.GenerationConfig.from_dict(generation_config)\n",
    "    \n",
    "    inputs = tokenizer(prompt_or_list, \n",
    "                       padding=True,\n",
    "                       return_tensors=\"pt\"\n",
    "                       ).to(device)\n",
    "    sparse_print(\"inputs:\")\n",
    "    sparse_print(inputs)\n",
    "\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    sparse_print(outputs)\n",
    "    \n",
    "    if not generation_config.return_dict_in_generate:\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        sparse_print(decoded_outputs)\n",
    "        \n",
    "        return outputs\n",
    "    else: # generation_config.return_dict_in_generate == True\n",
    "        outputs_len = outputs[\"sequences\"].shape[0]\n",
    "\n",
    "            \n",
    "        example_idx = random.randint(0, outputs_len-1)\n",
    "        sparse_print(f\"example_idx = {example_idx}\")\n",
    "\n",
    "        if outputs_len > 5:\n",
    "            example_decoded_output = tokenizer.decode(outputs[\"sequences\"][example_idx], skip_special_tokens=True)\n",
    "            sparse_print(example_decoded_output)\n",
    "        else: # outputs_len <= 5\n",
    "            decoded_outputs = tokenizer.batch_decode(outputs[\"sequences\"], skip_special_tokens=True)\n",
    "            sparse_print(decoded_outputs)\n",
    "        \n",
    "        if generation_config.output_scores:\n",
    "            example_scores = outputs[\"scores\"][example_idx]\n",
    "            sparse_print(example_scores.shape)\n",
    "            sparse_print(example_scores)\n",
    "        \n",
    "            example_probs = torch.nn.functional.softmax(example_scores, dim=-1)\n",
    "            sparse_print(example_probs[example_idx])\n",
    "            sparse_print(sum(example_probs[example_idx]))\n",
    "\n",
    "            # slow cpu\n",
    "            # for idx, prob in enumerate(first_probs[0]):\n",
    "            #     if prob > 1e-5:\n",
    "            #         print(f\"token_id: {idx}, token: {tokenizer.decode([idx])}, prob: {prob}\")\n",
    "\n",
    "            # fast gpu\n",
    "            # non_zero_idxs = torch.gt(first_probs.view(-1),1e-1)\n",
    "            threshold = 0.1\n",
    "            mask = example_probs > threshold\n",
    "            non_zero_idxs = torch.nonzero(mask)\n",
    "            sparse_print(non_zero_idxs)\n",
    "            non_zero_vals = example_probs.view(-1)[non_zero_idxs]\n",
    "            sparse_print(non_zero_vals)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "\n",
      "{'input_ids': tensor([[    1, 15043, 29892,   590,  1024,   338, 32000, 32000],\n",
      "        [    1,   450,  6673,   310,   278,  3303,  3900,   338],\n",
      "        [    1,   450,  7483,   310,  3444,   338, 32000, 32000],\n",
      "        [    1,   450,  5434,   310,   319, 29902,   338, 32000]],\n",
      "       device='cuda:3'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:3')}\n",
      "\n",
      "SampleDecoderOnlyOutput(sequences=tensor([[    1, 15043, 29892,   590,  1024,   338, 32000, 32000, 30488, 30488,\n",
      "         30488, 30488, 30488, 30488, 30488, 30488, 30488, 30488],\n",
      "        [    1,   450,  6673,   310,   278,  3303,  3900,   338,   278,  2343,\n",
      "           310,  2106,   322,  2343,   310,  5874,   310,   278],\n",
      "        [    1,   450,  7483,   310,  3444,   338, 32000, 32000, 30488,  3681,\n",
      "         29889,   450,  4272,   338,  5982,   297,   278,  6641],\n",
      "        [    1,   450,  5434,   310,   319, 29902,   338, 32000, 30488, 30093,\n",
      "           297,   278,  6567,   310,   278,  2305,    13,  1576]],\n",
      "       device='cuda:3'), scores=None, attentions=None, hidden_states=None)\n",
      "\n",
      "example_idx = 0\n",
      "\n",
      "['Hello, my name isЉЉЉЉЉЉЉЉЉЉ', 'The president of the United States is the head of state and head of government of the', 'The capital of France isЉ Paris. The city is located in the north', 'The future of AI isЉћ in the hands of the people\\nThe']\n",
      "\n",
      "Prompt    : Hello, my name is\n",
      "Generation: ЉЉЉЉЉЉЉЉЉЉ\n",
      "\n",
      "Prompt    : The president of the United States is\n",
      "Generation:  the head of state and head of government of the\n",
      "\n",
      "Prompt    : The capital of France is\n",
      "Generation: Љ Paris. The city is located in the north\n",
      "\n",
      "Prompt    : The future of AI is\n",
      "Generation: Љћ in the hands of the people\n",
      "The\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "generation_config = {\n",
    "    \"do_sample\": True,\n",
    "    # \"temperature\": 0.8,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"return_dict_in_generate\": True,\n",
    "}\n",
    "\n",
    "generation_config = transformers.GenerationConfig.from_dict(generation_config)\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    " \n",
    "outputs = generate(prompt_or_list=prompts, generation_config=generation_config)\n",
    "\n",
    "decoded_outputs = tokenizer.batch_decode(**outputs, skip_special_tokens=True)\n",
    "\n",
    "for prompt, decoded_output in zip(prompts, decoded_outputs):\n",
    "    print(f\"Prompt    : {prompt}\")\n",
    "    print(f\"Generation: {decoded_output[len(prompt):]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "print(generation_config.return_dict_in_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"A $90^\\\\circ$ rotation around the origin in the counter-clockwise direction is applied to $7 + 2i.$  What is the resulting complex number?\"\n",
    "steps = [\n",
    "    \"I know that a $90^\\\\circ$ rotation around the origin in the complex plane can be achieved by multiplying the complex number by $i$, since $i$ has a magnitude of 1 and an argument of $90^\\\\circ$.\",\n",
    "    \"So, I can write the rotation as $(7 + 2i)i = 7i + 2i^2.$\",\n",
    "    \"To simplify this expression, I recall that $i^2 = -1$, so I can substitute that and get $7i - 2.$\",\n",
    "    \"This is the resulting complex number after the rotation.\\n\\n# Answer\\n\\n7i - 2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating2word = {1: \"positive\", -1: \"negative\", 0: \"neutral\"}\n",
    "rating_words = list(rating2word.values())\n",
    "rating_token_ids = tokenizer(rating_words, add_special_tokens=False).input_ids\n",
    "print(rating_token_ids)\n",
    "\n",
    "# constraint = transformers.Constraint(type=\"must_include\", token=\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   319,   395, 29929, 29900,  3823,  6034, 29938, 13733,  2820,\n",
      "           278,  3978,   297,   278,  6795, 29899, 13058,  3538,  5305,   338,\n",
      "          7436,   304,   395, 29955,   718, 29871, 29906, 29875,  7449, 29871,\n",
      "          1724,   338,   278,  9819,  4280,  1353, 29973]], device='cuda:5'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:5')}\n",
      "BeamSearchDecoderOnlyOutput(sequences=tensor([[    1,   319,   395, 29929, 29900,  3823,  6034, 29938, 13733,  2820,\n",
      "           278,  3978,   297,   278,  6795, 29899, 13058,  3538,  5305,   338,\n",
      "          7436,   304,   395, 29955,   718, 29871, 29906, 29875,  7449, 29871,\n",
      "          1724,   338,   278,  9819,  4280,  1353, 29973,    13]],\n",
      "       device='cuda:5'), sequences_scores=tensor([-0.0062], device='cuda:5'), scores=(tensor([[-2.0333e+01, -2.0216e+01, -2.5555e+00,  ..., -2.0804e+01,\n",
      "         -1.9792e+01, -1.9454e+01],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09]], device='cuda:5'),), beam_indices=None, attentions=None, hidden_states=None)\n",
      "tensor([[-2.0333e+01, -2.0216e+01, -2.5555e+00,  ..., -2.0804e+01,\n",
      "         -1.9792e+01, -1.9454e+01],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09]], device='cuda:5')\n",
      "torch.Size([2, 32001])\n",
      "tensor([1.4780e-09, 1.6609e-09, 7.7650e-02,  ..., 9.2279e-10, 2.5368e-09,\n",
      "        3.5600e-09], device='cuda:5')\n",
      "tensor(0.9999, device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "generation_config = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 1, \n",
    "    \"force_words_ids\": rating_token_ids,\n",
    "    \"num_beams\": 2,\n",
    "    \"remove_invalid_values\": True,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True\n",
    "}\n",
    "\n",
    "generation_config = transformers.GenerationConfig.from_dict(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(problem + steps[0], generation_config=generation_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
