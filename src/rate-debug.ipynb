{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from functools import reduce\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import blobfile as bf\n",
    "import numpy as np\n",
    "import orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\"\n",
    "model_name_or_path = \"/data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer_name_or_path = \"/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470\"\n",
    "# gpu_ids = [1,3,5,7]\n",
    "gpu_ids = [0]\n",
    "tensor_parallel_size=len(gpu_ids)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in gpu_ids])\n",
    "samples_to_rate_path = \"/data/users/zhangjunlei/tyx/reward-by-prm800k/datasets/prm800k-scored-test-samples.jsonl\"\n",
    "input_ids_path = \"/data/users/zhangjunlei/tyx/reward-by-prm800k/datasets/gpt4-generated-math-solutions-till-each-step-input-ids-list.pkl\"\n",
    "rated_samples_path = \"/data/users/zhangjunlei/tyx/reward-by-prm800k/eval/rated-samples/gpt-4-generatations/llama-2-7b-2023-08-15-2-step-2040-ratings.jsonl\"\n",
    "\n",
    "__DEBUG__ = False\n",
    "# __DEBUG__ = True\n",
    "__DEBUG_FOR__ = {\n",
    "    # \"inference_sample_num\": 100\n",
    "    \"inference_sample_num\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_loads(s: str) -> Dict:\n",
    "    try:\n",
    "        return orjson.loads(s)\n",
    "    except Exception:\n",
    "        return json.loads(s)  # fallback\n",
    "\n",
    "\n",
    "def open_jsonl(file: str):\n",
    "    if file.endswith(\".gz\"):\n",
    "        return gzip.open(bf.BlobFile(file, \"rb\"))\n",
    "    return bf.BlobFile(file, \"r\")\n",
    "\n",
    "\n",
    "def _read_jsonl(file: str) -> List[Dict]:\n",
    "    assert bf.exists(file), file\n",
    "    with open_jsonl(file) as f:\n",
    "        return [json_loads(l) for l in f.readlines() if l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = _read_jsonl(samples_to_rate_path)\n",
    "# samples_by_problem = _key_by_problem(samples)  # group samples by problem\n",
    "# num_problems = len(samples_by_problem)  # num of problmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815631\n"
     ]
    }
   ],
   "source": [
    "print(len(generated_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'A $90^\\\\circ$ rotation around the origin in the counter-clockwise direction is applied to $7 + 2i.$  What is the resulting complex number?', 'answer': '7i - 2', 'is_correct': True, 'subject': 'Precalculus', 'level': 2, 'unique_id': 'test/precalculus/779.json', 'steps': ['I know that a $90^\\\\circ$ rotation around the origin in the complex plane can be achieved by multiplying the original complex number by $i.$', 'So, I can write $7 + 2i$ as $7i - 2$ after applying the rotation.', 'To check, I can draw the original and rotated complex numbers on the complex plane and see if they form a right angle at the origin.', 'The original complex number corresponds to the point $(7, 2)$ and the rotated one to the point $(-2, 7).$', 'I can use the dot product formula to see if they are perpendicular.', 'The dot product of two perpendicular vectors is zero, so I want to see if $(7, 2) \\\\cdot (-2, 7) = 0.$', 'I can calculate this by multiplying the corresponding components and adding them up: $(7)(-2) + (2)(7) = -14 + 14 = 0.$', 'This confirms that the rotation was correct.', 'Therefore, the resulting complex number is $7i - 2.$\\n\\n# Answer\\n\\n7i - 2'], 'rating_probs': [{'-1': 0.4955421880432429, '1': 0.4850565065558231, '0': 0.019401305400933915}, {'-1': 0.8115352052850885, '1': 0.16596184275124684, '0': 0.02250295196366458}, {'0': 0.6546451880933941, '1': 0.31537486890985433, '-1': 0.02997994299675163}, {'1': 0.8262738884082665, '0': 0.1682199724210449, '-1': 0.005506139170688614}, {'1': 0.49459962721441486, '0': 0.49145594479362104, '-1': 0.013944427991964166}, {'1': 0.8565057970399176, '0': 0.13033172825087222, '-1': 0.013162474709210219}, {'1': 0.9610508839385529, '0': 0.03856784254309562, '-1': 0.00038127351835143063}, {'1': 0.7603325694682361, '0': 0.23845109311131918, '-1': 0.0012163374204446772}, {'-1': 0.4995217602188542, '1': 0.2852564103239134, '0': 0.21522182945723228}], 'orm_score': 0.6872906684875488, 'prm_score': 0.044593952640983744, 'ground_truth_answer': '-2+7i'}\n"
     ]
    }
   ],
   "source": [
    "# sample = random.choice(all_samples)\n",
    "sample = generated_samples[0]\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-1': 0.4955421880432429, '1': 0.4850565065558231, '0': 0.019401305400933915}\n",
      "0.9999999999999999\n",
      "{'-1': 0.8115352052850885, '1': 0.16596184275124684, '0': 0.02250295196366458}\n",
      "0.9999999999999999\n",
      "{'0': 0.6546451880933941, '1': 0.31537486890985433, '-1': 0.02997994299675163}\n",
      "1.0\n",
      "{'1': 0.8262738884082665, '0': 0.1682199724210449, '-1': 0.005506139170688614}\n",
      "1.0\n",
      "{'1': 0.49459962721441486, '0': 0.49145594479362104, '-1': 0.013944427991964166}\n",
      "1.0000000000000002\n",
      "{'1': 0.8565057970399176, '0': 0.13033172825087222, '-1': 0.013162474709210219}\n",
      "1.0\n",
      "{'1': 0.9610508839385529, '0': 0.03856784254309562, '-1': 0.00038127351835143063}\n",
      "0.9999999999999999\n",
      "{'1': 0.7603325694682361, '0': 0.23845109311131918, '-1': 0.0012163374204446772}\n",
      "1.0\n",
      "{'-1': 0.4995217602188542, '1': 0.2852564103239134, '0': 0.21522182945723228}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for rating2prob_step in sample[\"rating_probs\"]:\n",
    "    print(rating2prob_step)\n",
    "    print(sum(list(rating2prob_step.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018523420058605313 0.16596184275124684\n",
      "0.044593952640983744\n"
     ]
    }
   ],
   "source": [
    "positive_probs = [rating_str2_prob['1'] for rating_str2_prob in sample[\"rating_probs\"]]\n",
    "positive_probs_product = reduce(lambda x, y: x * y, positive_probs)\n",
    "positive_probs_minimum = min(positive_probs)\n",
    "print(positive_probs_product, positive_probs_minimum)\n",
    "print(sample[\"prm_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{21104, 8178, 6374}\n",
      "{6374: '1', 8178: '-1', 21104: '0'}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, model_max_length=4096)\n",
    "\n",
    "rating2word = {1: \"positive\", -1: \"negative\", 0: \"neutral\"}\n",
    "rating_words = list(rating2word.values())\n",
    "rating_token_ids = tokenizer(rating_words, add_special_tokens=False).input_ids # [[6374], [8178], [21104]]\n",
    "rating_token_ids = set([token_id[0] for token_id in rating_token_ids]) # [6374, 8178, 21104]\n",
    "print(rating_token_ids)\n",
    "token_id2rating_str = {tokenizer(word, add_special_tokens=False).input_ids[0]: str(rating) for rating, word in rating2word.items()}\n",
    "print(token_id2rating_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map\n",
    "\n",
    "num_added_tokens = tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    }\n",
    ")\n",
    "assert num_added_tokens in [\n",
    "    0,\n",
    "    1,\n",
    "], \"LlamaTokenizer should only add one special token - the pad_token, or no tokens if pad token present.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "# print(type(tokenizer))\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_list = []\n",
    "# for sample in all_samples[:10000]:\n",
    "#     problem = sample[\"problem\"]\n",
    "#     steps = sample[\"steps\"]\n",
    "#     solution_text = problem + \"\\n\"\n",
    "#     solution_input_ids = tokenizer(\n",
    "#         problem + \"\\n\",\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=False,\n",
    "#         truncation=False,\n",
    "#         add_special_tokens=True,\n",
    "#         return_attention_mask=False,\n",
    "#     )[\"input_ids\"][0]\n",
    "\n",
    "#     solution_input_ids_list = []\n",
    "#     for step in steps:\n",
    "#         step = step.strip()\n",
    "\n",
    "#         step_input_ids = tokenizer(\n",
    "#             \"\\n\" + step + \"\\n\",\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=False,\n",
    "#             truncation=False,\n",
    "#             # add_special_tokens=True,\n",
    "#             add_special_tokens=False,\n",
    "#             return_attention_mask=False,\n",
    "#         )[\"input_ids\"][0]\n",
    "#         step_input_ids = step_input_ids[2:]  # remove \"\\n\" at beginning\n",
    "#         solution_input_ids = torch.cat((solution_input_ids, step_input_ids), dim=-1)[:-1]\n",
    "#         solution_input_ids_list.append(solution_input_ids)\n",
    "#     input_ids_list += solution_input_ids_list\n",
    "# print(input_ids_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = []\n",
    "for sample in generated_samples:\n",
    "    problem = sample[\"problem\"]\n",
    "    steps = sample[\"steps\"]\n",
    "    solution_so_far = problem\n",
    "    solution_until_step_idx = []\n",
    "    for step in steps:\n",
    "        solution_so_far += \"\\n\" + step\n",
    "        solution_until_step_idx.append(solution_so_far)\n",
    "    prompts += solution_until_step_idx\n",
    "    if __DEBUG__: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9703694\n",
      "A $90^\\circ$ rotation around the origin in the counter-clockwise direction is applied to $7 + 2i.$  What is the resulting complex number?\n",
      "I know that a $90^\\circ$ rotation around the origin in the complex plane can be achieved by multiplying the original complex number by $i.$\n"
     ]
    }
   ],
   "source": [
    "if prompts is not None:\n",
    "    print(len(prompts))\n",
    "    print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = None\n",
    "if prompts is None:\n",
    "    if os.path.exists(input_ids_path):\n",
    "        with open(input_ids_path, \"rb\") as f:\n",
    "            prompt_input_ids_list = pickle.load(f)\n",
    "    else:\n",
    "        # input_token_ids = tokenizer(all_prompts_to_rate[:1000], return_tensors=\"pt\", padding=\"longest\", add_special_tokens=True).input_ids\n",
    "        prompt_input_ids_list = tokenizer(prompts, add_special_tokens=True).input_ids\n",
    "\n",
    "        with open(input_ids_path, \"wb\") as f:\n",
    "            pickle.dump(prompt_input_ids_list, f)\n",
    "            \n",
    "        print(prompt_input_ids_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_input_ids_list[0]: [1, 319, 395, 29929, 29900, 3823, 6034, 29938, 13733, 2820, 278, 3978, 297, 278, 6795, 29899, 13058, 3538, 5305, 338, 7436, 304, 395, 29955, 718, 29871, 29906, 29875, 7449, 29871, 1724, 338, 278, 9819, 4280, 1353, 29973, 13, 29902, 1073, 393, 263, 395, 29929, 29900, 3823, 6034, 29938, 13733, 2820, 278, 3978, 297, 278, 4280, 10694, 508, 367, 14363, 491, 6674, 5890, 278, 2441, 4280, 1353, 491, 395, 29875, 7449]\n"
     ]
    }
   ],
   "source": [
    "if prompt_input_ids_list is not None:\n",
    "    print(\"prompt_input_ids_list[0]:\", prompt_input_ids_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-17 06:09:18 llm_engine.py:70] Initializing an LLM engine with config: model='/data/users/zhangjunlei/tyx/reward-by-prm800k/models/direct-prediction/meta-llama/Llama-2-7b-hf', tokenizer='/data/users/zhangjunlei/tyx/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/99eceeba6e8289bee767f0771166b5917e70e470', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n",
      "INFO 08-17 06:09:18 tokenizer.py:29] For some LLaMA-based models, initializing the fast tokenizer may take a long time. To eliminate the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n",
      "INFO 08-17 06:09:33 llm_engine.py:196] # GPU blocks: 7439, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "# _DATA_PARALLEL_GROUP = None\n",
    "llm = LLM(\n",
    "    model=model_name_or_path,\n",
    "    tokenizer=tokenizer_name_or_path,\n",
    "    tokenizer_mode=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=len(gpu_ids),\n",
    "    dtype=\"auto\",\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    top_k=top_k,\n",
    "    max_tokens=1,\n",
    "    logprobs=top_k,\n",
    ")\n",
    "\n",
    "# new_sample = sample.copy()\n",
    "\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "\n",
    "if __DEBUG_FOR__[\"inference_sample_num\"] is not None:\n",
    "    outputs = llm.generate(prompts=prompts[:__DEBUG_FOR__[\"inference_sample_num\"]], prompt_token_ids=prompt_input_ids_list[:__DEBUG_FOR__[\"inference_sample_num\"]], sampling_params=sampling_params)\n",
    "else:\n",
    "    outputs = llm.generate(prompts=prompts, prompt_token_ids=prompt_input_ids_list, sampling_params=sampling_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_probs_list: [{'-1': 0.930783418447706, '1': 0.05950298609472064, '0': 0.00971359545757343}, {'1': 0.9869514676596802, '-1': 0.007535465430872128, '0': 0.005513066909447719}, {'1': 0.8956494787243626, '0': 0.09440075491006221, '-1': 0.009949766365575163}, {'1': 0.9626037721204714, '0': 0.03293848730756566, '-1': 0.004457740571963018}, {'1': 0.9479232720388627, '0': 0.03675497231548548, '-1': 0.015321755645651719}, {'1': 0.8938154222986121, '0': 0.09420744258510651, '-1': 0.01197713511628137}, {'1': 0.760529949783203, '0': 0.2178954787687271, '-1': 0.021574571448069933}, {'1': 0.6781062627227672, '0': 0.3203147177144465, '-1': 0.0015790195627863536}, {'1': 0.9688331775035378, '-1': 0.022784772954677318, '0': 0.008382049541784885}, {'-1': 0.930783418447706, '1': 0.05950298609472064, '0': 0.00971359545757343}, {'1': 0.8079490703640116, '-1': 0.1404004751429728, '0': 0.051650454493015426}, {'1': 0.9740897662796335, '0': 0.015744694753628954, '-1': 0.010165538966737497}, {'1': 0.9909442233920155, '0': 0.007565952271657415, '-1': 0.001489824336327039}, {'1': 0.9908490174815335, '-1': 0.007565225175350753, '0': 0.0015857573431156786}, {'-1': 0.9307834047718984, '1': 0.05950299586041947, '0': 0.009713599367682223}, {'1': 0.8214090413245685, '-1': 0.11116560869580719, '0': 0.06742534997962439}, {'1': 0.9781321920259365, '0': 0.014852149373641743, '-1': 0.007015658600421771}, {'1': 0.9828103755986991, '0': 0.01588565077896944, '-1': 0.0013039736223314433}, {'1': 0.9725266080878028, '0': 0.0259169579789949, '-1': 0.001556433933202379}, {'1': 0.9755101272963584, '-1': 0.020246065762921064, '0': 0.004243806940720534}, {'-1': 0.7714213324191483, '1': 0.22101591872515283, '0': 0.007562748855698845}, {'1': 0.984494338640219, '-1': 0.009651643599851722, '0': 0.0058540177599292805}, {'1': 0.8328122581473196, '-1': 0.1639904373331917, '0': 0.003197304519488789}, {'-1': 0.7714213369787306, '1': 0.22101591344470747, '0': 0.007562749576561891}, {'1': 0.992523195692709, '-1': 0.004317819833861867, '0': 0.0031589844734291337}, {'1': 0.9479232784439854, '-1': 0.036754970373072016, '0': 0.015321751182942541}, {'1': 0.9844770857761519, '-1': 0.014042812658929415, '0': 0.0014801015649186925}, {'-1': 0.7714213324191483, '1': 0.22101591872515283, '0': 0.007562748855698845}, {'1': 0.992523195692709, '-1': 0.004317819833861867, '0': 0.0031589844734291337}, {'1': 0.9479232763898002, '-1': 0.036754974401110876, '0': 0.015321749209088846}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7714213369787306, '1': 0.22101591344470747, '0': 0.007562749576561891}, {'1': 0.9927131949758704, '-1': 0.004318645958179092, '0': 0.002968159065950455}, {'1': 0.9469874868650973, '-1': 0.03671869222871527, '0': 0.016293820906187455}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7714213369787306, '1': 0.22101591344470747, '0': 0.007562749576561891}, {'1': 0.9925231983397186, '-1': 0.004317818305226126, '0': 0.0031589833550553347}, {'1': 0.9529233730586579, '-1': 0.032607248243945335, '0': 0.014469378697396756}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7714213226045086, '1': 0.22101592908678694, '0': 0.0075627483087045595}, {'1': 0.9927829125441847, '-1': 0.004057276972497018, '0': 0.003159810483318257}, {'1': 0.9423801644553041, '-1': 0.04140529072327335, '0': 0.01621454482142251}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7717749639398334, '1': 0.2211172358551696, '0': 0.0071078002049970316}, {'1': 0.9927829125441847, '-1': 0.004057276972497018, '0': 0.003159810483318257}, {'1': 0.946987491291977, '-1': 0.036718691853212894, '0': 0.01629381685481016}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7714213369787306, '1': 0.22101591344470747, '0': 0.007562749576561891}, {'1': 0.9929730092238954, '-1': 0.004058054604403827, '0': 0.002968936171700721}, {'1': 0.9529233815266827, '-1': 0.032607244768089995, '0': 0.014469373705227368}, {'1': 0.9846483307388263, '-1': 0.014045256867784277, '0': 0.0013064123933895179}, {'-1': 0.7714213226045086, '1': 0.22101592908678694, '0': 0.0075627483087045595}, {'1': 0.9925231983397186, '-1': 0.004317818305226126, '0': 0.0031589833550553347}, {'1': 0.946987491291977, '-1': 0.036718691853212894, '0': 0.01629381685481016}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7714213369787306, '1': 0.22101591344470747, '0': 0.007562749576561891}, {'1': 0.9927829125441847, '-1': 0.004057276972497018, '0': 0.003159810483318257}, {'1': 0.9479232725905568, '-1': 0.03675497192610816, '0': 0.015321755483335062}, {'1': 0.9844770876263675, '-1': 0.014042810985130926, '0': 0.0014801013885016297}, {'-1': 0.7714213369787306, '1': 0.22101591344470747, '0': 0.007562749576561891}, {'1': 0.9925231983397186, '-1': 0.004317818305226126, '0': 0.0031589833550553347}, {'1': 0.946987491291977, '-1': 0.036718691853212894, '0': 0.01629381685481016}, {'1': 0.9846483307388263, '-1': 0.014045256867784277, '0': 0.0013064123933895179}, {'-1': 0.771774962631959, '1': 0.22111723548045728, '0': 0.007107801887583752}, {'1': 0.9927829125441847, '-1': 0.004057276972497018, '0': 0.003159810483318257}, {'1': 0.9479232725905568, '-1': 0.03675497192610816, '0': 0.015321755483335062}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7714213369787306, '1': 0.22101591344470747, '0': 0.007562749576561891}, {'1': 0.9925231983397186, '-1': 0.004317818305226126, '0': 0.0031589833550553347}, {'1': 0.9469874932236994, '-1': 0.036718687824480525, '0': 0.016293818951820135}, {'1': 0.9861928122845166, '-1': 0.012414338232132775, '0': 0.001392849483350571}, {'-1': 0.7714213324191483, '1': 0.22101591872515283, '0': 0.007562748855698845}, {'1': 0.9929730092238954, '-1': 0.004058054604403827, '0': 0.002968936171700721}, {'1': 0.9479232784439854, '-1': 0.036754970373072016, '0': 0.015321751182942541}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7714213324191483, '1': 0.22101591872515283, '0': 0.007562748855698845}, {'1': 0.9933959427561683, '-1': 0.0038138116813814663, '0': 0.0027902455624503985}, {'1': 0.952923380578971, '-1': 0.032607243035060796, '0': 0.014469376385968253}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7714213324191483, '1': 0.22101591872515283, '0': 0.007562748855698845}, {'1': 0.9929730092238954, '-1': 0.004058054604403827, '0': 0.002968936171700721}, {'1': 0.9469874868650973, '-1': 0.03671869222871527, '0': 0.016293820906187455}, {'1': 0.9844770876263675, '-1': 0.014042810985130926, '0': 0.0014801013885016297}, {'-1': 0.7714213324191483, '1': 0.22101591872515283, '0': 0.007562748855698845}, {'1': 0.9929730092238954, '-1': 0.004058054604403827, '0': 0.002968936171700721}, {'1': 0.9469874868650973, '-1': 0.03671869222871527, '0': 0.016293820906187455}, {'1': 0.9862760480475852, '-1': 0.012415380477450931, '0': 0.0013085714749638375}, {'-1': 0.7714213324191483, '1': 0.22101591872515283, '0': 0.007562748855698845}, {'1': 0.9925231983397186, '-1': 0.004317818305226126, '0': 0.0031589833550553347}, {'1': 0.9479232784439854, '-1': 0.036754970373072016, '0': 0.015321751182942541}, {'1': 0.9844770876263675, '-1': 0.014042810985130926, '0': 0.0014801013885016297}, {'-1': 0.7714213285550405, '1': 0.2210159242048551, '0': 0.00756274724010428}, {'1': 0.9925231983397186, '-1': 0.004317818305226126, '0': 0.0031589833550553347}, {'1': 0.9469874932236994, '-1': 0.036718687824480525, '0': 0.016293818951820135}, {'1': 0.9844770876263675, '-1': 0.014042810985130926, '0': 0.0014801013885016297}, {'-1': 0.7714213226045086, '1': 0.22101592908678694, '0': 0.0075627483087045595}, {'1': 0.9929730092238954, '-1': 0.004058054604403827, '0': 0.002968936171700721}, {'1': 0.946987491291977, '-1': 0.036718691853212894, '0': 0.01629381685481016}, {'1': 0.9844770876263675, '-1': 0.014042810985130926, '0': 0.0014801013885016297}, {'-1': 0.7714213226045086, '1': 0.22101592908678694, '0': 0.0075627483087045595}]\n",
      "len(rating_probs_list): 100\n"
     ]
    }
   ],
   "source": [
    "rating2prob_list = []\n",
    "for idx, output in enumerate(outputs):\n",
    "    if __DEBUG__:\n",
    "        print(output)\n",
    "            \n",
    "        token_id2logprob = output.outputs[0].logprobs[0]\n",
    "        print(\"logprobs:\", token_id2logprob)\n",
    "        \n",
    "        top_token_ids = token_id2logprob.keys()\n",
    "        \n",
    "        logprobs = list(token_id2logprob.values())\n",
    "        probs = np.exp(logprobs)\n",
    "        print(\"probs:\", probs)\n",
    "        sum_probs = sum(probs)\n",
    "        print(\"sum(probs):\", sum_probs)\n",
    "        \n",
    "        norm_probs = probs / sum_probs\n",
    "        print(\"norm_probs:\", norm_probs)\n",
    "        \n",
    "        if set(top_token_ids) != set(rating_token_ids):\n",
    "            print(\"idx:\", idx)\n",
    "            print(\"top_ids:\", top_token_ids)\n",
    "            print(\"logprobs:\", token_id2logprob)\n",
    "            \n",
    "        rating_strs = [token_id2rating_str.get(top_token_id, top_token_id) for top_token_id in top_token_ids ]\n",
    "        rating_probs = {\n",
    "            rating_str: norm_prob for rating_str, norm_prob in zip(rating_strs, norm_probs)\n",
    "        }\n",
    "        print(\"rating_probs:\", rating_probs)\n",
    "        rating2prob_list.append(rating_probs)\n",
    "        # break\n",
    "    else:\n",
    "\n",
    "        token_id2logprob = output.outputs[0].logprobs[0]\n",
    "\n",
    "        top_token_ids = token_id2logprob.keys()\n",
    "        \n",
    "        logprobs = list(token_id2logprob.values())\n",
    "        probs = np.exp(logprobs)\n",
    "        sum_probs = sum(probs)\n",
    "        \n",
    "        top_p = 0.95\n",
    "        if sum_probs < top_p:\n",
    "            print(f\"{idx}: sum_probs < {top_p}\",)\n",
    "        \n",
    "        norm_probs = probs / sum_probs\n",
    "        \n",
    "        if set(top_token_ids) != set(rating_token_ids):\n",
    "            print(f\"{idx}: set(top_token_ids) != set(rating_token_ids):\")\n",
    "            \n",
    "        rating_strs = [token_id2rating_str.get(top_token_id, top_token_id) for top_token_id in top_token_ids ]\n",
    "        rating_probs = {\n",
    "            rating_str: norm_prob for rating_str, norm_prob in zip(rating_strs, norm_probs)\n",
    "        }\n",
    "        rating2prob_list.append(rating_probs)\n",
    "print(\"rating_probs_list:\", rating2prob_list)\n",
    "print(\"len(rating_probs_list):\", len(rating2prob_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sample_step_num_so_far \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m step_idx, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sample[\u001b[39m\"\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m----> 5\u001b[0m     rating2prob \u001b[39m=\u001b[39m rating2prob_list[total_step_num_so_far]\n\u001b[1;32m      7\u001b[0m     rating2prob \u001b[39m=\u001b[39m {rating: \u001b[39mfloat\u001b[39m(prob) \u001b[39mfor\u001b[39;00m rating, prob \u001b[39min\u001b[39;00m rating2prob\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m      9\u001b[0m     sample[\u001b[39m\"\u001b[39m\u001b[39mrating_probs\u001b[39m\u001b[39m\"\u001b[39m][step_idx] \u001b[39m=\u001b[39m rating2prob\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 03:01:56,366\tWARNING worker.py:2037 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff3c9bf5cffeb263efd73cda5801000000 Worker ID: df527efa98152934c882a508d6916f6ea68b8ea5a63f04554d89c1c0 Node ID: 7d6bbdcfae3148bf64cffb363be44cd2a7e8288e15033f9b33d30424 Worker IP address: 172.16.75.141 Worker port: 38419 Worker PID: 4151663 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-08-17 03:02:38,692\tWARNING worker.py:2037 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff7aa847f246814ebaeab4f45801000000 Worker ID: 4d11745881f2dfdbc3262054dcf1a1d49359d0492c42d3ccbc1cfd6c Node ID: 7d6bbdcfae3148bf64cffb363be44cd2a7e8288e15033f9b33d30424 Worker IP address: 172.16.75.141 Worker port: 37507 Worker PID: 4151666 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-08-17 03:02:38,719\tWARNING worker.py:2037 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff6d98d15586993d22d765c7fd01000000 Worker ID: 4d13ae382be65fd4f89059b373bbd067a3cc44661ceeab728a5dbdbf Node ID: 7d6bbdcfae3148bf64cffb363be44cd2a7e8288e15033f9b33d30424 Worker IP address: 172.16.75.141 Worker port: 43125 Worker PID: 4151664 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-08-17 03:02:38,721\tWARNING worker.py:2037 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff2c95f4c325416b060ed8240601000000 Worker ID: 47cb1d0c17a57981480f5e68bffe3892310cfd68e8b37358d91d0532 Node ID: 7d6bbdcfae3148bf64cffb363be44cd2a7e8288e15033f9b33d30424 Worker IP address: 172.16.75.141 Worker port: 44385 Worker PID: 4151665 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    }
   ],
   "source": [
    "total_step_num_so_far = 0\n",
    "for sample in generated_samples:\n",
    "    sample_step_num_so_far = 0\n",
    "    for step_idx, step in enumerate(sample[\"steps\"]):\n",
    "        rating2prob = rating2prob_list[total_step_num_so_far]\n",
    "        \n",
    "        rating2prob = {rating: float(prob) for rating, prob in rating2prob.items()}\n",
    "        \n",
    "        sample[\"rating_probs\"][step_idx] = rating2prob\n",
    "        \n",
    "        total_step_num_so_far += 1\n",
    "        \n",
    "    sample[\"orm_score\"] = None\n",
    "\n",
    "    positive_probs = [rating2prob[\"1\"] for rating2prob in sample[\"rating_probs\"]]\n",
    "    sample[\"prm_score\"] = {\n",
    "        \"positive_probs_product\": float(np.prod(positive_probs)),\n",
    "        \"positive_probs_minimum\": float(np.min(positive_probs)),\n",
    "    } \n",
    "    \n",
    "    if __DEBUG__:\n",
    "        print(sample)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开文件以写入JSONL格式\n",
    "with open(rated_samples_path, 'w') as f:\n",
    "    # 迭代列表中的每个元素\n",
    "    for item in generated_samples:\n",
    "        # 将列表中的每个元素转换为JSON字符串\n",
    "        json_str = orjson.dumps(item).decode()\n",
    "        \n",
    "        # 将JSON字符串写入文件，并添加换行符以分隔每个元素\n",
    "        f.write(json_str + '\\n')\n",
    "        if __DEBUG__:\n",
    "            break\n",
    "f.close()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-instruct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
