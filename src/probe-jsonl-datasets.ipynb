{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "import os\n",
    "\n",
    "import blobfile as bf\n",
    "import numpy as np\n",
    "import orjson\n",
    "\n",
    "Sample = Dict[str, Any]\n",
    "\n",
    "## global variables\n",
    "\n",
    "scored_test_samples_jsonl_path = (\n",
    "    \"/data/tongyx361/reward-by-prm800k/datasets/scored-test-samples.jsonl\"\n",
    ")\n",
    "\n",
    "prm800k_jsonl_dirpath = \"/data/tongyx361/reward-by-prm800k/prm800k-main/prm800k/data\"\n",
    "\n",
    "prm800k_jsonl_path_phase = [\n",
    "    {\n",
    "        \"train\": os.path.join(prm800k_jsonl_dirpath, \"phase1_train.jsonl\"),\n",
    "        \"test\": os.path.join(prm800k_jsonl_dirpath, \"phase1_test.jsonl\"),\n",
    "    },\n",
    "    {\n",
    "        \"train\": os.path.join(prm800k_jsonl_dirpath, \"phase2_train.jsonl\"),\n",
    "        \"test\": os.path.join(prm800k_jsonl_dirpath, \"phase2_test.jsonl\"),\n",
    "    },\n",
    "]\n",
    "\n",
    "## functions\n",
    "\n",
    "\n",
    "def json_loads(s: str) -> Dict:\n",
    "    try:\n",
    "        return orjson.loads(s)\n",
    "    except Exception:\n",
    "        return json.loads(s)  # fallback\n",
    "\n",
    "\n",
    "def open_jsonl(file: str):\n",
    "    if file.endswith(\".gz\"):\n",
    "        return gzip.open(bf.BlobFile(file, \"rb\"))\n",
    "    return bf.BlobFile(file, \"r\")\n",
    "\n",
    "\n",
    "def read_jsonl(file: str) -> List[Dict]:\n",
    "    assert bf.exists(file), file\n",
    "    with open_jsonl(file) as f:\n",
    "        return [json_loads(l) for l in f.readlines() if l]\n",
    "\n",
    "\n",
    "def key_by_problem(samples: List[Dict]):\n",
    "    grouped_samples = defaultdict(list)\n",
    "    for sample in samples:\n",
    "        grouped_samples[sample[\"problem\"]].append(sample)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scored_test_samples \u001b[39m=\u001b[39m read_jsonl(scored_test_samples_jsonl_path)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(random\u001b[39m.\u001b[39mchoice(scored_test_samples))\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mread_jsonl\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39massert\u001b[39;00m bf\u001b[39m.\u001b[39mexists(file), file\n\u001b[1;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m open_jsonl(file) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m [json_loads(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39;49mreadlines() \u001b[39mif\u001b[39;00m l]\n",
      "File \u001b[0;32m/data/tongyx361/miniconda3/envs/nlp/lib/python3.10/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scored_test_samples = read_jsonl(scored_test_samples_jsonl_path)\n",
    "print(random.choice(scored_test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prm800k_dataset_phase = []\n",
    "\n",
    "for phase in prm800k_jsonl_path_phase:\n",
    "    train_dataset = read_jsonl(phase[\"train\"])\n",
    "    test_dataset = read_jsonl(phase[\"test\"])\n",
    "    phase_dataset = {\"train\": train_dataset, \"test\": test_dataset}\n",
    "    prm800k_dataset_phase.append(phase_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
