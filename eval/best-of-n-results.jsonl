{"model_name_or_path":"model-agnostic","metric":"majority_voting","num_samples_per_problem":2,"mean":0.4748,"std":0.016809521111560546,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175793"}
{"model_name_or_path":"model-agnostic","metric":"majority_voting","num_samples_per_problem":4,"mean":0.5492,"std":0.013362634470792046,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175824"}
{"model_name_or_path":"model-agnostic","metric":"majority_voting","num_samples_per_problem":8,"mean":0.6075999999999999,"std":0.009748846085563161,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175830"}
{"model_name_or_path":"model-agnostic","metric":"majority_voting","num_samples_per_problem":16,"mean":0.6523999999999999,"std":0.002800000000000002,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175835"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5082000000000001,"std":0.015606408939919535,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175842"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5358,"std":0.013984276885130684,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175846"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5448,"std":0.014593149077563738,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175850"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5540000000000002,"std":1.1102230246251565e-16,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175854"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4993999999999999,"std":0.014752626884728035,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175858"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5246000000000002,"std":0.013800000000000012,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175863"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5352,"std":0.014783774890061076,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175867"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5359999999999999,"std":1.1102230246251565e-16,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175871"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5092,"std":0.018998947339260684,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175875"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5344,"std":0.009372299611087993,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175880"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.542,"std":0.011419281938896169,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175884"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.548,"std":0.0,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175888"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.49879999999999997,"std":0.016104657711357932,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175892"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5246000000000001,"std":0.016249307677559697,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175896"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5354000000000001,"std":0.011833849753989623,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175899"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5440000000000002,"std":1.1102230246251565e-16,"num_trials":10,"time_stamp":"UTC+8 2023-08-22 19:56:41 175903"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.4928,"std":0.015263027222671144,"num_trials":10,"pass_rates":[[0.504,0.512,0.54,0.552],[0.492,0.524,0.552,0.552],[0.472,0.502,0.532,0.552],[0.478,0.516,0.534,0.552],[0.476,0.538,0.556,0.552],[0.52,0.538,0.546,0.552],[0.508,0.52,0.536,0.552],[0.49,0.506,0.534,0.552],[0.482,0.556,0.536,0.552],[0.506,0.51,0.544,0.552]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009364"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5222,"std":0.016160445538412622,"num_trials":10,"pass_rates":[[0.504,0.512,0.54,0.552],[0.492,0.524,0.552,0.552],[0.472,0.502,0.532,0.552],[0.478,0.516,0.534,0.552],[0.476,0.538,0.556,0.552],[0.52,0.538,0.546,0.552],[0.508,0.52,0.536,0.552],[0.49,0.506,0.534,0.552],[0.482,0.556,0.536,0.552],[0.506,0.51,0.544,0.552]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009455"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5410000000000001,"std":0.007810249675906661,"num_trials":10,"pass_rates":[[0.504,0.512,0.54,0.552],[0.492,0.524,0.552,0.552],[0.472,0.502,0.532,0.552],[0.478,0.516,0.534,0.552],[0.476,0.538,0.556,0.552],[0.52,0.538,0.546,0.552],[0.508,0.52,0.536,0.552],[0.49,0.506,0.534,0.552],[0.482,0.556,0.536,0.552],[0.506,0.51,0.544,0.552]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009466"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5519999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.504,0.512,0.54,0.552],[0.492,0.524,0.552,0.552],[0.472,0.502,0.532,0.552],[0.478,0.516,0.534,0.552],[0.476,0.538,0.556,0.552],[0.52,0.538,0.546,0.552],[0.508,0.52,0.536,0.552],[0.49,0.506,0.534,0.552],[0.482,0.556,0.536,0.552],[0.506,0.51,0.544,0.552]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009473"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.49060000000000004,"std":0.01612575579624101,"num_trials":10,"pass_rates":[[0.488,0.514,0.53,0.54],[0.508,0.502,0.526,0.538],[0.47,0.51,0.52,0.538],[0.476,0.526,0.534,0.538],[0.476,0.518,0.53,0.538],[0.514,0.508,0.536,0.538],[0.508,0.492,0.524,0.538],[0.488,0.502,0.52,0.538],[0.472,0.534,0.516,0.538],[0.506,0.52,0.532,0.538]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009482"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5126,"std":0.011833849753989623,"num_trials":10,"pass_rates":[[0.488,0.514,0.53,0.54],[0.508,0.502,0.526,0.538],[0.47,0.51,0.52,0.538],[0.476,0.526,0.534,0.538],[0.476,0.518,0.53,0.538],[0.514,0.508,0.536,0.538],[0.508,0.492,0.524,0.538],[0.488,0.502,0.52,0.538],[0.472,0.534,0.516,0.538],[0.506,0.52,0.532,0.538]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009490"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5268,"std":0.006337191807101949,"num_trials":10,"pass_rates":[[0.488,0.514,0.53,0.54],[0.508,0.502,0.526,0.538],[0.47,0.51,0.52,0.538],[0.476,0.526,0.534,0.538],[0.476,0.518,0.53,0.538],[0.514,0.508,0.536,0.538],[0.508,0.492,0.524,0.538],[0.488,0.502,0.52,0.538],[0.472,0.534,0.516,0.538],[0.506,0.52,0.532,0.538]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009497"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5382000000000001,"std":0.0006000000000000006,"num_trials":10,"pass_rates":[[0.488,0.514,0.53,0.54],[0.508,0.502,0.526,0.538],[0.47,0.51,0.52,0.538],[0.476,0.526,0.534,0.538],[0.476,0.518,0.53,0.538],[0.514,0.508,0.536,0.538],[0.508,0.492,0.524,0.538],[0.488,0.502,0.52,0.538],[0.472,0.534,0.516,0.538],[0.506,0.52,0.532,0.538]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009505"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.49239999999999995,"std":0.013200000000000012,"num_trials":10,"pass_rates":[[0.502,0.518,0.534,0.546],[0.494,0.52,0.542,0.546],[0.476,0.498,0.538,0.546],[0.484,0.516,0.538,0.546],[0.476,0.536,0.542,0.546],[0.514,0.538,0.542,0.546],[0.508,0.516,0.526,0.546],[0.486,0.506,0.532,0.546],[0.48,0.544,0.524,0.546],[0.504,0.508,0.542,0.546]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009512"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.52,"std":0.01419859147943909,"num_trials":10,"pass_rates":[[0.502,0.518,0.534,0.546],[0.494,0.52,0.542,0.546],[0.476,0.498,0.538,0.546],[0.484,0.516,0.538,0.546],[0.476,0.536,0.542,0.546],[0.514,0.538,0.542,0.546],[0.508,0.516,0.526,0.546],[0.486,0.506,0.532,0.546],[0.48,0.544,0.524,0.546],[0.504,0.508,0.542,0.546]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009520"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5359999999999999,"std":0.006449806198638846,"num_trials":10,"pass_rates":[[0.502,0.518,0.534,0.546],[0.494,0.52,0.542,0.546],[0.476,0.498,0.538,0.546],[0.484,0.516,0.538,0.546],[0.476,0.536,0.542,0.546],[0.514,0.538,0.542,0.546],[0.508,0.516,0.526,0.546],[0.486,0.506,0.532,0.546],[0.48,0.544,0.524,0.546],[0.504,0.508,0.542,0.546]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009527"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5460000000000002,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.502,0.518,0.534,0.546],[0.494,0.52,0.542,0.546],[0.476,0.498,0.538,0.546],[0.484,0.516,0.538,0.546],[0.476,0.536,0.542,0.546],[0.514,0.538,0.542,0.546],[0.508,0.516,0.526,0.546],[0.486,0.506,0.532,0.546],[0.48,0.544,0.524,0.546],[0.504,0.508,0.542,0.546]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009534"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.4946,"std":0.016026228502052514,"num_trials":10,"pass_rates":[[0.504,0.514,0.524,0.544],[0.512,0.5,0.524,0.542],[0.476,0.506,0.52,0.542],[0.486,0.522,0.534,0.542],[0.474,0.534,0.524,0.542],[0.516,0.512,0.528,0.542],[0.512,0.496,0.536,0.542],[0.492,0.5,0.518,0.542],[0.472,0.536,0.522,0.542],[0.502,0.502,0.526,0.542]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009541"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5121999999999999,"std":0.013577923257994956,"num_trials":10,"pass_rates":[[0.504,0.514,0.524,0.544],[0.512,0.5,0.524,0.542],[0.476,0.506,0.52,0.542],[0.486,0.522,0.534,0.542],[0.474,0.534,0.524,0.542],[0.516,0.512,0.528,0.542],[0.512,0.496,0.536,0.542],[0.492,0.5,0.518,0.542],[0.472,0.536,0.522,0.542],[0.502,0.502,0.526,0.542]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009549"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5256000000000001,"std":0.005425863986500219,"num_trials":10,"pass_rates":[[0.504,0.514,0.524,0.544],[0.512,0.5,0.524,0.542],[0.476,0.506,0.52,0.542],[0.486,0.522,0.534,0.542],[0.474,0.534,0.524,0.542],[0.516,0.512,0.528,0.542],[0.512,0.496,0.536,0.542],[0.492,0.5,0.518,0.542],[0.472,0.536,0.522,0.542],[0.502,0.502,0.526,0.542]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009556"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_2000","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5421999999999999,"std":0.0006000000000000005,"num_trials":10,"pass_rates":[[0.504,0.514,0.524,0.544],[0.512,0.5,0.524,0.542],[0.476,0.506,0.52,0.542],[0.486,0.522,0.534,0.542],[0.474,0.534,0.524,0.542],[0.516,0.512,0.528,0.542],[0.512,0.496,0.536,0.542],[0.492,0.5,0.518,0.542],[0.472,0.536,0.522,0.542],[0.502,0.502,0.526,0.542]],"time_stamp":"UTC+8 2023-08-24 02:42:05 009566"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.502,"std":0.009838699100999083,"num_trials":10,"pass_rates":[[0.506,0.524,0.51,0.53],[0.504,0.532,0.534,0.53],[0.488,0.514,0.52,0.53],[0.516,0.496,0.522,0.53],[0.506,0.53,0.526,0.53],[0.49,0.526,0.542,0.53],[0.504,0.52,0.568,0.53],[0.496,0.53,0.526,0.53],[0.492,0.504,0.54,0.53],[0.518,0.538,0.542,0.53]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700125"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5214000000000001,"std":0.012523577763562626,"num_trials":10,"pass_rates":[[0.506,0.524,0.51,0.53],[0.504,0.532,0.534,0.53],[0.488,0.514,0.52,0.53],[0.516,0.496,0.522,0.53],[0.506,0.53,0.526,0.53],[0.49,0.526,0.542,0.53],[0.504,0.52,0.568,0.53],[0.496,0.53,0.526,0.53],[0.492,0.504,0.54,0.53],[0.518,0.538,0.542,0.53]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700169"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.533,"std":0.015342750731208521,"num_trials":10,"pass_rates":[[0.506,0.524,0.51,0.53],[0.504,0.532,0.534,0.53],[0.488,0.514,0.52,0.53],[0.516,0.496,0.522,0.53],[0.506,0.53,0.526,0.53],[0.49,0.526,0.542,0.53],[0.504,0.52,0.568,0.53],[0.496,0.53,0.526,0.53],[0.492,0.504,0.54,0.53],[0.518,0.538,0.542,0.53]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700178"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5300000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.506,0.524,0.51,0.53],[0.504,0.532,0.534,0.53],[0.488,0.514,0.52,0.53],[0.516,0.496,0.522,0.53],[0.506,0.53,0.526,0.53],[0.49,0.526,0.542,0.53],[0.504,0.52,0.568,0.53],[0.496,0.53,0.526,0.53],[0.492,0.504,0.54,0.53],[0.518,0.538,0.542,0.53]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700186"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.496,"std":0.010620734437881413,"num_trials":10,"pass_rates":[[0.488,0.518,0.536,0.558],[0.506,0.532,0.538,0.558],[0.494,0.518,0.53,0.558],[0.51,0.52,0.542,0.558],[0.49,0.522,0.532,0.558],[0.478,0.524,0.542,0.558],[0.504,0.51,0.552,0.558],[0.484,0.516,0.544,0.558],[0.496,0.5,0.548,0.558],[0.51,0.532,0.532,0.558]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700194"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5192,"std":0.009086253353280446,"num_trials":10,"pass_rates":[[0.488,0.518,0.536,0.558],[0.506,0.532,0.538,0.558],[0.494,0.518,0.53,0.558],[0.51,0.52,0.542,0.558],[0.49,0.522,0.532,0.558],[0.478,0.524,0.542,0.558],[0.504,0.51,0.552,0.558],[0.484,0.516,0.544,0.558],[0.496,0.5,0.548,0.558],[0.51,0.532,0.532,0.558]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700202"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5396,"std":0.006916646586316239,"num_trials":10,"pass_rates":[[0.488,0.518,0.536,0.558],[0.506,0.532,0.538,0.558],[0.494,0.518,0.53,0.558],[0.51,0.52,0.542,0.558],[0.49,0.522,0.532,0.558],[0.478,0.524,0.542,0.558],[0.504,0.51,0.552,0.558],[0.484,0.516,0.544,0.558],[0.496,0.5,0.548,0.558],[0.51,0.532,0.532,0.558]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700209"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5579999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.488,0.518,0.536,0.558],[0.506,0.532,0.538,0.558],[0.494,0.518,0.53,0.558],[0.51,0.52,0.542,0.558],[0.49,0.522,0.532,0.558],[0.478,0.524,0.542,0.558],[0.504,0.51,0.552,0.558],[0.484,0.516,0.544,0.558],[0.496,0.5,0.548,0.558],[0.51,0.532,0.532,0.558]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700216"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5034000000000001,"std":0.007952358140828423,"num_trials":10,"pass_rates":[[0.512,0.524,0.522,0.536],[0.512,0.526,0.544,0.536],[0.496,0.508,0.524,0.536],[0.512,0.494,0.53,0.536],[0.506,0.53,0.528,0.536],[0.492,0.524,0.548,0.536],[0.502,0.522,0.564,0.536],[0.494,0.532,0.53,0.536],[0.496,0.502,0.538,0.536],[0.512,0.544,0.546,0.536]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700223"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5206,"std":0.014256226709757402,"num_trials":10,"pass_rates":[[0.512,0.524,0.522,0.536],[0.512,0.526,0.544,0.536],[0.496,0.508,0.524,0.536],[0.512,0.494,0.53,0.536],[0.506,0.53,0.528,0.536],[0.492,0.524,0.548,0.536],[0.502,0.522,0.564,0.536],[0.494,0.532,0.53,0.536],[0.496,0.502,0.538,0.536],[0.512,0.544,0.546,0.536]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700230"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5374000000000001,"std":0.012459534501738003,"num_trials":10,"pass_rates":[[0.512,0.524,0.522,0.536],[0.512,0.526,0.544,0.536],[0.496,0.508,0.524,0.536],[0.512,0.494,0.53,0.536],[0.506,0.53,0.528,0.536],[0.492,0.524,0.548,0.536],[0.502,0.522,0.564,0.536],[0.494,0.532,0.53,0.536],[0.496,0.502,0.538,0.536],[0.512,0.544,0.546,0.536]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700237"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5359999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.512,0.524,0.522,0.536],[0.512,0.526,0.544,0.536],[0.496,0.508,0.524,0.536],[0.512,0.494,0.53,0.536],[0.506,0.53,0.528,0.536],[0.492,0.524,0.548,0.536],[0.502,0.522,0.564,0.536],[0.494,0.532,0.53,0.536],[0.496,0.502,0.538,0.536],[0.512,0.544,0.546,0.536]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700244"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.4934,"std":0.010846197490365007,"num_trials":10,"pass_rates":[[0.488,0.508,0.536,0.554],[0.508,0.536,0.546,0.552],[0.492,0.514,0.528,0.554],[0.51,0.508,0.548,0.554],[0.496,0.508,0.538,0.554],[0.476,0.522,0.542,0.552],[0.496,0.508,0.552,0.554],[0.478,0.526,0.54,0.552],[0.488,0.506,0.544,0.554],[0.502,0.532,0.532,0.554]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700251"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5168,"std":0.01070327052820773,"num_trials":10,"pass_rates":[[0.488,0.508,0.536,0.554],[0.508,0.536,0.546,0.552],[0.492,0.514,0.528,0.554],[0.51,0.508,0.548,0.554],[0.496,0.508,0.538,0.554],[0.476,0.522,0.542,0.552],[0.496,0.508,0.552,0.554],[0.478,0.526,0.54,0.552],[0.488,0.506,0.544,0.554],[0.502,0.532,0.532,0.554]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700259"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5406000000000001,"std":0.006988562083862465,"num_trials":10,"pass_rates":[[0.488,0.508,0.536,0.554],[0.508,0.536,0.546,0.552],[0.492,0.514,0.528,0.554],[0.51,0.508,0.548,0.554],[0.496,0.508,0.538,0.554],[0.476,0.522,0.542,0.552],[0.496,0.508,0.552,0.554],[0.478,0.526,0.54,0.552],[0.488,0.506,0.544,0.554],[0.502,0.532,0.532,0.554]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700266"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1900","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5534000000000001,"std":0.0009165151389911689,"num_trials":10,"pass_rates":[[0.488,0.508,0.536,0.554],[0.508,0.536,0.546,0.552],[0.492,0.514,0.528,0.554],[0.51,0.508,0.548,0.554],[0.496,0.508,0.538,0.554],[0.476,0.522,0.542,0.552],[0.496,0.508,0.552,0.554],[0.478,0.526,0.54,0.552],[0.488,0.506,0.544,0.554],[0.502,0.532,0.532,0.554]],"time_stamp":"UTC+8 2023-08-24 03:06:31 700281"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5022,"std":0.010447966309287189,"num_trials":10,"pass_rates":[[0.498,0.548,0.53,0.572],[0.526,0.524,0.544,0.572],[0.504,0.516,0.534,0.572],[0.506,0.514,0.54,0.572],[0.488,0.53,0.542,0.572],[0.508,0.508,0.528,0.572],[0.49,0.55,0.542,0.572],[0.494,0.54,0.556,0.572],[0.5,0.534,0.552,0.572],[0.508,0.532,0.542,0.572]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319508"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5296000000000001,"std":0.0134699665923862,"num_trials":10,"pass_rates":[[0.498,0.548,0.53,0.572],[0.526,0.524,0.544,0.572],[0.504,0.516,0.534,0.572],[0.506,0.514,0.54,0.572],[0.488,0.53,0.542,0.572],[0.508,0.508,0.528,0.572],[0.49,0.55,0.542,0.572],[0.494,0.54,0.556,0.572],[0.5,0.534,0.552,0.572],[0.508,0.532,0.542,0.572]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319551"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.541,"std":0.008354639429682176,"num_trials":10,"pass_rates":[[0.498,0.548,0.53,0.572],[0.526,0.524,0.544,0.572],[0.504,0.516,0.534,0.572],[0.506,0.514,0.54,0.572],[0.488,0.53,0.542,0.572],[0.508,0.508,0.528,0.572],[0.49,0.55,0.542,0.572],[0.494,0.54,0.556,0.572],[0.5,0.534,0.552,0.572],[0.508,0.532,0.542,0.572]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319561"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.572,"std":0.0,"num_trials":10,"pass_rates":[[0.498,0.548,0.53,0.572],[0.526,0.524,0.544,0.572],[0.504,0.516,0.534,0.572],[0.506,0.514,0.54,0.572],[0.488,0.53,0.542,0.572],[0.508,0.508,0.528,0.572],[0.49,0.55,0.542,0.572],[0.494,0.54,0.556,0.572],[0.5,0.534,0.552,0.572],[0.508,0.532,0.542,0.572]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319582"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4946,"std":0.01277654100294755,"num_trials":10,"pass_rates":[[0.492,0.538,0.53,0.56],[0.518,0.516,0.534,0.56],[0.488,0.506,0.526,0.56],[0.5,0.508,0.53,0.56],[0.468,0.51,0.522,0.56],[0.508,0.504,0.53,0.56],[0.496,0.542,0.53,0.56],[0.484,0.522,0.554,0.56],[0.496,0.512,0.562,0.56],[0.496,0.524,0.544,0.56]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319591"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5182000000000001,"std":0.012536347155371866,"num_trials":10,"pass_rates":[[0.492,0.538,0.53,0.56],[0.518,0.516,0.534,0.56],[0.488,0.506,0.526,0.56],[0.5,0.508,0.53,0.56],[0.468,0.51,0.522,0.56],[0.508,0.504,0.53,0.56],[0.496,0.542,0.53,0.56],[0.484,0.522,0.554,0.56],[0.496,0.512,0.562,0.56],[0.496,0.524,0.544,0.56]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319599"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5362000000000002,"std":0.012278436382536673,"num_trials":10,"pass_rates":[[0.492,0.538,0.53,0.56],[0.518,0.516,0.534,0.56],[0.488,0.506,0.526,0.56],[0.5,0.508,0.53,0.56],[0.468,0.51,0.522,0.56],[0.508,0.504,0.53,0.56],[0.496,0.542,0.53,0.56],[0.484,0.522,0.554,0.56],[0.496,0.512,0.562,0.56],[0.496,0.524,0.544,0.56]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319606"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5600000000000002,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.492,0.538,0.53,0.56],[0.518,0.516,0.534,0.56],[0.488,0.506,0.526,0.56],[0.5,0.508,0.53,0.56],[0.468,0.51,0.522,0.56],[0.508,0.504,0.53,0.56],[0.496,0.542,0.53,0.56],[0.484,0.522,0.554,0.56],[0.496,0.512,0.562,0.56],[0.496,0.524,0.544,0.56]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319613"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.502,"std":0.010733126291999,"num_trials":10,"pass_rates":[[0.496,0.55,0.522,0.56],[0.524,0.512,0.534,0.56],[0.502,0.52,0.528,0.56],[0.506,0.514,0.538,0.56],[0.486,0.524,0.534,0.56],[0.514,0.506,0.53,0.56],[0.492,0.548,0.546,0.56],[0.494,0.536,0.556,0.56],[0.498,0.538,0.556,0.56],[0.508,0.532,0.544,0.56]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319621"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5280000000000001,"std":0.01442220510185597,"num_trials":10,"pass_rates":[[0.496,0.55,0.522,0.56],[0.524,0.512,0.534,0.56],[0.502,0.52,0.528,0.56],[0.506,0.514,0.538,0.56],[0.486,0.524,0.534,0.56],[0.514,0.506,0.53,0.56],[0.492,0.548,0.546,0.56],[0.494,0.536,0.556,0.56],[0.498,0.538,0.556,0.56],[0.508,0.532,0.544,0.56]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319628"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5388,"std":0.01092520022699814,"num_trials":10,"pass_rates":[[0.496,0.55,0.522,0.56],[0.524,0.512,0.534,0.56],[0.502,0.52,0.528,0.56],[0.506,0.514,0.538,0.56],[0.486,0.524,0.534,0.56],[0.514,0.506,0.53,0.56],[0.492,0.548,0.546,0.56],[0.494,0.536,0.556,0.56],[0.498,0.538,0.556,0.56],[0.508,0.532,0.544,0.56]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319635"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5600000000000002,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.496,0.55,0.522,0.56],[0.524,0.512,0.534,0.56],[0.502,0.52,0.528,0.56],[0.506,0.514,0.538,0.56],[0.486,0.524,0.534,0.56],[0.514,0.506,0.53,0.56],[0.492,0.548,0.546,0.56],[0.494,0.536,0.556,0.56],[0.498,0.538,0.556,0.56],[0.508,0.532,0.544,0.56]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319642"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.49060000000000004,"std":0.01120892501536164,"num_trials":10,"pass_rates":[[0.484,0.534,0.526,0.548],[0.506,0.51,0.528,0.548],[0.486,0.508,0.518,0.548],[0.502,0.502,0.518,0.548],[0.466,0.5,0.518,0.548],[0.498,0.512,0.534,0.548],[0.49,0.54,0.532,0.548],[0.492,0.506,0.552,0.548],[0.482,0.52,0.542,0.548],[0.5,0.51,0.546,0.548]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319649"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5142,"std":0.01260000000000001,"num_trials":10,"pass_rates":[[0.484,0.534,0.526,0.548],[0.506,0.51,0.528,0.548],[0.486,0.508,0.518,0.548],[0.502,0.502,0.518,0.548],[0.466,0.5,0.518,0.548],[0.498,0.512,0.534,0.548],[0.49,0.54,0.532,0.548],[0.492,0.506,0.552,0.548],[0.482,0.52,0.542,0.548],[0.5,0.51,0.546,0.548]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319656"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5313999999999999,"std":0.01156027681329475,"num_trials":10,"pass_rates":[[0.484,0.534,0.526,0.548],[0.506,0.51,0.528,0.548],[0.486,0.508,0.518,0.548],[0.502,0.502,0.518,0.548],[0.466,0.5,0.518,0.548],[0.498,0.512,0.534,0.548],[0.49,0.54,0.532,0.548],[0.492,0.506,0.552,0.548],[0.482,0.52,0.542,0.548],[0.5,0.51,0.546,0.548]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319663"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1800","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.548,"std":0.0,"num_trials":10,"pass_rates":[[0.484,0.534,0.526,0.548],[0.506,0.51,0.528,0.548],[0.486,0.508,0.518,0.548],[0.502,0.502,0.518,0.548],[0.466,0.5,0.518,0.548],[0.498,0.512,0.534,0.548],[0.49,0.54,0.532,0.548],[0.492,0.506,0.552,0.548],[0.482,0.52,0.542,0.548],[0.5,0.51,0.546,0.548]],"time_stamp":"UTC+8 2023-08-24 03:30:51 319670"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5024000000000001,"std":0.014907716122867391,"num_trials":10,"pass_rates":[[0.504,0.512,0.532,0.53],[0.506,0.526,0.538,0.53],[0.504,0.526,0.53,0.53],[0.502,0.542,0.532,0.53],[0.482,0.516,0.526,0.53],[0.484,0.52,0.53,0.53],[0.498,0.506,0.534,0.53],[0.494,0.518,0.526,0.53],[0.512,0.51,0.52,0.53],[0.538,0.504,0.542,0.53]],"time_stamp":"UTC+8 2023-08-24 03:55:15 593963"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.518,"std":0.010733126291999,"num_trials":10,"pass_rates":[[0.504,0.512,0.532,0.53],[0.506,0.526,0.538,0.53],[0.504,0.526,0.53,0.53],[0.502,0.542,0.532,0.53],[0.482,0.516,0.526,0.53],[0.484,0.52,0.53,0.53],[0.498,0.506,0.534,0.53],[0.494,0.518,0.526,0.53],[0.512,0.51,0.52,0.53],[0.538,0.504,0.542,0.53]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594016"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.531,"std":0.005949789912257412,"num_trials":10,"pass_rates":[[0.504,0.512,0.532,0.53],[0.506,0.526,0.538,0.53],[0.504,0.526,0.53,0.53],[0.502,0.542,0.532,0.53],[0.482,0.516,0.526,0.53],[0.484,0.52,0.53,0.53],[0.498,0.506,0.534,0.53],[0.494,0.518,0.526,0.53],[0.512,0.51,0.52,0.53],[0.538,0.504,0.542,0.53]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594026"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5300000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.504,0.512,0.532,0.53],[0.506,0.526,0.538,0.53],[0.504,0.526,0.53,0.53],[0.502,0.542,0.532,0.53],[0.482,0.516,0.526,0.53],[0.484,0.52,0.53,0.53],[0.498,0.506,0.534,0.53],[0.494,0.518,0.526,0.53],[0.512,0.51,0.52,0.53],[0.538,0.504,0.542,0.53]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594035"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.49139999999999995,"std":0.01408687332235227,"num_trials":10,"pass_rates":[[0.496,0.486,0.514,0.52],[0.478,0.538,0.52,0.52],[0.49,0.534,0.518,0.52],[0.506,0.522,0.51,0.52],[0.48,0.516,0.52,0.52],[0.464,0.494,0.542,0.52],[0.496,0.478,0.522,0.52],[0.486,0.494,0.514,0.52],[0.506,0.516,0.52,0.52],[0.512,0.506,0.532,0.52]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594043"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5084,"std":0.019179155351578978,"num_trials":10,"pass_rates":[[0.496,0.486,0.514,0.52],[0.478,0.538,0.52,0.52],[0.49,0.534,0.518,0.52],[0.506,0.522,0.51,0.52],[0.48,0.516,0.52,0.52],[0.464,0.494,0.542,0.52],[0.496,0.478,0.522,0.52],[0.486,0.494,0.514,0.52],[0.506,0.516,0.52,0.52],[0.512,0.506,0.532,0.52]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594050"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5212000000000001,"std":0.008908422980528043,"num_trials":10,"pass_rates":[[0.496,0.486,0.514,0.52],[0.478,0.538,0.52,0.52],[0.49,0.534,0.518,0.52],[0.506,0.522,0.51,0.52],[0.48,0.516,0.52,0.52],[0.464,0.494,0.542,0.52],[0.496,0.478,0.522,0.52],[0.486,0.494,0.514,0.52],[0.506,0.516,0.52,0.52],[0.512,0.506,0.532,0.52]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594057"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5199999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.496,0.486,0.514,0.52],[0.478,0.538,0.52,0.52],[0.49,0.534,0.518,0.52],[0.506,0.522,0.51,0.52],[0.48,0.516,0.52,0.52],[0.464,0.494,0.542,0.52],[0.496,0.478,0.522,0.52],[0.486,0.494,0.514,0.52],[0.506,0.516,0.52,0.52],[0.512,0.506,0.532,0.52]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594064"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5016,"std":0.012059850745345078,"num_trials":10,"pass_rates":[[0.508,0.508,0.528,0.528],[0.5,0.526,0.532,0.528],[0.504,0.532,0.53,0.528],[0.5,0.55,0.532,0.528],[0.492,0.512,0.518,0.528],[0.486,0.514,0.54,0.528],[0.496,0.498,0.528,0.528],[0.492,0.516,0.524,0.528],[0.506,0.512,0.512,0.528],[0.532,0.502,0.55,0.528]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594071"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5170000000000001,"std":0.014567086187704128,"num_trials":10,"pass_rates":[[0.508,0.508,0.528,0.528],[0.5,0.526,0.532,0.528],[0.504,0.532,0.53,0.528],[0.5,0.55,0.532,0.528],[0.492,0.512,0.518,0.528],[0.486,0.514,0.54,0.528],[0.496,0.498,0.528,0.528],[0.492,0.516,0.524,0.528],[0.506,0.512,0.512,0.528],[0.532,0.502,0.55,0.528]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594078"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5294,"std":0.010081666528902857,"num_trials":10,"pass_rates":[[0.508,0.508,0.528,0.528],[0.5,0.526,0.532,0.528],[0.504,0.532,0.53,0.528],[0.5,0.55,0.532,0.528],[0.492,0.512,0.518,0.528],[0.486,0.514,0.54,0.528],[0.496,0.498,0.528,0.528],[0.492,0.516,0.524,0.528],[0.506,0.512,0.512,0.528],[0.532,0.502,0.55,0.528]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594085"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5280000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.508,0.508,0.528,0.528],[0.5,0.526,0.532,0.528],[0.504,0.532,0.53,0.528],[0.5,0.55,0.532,0.528],[0.492,0.512,0.518,0.528],[0.486,0.514,0.54,0.528],[0.496,0.498,0.528,0.528],[0.492,0.516,0.524,0.528],[0.506,0.512,0.512,0.528],[0.532,0.502,0.55,0.528]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594092"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.49139999999999995,"std":0.01649363513601535,"num_trials":10,"pass_rates":[[0.496,0.486,0.51,0.516],[0.48,0.512,0.514,0.516],[0.482,0.52,0.51,0.516],[0.512,0.514,0.518,0.516],[0.474,0.524,0.51,0.516],[0.458,0.49,0.53,0.516],[0.504,0.476,0.5,0.516],[0.496,0.492,0.494,0.516],[0.502,0.508,0.51,0.516],[0.51,0.504,0.524,0.516]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594099"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5025999999999999,"std":0.015047923444781357,"num_trials":10,"pass_rates":[[0.496,0.486,0.51,0.516],[0.48,0.512,0.514,0.516],[0.482,0.52,0.51,0.516],[0.512,0.514,0.518,0.516],[0.474,0.524,0.51,0.516],[0.458,0.49,0.53,0.516],[0.504,0.476,0.5,0.516],[0.496,0.492,0.494,0.516],[0.502,0.508,0.51,0.516],[0.51,0.504,0.524,0.516]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594106"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.512,"std":0.009959919678390994,"num_trials":10,"pass_rates":[[0.496,0.486,0.51,0.516],[0.48,0.512,0.514,0.516],[0.482,0.52,0.51,0.516],[0.512,0.514,0.518,0.516],[0.474,0.524,0.51,0.516],[0.458,0.49,0.53,0.516],[0.504,0.476,0.5,0.516],[0.496,0.492,0.494,0.516],[0.502,0.508,0.51,0.516],[0.51,0.504,0.524,0.516]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594113"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1700","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.516,"std":0.0,"num_trials":10,"pass_rates":[[0.496,0.486,0.51,0.516],[0.48,0.512,0.514,0.516],[0.482,0.52,0.51,0.516],[0.512,0.514,0.518,0.516],[0.474,0.524,0.51,0.516],[0.458,0.49,0.53,0.516],[0.504,0.476,0.5,0.516],[0.496,0.492,0.494,0.516],[0.502,0.508,0.51,0.516],[0.51,0.504,0.524,0.516]],"time_stamp":"UTC+8 2023-08-24 03:55:15 594120"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5067999999999999,"std":0.01203993355463394,"num_trials":10,"pass_rates":[[0.51,0.512,0.53,0.532],[0.518,0.516,0.54,0.532],[0.524,0.51,0.518,0.532],[0.506,0.512,0.498,0.532],[0.5,0.518,0.534,0.532],[0.482,0.528,0.52,0.532],[0.496,0.542,0.548,0.532],[0.522,0.506,0.544,0.532],[0.504,0.502,0.514,0.532],[0.506,0.524,0.534,0.532]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940014"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.517,"std":0.011144505372604037,"num_trials":10,"pass_rates":[[0.51,0.512,0.53,0.532],[0.518,0.516,0.54,0.532],[0.524,0.51,0.518,0.532],[0.506,0.512,0.498,0.532],[0.5,0.518,0.534,0.532],[0.482,0.528,0.52,0.532],[0.496,0.542,0.548,0.532],[0.522,0.506,0.544,0.532],[0.504,0.502,0.514,0.532],[0.506,0.524,0.534,0.532]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940068"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.528,"std":0.014615060725156101,"num_trials":10,"pass_rates":[[0.51,0.512,0.53,0.532],[0.518,0.516,0.54,0.532],[0.524,0.51,0.518,0.532],[0.506,0.512,0.498,0.532],[0.5,0.518,0.534,0.532],[0.482,0.528,0.52,0.532],[0.496,0.542,0.548,0.532],[0.522,0.506,0.544,0.532],[0.504,0.502,0.514,0.532],[0.506,0.524,0.534,0.532]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940078"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.532,"std":0.0,"num_trials":10,"pass_rates":[[0.51,0.512,0.53,0.532],[0.518,0.516,0.54,0.532],[0.524,0.51,0.518,0.532],[0.506,0.512,0.498,0.532],[0.5,0.518,0.534,0.532],[0.482,0.528,0.52,0.532],[0.496,0.542,0.548,0.532],[0.522,0.506,0.544,0.532],[0.504,0.502,0.514,0.532],[0.506,0.524,0.534,0.532]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940086"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4948,"std":0.01656985214176639,"num_trials":10,"pass_rates":[[0.502,0.504,0.544,0.524],[0.508,0.502,0.51,0.524],[0.518,0.486,0.522,0.524],[0.496,0.522,0.492,0.524],[0.49,0.512,0.528,0.524],[0.466,0.516,0.52,0.524],[0.488,0.53,0.54,0.524],[0.52,0.506,0.528,0.524],[0.478,0.504,0.512,0.524],[0.482,0.512,0.52,0.524]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940094"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5094000000000001,"std":0.011456002793295759,"num_trials":10,"pass_rates":[[0.502,0.504,0.544,0.524],[0.508,0.502,0.51,0.524],[0.518,0.486,0.522,0.524],[0.496,0.522,0.492,0.524],[0.49,0.512,0.528,0.524],[0.466,0.516,0.52,0.524],[0.488,0.53,0.54,0.524],[0.52,0.506,0.528,0.524],[0.478,0.504,0.512,0.524],[0.482,0.512,0.52,0.524]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940102"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5216,"std":0.014249210504445513,"num_trials":10,"pass_rates":[[0.502,0.504,0.544,0.524],[0.508,0.502,0.51,0.524],[0.518,0.486,0.522,0.524],[0.496,0.522,0.492,0.524],[0.49,0.512,0.528,0.524],[0.466,0.516,0.52,0.524],[0.488,0.53,0.54,0.524],[0.52,0.506,0.528,0.524],[0.478,0.504,0.512,0.524],[0.482,0.512,0.52,0.524]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940109"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.524,"std":0.0,"num_trials":10,"pass_rates":[[0.502,0.504,0.544,0.524],[0.508,0.502,0.51,0.524],[0.518,0.486,0.522,0.524],[0.496,0.522,0.492,0.524],[0.49,0.512,0.528,0.524],[0.466,0.516,0.52,0.524],[0.488,0.53,0.54,0.524],[0.52,0.506,0.528,0.524],[0.478,0.504,0.512,0.524],[0.482,0.512,0.52,0.524]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940116"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5044,"std":0.01295530779256133,"num_trials":10,"pass_rates":[[0.51,0.514,0.534,0.528],[0.516,0.51,0.53,0.528],[0.518,0.504,0.52,0.528],[0.5,0.506,0.506,0.528],[0.494,0.512,0.54,0.528],[0.476,0.522,0.52,0.528],[0.496,0.538,0.548,0.528],[0.522,0.498,0.54,0.528],[0.504,0.5,0.512,0.528],[0.508,0.512,0.532,0.528]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940124"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5115999999999999,"std":0.011056219968868212,"num_trials":10,"pass_rates":[[0.51,0.514,0.534,0.528],[0.516,0.51,0.53,0.528],[0.518,0.504,0.52,0.528],[0.5,0.506,0.506,0.528],[0.494,0.512,0.54,0.528],[0.476,0.522,0.52,0.528],[0.496,0.538,0.548,0.528],[0.522,0.498,0.54,0.528],[0.504,0.5,0.512,0.528],[0.508,0.512,0.532,0.528]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940132"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5282,"std":0.012694880858046691,"num_trials":10,"pass_rates":[[0.51,0.514,0.534,0.528],[0.516,0.51,0.53,0.528],[0.518,0.504,0.52,0.528],[0.5,0.506,0.506,0.528],[0.494,0.512,0.54,0.528],[0.476,0.522,0.52,0.528],[0.496,0.538,0.548,0.528],[0.522,0.498,0.54,0.528],[0.504,0.5,0.512,0.528],[0.508,0.512,0.532,0.528]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940138"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5280000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.51,0.514,0.534,0.528],[0.516,0.51,0.53,0.528],[0.518,0.504,0.52,0.528],[0.5,0.506,0.506,0.528],[0.494,0.512,0.54,0.528],[0.476,0.522,0.52,0.528],[0.496,0.538,0.548,0.528],[0.522,0.498,0.54,0.528],[0.504,0.5,0.512,0.528],[0.508,0.512,0.532,0.528]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940145"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.49339999999999995,"std":0.015596153371905536,"num_trials":10,"pass_rates":[[0.512,0.502,0.534,0.526],[0.508,0.5,0.514,0.526],[0.506,0.484,0.52,0.526],[0.496,0.52,0.5,0.526],[0.494,0.502,0.52,0.526],[0.47,0.518,0.518,0.526],[0.478,0.528,0.52,0.526],[0.512,0.488,0.516,0.526],[0.488,0.508,0.52,0.526],[0.47,0.502,0.516,0.526]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940153"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5052,"std":0.013090454537562869,"num_trials":10,"pass_rates":[[0.512,0.502,0.534,0.526],[0.508,0.5,0.514,0.526],[0.506,0.484,0.52,0.526],[0.496,0.52,0.5,0.526],[0.494,0.502,0.52,0.526],[0.47,0.518,0.518,0.526],[0.478,0.528,0.52,0.526],[0.512,0.488,0.516,0.526],[0.488,0.508,0.52,0.526],[0.47,0.502,0.516,0.526]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940160"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5177999999999999,"std":0.007871467461661778,"num_trials":10,"pass_rates":[[0.512,0.502,0.534,0.526],[0.508,0.5,0.514,0.526],[0.506,0.484,0.52,0.526],[0.496,0.52,0.5,0.526],[0.494,0.502,0.52,0.526],[0.47,0.518,0.518,0.526],[0.478,0.528,0.52,0.526],[0.512,0.488,0.516,0.526],[0.488,0.508,0.52,0.526],[0.47,0.502,0.516,0.526]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940167"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1600","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5259999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.512,0.502,0.534,0.526],[0.508,0.5,0.514,0.526],[0.506,0.484,0.52,0.526],[0.496,0.52,0.5,0.526],[0.494,0.502,0.52,0.526],[0.47,0.518,0.518,0.526],[0.478,0.528,0.52,0.526],[0.512,0.488,0.516,0.526],[0.488,0.508,0.52,0.526],[0.47,0.502,0.516,0.526]],"time_stamp":"UTC+8 2023-08-24 04:19:05 940174"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5112000000000001,"std":0.009431860898041286,"num_trials":10,"pass_rates":[[0.534,0.49,0.526,0.536],[0.506,0.526,0.56,0.536],[0.502,0.546,0.56,0.536],[0.512,0.534,0.546,0.536],[0.51,0.542,0.538,0.536],[0.506,0.53,0.55,0.536],[0.512,0.532,0.526,0.536],[0.502,0.528,0.55,0.536],[0.522,0.524,0.532,0.536],[0.506,0.556,0.546,0.536]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973595"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5307999999999999,"std":0.016569852141766397,"num_trials":10,"pass_rates":[[0.534,0.49,0.526,0.536],[0.506,0.526,0.56,0.536],[0.502,0.546,0.56,0.536],[0.512,0.534,0.546,0.536],[0.51,0.542,0.538,0.536],[0.506,0.53,0.55,0.536],[0.512,0.532,0.526,0.536],[0.502,0.528,0.55,0.536],[0.522,0.524,0.532,0.536],[0.506,0.556,0.546,0.536]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973805"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5434,"std":0.011901260437449481,"num_trials":10,"pass_rates":[[0.534,0.49,0.526,0.536],[0.506,0.526,0.56,0.536],[0.502,0.546,0.56,0.536],[0.512,0.534,0.546,0.536],[0.51,0.542,0.538,0.536],[0.506,0.53,0.55,0.536],[0.512,0.532,0.526,0.536],[0.502,0.528,0.55,0.536],[0.522,0.524,0.532,0.536],[0.506,0.556,0.546,0.536]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973815"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5359999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.534,0.49,0.526,0.536],[0.506,0.526,0.56,0.536],[0.502,0.546,0.56,0.536],[0.512,0.534,0.546,0.536],[0.51,0.542,0.538,0.536],[0.506,0.53,0.55,0.536],[0.512,0.532,0.526,0.536],[0.502,0.528,0.55,0.536],[0.522,0.524,0.532,0.536],[0.506,0.556,0.546,0.536]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973823"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.5006,"std":0.011732007500849982,"num_trials":10,"pass_rates":[[0.522,0.476,0.534,0.524],[0.494,0.532,0.53,0.524],[0.492,0.526,0.564,0.524],[0.49,0.524,0.53,0.524],[0.492,0.534,0.544,0.524],[0.514,0.528,0.546,0.524],[0.506,0.512,0.532,0.524],[0.484,0.508,0.538,0.524],[0.512,0.52,0.514,0.524],[0.5,0.542,0.546,0.524]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973830"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5202,"std":0.01756018223140069,"num_trials":10,"pass_rates":[[0.522,0.476,0.534,0.524],[0.494,0.532,0.53,0.524],[0.492,0.526,0.564,0.524],[0.49,0.524,0.53,0.524],[0.492,0.534,0.544,0.524],[0.514,0.528,0.546,0.524],[0.506,0.512,0.532,0.524],[0.484,0.508,0.538,0.524],[0.512,0.52,0.514,0.524],[0.5,0.542,0.546,0.524]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973838"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5378000000000001,"std":0.012631706139710491,"num_trials":10,"pass_rates":[[0.522,0.476,0.534,0.524],[0.494,0.532,0.53,0.524],[0.492,0.526,0.564,0.524],[0.49,0.524,0.53,0.524],[0.492,0.534,0.544,0.524],[0.514,0.528,0.546,0.524],[0.506,0.512,0.532,0.524],[0.484,0.508,0.538,0.524],[0.512,0.52,0.514,0.524],[0.5,0.542,0.546,0.524]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973845"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.524,"std":0.0,"num_trials":10,"pass_rates":[[0.522,0.476,0.534,0.524],[0.494,0.532,0.53,0.524],[0.492,0.526,0.564,0.524],[0.49,0.524,0.53,0.524],[0.492,0.534,0.544,0.524],[0.514,0.528,0.546,0.524],[0.506,0.512,0.532,0.524],[0.484,0.508,0.538,0.524],[0.512,0.52,0.514,0.524],[0.5,0.542,0.546,0.524]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973856"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5106,"std":0.008720091742636665,"num_trials":10,"pass_rates":[[0.53,0.488,0.532,0.524],[0.502,0.516,0.556,0.524],[0.504,0.542,0.552,0.524],[0.512,0.524,0.536,0.524],[0.508,0.538,0.536,0.524],[0.508,0.518,0.55,0.524],[0.508,0.524,0.528,0.524],[0.504,0.524,0.54,0.524],[0.524,0.522,0.538,0.524],[0.506,0.552,0.546,0.524]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973864"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5248000000000002,"std":0.016424372134118265,"num_trials":10,"pass_rates":[[0.53,0.488,0.532,0.524],[0.502,0.516,0.556,0.524],[0.504,0.542,0.552,0.524],[0.512,0.524,0.536,0.524],[0.508,0.538,0.536,0.524],[0.508,0.518,0.55,0.524],[0.508,0.524,0.528,0.524],[0.504,0.524,0.54,0.524],[0.524,0.522,0.538,0.524],[0.506,0.552,0.546,0.524]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973872"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5414000000000001,"std":0.008720091742636665,"num_trials":10,"pass_rates":[[0.53,0.488,0.532,0.524],[0.502,0.516,0.556,0.524],[0.504,0.542,0.552,0.524],[0.512,0.524,0.536,0.524],[0.508,0.538,0.536,0.524],[0.508,0.518,0.55,0.524],[0.508,0.524,0.528,0.524],[0.504,0.524,0.54,0.524],[0.524,0.522,0.538,0.524],[0.506,0.552,0.546,0.524]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973878"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.524,"std":0.0,"num_trials":10,"pass_rates":[[0.53,0.488,0.532,0.524],[0.502,0.516,0.556,0.524],[0.504,0.542,0.552,0.524],[0.512,0.524,0.536,0.524],[0.508,0.538,0.536,0.524],[0.508,0.518,0.55,0.524],[0.508,0.524,0.528,0.524],[0.504,0.524,0.54,0.524],[0.524,0.522,0.538,0.524],[0.506,0.552,0.546,0.524]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973885"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.4998,"std":0.011258774356030066,"num_trials":10,"pass_rates":[[0.514,0.472,0.538,0.51],[0.492,0.512,0.508,0.508],[0.49,0.524,0.542,0.508],[0.488,0.512,0.51,0.508],[0.488,0.522,0.526,0.508],[0.516,0.522,0.526,0.508],[0.5,0.508,0.508,0.508],[0.494,0.512,0.516,0.51],[0.518,0.51,0.51,0.508],[0.498,0.524,0.53,0.51]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973892"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5117999999999999,"std":0.01451757555516762,"num_trials":10,"pass_rates":[[0.514,0.472,0.538,0.51],[0.492,0.512,0.508,0.508],[0.49,0.524,0.542,0.508],[0.488,0.512,0.51,0.508],[0.488,0.522,0.526,0.508],[0.516,0.522,0.526,0.508],[0.5,0.508,0.508,0.508],[0.494,0.512,0.516,0.51],[0.518,0.51,0.51,0.508],[0.498,0.524,0.53,0.51]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973899"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5214,"std":0.012101239605924689,"num_trials":10,"pass_rates":[[0.514,0.472,0.538,0.51],[0.492,0.512,0.508,0.508],[0.49,0.524,0.542,0.508],[0.488,0.512,0.51,0.508],[0.488,0.522,0.526,0.508],[0.516,0.522,0.526,0.508],[0.5,0.508,0.508,0.508],[0.494,0.512,0.516,0.51],[0.518,0.51,0.51,0.508],[0.498,0.524,0.53,0.51]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973906"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1500","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5085999999999999,"std":0.0009165151389911689,"num_trials":10,"pass_rates":[[0.514,0.472,0.538,0.51],[0.492,0.512,0.508,0.508],[0.49,0.524,0.542,0.508],[0.488,0.512,0.51,0.508],[0.488,0.522,0.526,0.508],[0.516,0.522,0.526,0.508],[0.5,0.508,0.508,0.508],[0.494,0.512,0.516,0.51],[0.518,0.51,0.51,0.508],[0.498,0.524,0.53,0.51]],"time_stamp":"UTC+8 2023-08-24 04:42:57 973913"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5006000000000002,"std":0.014228141129465941,"num_trials":10,"pass_rates":[[0.506,0.528,0.526,0.556],[0.526,0.518,0.544,0.556],[0.504,0.55,0.548,0.556],[0.51,0.51,0.532,0.556],[0.474,0.514,0.53,0.556],[0.496,0.524,0.55,0.556],[0.51,0.502,0.548,0.556],[0.502,0.52,0.532,0.556],[0.48,0.516,0.552,0.556],[0.498,0.528,0.532,0.556]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409501"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5210000000000001,"std":0.01230447073221763,"num_trials":10,"pass_rates":[[0.506,0.528,0.526,0.556],[0.526,0.518,0.544,0.556],[0.504,0.55,0.548,0.556],[0.51,0.51,0.532,0.556],[0.474,0.514,0.53,0.556],[0.496,0.524,0.55,0.556],[0.51,0.502,0.548,0.556],[0.502,0.52,0.532,0.556],[0.48,0.516,0.552,0.556],[0.498,0.528,0.532,0.556]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409554"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5394,"std":0.0093402355430685,"num_trials":10,"pass_rates":[[0.506,0.528,0.526,0.556],[0.526,0.518,0.544,0.556],[0.504,0.55,0.548,0.556],[0.51,0.51,0.532,0.556],[0.474,0.514,0.53,0.556],[0.496,0.524,0.55,0.556],[0.51,0.502,0.548,0.556],[0.502,0.52,0.532,0.556],[0.48,0.516,0.552,0.556],[0.498,0.528,0.532,0.556]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409564"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.556,"std":0.0,"num_trials":10,"pass_rates":[[0.506,0.528,0.526,0.556],[0.526,0.518,0.544,0.556],[0.504,0.55,0.548,0.556],[0.51,0.51,0.532,0.556],[0.474,0.514,0.53,0.556],[0.496,0.524,0.55,0.556],[0.51,0.502,0.548,0.556],[0.502,0.52,0.532,0.556],[0.48,0.516,0.552,0.556],[0.498,0.528,0.532,0.556]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409572"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.49559999999999993,"std":0.014192955999368151,"num_trials":10,"pass_rates":[[0.506,0.514,0.546,0.56],[0.528,0.512,0.514,0.56],[0.494,0.546,0.548,0.56],[0.494,0.508,0.534,0.56],[0.474,0.504,0.534,0.558],[0.478,0.524,0.556,0.56],[0.502,0.492,0.536,0.558],[0.49,0.516,0.528,0.56],[0.494,0.504,0.548,0.56],[0.496,0.522,0.532,0.56]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409580"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5142,"std":0.013840520221436777,"num_trials":10,"pass_rates":[[0.506,0.514,0.546,0.56],[0.528,0.512,0.514,0.56],[0.494,0.546,0.548,0.56],[0.494,0.508,0.534,0.56],[0.474,0.504,0.534,0.558],[0.478,0.524,0.556,0.56],[0.502,0.492,0.536,0.558],[0.49,0.516,0.528,0.56],[0.494,0.504,0.548,0.56],[0.496,0.522,0.532,0.56]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409587"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5376000000000001,"std":0.011551623262554931,"num_trials":10,"pass_rates":[[0.506,0.514,0.546,0.56],[0.528,0.512,0.514,0.56],[0.494,0.546,0.548,0.56],[0.494,0.508,0.534,0.56],[0.474,0.504,0.534,0.558],[0.478,0.524,0.556,0.56],[0.502,0.492,0.536,0.558],[0.49,0.516,0.528,0.56],[0.494,0.504,0.548,0.56],[0.496,0.522,0.532,0.56]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409594"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5596000000000002,"std":0.0008000000000000007,"num_trials":10,"pass_rates":[[0.506,0.514,0.546,0.56],[0.528,0.512,0.514,0.56],[0.494,0.546,0.548,0.56],[0.494,0.508,0.534,0.56],[0.474,0.504,0.534,0.558],[0.478,0.524,0.556,0.56],[0.502,0.492,0.536,0.558],[0.49,0.516,0.528,0.56],[0.494,0.504,0.548,0.56],[0.496,0.522,0.532,0.56]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409601"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5002,"std":0.014763468427168474,"num_trials":10,"pass_rates":[[0.508,0.52,0.524,0.558],[0.524,0.52,0.534,0.558],[0.502,0.546,0.546,0.558],[0.512,0.506,0.528,0.558],[0.474,0.514,0.53,0.558],[0.494,0.518,0.552,0.558],[0.512,0.502,0.544,0.558],[0.504,0.512,0.534,0.558],[0.478,0.516,0.554,0.558],[0.494,0.518,0.532,0.558]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409608"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5172,"std":0.011142710621747305,"num_trials":10,"pass_rates":[[0.508,0.52,0.524,0.558],[0.524,0.52,0.534,0.558],[0.502,0.546,0.546,0.558],[0.512,0.506,0.528,0.558],[0.474,0.514,0.53,0.558],[0.494,0.518,0.552,0.558],[0.512,0.502,0.544,0.558],[0.504,0.512,0.534,0.558],[0.478,0.516,0.554,0.558],[0.494,0.518,0.532,0.558]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409616"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5378000000000001,"std":0.009897474425326907,"num_trials":10,"pass_rates":[[0.508,0.52,0.524,0.558],[0.524,0.52,0.534,0.558],[0.502,0.546,0.546,0.558],[0.512,0.506,0.528,0.558],[0.474,0.514,0.53,0.558],[0.494,0.518,0.552,0.558],[0.512,0.502,0.544,0.558],[0.504,0.512,0.534,0.558],[0.478,0.516,0.554,0.558],[0.494,0.518,0.532,0.558]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409623"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5579999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.508,0.52,0.524,0.558],[0.524,0.52,0.534,0.558],[0.502,0.546,0.546,0.558],[0.512,0.506,0.528,0.558],[0.474,0.514,0.53,0.558],[0.494,0.518,0.552,0.558],[0.512,0.502,0.544,0.558],[0.504,0.512,0.534,0.558],[0.478,0.516,0.554,0.558],[0.494,0.518,0.532,0.558]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409629"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.49060000000000004,"std":0.013116401945655675,"num_trials":10,"pass_rates":[[0.488,0.494,0.53,0.534],[0.516,0.51,0.514,0.534],[0.494,0.534,0.538,0.534],[0.496,0.506,0.52,0.534],[0.462,0.5,0.528,0.532],[0.48,0.516,0.548,0.534],[0.498,0.482,0.52,0.532],[0.488,0.506,0.516,0.534],[0.488,0.5,0.54,0.534],[0.496,0.516,0.534,0.534]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409637"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5064,"std":0.013380582946942197,"num_trials":10,"pass_rates":[[0.488,0.494,0.53,0.534],[0.516,0.51,0.514,0.534],[0.494,0.534,0.538,0.534],[0.496,0.506,0.52,0.534],[0.462,0.5,0.528,0.532],[0.48,0.516,0.548,0.534],[0.498,0.482,0.52,0.532],[0.488,0.506,0.516,0.534],[0.488,0.5,0.54,0.534],[0.496,0.516,0.534,0.534]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409644"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5288,"std":0.010703270528207732,"num_trials":10,"pass_rates":[[0.488,0.494,0.53,0.534],[0.516,0.51,0.514,0.534],[0.494,0.534,0.538,0.534],[0.496,0.506,0.52,0.534],[0.462,0.5,0.528,0.532],[0.48,0.516,0.548,0.534],[0.498,0.482,0.52,0.532],[0.488,0.506,0.516,0.534],[0.488,0.5,0.54,0.534],[0.496,0.516,0.534,0.534]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409651"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1400","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5336,"std":0.0008000000000000007,"num_trials":10,"pass_rates":[[0.488,0.494,0.53,0.534],[0.516,0.51,0.514,0.534],[0.494,0.534,0.538,0.534],[0.496,0.506,0.52,0.534],[0.462,0.5,0.528,0.532],[0.48,0.516,0.548,0.534],[0.498,0.482,0.52,0.532],[0.488,0.506,0.516,0.534],[0.488,0.5,0.54,0.534],[0.496,0.516,0.534,0.534]],"time_stamp":"UTC+8 2023-08-24 05:06:39 409657"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.49399999999999994,"std":0.01569713349627887,"num_trials":10,"pass_rates":[[0.464,0.516,0.526,0.514],[0.506,0.504,0.53,0.514],[0.51,0.482,0.542,0.514],[0.47,0.51,0.524,0.514],[0.506,0.492,0.528,0.514],[0.484,0.516,0.526,0.514],[0.492,0.512,0.514,0.514],[0.506,0.51,0.534,0.514],[0.508,0.506,0.546,0.514],[0.494,0.526,0.518,0.514]],"time_stamp":"UTC+8 2023-08-24 05:30:56 608675"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5074,"std":0.011934822998268566,"num_trials":10,"pass_rates":[[0.464,0.516,0.526,0.514],[0.506,0.504,0.53,0.514],[0.51,0.482,0.542,0.514],[0.47,0.51,0.524,0.514],[0.506,0.492,0.528,0.514],[0.484,0.516,0.526,0.514],[0.492,0.512,0.514,0.514],[0.506,0.51,0.534,0.514],[0.508,0.506,0.546,0.514],[0.494,0.526,0.518,0.514]],"time_stamp":"UTC+8 2023-08-24 05:30:56 608990"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5288,"std":0.009346657156438346,"num_trials":10,"pass_rates":[[0.464,0.516,0.526,0.514],[0.506,0.504,0.53,0.514],[0.51,0.482,0.542,0.514],[0.47,0.51,0.524,0.514],[0.506,0.492,0.528,0.514],[0.484,0.516,0.526,0.514],[0.492,0.512,0.514,0.514],[0.506,0.51,0.534,0.514],[0.508,0.506,0.546,0.514],[0.494,0.526,0.518,0.514]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609003"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5140000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.464,0.516,0.526,0.514],[0.506,0.504,0.53,0.514],[0.51,0.482,0.542,0.514],[0.47,0.51,0.524,0.514],[0.506,0.492,0.528,0.514],[0.484,0.516,0.526,0.514],[0.492,0.512,0.514,0.514],[0.506,0.51,0.534,0.514],[0.508,0.506,0.546,0.514],[0.494,0.526,0.518,0.514]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609011"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4888,"std":0.009887365675446628,"num_trials":10,"pass_rates":[[0.48,0.516,0.518,0.538],[0.5,0.526,0.538,0.538],[0.51,0.478,0.548,0.538],[0.474,0.516,0.536,0.538],[0.49,0.498,0.532,0.538],[0.486,0.518,0.534,0.538],[0.484,0.516,0.538,0.538],[0.482,0.482,0.546,0.538],[0.494,0.512,0.526,0.538],[0.488,0.514,0.532,0.538]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609020"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5075999999999999,"std":0.015304901175767207,"num_trials":10,"pass_rates":[[0.48,0.516,0.518,0.538],[0.5,0.526,0.538,0.538],[0.51,0.478,0.548,0.538],[0.474,0.516,0.536,0.538],[0.49,0.498,0.532,0.538],[0.486,0.518,0.534,0.538],[0.484,0.516,0.538,0.538],[0.482,0.482,0.546,0.538],[0.494,0.512,0.526,0.538],[0.488,0.514,0.532,0.538]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609027"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5348,"std":0.008352245207128448,"num_trials":10,"pass_rates":[[0.48,0.516,0.518,0.538],[0.5,0.526,0.538,0.538],[0.51,0.478,0.548,0.538],[0.474,0.516,0.536,0.538],[0.49,0.498,0.532,0.538],[0.486,0.518,0.534,0.538],[0.484,0.516,0.538,0.538],[0.482,0.482,0.546,0.538],[0.494,0.512,0.526,0.538],[0.488,0.514,0.532,0.538]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609034"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5380000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.48,0.516,0.518,0.538],[0.5,0.526,0.538,0.538],[0.51,0.478,0.548,0.538],[0.474,0.516,0.536,0.538],[0.49,0.498,0.532,0.538],[0.486,0.518,0.534,0.538],[0.484,0.516,0.538,0.538],[0.482,0.482,0.546,0.538],[0.494,0.512,0.526,0.538],[0.488,0.514,0.532,0.538]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609041"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.4912000000000001,"std":0.012464349160706307,"num_trials":10,"pass_rates":[[0.47,0.512,0.532,0.526],[0.49,0.496,0.522,0.526],[0.5,0.476,0.534,0.526],[0.468,0.514,0.536,0.526],[0.506,0.482,0.536,0.526],[0.49,0.516,0.53,0.526],[0.492,0.514,0.512,0.526],[0.492,0.506,0.516,0.526],[0.506,0.508,0.548,0.526],[0.498,0.514,0.512,0.526]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609048"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5038,"std":0.013636715146984639,"num_trials":10,"pass_rates":[[0.47,0.512,0.532,0.526],[0.49,0.496,0.522,0.526],[0.5,0.476,0.534,0.526],[0.468,0.514,0.536,0.526],[0.506,0.482,0.536,0.526],[0.49,0.516,0.53,0.526],[0.492,0.514,0.512,0.526],[0.492,0.506,0.516,0.526],[0.506,0.508,0.548,0.526],[0.498,0.514,0.512,0.526]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609055"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5278,"std":0.01129424632279641,"num_trials":10,"pass_rates":[[0.47,0.512,0.532,0.526],[0.49,0.496,0.522,0.526],[0.5,0.476,0.534,0.526],[0.468,0.514,0.536,0.526],[0.506,0.482,0.536,0.526],[0.49,0.516,0.53,0.526],[0.492,0.514,0.512,0.526],[0.492,0.506,0.516,0.526],[0.506,0.508,0.548,0.526],[0.498,0.514,0.512,0.526]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609062"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5259999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.47,0.512,0.532,0.526],[0.49,0.496,0.522,0.526],[0.5,0.476,0.534,0.526],[0.468,0.514,0.536,0.526],[0.506,0.482,0.536,0.526],[0.49,0.516,0.53,0.526],[0.492,0.514,0.512,0.526],[0.492,0.506,0.516,0.526],[0.506,0.508,0.548,0.526],[0.498,0.514,0.512,0.526]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609069"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.47859999999999997,"std":0.01472548810735997,"num_trials":10,"pass_rates":[[0.47,0.498,0.506,0.536],[0.502,0.516,0.52,0.536],[0.494,0.462,0.526,0.536],[0.454,0.498,0.538,0.536],[0.492,0.482,0.53,0.536],[0.474,0.504,0.52,0.536],[0.46,0.504,0.532,0.536],[0.47,0.488,0.522,0.536],[0.486,0.502,0.54,0.536],[0.484,0.49,0.524,0.536]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609076"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.4944,"std":0.014136477637657834,"num_trials":10,"pass_rates":[[0.47,0.498,0.506,0.536],[0.502,0.516,0.52,0.536],[0.494,0.462,0.526,0.536],[0.454,0.498,0.538,0.536],[0.492,0.482,0.53,0.536],[0.474,0.504,0.52,0.536],[0.46,0.504,0.532,0.536],[0.47,0.488,0.522,0.536],[0.486,0.502,0.54,0.536],[0.484,0.49,0.524,0.536]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609083"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5258,"std":0.009400000000000009,"num_trials":10,"pass_rates":[[0.47,0.498,0.506,0.536],[0.502,0.516,0.52,0.536],[0.494,0.462,0.526,0.536],[0.454,0.498,0.538,0.536],[0.492,0.482,0.53,0.536],[0.474,0.504,0.52,0.536],[0.46,0.504,0.532,0.536],[0.47,0.488,0.522,0.536],[0.486,0.502,0.54,0.536],[0.484,0.49,0.524,0.536]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609089"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1300","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5359999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.47,0.498,0.506,0.536],[0.502,0.516,0.52,0.536],[0.494,0.462,0.526,0.536],[0.454,0.498,0.538,0.536],[0.492,0.482,0.53,0.536],[0.474,0.504,0.52,0.536],[0.46,0.504,0.532,0.536],[0.47,0.488,0.522,0.536],[0.486,0.502,0.54,0.536],[0.484,0.49,0.524,0.536]],"time_stamp":"UTC+8 2023-08-24 05:30:56 609097"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.48719999999999997,"std":0.012172099243762353,"num_trials":10,"pass_rates":[[0.502,0.514,0.54,0.536],[0.494,0.546,0.526,0.536],[0.486,0.532,0.538,0.536],[0.47,0.514,0.544,0.536],[0.502,0.528,0.536,0.536],[0.49,0.514,0.53,0.536],[0.5,0.502,0.538,0.536],[0.48,0.536,0.53,0.536],[0.482,0.53,0.542,0.536],[0.466,0.496,0.536,0.536]],"time_stamp":"UTC+8 2023-08-24 05:54:56 868978"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5212,"std":0.014945233353815536,"num_trials":10,"pass_rates":[[0.502,0.514,0.54,0.536],[0.494,0.546,0.526,0.536],[0.486,0.532,0.538,0.536],[0.47,0.514,0.544,0.536],[0.502,0.528,0.536,0.536],[0.49,0.514,0.53,0.536],[0.5,0.502,0.538,0.536],[0.48,0.536,0.53,0.536],[0.482,0.53,0.542,0.536],[0.466,0.496,0.536,0.536]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869029"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5360000000000001,"std":0.005440588203494183,"num_trials":10,"pass_rates":[[0.502,0.514,0.54,0.536],[0.494,0.546,0.526,0.536],[0.486,0.532,0.538,0.536],[0.47,0.514,0.544,0.536],[0.502,0.528,0.536,0.536],[0.49,0.514,0.53,0.536],[0.5,0.502,0.538,0.536],[0.48,0.536,0.53,0.536],[0.482,0.53,0.542,0.536],[0.466,0.496,0.536,0.536]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869039"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5359999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.502,0.514,0.54,0.536],[0.494,0.546,0.526,0.536],[0.486,0.532,0.538,0.536],[0.47,0.514,0.544,0.536],[0.502,0.528,0.536,0.536],[0.49,0.514,0.53,0.536],[0.5,0.502,0.538,0.536],[0.48,0.536,0.53,0.536],[0.482,0.53,0.542,0.536],[0.466,0.496,0.536,0.536]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869047"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4826,"std":0.011280070921762857,"num_trials":10,"pass_rates":[[0.504,0.522,0.522,0.516],[0.484,0.522,0.534,0.516],[0.476,0.506,0.516,0.516],[0.458,0.516,0.504,0.516],[0.482,0.518,0.514,0.516],[0.49,0.5,0.51,0.516],[0.486,0.496,0.526,0.516],[0.478,0.506,0.52,0.516],[0.49,0.52,0.52,0.516],[0.478,0.506,0.506,0.516]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869055"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5112,"std":0.00904212364436586,"num_trials":10,"pass_rates":[[0.504,0.522,0.522,0.516],[0.484,0.522,0.534,0.516],[0.476,0.506,0.516,0.516],[0.458,0.516,0.504,0.516],[0.482,0.518,0.514,0.516],[0.49,0.5,0.51,0.516],[0.486,0.496,0.526,0.516],[0.478,0.506,0.52,0.516],[0.49,0.52,0.52,0.516],[0.478,0.506,0.506,0.516]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869063"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5171999999999999,"std":0.008726969691708579,"num_trials":10,"pass_rates":[[0.504,0.522,0.522,0.516],[0.484,0.522,0.534,0.516],[0.476,0.506,0.516,0.516],[0.458,0.516,0.504,0.516],[0.482,0.518,0.514,0.516],[0.49,0.5,0.51,0.516],[0.486,0.496,0.526,0.516],[0.478,0.506,0.52,0.516],[0.49,0.52,0.52,0.516],[0.478,0.506,0.506,0.516]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869070"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.516,"std":0.0,"num_trials":10,"pass_rates":[[0.504,0.522,0.522,0.516],[0.484,0.522,0.534,0.516],[0.476,0.506,0.516,0.516],[0.458,0.516,0.504,0.516],[0.482,0.518,0.514,0.516],[0.49,0.5,0.51,0.516],[0.486,0.496,0.526,0.516],[0.478,0.506,0.52,0.516],[0.49,0.52,0.52,0.516],[0.478,0.506,0.506,0.516]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869077"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.48819999999999997,"std":0.010486181383134672,"num_trials":10,"pass_rates":[[0.5,0.518,0.546,0.53],[0.498,0.536,0.534,0.53],[0.48,0.526,0.536,0.53],[0.472,0.516,0.542,0.53],[0.496,0.532,0.534,0.53],[0.49,0.498,0.532,0.53],[0.502,0.51,0.528,0.53],[0.484,0.546,0.526,0.53],[0.488,0.534,0.54,0.53],[0.472,0.498,0.54,0.53]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869085"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5214000000000001,"std":0.015415576538034522,"num_trials":10,"pass_rates":[[0.5,0.518,0.546,0.53],[0.498,0.536,0.534,0.53],[0.48,0.526,0.536,0.53],[0.472,0.516,0.542,0.53],[0.496,0.532,0.534,0.53],[0.49,0.498,0.532,0.53],[0.502,0.51,0.528,0.53],[0.484,0.546,0.526,0.53],[0.488,0.534,0.54,0.53],[0.472,0.498,0.54,0.53]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869093"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5358,"std":0.005963220606350235,"num_trials":10,"pass_rates":[[0.5,0.518,0.546,0.53],[0.498,0.536,0.534,0.53],[0.48,0.526,0.536,0.53],[0.472,0.516,0.542,0.53],[0.496,0.532,0.534,0.53],[0.49,0.498,0.532,0.53],[0.502,0.51,0.528,0.53],[0.484,0.546,0.526,0.53],[0.488,0.534,0.54,0.53],[0.472,0.498,0.54,0.53]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869100"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5300000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.5,0.518,0.546,0.53],[0.498,0.536,0.534,0.53],[0.48,0.526,0.536,0.53],[0.472,0.516,0.542,0.53],[0.496,0.532,0.534,0.53],[0.49,0.498,0.532,0.53],[0.502,0.51,0.528,0.53],[0.484,0.546,0.526,0.53],[0.488,0.534,0.54,0.53],[0.472,0.498,0.54,0.53]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869107"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.48179999999999995,"std":0.00989747442532689,"num_trials":10,"pass_rates":[[0.494,0.532,0.532,0.52],[0.484,0.514,0.524,0.52],[0.468,0.516,0.522,0.52],[0.466,0.526,0.51,0.52],[0.484,0.532,0.51,0.52],[0.49,0.496,0.516,0.52],[0.496,0.5,0.516,0.52],[0.474,0.494,0.528,0.52],[0.486,0.512,0.524,0.52],[0.476,0.506,0.506,0.52]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869114"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5128,"std":0.013302631318652725,"num_trials":10,"pass_rates":[[0.494,0.532,0.532,0.52],[0.484,0.514,0.524,0.52],[0.468,0.516,0.522,0.52],[0.466,0.526,0.51,0.52],[0.484,0.532,0.51,0.52],[0.49,0.496,0.516,0.52],[0.496,0.5,0.516,0.52],[0.474,0.494,0.528,0.52],[0.486,0.512,0.524,0.52],[0.476,0.506,0.506,0.52]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869121"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5187999999999999,"std":0.008109253973085324,"num_trials":10,"pass_rates":[[0.494,0.532,0.532,0.52],[0.484,0.514,0.524,0.52],[0.468,0.516,0.522,0.52],[0.466,0.526,0.51,0.52],[0.484,0.532,0.51,0.52],[0.49,0.496,0.516,0.52],[0.496,0.5,0.516,0.52],[0.474,0.494,0.528,0.52],[0.486,0.512,0.524,0.52],[0.476,0.506,0.506,0.52]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869128"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1200","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5199999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.494,0.532,0.532,0.52],[0.484,0.514,0.524,0.52],[0.468,0.516,0.522,0.52],[0.466,0.526,0.51,0.52],[0.484,0.532,0.51,0.52],[0.49,0.496,0.516,0.52],[0.496,0.5,0.516,0.52],[0.474,0.494,0.528,0.52],[0.486,0.512,0.524,0.52],[0.476,0.506,0.506,0.52]],"time_stamp":"UTC+8 2023-08-24 05:54:56 869135"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5128,"std":0.00980612053770502,"num_trials":10,"pass_rates":[[0.512,0.52,0.538,0.548],[0.522,0.518,0.536,0.548],[0.51,0.542,0.534,0.548],[0.514,0.53,0.548,0.548],[0.496,0.532,0.538,0.548],[0.524,0.534,0.546,0.548],[0.502,0.51,0.552,0.548],[0.502,0.538,0.536,0.548],[0.52,0.53,0.55,0.548],[0.526,0.53,0.532,0.548]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567069"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5284000000000001,"std":0.009200000000000009,"num_trials":10,"pass_rates":[[0.512,0.52,0.538,0.548],[0.522,0.518,0.536,0.548],[0.51,0.542,0.534,0.548],[0.514,0.53,0.548,0.548],[0.496,0.532,0.538,0.548],[0.524,0.534,0.546,0.548],[0.502,0.51,0.552,0.548],[0.502,0.538,0.536,0.548],[0.52,0.53,0.55,0.548],[0.526,0.53,0.532,0.548]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567118"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.541,"std":0.006884765791223408,"num_trials":10,"pass_rates":[[0.512,0.52,0.538,0.548],[0.522,0.518,0.536,0.548],[0.51,0.542,0.534,0.548],[0.514,0.53,0.548,0.548],[0.496,0.532,0.538,0.548],[0.524,0.534,0.546,0.548],[0.502,0.51,0.552,0.548],[0.502,0.538,0.536,0.548],[0.52,0.53,0.55,0.548],[0.526,0.53,0.532,0.548]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567128"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.548,"std":0.0,"num_trials":10,"pass_rates":[[0.512,0.52,0.538,0.548],[0.522,0.518,0.536,0.548],[0.51,0.542,0.534,0.548],[0.514,0.53,0.548,0.548],[0.496,0.532,0.538,0.548],[0.524,0.534,0.546,0.548],[0.502,0.51,0.552,0.548],[0.502,0.538,0.536,0.548],[0.52,0.53,0.55,0.548],[0.526,0.53,0.532,0.548]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567135"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.5062,"std":0.010970870521521991,"num_trials":10,"pass_rates":[[0.506,0.528,0.54,0.546],[0.508,0.488,0.514,0.546],[0.496,0.51,0.514,0.546],[0.502,0.522,0.532,0.546],[0.49,0.524,0.522,0.546],[0.524,0.516,0.536,0.546],[0.494,0.506,0.53,0.546],[0.504,0.516,0.524,0.546],[0.522,0.52,0.52,0.546],[0.516,0.526,0.54,0.546]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567142"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5156000000000001,"std":0.011306635220082066,"num_trials":10,"pass_rates":[[0.506,0.528,0.54,0.546],[0.508,0.488,0.514,0.546],[0.496,0.51,0.514,0.546],[0.502,0.522,0.532,0.546],[0.49,0.524,0.522,0.546],[0.524,0.516,0.536,0.546],[0.494,0.506,0.53,0.546],[0.504,0.516,0.524,0.546],[0.522,0.52,0.52,0.546],[0.516,0.526,0.54,0.546]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567150"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5271999999999999,"std":0.009346657156438346,"num_trials":10,"pass_rates":[[0.506,0.528,0.54,0.546],[0.508,0.488,0.514,0.546],[0.496,0.51,0.514,0.546],[0.502,0.522,0.532,0.546],[0.49,0.524,0.522,0.546],[0.524,0.516,0.536,0.546],[0.494,0.506,0.53,0.546],[0.504,0.516,0.524,0.546],[0.522,0.52,0.52,0.546],[0.516,0.526,0.54,0.546]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567157"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5460000000000002,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.506,0.528,0.54,0.546],[0.508,0.488,0.514,0.546],[0.496,0.51,0.514,0.546],[0.502,0.522,0.532,0.546],[0.49,0.524,0.522,0.546],[0.524,0.516,0.536,0.546],[0.494,0.506,0.53,0.546],[0.504,0.516,0.524,0.546],[0.522,0.52,0.52,0.546],[0.516,0.526,0.54,0.546]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567164"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5134000000000001,"std":0.011137324633860694,"num_trials":10,"pass_rates":[[0.506,0.522,0.546,0.542],[0.518,0.522,0.53,0.542],[0.51,0.532,0.528,0.542],[0.518,0.532,0.542,0.542],[0.502,0.538,0.534,0.542],[0.522,0.538,0.542,0.542],[0.496,0.512,0.548,0.542],[0.504,0.544,0.538,0.542],[0.524,0.526,0.53,0.542],[0.534,0.53,0.53,0.542]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567172"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5296000000000001,"std":0.008935323161475479,"num_trials":10,"pass_rates":[[0.506,0.522,0.546,0.542],[0.518,0.522,0.53,0.542],[0.51,0.532,0.528,0.542],[0.518,0.532,0.542,0.542],[0.502,0.538,0.534,0.542],[0.522,0.538,0.542,0.542],[0.496,0.512,0.548,0.542],[0.504,0.544,0.538,0.542],[0.524,0.526,0.53,0.542],[0.534,0.53,0.53,0.542]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567179"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5368,"std":0.006997142273814367,"num_trials":10,"pass_rates":[[0.506,0.522,0.546,0.542],[0.518,0.522,0.53,0.542],[0.51,0.532,0.528,0.542],[0.518,0.532,0.542,0.542],[0.502,0.538,0.534,0.542],[0.522,0.538,0.542,0.542],[0.496,0.512,0.548,0.542],[0.504,0.544,0.538,0.542],[0.524,0.526,0.53,0.542],[0.534,0.53,0.53,0.542]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567187"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5419999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.506,0.522,0.546,0.542],[0.518,0.522,0.53,0.542],[0.51,0.532,0.528,0.542],[0.518,0.532,0.542,0.542],[0.502,0.538,0.534,0.542],[0.522,0.538,0.542,0.542],[0.496,0.512,0.548,0.542],[0.504,0.544,0.538,0.542],[0.524,0.526,0.53,0.542],[0.534,0.53,0.53,0.542]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567194"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.49879999999999997,"std":0.016079800993793438,"num_trials":10,"pass_rates":[[0.502,0.506,0.532,0.552],[0.498,0.482,0.522,0.552],[0.476,0.516,0.52,0.552],[0.474,0.532,0.526,0.552],[0.49,0.516,0.522,0.552],[0.522,0.512,0.534,0.552],[0.488,0.498,0.504,0.552],[0.504,0.52,0.514,0.552],[0.514,0.516,0.512,0.552],[0.52,0.532,0.53,0.552]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567201"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5130000000000001,"std":0.014261837188805665,"num_trials":10,"pass_rates":[[0.502,0.506,0.532,0.552],[0.498,0.482,0.522,0.552],[0.476,0.516,0.52,0.552],[0.474,0.532,0.526,0.552],[0.49,0.516,0.522,0.552],[0.522,0.512,0.534,0.552],[0.488,0.498,0.504,0.552],[0.504,0.52,0.514,0.552],[0.514,0.516,0.512,0.552],[0.52,0.532,0.53,0.552]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567209"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5216000000000001,"std":0.00902441133814279,"num_trials":10,"pass_rates":[[0.502,0.506,0.532,0.552],[0.498,0.482,0.522,0.552],[0.476,0.516,0.52,0.552],[0.474,0.532,0.526,0.552],[0.49,0.516,0.522,0.552],[0.522,0.512,0.534,0.552],[0.488,0.498,0.504,0.552],[0.504,0.52,0.514,0.552],[0.514,0.516,0.512,0.552],[0.52,0.532,0.53,0.552]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567216"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1100","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5519999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.502,0.506,0.532,0.552],[0.498,0.482,0.522,0.552],[0.476,0.516,0.52,0.552],[0.474,0.532,0.526,0.552],[0.49,0.516,0.522,0.552],[0.522,0.512,0.534,0.552],[0.488,0.498,0.504,0.552],[0.504,0.52,0.514,0.552],[0.514,0.516,0.512,0.552],[0.52,0.532,0.53,0.552]],"time_stamp":"UTC+8 2023-08-24 06:19:01 567222"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5032,"std":0.013332666649999178,"num_trials":10,"pass_rates":[[0.51,0.49,0.526,0.54],[0.51,0.536,0.538,0.54],[0.486,0.506,0.52,0.54],[0.494,0.532,0.54,0.54],[0.504,0.508,0.526,0.54],[0.476,0.53,0.54,0.54],[0.516,0.518,0.524,0.54],[0.51,0.534,0.546,0.54],[0.504,0.508,0.546,0.54],[0.522,0.548,0.524,0.54]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943352"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.521,"std":0.016905620367203344,"num_trials":10,"pass_rates":[[0.51,0.49,0.526,0.54],[0.51,0.536,0.538,0.54],[0.486,0.506,0.52,0.54],[0.494,0.532,0.54,0.54],[0.504,0.508,0.526,0.54],[0.476,0.53,0.54,0.54],[0.516,0.518,0.524,0.54],[0.51,0.534,0.546,0.54],[0.504,0.508,0.546,0.54],[0.522,0.548,0.524,0.54]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943561"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5330000000000001,"std":0.009433981132056611,"num_trials":10,"pass_rates":[[0.51,0.49,0.526,0.54],[0.51,0.536,0.538,0.54],[0.486,0.506,0.52,0.54],[0.494,0.532,0.54,0.54],[0.504,0.508,0.526,0.54],[0.476,0.53,0.54,0.54],[0.516,0.518,0.524,0.54],[0.51,0.534,0.546,0.54],[0.504,0.508,0.546,0.54],[0.522,0.548,0.524,0.54]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943571"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.54,"std":0.0,"num_trials":10,"pass_rates":[[0.51,0.49,0.526,0.54],[0.51,0.536,0.538,0.54],[0.486,0.506,0.52,0.54],[0.494,0.532,0.54,0.54],[0.504,0.508,0.526,0.54],[0.476,0.53,0.54,0.54],[0.516,0.518,0.524,0.54],[0.51,0.534,0.546,0.54],[0.504,0.508,0.546,0.54],[0.522,0.548,0.524,0.54]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943579"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.48919999999999997,"std":0.013862178760930767,"num_trials":10,"pass_rates":[[0.504,0.486,0.508,0.512],[0.486,0.516,0.502,0.512],[0.49,0.506,0.502,0.512],[0.466,0.506,0.522,0.512],[0.474,0.494,0.502,0.512],[0.486,0.518,0.522,0.512],[0.49,0.48,0.502,0.512],[0.508,0.508,0.516,0.512],[0.478,0.508,0.508,0.512],[0.51,0.518,0.5,0.512]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943587"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5039999999999999,"std":0.012553883861180185,"num_trials":10,"pass_rates":[[0.504,0.486,0.508,0.512],[0.486,0.516,0.502,0.512],[0.49,0.506,0.502,0.512],[0.466,0.506,0.522,0.512],[0.474,0.494,0.502,0.512],[0.486,0.518,0.522,0.512],[0.49,0.48,0.502,0.512],[0.508,0.508,0.516,0.512],[0.478,0.508,0.508,0.512],[0.51,0.518,0.5,0.512]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943595"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5084,"std":0.008138795979750328,"num_trials":10,"pass_rates":[[0.504,0.486,0.508,0.512],[0.486,0.516,0.502,0.512],[0.49,0.506,0.502,0.512],[0.466,0.506,0.522,0.512],[0.474,0.494,0.502,0.512],[0.486,0.518,0.522,0.512],[0.49,0.48,0.502,0.512],[0.508,0.508,0.516,0.512],[0.478,0.508,0.508,0.512],[0.51,0.518,0.5,0.512]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943602"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5120000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.504,0.486,0.508,0.512],[0.486,0.516,0.502,0.512],[0.49,0.506,0.502,0.512],[0.466,0.506,0.522,0.512],[0.474,0.494,0.502,0.512],[0.486,0.518,0.522,0.512],[0.49,0.48,0.502,0.512],[0.508,0.508,0.516,0.512],[0.478,0.508,0.508,0.512],[0.51,0.518,0.5,0.512]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943609"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5008000000000001,"std":0.013629380029920668,"num_trials":10,"pass_rates":[[0.502,0.484,0.524,0.544],[0.512,0.536,0.538,0.544],[0.482,0.506,0.52,0.544],[0.492,0.536,0.532,0.544],[0.502,0.512,0.524,0.544],[0.474,0.526,0.54,0.544],[0.52,0.528,0.526,0.544],[0.504,0.524,0.536,0.544],[0.506,0.508,0.548,0.544],[0.514,0.538,0.528,0.544]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943619"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5198000000000002,"std":0.016283734215467915,"num_trials":10,"pass_rates":[[0.502,0.484,0.524,0.544],[0.512,0.536,0.538,0.544],[0.482,0.506,0.52,0.544],[0.492,0.536,0.532,0.544],[0.502,0.512,0.524,0.544],[0.474,0.526,0.54,0.544],[0.52,0.528,0.526,0.544],[0.504,0.524,0.536,0.544],[0.506,0.508,0.548,0.544],[0.514,0.538,0.528,0.544]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943627"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5316000000000001,"std":0.008333066662399871,"num_trials":10,"pass_rates":[[0.502,0.484,0.524,0.544],[0.512,0.536,0.538,0.544],[0.482,0.506,0.52,0.544],[0.492,0.536,0.532,0.544],[0.502,0.512,0.524,0.544],[0.474,0.526,0.54,0.544],[0.52,0.528,0.526,0.544],[0.504,0.524,0.536,0.544],[0.506,0.508,0.548,0.544],[0.514,0.538,0.528,0.544]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943634"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5440000000000002,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.502,0.484,0.524,0.544],[0.512,0.536,0.538,0.544],[0.482,0.506,0.52,0.544],[0.492,0.536,0.532,0.544],[0.502,0.512,0.524,0.544],[0.474,0.526,0.54,0.544],[0.52,0.528,0.526,0.544],[0.504,0.524,0.536,0.544],[0.506,0.508,0.548,0.544],[0.514,0.538,0.528,0.544]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943641"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.48599999999999993,"std":0.014913081505845801,"num_trials":10,"pass_rates":[[0.502,0.48,0.514,0.502],[0.486,0.5,0.504,0.502],[0.49,0.488,0.492,0.502],[0.458,0.498,0.5,0.502],[0.476,0.478,0.492,0.502],[0.484,0.504,0.506,0.502],[0.48,0.48,0.494,0.502],[0.502,0.492,0.504,0.502],[0.472,0.494,0.496,0.502],[0.51,0.512,0.5,0.502]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943649"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.49260000000000004,"std":0.010697663296253076,"num_trials":10,"pass_rates":[[0.502,0.48,0.514,0.502],[0.486,0.5,0.504,0.502],[0.49,0.488,0.492,0.502],[0.458,0.498,0.5,0.502],[0.476,0.478,0.492,0.502],[0.484,0.504,0.506,0.502],[0.48,0.48,0.494,0.502],[0.502,0.492,0.504,0.502],[0.472,0.494,0.496,0.502],[0.51,0.512,0.5,0.502]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943656"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5002000000000001,"std":0.006660330322138691,"num_trials":10,"pass_rates":[[0.502,0.48,0.514,0.502],[0.486,0.5,0.504,0.502],[0.49,0.488,0.492,0.502],[0.458,0.498,0.5,0.502],[0.476,0.478,0.492,0.502],[0.484,0.504,0.506,0.502],[0.48,0.48,0.494,0.502],[0.502,0.492,0.504,0.502],[0.472,0.494,0.496,0.502],[0.51,0.512,0.5,0.502]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943663"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_1000","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5019999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.502,0.48,0.514,0.502],[0.486,0.5,0.504,0.502],[0.49,0.488,0.492,0.502],[0.458,0.498,0.5,0.502],[0.476,0.478,0.492,0.502],[0.484,0.504,0.506,0.502],[0.48,0.48,0.494,0.502],[0.502,0.492,0.504,0.502],[0.472,0.494,0.496,0.502],[0.51,0.512,0.5,0.502]],"time_stamp":"UTC+8 2023-08-24 06:43:11 943670"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.49540000000000006,"std":0.018221964767828967,"num_trials":10,"pass_rates":[[0.494,0.512,0.534,0.524],[0.51,0.516,0.518,0.524],[0.488,0.52,0.498,0.524],[0.484,0.512,0.536,0.524],[0.508,0.508,0.522,0.524],[0.484,0.518,0.522,0.524],[0.48,0.518,0.508,0.524],[0.528,0.504,0.546,0.524],[0.464,0.53,0.528,0.524],[0.514,0.506,0.55,0.524]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593856"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5144000000000001,"std":0.007310266752998833,"num_trials":10,"pass_rates":[[0.494,0.512,0.534,0.524],[0.51,0.516,0.518,0.524],[0.488,0.52,0.498,0.524],[0.484,0.512,0.536,0.524],[0.508,0.508,0.522,0.524],[0.484,0.518,0.522,0.524],[0.48,0.518,0.508,0.524],[0.528,0.504,0.546,0.524],[0.464,0.53,0.528,0.524],[0.514,0.506,0.55,0.524]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593911"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5262000000000001,"std":0.015321879780235859,"num_trials":10,"pass_rates":[[0.494,0.512,0.534,0.524],[0.51,0.516,0.518,0.524],[0.488,0.52,0.498,0.524],[0.484,0.512,0.536,0.524],[0.508,0.508,0.522,0.524],[0.484,0.518,0.522,0.524],[0.48,0.518,0.508,0.524],[0.528,0.504,0.546,0.524],[0.464,0.53,0.528,0.524],[0.514,0.506,0.55,0.524]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593921"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.524,"std":0.0,"num_trials":10,"pass_rates":[[0.494,0.512,0.534,0.524],[0.51,0.516,0.518,0.524],[0.488,0.52,0.498,0.524],[0.484,0.512,0.536,0.524],[0.508,0.508,0.522,0.524],[0.484,0.518,0.522,0.524],[0.48,0.518,0.508,0.524],[0.528,0.504,0.546,0.524],[0.464,0.53,0.528,0.524],[0.514,0.506,0.55,0.524]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593929"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4846000000000001,"std":0.015799999999999998,"num_trials":10,"pass_rates":[[0.482,0.506,0.516,0.508],[0.484,0.486,0.532,0.508],[0.484,0.514,0.52,0.508],[0.486,0.5,0.532,0.508],[0.5,0.494,0.534,0.508],[0.468,0.51,0.514,0.508],[0.474,0.518,0.508,0.508],[0.514,0.492,0.518,0.508],[0.456,0.498,0.512,0.508],[0.498,0.51,0.526,0.508]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593937"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5027999999999999,"std":0.00988736567544663,"num_trials":10,"pass_rates":[[0.482,0.506,0.516,0.508],[0.484,0.486,0.532,0.508],[0.484,0.514,0.52,0.508],[0.486,0.5,0.532,0.508],[0.5,0.494,0.534,0.508],[0.468,0.51,0.514,0.508],[0.474,0.518,0.508,0.508],[0.514,0.492,0.518,0.508],[0.456,0.498,0.512,0.508],[0.498,0.51,0.526,0.508]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593944"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5212,"std":0.008772684879784531,"num_trials":10,"pass_rates":[[0.482,0.506,0.516,0.508],[0.484,0.486,0.532,0.508],[0.484,0.514,0.52,0.508],[0.486,0.5,0.532,0.508],[0.5,0.494,0.534,0.508],[0.468,0.51,0.514,0.508],[0.474,0.518,0.508,0.508],[0.514,0.492,0.518,0.508],[0.456,0.498,0.512,0.508],[0.498,0.51,0.526,0.508]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593951"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.508,"std":0.0,"num_trials":10,"pass_rates":[[0.482,0.506,0.516,0.508],[0.484,0.486,0.532,0.508],[0.484,0.514,0.52,0.508],[0.486,0.5,0.532,0.508],[0.5,0.494,0.534,0.508],[0.468,0.51,0.514,0.508],[0.474,0.518,0.508,0.508],[0.514,0.492,0.518,0.508],[0.456,0.498,0.512,0.508],[0.498,0.51,0.526,0.508]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593958"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.49540000000000006,"std":0.01808977611801761,"num_trials":10,"pass_rates":[[0.486,0.514,0.536,0.528],[0.502,0.512,0.522,0.528],[0.49,0.518,0.504,0.528],[0.488,0.512,0.546,0.528],[0.51,0.498,0.52,0.528],[0.488,0.506,0.528,0.528],[0.478,0.518,0.51,0.528],[0.53,0.506,0.548,0.528],[0.466,0.53,0.53,0.528],[0.516,0.508,0.538,0.528]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593965"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5122000000000001,"std":0.008268010643437762,"num_trials":10,"pass_rates":[[0.486,0.514,0.536,0.528],[0.502,0.512,0.522,0.528],[0.49,0.518,0.504,0.528],[0.488,0.512,0.546,0.528],[0.51,0.498,0.52,0.528],[0.488,0.506,0.528,0.528],[0.478,0.518,0.51,0.528],[0.53,0.506,0.548,0.528],[0.466,0.53,0.53,0.528],[0.516,0.508,0.538,0.528]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593973"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5282000000000001,"std":0.013753544997563368,"num_trials":10,"pass_rates":[[0.486,0.514,0.536,0.528],[0.502,0.512,0.522,0.528],[0.49,0.518,0.504,0.528],[0.488,0.512,0.546,0.528],[0.51,0.498,0.52,0.528],[0.488,0.506,0.528,0.528],[0.478,0.518,0.51,0.528],[0.53,0.506,0.548,0.528],[0.466,0.53,0.53,0.528],[0.516,0.508,0.538,0.528]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593980"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5280000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.486,0.514,0.536,0.528],[0.502,0.512,0.522,0.528],[0.49,0.518,0.504,0.528],[0.488,0.512,0.546,0.528],[0.51,0.498,0.52,0.528],[0.488,0.506,0.528,0.528],[0.478,0.518,0.51,0.528],[0.53,0.506,0.548,0.528],[0.466,0.53,0.53,0.528],[0.516,0.508,0.538,0.528]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593987"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.48080000000000006,"std":0.01515783625719713,"num_trials":10,"pass_rates":[[0.474,0.488,0.512,0.512],[0.476,0.476,0.512,0.512],[0.494,0.508,0.506,0.512],[0.484,0.498,0.54,0.512],[0.502,0.494,0.522,0.512],[0.474,0.506,0.504,0.512],[0.466,0.512,0.494,0.512],[0.498,0.488,0.512,0.512],[0.45,0.49,0.516,0.512],[0.49,0.502,0.52,0.512]],"time_stamp":"UTC+8 2023-08-24 07:07:14 593994"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.4962,"std":0.010524257693538305,"num_trials":10,"pass_rates":[[0.474,0.488,0.512,0.512],[0.476,0.476,0.512,0.512],[0.494,0.508,0.506,0.512],[0.484,0.498,0.54,0.512],[0.502,0.494,0.522,0.512],[0.474,0.506,0.504,0.512],[0.466,0.512,0.494,0.512],[0.498,0.488,0.512,0.512],[0.45,0.49,0.516,0.512],[0.49,0.502,0.52,0.512]],"time_stamp":"UTC+8 2023-08-24 07:07:14 594001"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5138,"std":0.011643023662262317,"num_trials":10,"pass_rates":[[0.474,0.488,0.512,0.512],[0.476,0.476,0.512,0.512],[0.494,0.508,0.506,0.512],[0.484,0.498,0.54,0.512],[0.502,0.494,0.522,0.512],[0.474,0.506,0.504,0.512],[0.466,0.512,0.494,0.512],[0.498,0.488,0.512,0.512],[0.45,0.49,0.516,0.512],[0.49,0.502,0.52,0.512]],"time_stamp":"UTC+8 2023-08-24 07:07:14 594008"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_900","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5120000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.474,0.488,0.512,0.512],[0.476,0.476,0.512,0.512],[0.494,0.508,0.506,0.512],[0.484,0.498,0.54,0.512],[0.502,0.494,0.522,0.512],[0.474,0.506,0.504,0.512],[0.466,0.512,0.494,0.512],[0.498,0.488,0.512,0.512],[0.45,0.49,0.516,0.512],[0.49,0.502,0.52,0.512]],"time_stamp":"UTC+8 2023-08-24 07:07:14 594015"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.49959999999999993,"std":0.01155162326255493,"num_trials":10,"pass_rates":[[0.488,0.498,0.522,0.534],[0.494,0.528,0.514,0.534],[0.514,0.524,0.516,0.534],[0.488,0.516,0.544,0.534],[0.518,0.508,0.55,0.534],[0.494,0.534,0.51,0.534],[0.5,0.518,0.542,0.534],[0.486,0.532,0.53,0.534],[0.498,0.494,0.546,0.534],[0.516,0.526,0.522,0.534]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396562"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5177999999999999,"std":0.013159027319676797,"num_trials":10,"pass_rates":[[0.488,0.498,0.522,0.534],[0.494,0.528,0.514,0.534],[0.514,0.524,0.516,0.534],[0.488,0.516,0.544,0.534],[0.518,0.508,0.55,0.534],[0.494,0.534,0.51,0.534],[0.5,0.518,0.542,0.534],[0.486,0.532,0.53,0.534],[0.498,0.494,0.546,0.534],[0.516,0.526,0.522,0.534]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396616"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5296000000000001,"std":0.014051334456200248,"num_trials":10,"pass_rates":[[0.488,0.498,0.522,0.534],[0.494,0.528,0.514,0.534],[0.514,0.524,0.516,0.534],[0.488,0.516,0.544,0.534],[0.518,0.508,0.55,0.534],[0.494,0.534,0.51,0.534],[0.5,0.518,0.542,0.534],[0.486,0.532,0.53,0.534],[0.498,0.494,0.546,0.534],[0.516,0.526,0.522,0.534]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396627"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5339999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.488,0.498,0.522,0.534],[0.494,0.528,0.514,0.534],[0.514,0.524,0.516,0.534],[0.488,0.516,0.544,0.534],[0.518,0.508,0.55,0.534],[0.494,0.534,0.51,0.534],[0.5,0.518,0.542,0.534],[0.486,0.532,0.53,0.534],[0.498,0.494,0.546,0.534],[0.516,0.526,0.522,0.534]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396634"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.49000000000000005,"std":0.01069579356569676,"num_trials":10,"pass_rates":[[0.476,0.496,0.516,0.536],[0.48,0.496,0.518,0.536],[0.49,0.502,0.526,0.536],[0.496,0.488,0.514,0.536],[0.506,0.49,0.538,0.536],[0.494,0.506,0.522,0.536],[0.492,0.518,0.534,0.536],[0.47,0.53,0.534,0.536],[0.5,0.482,0.536,0.536],[0.496,0.502,0.522,0.536]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396643"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.501,"std":0.013630847369111002,"num_trials":10,"pass_rates":[[0.476,0.496,0.516,0.536],[0.48,0.496,0.518,0.536],[0.49,0.502,0.526,0.536],[0.496,0.488,0.514,0.536],[0.506,0.49,0.538,0.536],[0.494,0.506,0.522,0.536],[0.492,0.518,0.534,0.536],[0.47,0.53,0.534,0.536],[0.5,0.482,0.536,0.536],[0.496,0.502,0.522,0.536]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396651"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.526,"std":0.008438009243891603,"num_trials":10,"pass_rates":[[0.476,0.496,0.516,0.536],[0.48,0.496,0.518,0.536],[0.49,0.502,0.526,0.536],[0.496,0.488,0.514,0.536],[0.506,0.49,0.538,0.536],[0.494,0.506,0.522,0.536],[0.492,0.518,0.534,0.536],[0.47,0.53,0.534,0.536],[0.5,0.482,0.536,0.536],[0.496,0.502,0.522,0.536]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396658"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5359999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.476,0.496,0.516,0.536],[0.48,0.496,0.518,0.536],[0.49,0.502,0.526,0.536],[0.496,0.488,0.514,0.536],[0.506,0.49,0.538,0.536],[0.494,0.506,0.522,0.536],[0.492,0.518,0.534,0.536],[0.47,0.53,0.534,0.536],[0.5,0.482,0.536,0.536],[0.496,0.502,0.522,0.536]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396665"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5012,"std":0.010007996802557453,"num_trials":10,"pass_rates":[[0.486,0.504,0.528,0.53],[0.496,0.528,0.522,0.53],[0.51,0.522,0.512,0.53],[0.498,0.51,0.54,0.53],[0.512,0.508,0.552,0.53],[0.504,0.532,0.514,0.53],[0.504,0.518,0.544,0.53],[0.486,0.536,0.528,0.53],[0.498,0.492,0.542,0.53],[0.518,0.52,0.516,0.53]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396672"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.517,"std":0.012907362240210054,"num_trials":10,"pass_rates":[[0.486,0.504,0.528,0.53],[0.496,0.528,0.522,0.53],[0.51,0.522,0.512,0.53],[0.498,0.51,0.54,0.53],[0.512,0.508,0.552,0.53],[0.504,0.532,0.514,0.53],[0.504,0.518,0.544,0.53],[0.486,0.536,0.528,0.53],[0.498,0.492,0.542,0.53],[0.518,0.52,0.516,0.53]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396680"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5298,"std":0.01331014650557988,"num_trials":10,"pass_rates":[[0.486,0.504,0.528,0.53],[0.496,0.528,0.522,0.53],[0.51,0.522,0.512,0.53],[0.498,0.51,0.54,0.53],[0.512,0.508,0.552,0.53],[0.504,0.532,0.514,0.53],[0.504,0.518,0.544,0.53],[0.486,0.536,0.528,0.53],[0.498,0.492,0.542,0.53],[0.518,0.52,0.516,0.53]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396687"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5300000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.486,0.504,0.528,0.53],[0.496,0.528,0.522,0.53],[0.51,0.522,0.512,0.53],[0.498,0.51,0.54,0.53],[0.512,0.508,0.552,0.53],[0.504,0.532,0.514,0.53],[0.504,0.518,0.544,0.53],[0.486,0.536,0.528,0.53],[0.498,0.492,0.542,0.53],[0.518,0.52,0.516,0.53]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396694"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.487,"std":0.006587867636800248,"num_trials":10,"pass_rates":[[0.482,0.504,0.5,0.512],[0.474,0.504,0.51,0.512],[0.492,0.49,0.512,0.512],[0.49,0.48,0.506,0.512],[0.492,0.482,0.516,0.512],[0.482,0.506,0.508,0.512],[0.494,0.52,0.52,0.512],[0.48,0.522,0.53,0.512],[0.494,0.48,0.526,0.512],[0.49,0.506,0.518,0.512]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396702"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.49939999999999996,"std":0.01486068639060795,"num_trials":10,"pass_rates":[[0.482,0.504,0.5,0.512],[0.474,0.504,0.51,0.512],[0.492,0.49,0.512,0.512],[0.49,0.48,0.506,0.512],[0.492,0.482,0.516,0.512],[0.482,0.506,0.508,0.512],[0.494,0.52,0.52,0.512],[0.48,0.522,0.53,0.512],[0.494,0.48,0.526,0.512],[0.49,0.506,0.518,0.512]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396710"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5146,"std":0.008765842800324459,"num_trials":10,"pass_rates":[[0.482,0.504,0.5,0.512],[0.474,0.504,0.51,0.512],[0.492,0.49,0.512,0.512],[0.49,0.48,0.506,0.512],[0.492,0.482,0.516,0.512],[0.482,0.506,0.508,0.512],[0.494,0.52,0.52,0.512],[0.48,0.522,0.53,0.512],[0.494,0.48,0.526,0.512],[0.49,0.506,0.518,0.512]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396717"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_800","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5120000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.482,0.504,0.5,0.512],[0.474,0.504,0.51,0.512],[0.492,0.49,0.512,0.512],[0.49,0.48,0.506,0.512],[0.492,0.482,0.516,0.512],[0.482,0.506,0.508,0.512],[0.494,0.52,0.52,0.512],[0.48,0.522,0.53,0.512],[0.494,0.48,0.526,0.512],[0.49,0.506,0.518,0.512]],"time_stamp":"UTC+8 2023-08-24 07:31:14 396723"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.4932,"std":0.010283968105745962,"num_trials":10,"pass_rates":[[0.488,0.51,0.504,0.512],[0.5,0.488,0.504,0.512],[0.498,0.488,0.508,0.512],[0.49,0.494,0.516,0.512],[0.472,0.508,0.514,0.512],[0.5,0.514,0.518,0.512],[0.488,0.496,0.494,0.512],[0.486,0.526,0.514,0.512],[0.498,0.506,0.494,0.512],[0.512,0.512,0.508,0.512]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148823"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5042,"std":0.011745637488020828,"num_trials":10,"pass_rates":[[0.488,0.51,0.504,0.512],[0.5,0.488,0.504,0.512],[0.498,0.488,0.508,0.512],[0.49,0.494,0.516,0.512],[0.472,0.508,0.514,0.512],[0.5,0.514,0.518,0.512],[0.488,0.496,0.494,0.512],[0.486,0.526,0.514,0.512],[0.498,0.506,0.494,0.512],[0.512,0.512,0.508,0.512]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148909"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5074,"std":0.008101851640211644,"num_trials":10,"pass_rates":[[0.488,0.51,0.504,0.512],[0.5,0.488,0.504,0.512],[0.498,0.488,0.508,0.512],[0.49,0.494,0.516,0.512],[0.472,0.508,0.514,0.512],[0.5,0.514,0.518,0.512],[0.488,0.496,0.494,0.512],[0.486,0.526,0.514,0.512],[0.498,0.506,0.494,0.512],[0.512,0.512,0.508,0.512]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148920"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5120000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.488,0.51,0.504,0.512],[0.5,0.488,0.504,0.512],[0.498,0.488,0.508,0.512],[0.49,0.494,0.516,0.512],[0.472,0.508,0.514,0.512],[0.5,0.514,0.518,0.512],[0.488,0.496,0.494,0.512],[0.486,0.526,0.514,0.512],[0.498,0.506,0.494,0.512],[0.512,0.512,0.508,0.512]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148928"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4852,"std":0.010476640682967036,"num_trials":10,"pass_rates":[[0.488,0.496,0.502,0.514],[0.5,0.492,0.502,0.514],[0.488,0.476,0.494,0.512],[0.488,0.502,0.508,0.514],[0.47,0.492,0.524,0.514],[0.5,0.498,0.496,0.512],[0.468,0.498,0.492,0.512],[0.476,0.498,0.5,0.51],[0.49,0.484,0.486,0.512],[0.484,0.496,0.52,0.514]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148936"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.4932,"std":0.007386474125047762,"num_trials":10,"pass_rates":[[0.488,0.496,0.502,0.514],[0.5,0.492,0.502,0.514],[0.488,0.476,0.494,0.512],[0.488,0.502,0.508,0.514],[0.47,0.492,0.524,0.514],[0.5,0.498,0.496,0.512],[0.468,0.498,0.492,0.512],[0.476,0.498,0.5,0.51],[0.49,0.484,0.486,0.512],[0.484,0.496,0.52,0.514]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148945"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5024000000000001,"std":0.011412274094149697,"num_trials":10,"pass_rates":[[0.488,0.496,0.502,0.514],[0.5,0.492,0.502,0.514],[0.488,0.476,0.494,0.512],[0.488,0.502,0.508,0.514],[0.47,0.492,0.524,0.514],[0.5,0.498,0.496,0.512],[0.468,0.498,0.492,0.512],[0.476,0.498,0.5,0.51],[0.49,0.484,0.486,0.512],[0.484,0.496,0.52,0.514]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148952"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5128000000000001,"std":0.001326649916142161,"num_trials":10,"pass_rates":[[0.488,0.496,0.502,0.514],[0.5,0.492,0.502,0.514],[0.488,0.476,0.494,0.512],[0.488,0.502,0.508,0.514],[0.47,0.492,0.524,0.514],[0.5,0.498,0.496,0.512],[0.468,0.498,0.492,0.512],[0.476,0.498,0.5,0.51],[0.49,0.484,0.486,0.512],[0.484,0.496,0.52,0.514]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148959"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.49119999999999997,"std":0.010127191120937738,"num_trials":10,"pass_rates":[[0.488,0.5,0.508,0.522],[0.5,0.508,0.51,0.522],[0.496,0.496,0.51,0.522],[0.488,0.5,0.524,0.522],[0.472,0.5,0.514,0.522],[0.496,0.5,0.518,0.522],[0.484,0.498,0.512,0.522],[0.48,0.52,0.518,0.522],[0.502,0.502,0.492,0.522],[0.506,0.52,0.52,0.522]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148965"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5044000000000001,"std":0.008333066662399871,"num_trials":10,"pass_rates":[[0.488,0.5,0.508,0.522],[0.5,0.508,0.51,0.522],[0.496,0.496,0.51,0.522],[0.488,0.5,0.524,0.522],[0.472,0.5,0.514,0.522],[0.496,0.5,0.518,0.522],[0.484,0.498,0.512,0.522],[0.48,0.52,0.518,0.522],[0.502,0.502,0.492,0.522],[0.506,0.52,0.52,0.522]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148973"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5126,"std":0.008392854103342923,"num_trials":10,"pass_rates":[[0.488,0.5,0.508,0.522],[0.5,0.508,0.51,0.522],[0.496,0.496,0.51,0.522],[0.488,0.5,0.524,0.522],[0.472,0.5,0.514,0.522],[0.496,0.5,0.518,0.522],[0.484,0.498,0.512,0.522],[0.48,0.52,0.518,0.522],[0.502,0.502,0.492,0.522],[0.506,0.52,0.52,0.522]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148980"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5220000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.488,0.5,0.508,0.522],[0.5,0.508,0.51,0.522],[0.496,0.496,0.51,0.522],[0.488,0.5,0.524,0.522],[0.472,0.5,0.514,0.522],[0.496,0.5,0.518,0.522],[0.484,0.498,0.512,0.522],[0.48,0.52,0.518,0.522],[0.502,0.502,0.492,0.522],[0.506,0.52,0.52,0.522]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148988"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.4864,"std":0.009329523031752489,"num_trials":10,"pass_rates":[[0.484,0.5,0.508,0.518],[0.502,0.502,0.514,0.518],[0.484,0.484,0.494,0.518],[0.486,0.504,0.508,0.518],[0.472,0.494,0.532,0.518],[0.5,0.496,0.51,0.518],[0.476,0.488,0.506,0.518],[0.478,0.506,0.508,0.518],[0.492,0.488,0.494,0.518],[0.49,0.494,0.524,0.518]],"time_stamp":"UTC+8 2023-08-24 07:31:20 148995"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.49559999999999993,"std":0.007031358332498784,"num_trials":10,"pass_rates":[[0.484,0.5,0.508,0.518],[0.502,0.502,0.514,0.518],[0.484,0.484,0.494,0.518],[0.486,0.504,0.508,0.518],[0.472,0.494,0.532,0.518],[0.5,0.496,0.51,0.518],[0.476,0.488,0.506,0.518],[0.478,0.506,0.508,0.518],[0.492,0.488,0.494,0.518],[0.49,0.494,0.524,0.518]],"time_stamp":"UTC+8 2023-08-24 07:31:20 149002"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5098,"std":0.011115754585272213,"num_trials":10,"pass_rates":[[0.484,0.5,0.508,0.518],[0.502,0.502,0.514,0.518],[0.484,0.484,0.494,0.518],[0.486,0.504,0.508,0.518],[0.472,0.494,0.532,0.518],[0.5,0.496,0.51,0.518],[0.476,0.488,0.506,0.518],[0.478,0.506,0.508,0.518],[0.492,0.488,0.494,0.518],[0.49,0.494,0.524,0.518]],"time_stamp":"UTC+8 2023-08-24 07:31:20 149008"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_700","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5179999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.484,0.5,0.508,0.518],[0.502,0.502,0.514,0.518],[0.484,0.484,0.494,0.518],[0.486,0.504,0.508,0.518],[0.472,0.494,0.532,0.518],[0.5,0.496,0.51,0.518],[0.476,0.488,0.506,0.518],[0.478,0.506,0.508,0.518],[0.492,0.488,0.494,0.518],[0.49,0.494,0.524,0.518]],"time_stamp":"UTC+8 2023-08-24 07:31:20 149016"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5042000000000001,"std":0.012663332894621395,"num_trials":10,"pass_rates":[[0.498,0.518,0.548,0.52],[0.492,0.516,0.538,0.52],[0.492,0.52,0.52,0.52],[0.526,0.512,0.524,0.52],[0.49,0.514,0.536,0.52],[0.508,0.514,0.528,0.52],[0.516,0.516,0.518,0.52],[0.49,0.492,0.512,0.52],[0.514,0.514,0.516,0.52],[0.516,0.534,0.518,0.52]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935190"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.515,"std":0.00968504001024261,"num_trials":10,"pass_rates":[[0.498,0.518,0.548,0.52],[0.492,0.516,0.538,0.52],[0.492,0.52,0.52,0.52],[0.526,0.512,0.524,0.52],[0.49,0.514,0.536,0.52],[0.508,0.514,0.528,0.52],[0.516,0.516,0.518,0.52],[0.49,0.492,0.512,0.52],[0.514,0.514,0.516,0.52],[0.516,0.534,0.518,0.52]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935237"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5258,"std":0.010934349546269324,"num_trials":10,"pass_rates":[[0.498,0.518,0.548,0.52],[0.492,0.516,0.538,0.52],[0.492,0.52,0.52,0.52],[0.526,0.512,0.524,0.52],[0.49,0.514,0.536,0.52],[0.508,0.514,0.528,0.52],[0.516,0.516,0.518,0.52],[0.49,0.492,0.512,0.52],[0.514,0.514,0.516,0.52],[0.516,0.534,0.518,0.52]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935246"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5199999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.498,0.518,0.548,0.52],[0.492,0.516,0.538,0.52],[0.492,0.52,0.52,0.52],[0.526,0.512,0.524,0.52],[0.49,0.514,0.536,0.52],[0.508,0.514,0.528,0.52],[0.516,0.516,0.518,0.52],[0.49,0.492,0.512,0.52],[0.514,0.514,0.516,0.52],[0.516,0.534,0.518,0.52]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935254"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4944,"std":0.015894653189044435,"num_trials":10,"pass_rates":[[0.498,0.504,0.542,0.536],[0.48,0.518,0.53,0.536],[0.484,0.516,0.518,0.536],[0.52,0.498,0.538,0.536],[0.476,0.508,0.528,0.536],[0.496,0.514,0.524,0.536],[0.514,0.504,0.512,0.536],[0.472,0.494,0.516,0.536],[0.512,0.518,0.512,0.536],[0.492,0.502,0.51,0.536]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935262"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5075999999999999,"std":0.008138795979750328,"num_trials":10,"pass_rates":[[0.498,0.504,0.542,0.536],[0.48,0.518,0.53,0.536],[0.484,0.516,0.518,0.536],[0.52,0.498,0.538,0.536],[0.476,0.508,0.528,0.536],[0.496,0.514,0.524,0.536],[0.514,0.504,0.512,0.536],[0.472,0.494,0.516,0.536],[0.512,0.518,0.512,0.536],[0.492,0.502,0.51,0.536]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935276"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.523,"std":0.010705138952858118,"num_trials":10,"pass_rates":[[0.498,0.504,0.542,0.536],[0.48,0.518,0.53,0.536],[0.484,0.516,0.518,0.536],[0.52,0.498,0.538,0.536],[0.476,0.508,0.528,0.536],[0.496,0.514,0.524,0.536],[0.514,0.504,0.512,0.536],[0.472,0.494,0.516,0.536],[0.512,0.518,0.512,0.536],[0.492,0.502,0.51,0.536]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935283"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5359999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.498,0.504,0.542,0.536],[0.48,0.518,0.53,0.536],[0.484,0.516,0.518,0.536],[0.52,0.498,0.538,0.536],[0.476,0.508,0.528,0.536],[0.496,0.514,0.524,0.536],[0.514,0.504,0.512,0.536],[0.472,0.494,0.516,0.536],[0.512,0.518,0.512,0.536],[0.492,0.502,0.51,0.536]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935290"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5048,"std":0.013242356285797492,"num_trials":10,"pass_rates":[[0.496,0.512,0.54,0.51],[0.492,0.524,0.522,0.51],[0.496,0.518,0.522,0.51],[0.528,0.514,0.514,0.51],[0.49,0.522,0.524,0.51],[0.512,0.514,0.514,0.51],[0.514,0.508,0.51,0.51],[0.488,0.492,0.514,0.51],[0.518,0.514,0.518,0.51],[0.514,0.536,0.518,0.51]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935297"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5154,"std":0.010809255293497336,"num_trials":10,"pass_rates":[[0.496,0.512,0.54,0.51],[0.492,0.524,0.522,0.51],[0.496,0.518,0.522,0.51],[0.528,0.514,0.514,0.51],[0.49,0.522,0.524,0.51],[0.512,0.514,0.514,0.51],[0.514,0.508,0.51,0.51],[0.488,0.492,0.514,0.51],[0.518,0.514,0.518,0.51],[0.514,0.536,0.518,0.51]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935305"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5196,"std":0.007989993742175277,"num_trials":10,"pass_rates":[[0.496,0.512,0.54,0.51],[0.492,0.524,0.522,0.51],[0.496,0.518,0.522,0.51],[0.528,0.514,0.514,0.51],[0.49,0.522,0.524,0.51],[0.512,0.514,0.514,0.51],[0.514,0.508,0.51,0.51],[0.488,0.492,0.514,0.51],[0.518,0.514,0.518,0.51],[0.514,0.536,0.518,0.51]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935311"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5099999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.496,0.512,0.54,0.51],[0.492,0.524,0.522,0.51],[0.496,0.518,0.522,0.51],[0.528,0.514,0.514,0.51],[0.49,0.522,0.524,0.51],[0.512,0.514,0.514,0.51],[0.514,0.508,0.51,0.51],[0.488,0.492,0.514,0.51],[0.518,0.514,0.518,0.51],[0.514,0.536,0.518,0.51]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935318"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.49400000000000005,"std":0.015231546211727828,"num_trials":10,"pass_rates":[[0.494,0.502,0.536,0.526],[0.49,0.51,0.538,0.526],[0.478,0.518,0.514,0.526],[0.52,0.502,0.53,0.526],[0.478,0.508,0.522,0.526],[0.5,0.514,0.532,0.526],[0.514,0.51,0.518,0.526],[0.47,0.498,0.516,0.526],[0.504,0.512,0.5,0.526],[0.492,0.498,0.508,0.526]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935326"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5072,"std":0.006523802572120043,"num_trials":10,"pass_rates":[[0.494,0.502,0.536,0.526],[0.49,0.51,0.538,0.526],[0.478,0.518,0.514,0.526],[0.52,0.502,0.53,0.526],[0.478,0.508,0.522,0.526],[0.5,0.514,0.532,0.526],[0.514,0.51,0.518,0.526],[0.47,0.498,0.516,0.526],[0.504,0.512,0.5,0.526],[0.492,0.498,0.508,0.526]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935333"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5214000000000001,"std":0.01186760295931745,"num_trials":10,"pass_rates":[[0.494,0.502,0.536,0.526],[0.49,0.51,0.538,0.526],[0.478,0.518,0.514,0.526],[0.52,0.502,0.53,0.526],[0.478,0.508,0.522,0.526],[0.5,0.514,0.532,0.526],[0.514,0.51,0.518,0.526],[0.47,0.498,0.516,0.526],[0.504,0.512,0.5,0.526],[0.492,0.498,0.508,0.526]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935340"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_600","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5259999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.494,0.502,0.536,0.526],[0.49,0.51,0.538,0.526],[0.478,0.518,0.514,0.526],[0.52,0.502,0.53,0.526],[0.478,0.508,0.522,0.526],[0.5,0.514,0.532,0.526],[0.514,0.51,0.518,0.526],[0.47,0.498,0.516,0.526],[0.504,0.512,0.5,0.526],[0.492,0.498,0.508,0.526]],"time_stamp":"UTC+8 2023-08-24 07:55:40 935348"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5071999999999999,"std":0.02045873896407109,"num_trials":10,"pass_rates":[[0.51,0.522,0.528,0.54],[0.478,0.52,0.538,0.54],[0.51,0.52,0.55,0.54],[0.544,0.516,0.516,0.54],[0.49,0.514,0.526,0.54],[0.52,0.544,0.532,0.54],[0.528,0.546,0.534,0.54],[0.488,0.514,0.54,0.54],[0.484,0.522,0.526,0.54],[0.52,0.534,0.53,0.54]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671362"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5252000000000001,"std":0.011285388783732718,"num_trials":10,"pass_rates":[[0.51,0.522,0.528,0.54],[0.478,0.52,0.538,0.54],[0.51,0.52,0.55,0.54],[0.544,0.516,0.516,0.54],[0.49,0.514,0.526,0.54],[0.52,0.544,0.532,0.54],[0.528,0.546,0.534,0.54],[0.488,0.514,0.54,0.54],[0.484,0.522,0.526,0.54],[0.52,0.534,0.53,0.54]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671417"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.532,"std":0.008809086218218104,"num_trials":10,"pass_rates":[[0.51,0.522,0.528,0.54],[0.478,0.52,0.538,0.54],[0.51,0.52,0.55,0.54],[0.544,0.516,0.516,0.54],[0.49,0.514,0.526,0.54],[0.52,0.544,0.532,0.54],[0.528,0.546,0.534,0.54],[0.488,0.514,0.54,0.54],[0.484,0.522,0.526,0.54],[0.52,0.534,0.53,0.54]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671427"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.54,"std":0.0,"num_trials":10,"pass_rates":[[0.51,0.522,0.528,0.54],[0.478,0.52,0.538,0.54],[0.51,0.52,0.55,0.54],[0.544,0.516,0.516,0.54],[0.49,0.514,0.526,0.54],[0.52,0.544,0.532,0.54],[0.528,0.546,0.534,0.54],[0.488,0.514,0.54,0.54],[0.484,0.522,0.526,0.54],[0.52,0.534,0.53,0.54]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671435"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4943999999999999,"std":0.016918628786045286,"num_trials":10,"pass_rates":[[0.508,0.484,0.526,0.522],[0.486,0.522,0.52,0.522],[0.484,0.504,0.508,0.522],[0.518,0.496,0.52,0.522],[0.468,0.496,0.512,0.522],[0.504,0.52,0.528,0.522],[0.51,0.518,0.542,0.522],[0.47,0.48,0.496,0.522],[0.486,0.506,0.518,0.522],[0.51,0.506,0.506,0.522]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671443"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5032,"std":0.013717142559585808,"num_trials":10,"pass_rates":[[0.508,0.484,0.526,0.522],[0.486,0.522,0.52,0.522],[0.484,0.504,0.508,0.522],[0.518,0.496,0.52,0.522],[0.468,0.496,0.512,0.522],[0.504,0.52,0.528,0.522],[0.51,0.518,0.542,0.522],[0.47,0.48,0.496,0.522],[0.486,0.506,0.518,0.522],[0.51,0.506,0.506,0.522]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671451"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5176,"std":0.012289833196589784,"num_trials":10,"pass_rates":[[0.508,0.484,0.526,0.522],[0.486,0.522,0.52,0.522],[0.484,0.504,0.508,0.522],[0.518,0.496,0.52,0.522],[0.468,0.496,0.512,0.522],[0.504,0.52,0.528,0.522],[0.51,0.518,0.542,0.522],[0.47,0.48,0.496,0.522],[0.486,0.506,0.518,0.522],[0.51,0.506,0.506,0.522]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671459"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5220000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.508,0.484,0.526,0.522],[0.486,0.522,0.52,0.522],[0.484,0.504,0.508,0.522],[0.518,0.496,0.52,0.522],[0.468,0.496,0.512,0.522],[0.504,0.52,0.528,0.522],[0.51,0.518,0.542,0.522],[0.47,0.48,0.496,0.522],[0.486,0.506,0.518,0.522],[0.51,0.506,0.506,0.522]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671466"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5064,"std":0.02219549503840815,"num_trials":10,"pass_rates":[[0.512,0.522,0.532,0.518],[0.478,0.522,0.538,0.518],[0.514,0.524,0.546,0.518],[0.54,0.524,0.524,0.518],[0.49,0.528,0.524,0.518],[0.526,0.542,0.542,0.518],[0.526,0.544,0.536,0.518],[0.48,0.512,0.526,0.518],[0.476,0.522,0.528,0.518],[0.522,0.532,0.52,0.518]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671474"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5272,"std":0.009260669522232187,"num_trials":10,"pass_rates":[[0.512,0.522,0.532,0.518],[0.478,0.522,0.538,0.518],[0.514,0.524,0.546,0.518],[0.54,0.524,0.524,0.518],[0.49,0.528,0.524,0.518],[0.526,0.542,0.542,0.518],[0.526,0.544,0.536,0.518],[0.48,0.512,0.526,0.518],[0.476,0.522,0.528,0.518],[0.522,0.532,0.52,0.518]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671481"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5316000000000001,"std":0.008187795796183495,"num_trials":10,"pass_rates":[[0.512,0.522,0.532,0.518],[0.478,0.522,0.538,0.518],[0.514,0.524,0.546,0.518],[0.54,0.524,0.524,0.518],[0.49,0.528,0.524,0.518],[0.526,0.542,0.542,0.518],[0.526,0.544,0.536,0.518],[0.48,0.512,0.526,0.518],[0.476,0.522,0.528,0.518],[0.522,0.532,0.52,0.518]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671489"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5179999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.512,0.522,0.532,0.518],[0.478,0.522,0.538,0.518],[0.514,0.524,0.546,0.518],[0.54,0.524,0.524,0.518],[0.49,0.528,0.524,0.518],[0.526,0.542,0.542,0.518],[0.526,0.544,0.536,0.518],[0.48,0.512,0.526,0.518],[0.476,0.522,0.528,0.518],[0.522,0.532,0.52,0.518]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671495"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.49139999999999995,"std":0.018023318229449314,"num_trials":10,"pass_rates":[[0.51,0.494,0.536,0.52],[0.478,0.536,0.53,0.52],[0.482,0.492,0.502,0.52],[0.52,0.498,0.514,0.52],[0.466,0.498,0.524,0.52],[0.504,0.508,0.526,0.52],[0.504,0.508,0.548,0.52],[0.464,0.484,0.498,0.52],[0.486,0.496,0.504,0.52],[0.5,0.508,0.51,0.52]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671502"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5022,"std":0.01348925498313381,"num_trials":10,"pass_rates":[[0.51,0.494,0.536,0.52],[0.478,0.536,0.53,0.52],[0.482,0.492,0.502,0.52],[0.52,0.498,0.514,0.52],[0.466,0.498,0.524,0.52],[0.504,0.508,0.526,0.52],[0.504,0.508,0.548,0.52],[0.464,0.484,0.498,0.52],[0.486,0.496,0.504,0.52],[0.5,0.508,0.51,0.52]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671510"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5192,"std":0.01544538766104627,"num_trials":10,"pass_rates":[[0.51,0.494,0.536,0.52],[0.478,0.536,0.53,0.52],[0.482,0.492,0.502,0.52],[0.52,0.498,0.514,0.52],[0.466,0.498,0.524,0.52],[0.504,0.508,0.526,0.52],[0.504,0.508,0.548,0.52],[0.464,0.484,0.498,0.52],[0.486,0.496,0.504,0.52],[0.5,0.508,0.51,0.52]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671517"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_500","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5199999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.51,0.494,0.536,0.52],[0.478,0.536,0.53,0.52],[0.482,0.492,0.502,0.52],[0.52,0.498,0.514,0.52],[0.466,0.498,0.524,0.52],[0.504,0.508,0.526,0.52],[0.504,0.508,0.548,0.52],[0.464,0.484,0.498,0.52],[0.486,0.496,0.504,0.52],[0.5,0.508,0.51,0.52]],"time_stamp":"UTC+8 2023-08-24 08:20:08 671524"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.4986,"std":0.009593747964169175,"num_trials":10,"pass_rates":[[0.506,0.514,0.502,0.524],[0.484,0.506,0.51,0.524],[0.506,0.5,0.514,0.524],[0.502,0.524,0.528,0.524],[0.482,0.512,0.532,0.524],[0.492,0.514,0.508,0.524],[0.496,0.532,0.524,0.524],[0.502,0.52,0.52,0.524],[0.502,0.512,0.524,0.524],[0.514,0.482,0.488,0.524]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957127"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5116,"std":0.013016912076218396,"num_trials":10,"pass_rates":[[0.506,0.514,0.502,0.524],[0.484,0.506,0.51,0.524],[0.506,0.5,0.514,0.524],[0.502,0.524,0.528,0.524],[0.482,0.512,0.532,0.524],[0.492,0.514,0.508,0.524],[0.496,0.532,0.524,0.524],[0.502,0.52,0.52,0.524],[0.502,0.512,0.524,0.524],[0.514,0.482,0.488,0.524]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957200"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.515,"std":0.01272006289292629,"num_trials":10,"pass_rates":[[0.506,0.514,0.502,0.524],[0.484,0.506,0.51,0.524],[0.506,0.5,0.514,0.524],[0.502,0.524,0.528,0.524],[0.482,0.512,0.532,0.524],[0.492,0.514,0.508,0.524],[0.496,0.532,0.524,0.524],[0.502,0.52,0.52,0.524],[0.502,0.512,0.524,0.524],[0.514,0.482,0.488,0.524]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957210"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.524,"std":0.0,"num_trials":10,"pass_rates":[[0.506,0.514,0.502,0.524],[0.484,0.506,0.51,0.524],[0.506,0.5,0.514,0.524],[0.502,0.524,0.528,0.524],[0.482,0.512,0.532,0.524],[0.492,0.514,0.508,0.524],[0.496,0.532,0.524,0.524],[0.502,0.52,0.52,0.524],[0.502,0.512,0.524,0.524],[0.514,0.482,0.488,0.524]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957219"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4991999999999999,"std":0.014647866738880456,"num_trials":10,"pass_rates":[[0.496,0.526,0.496,0.53],[0.494,0.496,0.518,0.53],[0.5,0.502,0.528,0.53],[0.508,0.5,0.504,0.53],[0.48,0.51,0.526,0.53],[0.474,0.514,0.516,0.53],[0.496,0.536,0.5,0.53],[0.528,0.51,0.518,0.53],[0.512,0.5,0.534,0.53],[0.504,0.48,0.502,0.53]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957227"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5074,"std":0.014967965793654139,"num_trials":10,"pass_rates":[[0.496,0.526,0.496,0.53],[0.494,0.496,0.518,0.53],[0.5,0.502,0.528,0.53],[0.508,0.5,0.504,0.53],[0.48,0.51,0.526,0.53],[0.474,0.514,0.516,0.53],[0.496,0.536,0.5,0.53],[0.528,0.51,0.518,0.53],[0.512,0.5,0.534,0.53],[0.504,0.48,0.502,0.53]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957235"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5142,"std":0.01240806189539689,"num_trials":10,"pass_rates":[[0.496,0.526,0.496,0.53],[0.494,0.496,0.518,0.53],[0.5,0.502,0.528,0.53],[0.508,0.5,0.504,0.53],[0.48,0.51,0.526,0.53],[0.474,0.514,0.516,0.53],[0.496,0.536,0.5,0.53],[0.528,0.51,0.518,0.53],[0.512,0.5,0.534,0.53],[0.504,0.48,0.502,0.53]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957242"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5300000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.496,0.526,0.496,0.53],[0.494,0.496,0.518,0.53],[0.5,0.502,0.528,0.53],[0.508,0.5,0.504,0.53],[0.48,0.51,0.526,0.53],[0.474,0.514,0.516,0.53],[0.496,0.536,0.5,0.53],[0.528,0.51,0.518,0.53],[0.512,0.5,0.534,0.53],[0.504,0.48,0.502,0.53]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957249"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.499,"std":0.011144505372604037,"num_trials":10,"pass_rates":[[0.506,0.52,0.498,0.524],[0.484,0.51,0.512,0.524],[0.502,0.5,0.52,0.524],[0.504,0.53,0.518,0.524],[0.478,0.504,0.534,0.524],[0.504,0.518,0.53,0.524],[0.494,0.536,0.518,0.524],[0.498,0.52,0.524,0.524],[0.5,0.508,0.536,0.524],[0.52,0.476,0.498,0.524]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957256"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5122,"std":0.01608601877407833,"num_trials":10,"pass_rates":[[0.506,0.52,0.498,0.524],[0.484,0.51,0.512,0.524],[0.502,0.5,0.52,0.524],[0.504,0.53,0.518,0.524],[0.478,0.504,0.534,0.524],[0.504,0.518,0.53,0.524],[0.494,0.536,0.518,0.524],[0.498,0.52,0.524,0.524],[0.5,0.508,0.536,0.524],[0.52,0.476,0.498,0.524]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957266"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5187999999999999,"std":0.01262378707044761,"num_trials":10,"pass_rates":[[0.506,0.52,0.498,0.524],[0.484,0.51,0.512,0.524],[0.502,0.5,0.52,0.524],[0.504,0.53,0.518,0.524],[0.478,0.504,0.534,0.524],[0.504,0.518,0.53,0.524],[0.494,0.536,0.518,0.524],[0.498,0.52,0.524,0.524],[0.5,0.508,0.536,0.524],[0.52,0.476,0.498,0.524]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957279"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.524,"std":0.0,"num_trials":10,"pass_rates":[[0.506,0.52,0.498,0.524],[0.484,0.51,0.512,0.524],[0.502,0.5,0.52,0.524],[0.504,0.53,0.518,0.524],[0.478,0.504,0.534,0.524],[0.504,0.518,0.53,0.524],[0.494,0.536,0.518,0.524],[0.498,0.52,0.524,0.524],[0.5,0.508,0.536,0.524],[0.52,0.476,0.498,0.524]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957286"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.491,"std":0.01456708618770412,"num_trials":10,"pass_rates":[[0.482,0.532,0.506,0.516],[0.488,0.478,0.498,0.516],[0.484,0.498,0.528,0.516],[0.502,0.482,0.51,0.516],[0.466,0.506,0.51,0.516],[0.478,0.51,0.506,0.516],[0.5,0.516,0.498,0.516],[0.522,0.504,0.496,0.516],[0.492,0.49,0.542,0.516],[0.496,0.47,0.508,0.516]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957293"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.4986,"std":0.01795661438022214,"num_trials":10,"pass_rates":[[0.482,0.532,0.506,0.516],[0.488,0.478,0.498,0.516],[0.484,0.498,0.528,0.516],[0.502,0.482,0.51,0.516],[0.466,0.506,0.51,0.516],[0.478,0.51,0.506,0.516],[0.5,0.516,0.498,0.516],[0.522,0.504,0.496,0.516],[0.492,0.49,0.542,0.516],[0.496,0.47,0.508,0.516]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957301"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5102,"std":0.01366601624468522,"num_trials":10,"pass_rates":[[0.482,0.532,0.506,0.516],[0.488,0.478,0.498,0.516],[0.484,0.498,0.528,0.516],[0.502,0.482,0.51,0.516],[0.466,0.506,0.51,0.516],[0.478,0.51,0.506,0.516],[0.5,0.516,0.498,0.516],[0.522,0.504,0.496,0.516],[0.492,0.49,0.542,0.516],[0.496,0.47,0.508,0.516]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957308"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_400","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.516,"std":0.0,"num_trials":10,"pass_rates":[[0.482,0.532,0.506,0.516],[0.488,0.478,0.498,0.516],[0.484,0.498,0.528,0.516],[0.502,0.482,0.51,0.516],[0.466,0.506,0.51,0.516],[0.478,0.51,0.506,0.516],[0.5,0.516,0.498,0.516],[0.522,0.504,0.496,0.516],[0.492,0.49,0.542,0.516],[0.496,0.47,0.508,0.516]],"time_stamp":"UTC+8 2023-08-24 08:44:12 957315"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.502,"std":0.01837389452456937,"num_trials":10,"pass_rates":[[0.522,0.538,0.518,0.544],[0.514,0.524,0.514,0.544],[0.49,0.526,0.526,0.544],[0.51,0.542,0.534,0.544],[0.464,0.554,0.528,0.544],[0.522,0.5,0.522,0.544],[0.508,0.534,0.516,0.544],[0.494,0.518,0.558,0.544],[0.48,0.502,0.556,0.544],[0.516,0.538,0.528,0.544]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753394"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5276,"std":0.016390240998838318,"num_trials":10,"pass_rates":[[0.522,0.538,0.518,0.544],[0.514,0.524,0.514,0.544],[0.49,0.526,0.526,0.544],[0.51,0.542,0.534,0.544],[0.464,0.554,0.528,0.544],[0.522,0.5,0.522,0.544],[0.508,0.534,0.516,0.544],[0.494,0.518,0.558,0.544],[0.48,0.502,0.556,0.544],[0.516,0.538,0.528,0.544]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753446"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.53,"std":0.014696938456699083,"num_trials":10,"pass_rates":[[0.522,0.538,0.518,0.544],[0.514,0.524,0.514,0.544],[0.49,0.526,0.526,0.544],[0.51,0.542,0.534,0.544],[0.464,0.554,0.528,0.544],[0.522,0.5,0.522,0.544],[0.508,0.534,0.516,0.544],[0.494,0.518,0.558,0.544],[0.48,0.502,0.556,0.544],[0.516,0.538,0.528,0.544]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753456"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.5440000000000002,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.522,0.538,0.518,0.544],[0.514,0.524,0.514,0.544],[0.49,0.526,0.526,0.544],[0.51,0.542,0.534,0.544],[0.464,0.554,0.528,0.544],[0.522,0.5,0.522,0.544],[0.508,0.534,0.516,0.544],[0.494,0.518,0.558,0.544],[0.48,0.502,0.556,0.544],[0.516,0.538,0.528,0.544]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753463"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.48840000000000006,"std":0.017973313550928783,"num_trials":10,"pass_rates":[[0.51,0.514,0.486,0.492],[0.496,0.496,0.478,0.492],[0.474,0.512,0.518,0.492],[0.478,0.536,0.53,0.492],[0.458,0.536,0.492,0.49],[0.518,0.508,0.492,0.49],[0.486,0.514,0.48,0.492],[0.49,0.496,0.506,0.492],[0.47,0.496,0.494,0.492],[0.504,0.51,0.482,0.492]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753472"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5118,"std":0.01395564401953562,"num_trials":10,"pass_rates":[[0.51,0.514,0.486,0.492],[0.496,0.496,0.478,0.492],[0.474,0.512,0.518,0.492],[0.478,0.536,0.53,0.492],[0.458,0.536,0.492,0.49],[0.518,0.508,0.492,0.49],[0.486,0.514,0.48,0.492],[0.49,0.496,0.506,0.492],[0.47,0.496,0.494,0.492],[0.504,0.51,0.482,0.492]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753479"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.4958,"std":0.016283734215467915,"num_trials":10,"pass_rates":[[0.51,0.514,0.486,0.492],[0.496,0.496,0.478,0.492],[0.474,0.512,0.518,0.492],[0.478,0.536,0.53,0.492],[0.458,0.536,0.492,0.49],[0.518,0.508,0.492,0.49],[0.486,0.514,0.48,0.492],[0.49,0.496,0.506,0.492],[0.47,0.496,0.494,0.492],[0.504,0.51,0.482,0.492]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753487"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.49160000000000004,"std":0.0008000000000000007,"num_trials":10,"pass_rates":[[0.51,0.514,0.486,0.492],[0.496,0.496,0.478,0.492],[0.474,0.512,0.518,0.492],[0.478,0.536,0.53,0.492],[0.458,0.536,0.492,0.49],[0.518,0.508,0.492,0.49],[0.486,0.514,0.48,0.492],[0.49,0.496,0.506,0.492],[0.47,0.496,0.494,0.492],[0.504,0.51,0.482,0.492]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753494"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5035999999999999,"std":0.019612241075410024,"num_trials":10,"pass_rates":[[0.522,0.536,0.51,0.536],[0.518,0.518,0.5,0.536],[0.49,0.532,0.52,0.536],[0.512,0.536,0.536,0.536],[0.46,0.548,0.516,0.536],[0.526,0.51,0.508,0.536],[0.508,0.544,0.514,0.536],[0.498,0.514,0.544,0.536],[0.484,0.508,0.546,0.536],[0.518,0.532,0.526,0.536]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753501"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5277999999999999,"std":0.01354843164355197,"num_trials":10,"pass_rates":[[0.522,0.536,0.51,0.536],[0.518,0.518,0.5,0.536],[0.49,0.532,0.52,0.536],[0.512,0.536,0.536,0.536],[0.46,0.548,0.516,0.536],[0.526,0.51,0.508,0.536],[0.508,0.544,0.514,0.536],[0.498,0.514,0.544,0.536],[0.484,0.508,0.546,0.536],[0.518,0.532,0.526,0.536]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753509"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.522,"std":0.014832396974191338,"num_trials":10,"pass_rates":[[0.522,0.536,0.51,0.536],[0.518,0.518,0.5,0.536],[0.49,0.532,0.52,0.536],[0.512,0.536,0.536,0.536],[0.46,0.548,0.516,0.536],[0.526,0.51,0.508,0.536],[0.508,0.544,0.514,0.536],[0.498,0.514,0.544,0.536],[0.484,0.508,0.546,0.536],[0.518,0.532,0.526,0.536]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753515"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5359999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.522,0.536,0.51,0.536],[0.518,0.518,0.5,0.536],[0.49,0.532,0.52,0.536],[0.512,0.536,0.536,0.536],[0.46,0.548,0.516,0.536],[0.526,0.51,0.508,0.536],[0.508,0.544,0.514,0.536],[0.498,0.514,0.544,0.536],[0.484,0.508,0.546,0.536],[0.518,0.532,0.526,0.536]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753522"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.48319999999999996,"std":0.018159295140505866,"num_trials":10,"pass_rates":[[0.502,0.488,0.504,0.482],[0.504,0.5,0.48,0.482],[0.474,0.514,0.512,0.482],[0.474,0.51,0.51,0.482],[0.448,0.518,0.472,0.482],[0.508,0.496,0.494,0.482],[0.478,0.518,0.49,0.482],[0.496,0.492,0.484,0.482],[0.466,0.492,0.504,0.482],[0.482,0.492,0.492,0.482]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753529"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.502,"std":0.01120714058089753,"num_trials":10,"pass_rates":[[0.502,0.488,0.504,0.482],[0.504,0.5,0.48,0.482],[0.474,0.514,0.512,0.482],[0.474,0.51,0.51,0.482],[0.448,0.518,0.472,0.482],[0.508,0.496,0.494,0.482],[0.478,0.518,0.49,0.482],[0.496,0.492,0.484,0.482],[0.466,0.492,0.504,0.482],[0.482,0.492,0.492,0.482]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753536"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.4942000000000001,"std":0.01256821387469199,"num_trials":10,"pass_rates":[[0.502,0.488,0.504,0.482],[0.504,0.5,0.48,0.482],[0.474,0.514,0.512,0.482],[0.474,0.51,0.51,0.482],[0.448,0.518,0.472,0.482],[0.508,0.496,0.494,0.482],[0.478,0.518,0.49,0.482],[0.496,0.492,0.484,0.482],[0.466,0.492,0.504,0.482],[0.482,0.492,0.492,0.482]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753543"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_300","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.4820000000000001,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.502,0.488,0.504,0.482],[0.504,0.5,0.48,0.482],[0.474,0.514,0.512,0.482],[0.474,0.51,0.51,0.482],[0.448,0.518,0.472,0.482],[0.508,0.496,0.494,0.482],[0.478,0.518,0.49,0.482],[0.496,0.492,0.484,0.482],[0.466,0.492,0.504,0.482],[0.482,0.492,0.492,0.482]],"time_stamp":"UTC+8 2023-08-24 09:08:19 753550"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"positive_probs_product","num_samples_per_problem":2,"mean":0.5057999999999999,"std":0.011115754585272213,"num_trials":10,"pass_rates":[[0.498,0.53,0.524,0.532],[0.502,0.496,0.518,0.532],[0.488,0.52,0.52,0.532],[0.516,0.5,0.504,0.532],[0.506,0.52,0.536,0.532],[0.524,0.512,0.54,0.532],[0.496,0.52,0.528,0.532],[0.514,0.502,0.542,0.532],[0.496,0.524,0.526,0.532],[0.518,0.524,0.55,0.532]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849497"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"positive_probs_product","num_samples_per_problem":4,"mean":0.5148,"std":0.011070682002478448,"num_trials":10,"pass_rates":[[0.498,0.53,0.524,0.532],[0.502,0.496,0.518,0.532],[0.488,0.52,0.52,0.532],[0.516,0.5,0.504,0.532],[0.506,0.52,0.536,0.532],[0.524,0.512,0.54,0.532],[0.496,0.52,0.528,0.532],[0.514,0.502,0.542,0.532],[0.496,0.524,0.526,0.532],[0.518,0.524,0.55,0.532]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849545"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"positive_probs_product","num_samples_per_problem":8,"mean":0.5287999999999999,"std":0.012812493902437584,"num_trials":10,"pass_rates":[[0.498,0.53,0.524,0.532],[0.502,0.496,0.518,0.532],[0.488,0.52,0.52,0.532],[0.516,0.5,0.504,0.532],[0.506,0.52,0.536,0.532],[0.524,0.512,0.54,0.532],[0.496,0.52,0.528,0.532],[0.514,0.502,0.542,0.532],[0.496,0.524,0.526,0.532],[0.518,0.524,0.55,0.532]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849555"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"positive_probs_product","num_samples_per_problem":16,"mean":0.532,"std":0.0,"num_trials":10,"pass_rates":[[0.498,0.53,0.524,0.532],[0.502,0.496,0.518,0.532],[0.488,0.52,0.52,0.532],[0.516,0.5,0.504,0.532],[0.506,0.52,0.536,0.532],[0.524,0.512,0.54,0.532],[0.496,0.52,0.528,0.532],[0.514,0.502,0.542,0.532],[0.496,0.524,0.526,0.532],[0.518,0.524,0.55,0.532]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849562"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"positive_probs_minimum","num_samples_per_problem":2,"mean":0.4958,"std":0.010712609392673673,"num_trials":10,"pass_rates":[[0.486,0.504,0.508,0.52],[0.486,0.498,0.498,0.518],[0.49,0.504,0.52,0.52],[0.49,0.516,0.506,0.518],[0.498,0.494,0.522,0.518],[0.506,0.504,0.516,0.52],[0.492,0.526,0.522,0.516],[0.522,0.486,0.504,0.518],[0.488,0.528,0.498,0.52],[0.5,0.534,0.528,0.518]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849571"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"positive_probs_minimum","num_samples_per_problem":4,"mean":0.5094000000000001,"std":0.01507448174896903,"num_trials":10,"pass_rates":[[0.486,0.504,0.508,0.52],[0.486,0.498,0.498,0.518],[0.49,0.504,0.52,0.52],[0.49,0.516,0.506,0.518],[0.498,0.494,0.522,0.518],[0.506,0.504,0.516,0.52],[0.492,0.526,0.522,0.516],[0.522,0.486,0.504,0.518],[0.488,0.528,0.498,0.52],[0.5,0.534,0.528,0.518]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849579"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"positive_probs_minimum","num_samples_per_problem":8,"mean":0.5122,"std":0.010215674231297716,"num_trials":10,"pass_rates":[[0.486,0.504,0.508,0.52],[0.486,0.498,0.498,0.518],[0.49,0.504,0.52,0.52],[0.49,0.516,0.506,0.518],[0.498,0.494,0.522,0.518],[0.506,0.504,0.516,0.52],[0.492,0.526,0.522,0.516],[0.522,0.486,0.504,0.518],[0.488,0.528,0.498,0.52],[0.5,0.534,0.528,0.518]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849586"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"positive_probs_minimum","num_samples_per_problem":16,"mean":0.5186000000000001,"std":0.001280624847486571,"num_trials":10,"pass_rates":[[0.486,0.504,0.508,0.52],[0.486,0.498,0.498,0.518],[0.49,0.504,0.52,0.52],[0.49,0.516,0.506,0.518],[0.498,0.494,0.522,0.518],[0.506,0.504,0.516,0.52],[0.492,0.526,0.522,0.516],[0.522,0.486,0.504,0.518],[0.488,0.528,0.498,0.52],[0.5,0.534,0.528,0.518]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849593"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"non_negative_probs_product","num_samples_per_problem":2,"mean":0.5050000000000001,"std":0.01281405478371309,"num_trials":10,"pass_rates":[[0.492,0.528,0.53,0.534],[0.498,0.496,0.52,0.534],[0.484,0.522,0.52,0.534],[0.516,0.494,0.502,0.534],[0.506,0.52,0.524,0.534],[0.526,0.512,0.536,0.534],[0.496,0.52,0.518,0.534],[0.518,0.502,0.528,0.534],[0.498,0.522,0.516,0.534],[0.516,0.528,0.546,0.534]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849600"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"non_negative_probs_product","num_samples_per_problem":4,"mean":0.5144,"std":0.012092973166264788,"num_trials":10,"pass_rates":[[0.492,0.528,0.53,0.534],[0.498,0.496,0.52,0.534],[0.484,0.522,0.52,0.534],[0.516,0.494,0.502,0.534],[0.506,0.52,0.524,0.534],[0.526,0.512,0.536,0.534],[0.496,0.52,0.518,0.534],[0.518,0.502,0.528,0.534],[0.498,0.522,0.516,0.534],[0.516,0.528,0.546,0.534]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849607"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"non_negative_probs_product","num_samples_per_problem":8,"mean":0.5240000000000001,"std":0.011384199576606175,"num_trials":10,"pass_rates":[[0.492,0.528,0.53,0.534],[0.498,0.496,0.52,0.534],[0.484,0.522,0.52,0.534],[0.516,0.494,0.502,0.534],[0.506,0.52,0.524,0.534],[0.526,0.512,0.536,0.534],[0.496,0.52,0.518,0.534],[0.518,0.502,0.528,0.534],[0.498,0.522,0.516,0.534],[0.516,0.528,0.546,0.534]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849614"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"non_negative_probs_product","num_samples_per_problem":16,"mean":0.5339999999999999,"std":1.1102230246251565e-16,"num_trials":10,"pass_rates":[[0.492,0.528,0.53,0.534],[0.498,0.496,0.52,0.534],[0.484,0.522,0.52,0.534],[0.516,0.494,0.502,0.534],[0.506,0.52,0.524,0.534],[0.526,0.512,0.536,0.534],[0.496,0.52,0.518,0.534],[0.518,0.502,0.528,0.534],[0.498,0.522,0.516,0.534],[0.516,0.528,0.546,0.534]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849624"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"non_negative_probs_minimum","num_samples_per_problem":2,"mean":0.4959999999999999,"std":0.011696153213770768,"num_trials":10,"pass_rates":[[0.498,0.524,0.506,0.506],[0.476,0.484,0.5,0.506],[0.478,0.51,0.514,0.508],[0.492,0.516,0.484,0.508],[0.504,0.49,0.502,0.504],[0.498,0.504,0.496,0.508],[0.498,0.51,0.504,0.508],[0.518,0.484,0.504,0.502],[0.494,0.502,0.498,0.508],[0.504,0.524,0.51,0.504]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849631"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"non_negative_probs_minimum","num_samples_per_problem":4,"mean":0.5048,"std":0.014176036117335493,"num_trials":10,"pass_rates":[[0.498,0.524,0.506,0.506],[0.476,0.484,0.5,0.506],[0.478,0.51,0.514,0.508],[0.492,0.516,0.484,0.508],[0.504,0.49,0.502,0.504],[0.498,0.504,0.496,0.508],[0.498,0.51,0.504,0.508],[0.518,0.484,0.504,0.502],[0.494,0.502,0.498,0.508],[0.504,0.524,0.51,0.504]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849639"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"non_negative_probs_minimum","num_samples_per_problem":8,"mean":0.5018,"std":0.007820485918406868,"num_trials":10,"pass_rates":[[0.498,0.524,0.506,0.506],[0.476,0.484,0.5,0.506],[0.478,0.51,0.514,0.508],[0.492,0.516,0.484,0.508],[0.504,0.49,0.502,0.504],[0.498,0.504,0.496,0.508],[0.498,0.51,0.504,0.508],[0.518,0.484,0.504,0.502],[0.494,0.502,0.498,0.508],[0.504,0.524,0.51,0.504]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849648"}
{"model_name_or_path":"direct-prediction/meta-llama/Llama-2-7b-hf/step_200","metric":"non_negative_probs_minimum","num_samples_per_problem":16,"mean":0.5062,"std":0.002088061301782112,"num_trials":10,"pass_rates":[[0.498,0.524,0.506,0.506],[0.476,0.484,0.5,0.506],[0.478,0.51,0.514,0.508],[0.492,0.516,0.484,0.508],[0.504,0.49,0.502,0.504],[0.498,0.504,0.496,0.508],[0.498,0.51,0.504,0.508],[0.518,0.484,0.504,0.502],[0.494,0.502,0.498,0.508],[0.504,0.524,0.51,0.504]],"time_stamp":"UTC+8 2023-08-24 09:32:18 849656"}
